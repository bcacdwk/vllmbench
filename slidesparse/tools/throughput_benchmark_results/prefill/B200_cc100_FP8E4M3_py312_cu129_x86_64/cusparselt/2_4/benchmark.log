
========== M=16 ==========
Time: 2026-01-26 13:16:18
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:16:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=473215) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=473215) WARNING 01-26 13:16:37 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=473215) WARNING 01-26 13:16:44 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 43.44 requests/s, 738.48 total tokens/s, 43.44 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-26 13:16:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:16:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:16:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:16:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:16:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:16:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:16:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:16:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:16:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:16:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:16:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:16:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:16:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:16:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:16:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:16:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:16:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:16:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:16:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:16:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:16:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:16:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:16:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:16:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:16:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:16:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:16:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:16:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=473215) [2026-01-26 13:16:32] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=473215) [2026-01-26 13:16:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=473215) [2026-01-26 13:16:32] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=473215) [2026-01-26 13:16:32] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=473215) [2026-01-26 13:16:32] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=473215) [2026-01-26 13:16:32] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=473215) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=473215) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.42it/s]
(EngineCore_DP0 pid=473215) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.42it/s]
(EngineCore_DP0 pid=473215) 
(EngineCore_DP0 pid=473215) [2026-01-26 13:16:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=473215) [2026-01-26 13:16:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=473215) [2026-01-26 13:16:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=473215) [2026-01-26 13:16:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=473215) [2026-01-26 13:16:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=473215) [2026-01-26 13:16:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=473215) [2026-01-26 13:16:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=473215) [2026-01-26 13:16:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=473215) 2026-01-26 13:16:44,895 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=473215) 2026-01-26 13:16:44,916 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=473215) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.81it/s]
(EngineCore_DP0 pid=473215) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.71it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 487.26it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 575.60it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:00, 130.11it/s, est. speed input: 2081.97 toks/s, output: 130.12 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:01, 60.67it/s, est. speed input: 1055.14 toks/s, output: 65.95 toks/s]  
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 53.98it/s, est. speed input: 949.22 toks/s, output: 59.33 toks/s] 
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 50.59it/s, est. speed input: 897.48 toks/s, output: 56.09 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 48.61it/s, est. speed input: 867.55 toks/s, output: 54.22 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 47.23it/s, est. speed input: 845.83 toks/s, output: 52.86 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 46.35it/s, est. speed input: 831.54 toks/s, output: 51.97 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 45.60it/s, est. speed input: 819.37 toks/s, output: 51.21 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 45.09it/s, est. speed input: 809.56 toks/s, output: 50.60 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 44.69it/s, est. speed input: 801.11 toks/s, output: 50.07 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 44.44it/s, est. speed input: 794.03 toks/s, output: 49.63 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 44.20it/s, est. speed input: 787.67 toks/s, output: 49.23 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 43.33it/s, est. speed input: 779.40 toks/s, output: 48.71 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 43.26it/s, est. speed input: 774.09 toks/s, output: 48.38 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 43.41it/s, est. speed input: 770.05 toks/s, output: 48.13 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 43.50it/s, est. speed input: 766.35 toks/s, output: 47.90 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 43.17it/s, est. speed input: 761.88 toks/s, output: 47.62 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 43.32it/s, est. speed input: 758.90 toks/s, output: 47.43 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 43.44it/s, est. speed input: 756.23 toks/s, output: 47.26 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 43.40it/s, est. speed input: 753.48 toks/s, output: 47.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.40it/s, est. speed input: 752.05 toks/s, output: 47.00 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 47.00it/s, est. speed input: 752.05 toks/s, output: 47.00 toks/s]
[rank0]:[W126 13:16:50.420531714 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 13:16:51
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:16:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=474314) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=474314) WARNING 01-26 13:17:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=474314) WARNING 01-26 13:17:18 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 42.36 requests/s, 5463.84 total tokens/s, 42.36 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-26 13:16:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:16:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:16:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:16:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:16:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:16:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:16:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:16:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:16:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:16:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:16:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:16:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:16:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:16:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:17:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:17:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:17:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:17:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:17:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:17:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:17:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:17:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:17:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:17:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:17:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:17:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:17:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:17:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=474314) [2026-01-26 13:17:06] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=474314) [2026-01-26 13:17:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=474314) [2026-01-26 13:17:06] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=474314) [2026-01-26 13:17:06] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=474314) [2026-01-26 13:17:06] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=474314) [2026-01-26 13:17:06] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=474314) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=474314) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.63it/s]
(EngineCore_DP0 pid=474314) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.63it/s]
(EngineCore_DP0 pid=474314) 
(EngineCore_DP0 pid=474314) [2026-01-26 13:17:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=474314) [2026-01-26 13:17:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=474314) [2026-01-26 13:17:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=474314) [2026-01-26 13:17:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=474314) [2026-01-26 13:17:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=474314) [2026-01-26 13:17:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=474314) [2026-01-26 13:17:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=474314) [2026-01-26 13:17:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=474314) 2026-01-26 13:17:18,231 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=474314) 2026-01-26 13:17:18,254 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=474314) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 21.45it/s]
(EngineCore_DP0 pid=474314) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 21.06it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 360.69it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 419.34it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:01, 112.70it/s, est. speed input: 14426.79 toks/s, output: 112.70 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 59.93it/s, est. speed input: 8276.22 toks/s, output: 64.66 toks/s]   
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 53.30it/s, est. speed input: 7444.74 toks/s, output: 58.16 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 50.51it/s, est. speed input: 7103.74 toks/s, output: 55.50 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 48.55it/s, est. speed input: 6869.15 toks/s, output: 53.66 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:00<00:01, 47.14it/s, est. speed input: 6694.72 toks/s, output: 52.30 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 46.23it/s, est. speed input: 6579.89 toks/s, output: 51.41 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 45.63it/s, est. speed input: 6491.36 toks/s, output: 50.71 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 45.12it/s, est. speed input: 6415.04 toks/s, output: 50.12 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 44.76it/s, est. speed input: 6351.52 toks/s, output: 49.62 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 44.55it/s, est. speed input: 6298.77 toks/s, output: 49.21 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 44.42it/s, est. speed input: 6254.07 toks/s, output: 48.86 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:00, 44.34it/s, est. speed input: 6215.43 toks/s, output: 48.56 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 44.26it/s, est. speed input: 6180.99 toks/s, output: 48.29 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 44.47it/s, est. speed input: 6157.44 toks/s, output: 48.10 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 44.14it/s, est. speed input: 6124.45 toks/s, output: 47.85 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 44.38it/s, est. speed input: 6105.91 toks/s, output: 47.70 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 44.47it/s, est. speed input: 6087.37 toks/s, output: 47.56 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 44.51it/s, est. speed input: 6070.05 toks/s, output: 47.42 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 44.50it/s, est. speed input: 6053.43 toks/s, output: 47.29 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 44.51it/s, est. speed input: 6038.65 toks/s, output: 47.18 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.51it/s, est. speed input: 6033.10 toks/s, output: 47.13 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 47.13it/s, est. speed input: 6033.10 toks/s, output: 47.13 toks/s]
[rank0]:[W126 13:17:23.384277664 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 13:17:24
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:17:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=475337) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=475337) WARNING 01-26 13:17:44 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=475337) WARNING 01-26 13:17:51 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 40.43 requests/s, 10390.76 total tokens/s, 40.43 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-26 13:17:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:17:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:17:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:17:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:17:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:17:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:17:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:17:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:17:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:17:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:17:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:17:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:17:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:17:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:17:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:17:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:17:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:17:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:17:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:17:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:17:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:17:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:17:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:17:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:17:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:17:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:17:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:17:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=475337) [2026-01-26 13:17:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=475337) [2026-01-26 13:17:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=475337) [2026-01-26 13:17:39] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=475337) [2026-01-26 13:17:39] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=475337) [2026-01-26 13:17:39] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=475337) [2026-01-26 13:17:39] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=475337) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=475337) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.30it/s]
(EngineCore_DP0 pid=475337) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.30it/s]
(EngineCore_DP0 pid=475337) 
(EngineCore_DP0 pid=475337) [2026-01-26 13:17:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=475337) [2026-01-26 13:17:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=475337) [2026-01-26 13:17:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=475337) [2026-01-26 13:17:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=475337) [2026-01-26 13:17:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=475337) [2026-01-26 13:17:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=475337) [2026-01-26 13:17:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=475337) [2026-01-26 13:17:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=475337) 2026-01-26 13:17:51,831 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=475337) 2026-01-26 13:17:51,853 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=475337) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.67it/s]
(EngineCore_DP0 pid=475337) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.09it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  95%|█████████▌| 122/128 [00:00<00:00, 1218.93it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1222.64it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:19,  6.50it/s, est. speed input: 1664.58 toks/s, output: 6.50 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:04, 25.55it/s, est. speed input: 5704.70 toks/s, output: 22.28 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 33.31it/s, est. speed input: 7352.76 toks/s, output: 28.72 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 37.26it/s, est. speed input: 8239.91 toks/s, output: 32.19 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 39.55it/s, est. speed input: 8794.60 toks/s, output: 34.35 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 40.92it/s, est. speed input: 9170.57 toks/s, output: 35.82 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 41.88it/s, est. speed input: 9451.53 toks/s, output: 36.92 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 42.57it/s, est. speed input: 9669.61 toks/s, output: 37.77 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 42.99it/s, est. speed input: 9837.19 toks/s, output: 38.43 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:01, 43.17it/s, est. speed input: 9965.40 toks/s, output: 38.93 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 43.35it/s, est. speed input: 10074.55 toks/s, output: 39.35 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 43.54it/s, est. speed input: 10169.89 toks/s, output: 39.73 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 43.63it/s, est. speed input: 10248.86 toks/s, output: 40.03 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 43.69it/s, est. speed input: 10316.27 toks/s, output: 40.30 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 43.75it/s, est. speed input: 10376.27 toks/s, output: 40.53 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 43.48it/s, est. speed input: 10413.62 toks/s, output: 40.68 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 43.58it/s, est. speed input: 10459.93 toks/s, output: 40.86 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:00, 43.70it/s, est. speed input: 10503.16 toks/s, output: 41.03 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 43.40it/s, est. speed input: 10525.87 toks/s, output: 41.12 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 43.56it/s, est. speed input: 10561.37 toks/s, output: 41.26 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 43.68it/s, est. speed input: 10593.79 toks/s, output: 41.38 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 43.78it/s, est. speed input: 10623.64 toks/s, output: 41.50 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 43.63it/s, est. speed input: 10643.40 toks/s, output: 41.58 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 43.73it/s, est. speed input: 10668.54 toks/s, output: 41.67 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 43.29it/s, est. speed input: 10674.83 toks/s, output: 41.70 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 43.55it/s, est. speed input: 10698.78 toks/s, output: 41.79 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 43.55it/s, est. speed input: 10707.98 toks/s, output: 41.83 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.83it/s, est. speed input: 10707.98 toks/s, output: 41.83 toks/s]
[rank0]:[W126 13:17:56.208372372 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 14:02:40
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:02:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=549365) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=549365) WARNING 01-26 14:03:00 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=549365) WARNING 01-26 14:03:07 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 43.39 requests/s, 22257.29 total tokens/s, 43.39 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 14:02:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:02:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:02:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:02:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:02:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:02:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:02:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:02:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:02:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:02:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:02:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:02:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:02:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:02:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:02:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:02:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:02:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:02:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:02:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=549365) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=549365) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.21it/s]
(EngineCore_DP0 pid=549365) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.21it/s]
(EngineCore_DP0 pid=549365) 
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=549365) [2026-01-26 14:02:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=549365) 2026-01-26 14:03:06,998 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=549365) 2026-01-26 14:03:07,021 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=549365) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.99it/s]
(EngineCore_DP0 pid=549365) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 21.07it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  55%|█████▍    | 70/128 [00:00<00:00, 692.54it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 333.76it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:00, 172.37it/s, est. speed input: 88257.23 toks/s, output: 172.37 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:01, 65.42it/s, est. speed input: 37133.04 toks/s, output: 72.52 toks/s]  
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 56.85it/s, est. speed input: 32704.20 toks/s, output: 63.87 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:00<00:01, 52.88it/s, est. speed input: 30741.45 toks/s, output: 60.04 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 50.39it/s, est. speed input: 29546.76 toks/s, output: 57.71 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 48.83it/s, est. speed input: 28787.71 toks/s, output: 56.23 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 47.58it/s, est. speed input: 28171.97 toks/s, output: 55.02 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 46.78it/s, est. speed input: 27750.41 toks/s, output: 54.20 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 46.10it/s, est. speed input: 27383.35 toks/s, output: 53.48 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 45.18it/s, est. speed input: 27007.07 toks/s, output: 52.75 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 44.90it/s, est. speed input: 26733.10 toks/s, output: 52.21 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:01<00:00, 44.65it/s, est. speed input: 26487.02 toks/s, output: 51.73 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 44.58it/s, est. speed input: 26279.75 toks/s, output: 51.33 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 44.35it/s, est. speed input: 26076.03 toks/s, output: 50.93 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 44.30it/s, est. speed input: 25904.20 toks/s, output: 50.59 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 44.35it/s, est. speed input: 25756.20 toks/s, output: 50.30 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 44.33it/s, est. speed input: 25616.58 toks/s, output: 50.03 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.33it/s, est. speed input: 25544.14 toks/s, output: 49.89 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 49.89it/s, est. speed input: 25544.14 toks/s, output: 49.89 toks/s]
[rank0]:[W126 14:03:11.145822209 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 14:03:13
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:03:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=550386) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=550386) WARNING 01-26 14:03:33 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=550386) WARNING 01-26 14:03:40 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 41.14 requests/s, 42170.06 total tokens/s, 41.14 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 14:03:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:03:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:03:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:03:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:03:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:03:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:03:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:03:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:03:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:03:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:03:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:03:28] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:03:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:03:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:03:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:03:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:03:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:03:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:28] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=550386) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=550386) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=550386) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=550386) 
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=550386) [2026-01-26 14:03:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=550386) 2026-01-26 14:03:40,606 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=550386) 2026-01-26 14:03:40,628 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=550386) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.48it/s]
(EngineCore_DP0 pid=550386) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 19.99it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|███       | 39/128 [00:00<00:00, 381.25it/s]
Adding requests:  66%|██████▌   | 84/128 [00:00<00:00, 421.06it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 424.25it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:01, 104.74it/s, est. speed input: 107261.31 toks/s, output: 104.74 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:01, 57.06it/s, est. speed input: 62715.94 toks/s, output: 61.25 toks/s]   
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 51.36it/s, est. speed input: 57002.86 toks/s, output: 55.67 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:01, 48.70it/s, est. speed input: 54372.89 toks/s, output: 53.10 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 47.04it/s, est. speed input: 52686.37 toks/s, output: 51.45 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 45.95it/s, est. speed input: 51607.82 toks/s, output: 50.40 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 45.20it/s, est. speed input: 50794.72 toks/s, output: 49.60 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 44.65it/s, est. speed input: 50141.75 toks/s, output: 48.97 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 44.30it/s, est. speed input: 49624.09 toks/s, output: 48.46 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 43.99it/s, est. speed input: 49175.28 toks/s, output: 48.02 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 43.79it/s, est. speed input: 48800.71 toks/s, output: 47.66 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 43.60it/s, est. speed input: 48467.48 toks/s, output: 47.33 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 43.21it/s, est. speed input: 48116.68 toks/s, output: 46.99 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:00, 43.27it/s, est. speed input: 47887.89 toks/s, output: 46.77 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 43.32it/s, est. speed input: 47687.02 toks/s, output: 46.57 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 42.90it/s, est. speed input: 47414.02 toks/s, output: 46.30 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 43.09it/s, est. speed input: 47265.29 toks/s, output: 46.16 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 43.44it/s, est. speed input: 47172.04 toks/s, output: 46.07 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 42.88it/s, est. speed input: 46946.41 toks/s, output: 45.85 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 43.12it/s, est. speed input: 46846.43 toks/s, output: 45.75 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 43.32it/s, est. speed input: 46759.70 toks/s, output: 45.66 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 43.53it/s, est. speed input: 46691.25 toks/s, output: 45.60 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.53it/s, est. speed input: 46672.14 toks/s, output: 45.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.58it/s, est. speed input: 46672.14 toks/s, output: 45.58 toks/s]
[rank0]:[W126 14:03:45.994239662 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 14:03:47
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:03:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=551441) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=551441) WARNING 01-26 14:04:08 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=551441) WARNING 01-26 14:04:15 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 84.91 requests/s, 87028.03 total tokens/s, 84.91 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 14:03:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:03:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:03:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:03:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:03:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:03:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:03:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:03:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:03:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:03:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:04:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:04:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:04:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:04:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:04:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:04:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:04:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:04:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:04:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:03] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:03] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:03] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:03] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:03] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=551441) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=551441) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.10it/s]
(EngineCore_DP0 pid=551441) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.10it/s]
(EngineCore_DP0 pid=551441) 
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=551441) [2026-01-26 14:04:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=551441) 2026-01-26 14:04:15,063 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=551441) 2026-01-26 14:04:15,084 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=551441) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 19.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 19.34it/s]
(EngineCore_DP0 pid=551441) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 23.65it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█         | 28/256 [00:00<00:00, 273.48it/s]
Adding requests:  27%|██▋       | 70/256 [00:00<00:00, 356.53it/s]
Adding requests:  41%|████▏     | 106/256 [00:00<00:00, 316.31it/s]
Adding requests:  58%|█████▊    | 148/256 [00:00<00:00, 352.38it/s]
Adding requests:  75%|███████▍  | 191/256 [00:00<00:00, 378.09it/s]
Adding requests:  93%|█████████▎| 238/256 [00:00<00:00, 405.54it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 378.54it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:00<00:00, 558.15it/s, est. speed input: 571639.92 toks/s, output: 558.18 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:00<00:01, 136.61it/s, est. speed input: 158112.73 toks/s, output: 154.41 toks/s]
Processed prompts:  56%|█████▌    | 143/256 [00:01<00:00, 118.96it/s, est. speed input: 138727.24 toks/s, output: 135.48 toks/s]
Processed prompts:  64%|██████▎   | 163/256 [00:01<00:00, 110.35it/s, est. speed input: 130287.79 toks/s, output: 127.23 toks/s]
Processed prompts:  70%|██████▉   | 179/256 [00:01<00:00, 104.91it/s, est. speed input: 125336.29 toks/s, output: 122.40 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:01<00:00, 99.56it/s, est. speed input: 121306.90 toks/s, output: 118.46 toks/s] 
Processed prompts:  80%|███████▉  | 204/256 [00:01<00:00, 97.35it/s, est. speed input: 119045.22 toks/s, output: 116.25 toks/s]
Processed prompts:  84%|████████▍ | 215/256 [00:01<00:00, 97.45it/s, est. speed input: 117910.15 toks/s, output: 115.15 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:02<00:00, 93.40it/s, est. speed input: 115640.66 toks/s, output: 112.93 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 92.33it/s, est. speed input: 114332.88 toks/s, output: 111.65 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:02<00:00, 91.55it/s, est. speed input: 113177.95 toks/s, output: 110.53 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 90.97it/s, est. speed input: 112142.65 toks/s, output: 109.51 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 90.97it/s, est. speed input: 112142.65 toks/s, output: 109.51 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 109.51it/s, est. speed input: 112142.65 toks/s, output: 109.51 toks/s]
[rank0]:[W126 14:04:20.769801221 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 14:04:22
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:04:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=552488) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=552488) WARNING 01-26 14:04:43 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=552488) WARNING 01-26 14:04:50 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 159.46 requests/s, 163449.75 total tokens/s, 159.46 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 14:04:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:04:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:04:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:04:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:04:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:04:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:04:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:04:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:04:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:04:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:04:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:04:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:04:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:04:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:04:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:04:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:04:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:04:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:04:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=552488) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=552488) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.06it/s]
(EngineCore_DP0 pid=552488) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.06it/s]
(EngineCore_DP0 pid=552488) 
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=552488) [2026-01-26 14:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=552488) 2026-01-26 14:04:50,575 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=552488) 2026-01-26 14:04:50,597 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=552488) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 22.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 17.81it/s]
(EngineCore_DP0 pid=552488) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 19.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 20.06it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 38/512 [00:00<00:01, 375.35it/s]
Adding requests:  16%|█▌        | 83/512 [00:00<00:01, 414.61it/s]
Adding requests:  25%|██▌       | 128/512 [00:00<00:00, 427.77it/s]
Adding requests:  34%|███▎      | 172/512 [00:00<00:00, 429.19it/s]
Adding requests:  42%|████▏     | 217/512 [00:00<00:00, 435.33it/s]
Adding requests:  51%|█████▏    | 263/512 [00:00<00:00, 441.56it/s]
Adding requests:  60%|██████    | 308/512 [00:00<00:00, 442.76it/s]
Adding requests:  69%|██████▉   | 354/512 [00:00<00:00, 448.04it/s]
Adding requests:  78%|███████▊  | 400/512 [00:00<00:00, 449.26it/s]
Adding requests:  87%|████████▋ | 445/512 [00:01<00:00, 446.80it/s]
Adding requests:  96%|█████████▌| 491/512 [00:01<00:00, 450.26it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 441.09it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:00<00:00, 1517.69it/s, est. speed input: 1554209.96 toks/s, output: 1517.72 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:01<00:00, 289.68it/s, est. speed input: 342366.00 toks/s, output: 334.34 toks/s]   
Processed prompts:  80%|████████  | 411/512 [00:01<00:00, 241.96it/s, est. speed input: 290827.25 toks/s, output: 284.01 toks/s]
Processed prompts:  90%|████████▉ | 459/512 [00:01<00:00, 221.28it/s, est. speed input: 270563.42 toks/s, output: 264.22 toks/s]
Processed prompts:  97%|█████████▋| 495/512 [00:01<00:00, 210.47it/s, est. speed input: 260367.83 toks/s, output: 254.26 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 210.47it/s, est. speed input: 255891.14 toks/s, output: 249.89 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 249.88it/s, est. speed input: 255891.14 toks/s, output: 249.89 toks/s]
[rank0]:[W126 14:04:56.412511425 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 14:04:57
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:05:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=553581) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=553581) WARNING 01-26 14:05:21 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=553581) WARNING 01-26 14:05:28 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 300.32 requests/s, 307824.88 total tokens/s, 300.32 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 14:05:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:05:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:05:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:05:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:05:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:05:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:05:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:05:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:05:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:05:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:05:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:05:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:05:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:05:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:05:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:05:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:05:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:05:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=553581) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=553581) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.09it/s]
(EngineCore_DP0 pid=553581) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.09it/s]
(EngineCore_DP0 pid=553581) 
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=553581) [2026-01-26 14:05:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=553581) 2026-01-26 14:05:28,876 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=553581) 2026-01-26 14:05:28,898 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=553581) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 22.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 20.71it/s]
(EngineCore_DP0 pid=553581) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.10it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 23.10it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 38/1024 [00:00<00:02, 376.58it/s]
Adding requests:   8%|▊         | 83/1024 [00:00<00:02, 418.00it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 432.38it/s]
Adding requests:  17%|█▋        | 173/1024 [00:00<00:01, 436.14it/s]
Adding requests:  21%|██▏       | 219/1024 [00:00<00:01, 442.99it/s]
Adding requests:  26%|██▌       | 267/1024 [00:00<00:01, 454.66it/s]
Adding requests:  31%|███       | 313/1024 [00:00<00:01, 450.55it/s]
Adding requests:  35%|███▌      | 359/1024 [00:00<00:01, 453.12it/s]
Adding requests:  40%|███▉      | 407/1024 [00:00<00:01, 458.38it/s]
Adding requests:  44%|████▍     | 454/1024 [00:01<00:01, 461.01it/s]
Adding requests:  49%|████▉     | 501/1024 [00:01<00:01, 460.48it/s]
Adding requests:  54%|█████▎    | 548/1024 [00:01<00:01, 456.17it/s]
Adding requests:  58%|█████▊    | 596/1024 [00:01<00:00, 462.25it/s]
Adding requests:  63%|██████▎   | 644/1024 [00:01<00:00, 467.04it/s]
Adding requests:  68%|██████▊   | 693/1024 [00:01<00:00, 473.13it/s]
Adding requests:  72%|███████▏  | 741/1024 [00:01<00:00, 471.11it/s]
Adding requests:  77%|███████▋  | 789/1024 [00:01<00:00, 469.52it/s]
Adding requests:  82%|████████▏ | 836/1024 [00:01<00:00, 460.37it/s]
Adding requests:  86%|████████▋ | 884/1024 [00:01<00:00, 465.86it/s]
Adding requests:  91%|█████████ | 932/1024 [00:02<00:00, 469.67it/s]
Adding requests:  96%|█████████▌| 980/1024 [00:02<00:00, 469.88it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 459.65it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:00<00:00, 5848.72it/s, est. speed input: 5989526.71 toks/s, output: 5848.82 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 5848.72it/s, est. speed input: 888116.75 toks/s, output: 867.30 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 867.21it/s, est. speed input: 888116.75 toks/s, output: 867.30 toks/s] 
[rank0]:[W126 14:05:35.254307884 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 14:05:36
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:05:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=554745) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=554745) WARNING 01-26 14:06:05 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=554745) WARNING 01-26 14:06:12 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 449.92 requests/s, 461165.37 total tokens/s, 449.92 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 14:05:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:05:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:05:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:05:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:05:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:05:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:05:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:05:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:05:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:05:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:05:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:06:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:06:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:06:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:06:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:06:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:06:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:06:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:06:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:00] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=554745) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=554745) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.37it/s]
(EngineCore_DP0 pid=554745) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.37it/s]
(EngineCore_DP0 pid=554745) 
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=554745) [2026-01-26 14:06:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=554745) 2026-01-26 14:06:12,713 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=554745) 2026-01-26 14:06:12,736 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=554745) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 21.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 23.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 21.11it/s]
(EngineCore_DP0 pid=554745) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 22.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 23.04it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 38/2048 [00:00<00:05, 379.23it/s]
Adding requests:   4%|▍         | 83/2048 [00:00<00:04, 417.60it/s]
Adding requests:   6%|▋         | 128/2048 [00:00<00:04, 429.12it/s]
Adding requests:   8%|▊         | 172/2048 [00:00<00:04, 432.57it/s]
Adding requests:  11%|█         | 217/2048 [00:00<00:04, 438.36it/s]
Adding requests:  13%|█▎        | 265/2048 [00:00<00:03, 450.33it/s]
Adding requests:  15%|█▌        | 311/2048 [00:00<00:03, 449.74it/s]
Adding requests:  17%|█▋        | 358/2048 [00:00<00:03, 453.07it/s]
Adding requests:  20%|█▉        | 405/2048 [00:00<00:03, 457.43it/s]
Adding requests:  22%|██▏       | 452/2048 [00:01<00:03, 459.70it/s]
Adding requests:  24%|██▍       | 498/2048 [00:01<00:03, 456.47it/s]
Adding requests:  27%|██▋       | 544/2048 [00:01<00:03, 450.29it/s]
Adding requests:  29%|██▉       | 593/2048 [00:01<00:03, 459.28it/s]
Adding requests:  31%|███▏      | 641/2048 [00:01<00:03, 463.05it/s]
Adding requests:  34%|███▎      | 689/2048 [00:01<00:02, 467.36it/s]
Adding requests:  36%|███▌      | 736/2048 [00:01<00:02, 463.28it/s]
Adding requests:  38%|███▊      | 783/2048 [00:01<00:02, 451.47it/s]
Adding requests:  40%|████      | 829/2048 [00:01<00:02, 439.94it/s]
Adding requests:  43%|████▎     | 874/2048 [00:01<00:02, 440.66it/s]
Adding requests:  45%|████▍     | 919/2048 [00:02<00:02, 442.84it/s]
Adding requests:  47%|████▋     | 964/2048 [00:02<00:02, 443.53it/s]
Adding requests:  49%|████▉     | 1009/2048 [00:02<00:02, 443.25it/s]
Adding requests:  51%|█████▏    | 1054/2048 [00:02<00:02, 443.50it/s]
Adding requests:  54%|█████▎    | 1099/2048 [00:02<00:02, 440.72it/s]
Adding requests:  56%|█████▌    | 1144/2048 [00:02<00:02, 435.83it/s]
Adding requests:  58%|█████▊    | 1190/2048 [00:02<00:01, 441.30it/s]
Adding requests:  60%|██████    | 1238/2048 [00:02<00:01, 452.20it/s]
Adding requests:  63%|██████▎   | 1285/2048 [00:02<00:01, 455.34it/s]
Adding requests:  65%|██████▌   | 1334/2048 [00:02<00:01, 462.34it/s]
Adding requests:  67%|██████▋   | 1381/2048 [00:03<00:01, 462.09it/s]
Adding requests:  70%|██████▉   | 1428/2048 [00:03<00:01, 464.11it/s]
Adding requests:  72%|███████▏  | 1477/2048 [00:03<00:01, 469.05it/s]
Adding requests:  74%|███████▍  | 1524/2048 [00:03<00:01, 464.89it/s]
Adding requests:  77%|███████▋  | 1572/2048 [00:03<00:01, 468.81it/s]
Adding requests:  79%|███████▉  | 1619/2048 [00:03<00:00, 452.37it/s]
Adding requests:  81%|████████▏ | 1666/2048 [00:03<00:00, 456.14it/s]
Adding requests:  84%|████████▎ | 1714/2048 [00:03<00:00, 460.57it/s]
Adding requests:  86%|████████▌ | 1761/2048 [00:03<00:00, 461.45it/s]
Adding requests:  88%|████████▊ | 1808/2048 [00:03<00:00, 462.64it/s]
Adding requests:  91%|█████████ | 1856/2048 [00:04<00:00, 465.41it/s]
Adding requests:  93%|█████████▎| 1903/2048 [00:04<00:00, 465.76it/s]
Adding requests:  95%|█████████▌| 1950/2048 [00:04<00:00, 466.28it/s]
Adding requests:  98%|█████████▊| 1998/2048 [00:04<00:00, 468.01it/s]
Adding requests: 100%|█████████▉| 2046/2048 [00:04<00:00, 470.22it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 454.58it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 45370.17it/s, est. speed input: 46473292.52 toks/s, output: 45379.52 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 45265.22it/s, est. speed input: 46473292.52 toks/s, output: 45379.52 toks/s]
[rank0]:[W126 14:06:19.991271887 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 14:06:21
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:06:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=556042) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=556042) WARNING 01-26 14:07:00 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=556042) WARNING 01-26 14:07:07 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 463.59 requests/s, 475180.22 total tokens/s, 463.59 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 14:06:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:06:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:06:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:06:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:06:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:06:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:06:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:06:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:06:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:06:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:06:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:06:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:06:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:06:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:06:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:06:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:06:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:06:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:06:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=556042) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=556042) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=556042) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.64it/s]
(EngineCore_DP0 pid=556042) 
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=556042) [2026-01-26 14:06:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=556042) [rank0]:W0126 14:07:03.335000 556042 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=556042) [rank0]:W0126 14:07:03.403000 556042 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=556042) [rank0]:W0126 14:07:04.239000 556042 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=556042) [rank0]:W0126 14:07:04.336000 556042 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=556042) 2026-01-26 14:07:06,960 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=556042) 2026-01-26 14:07:06,983 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=556042) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 18.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 21.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 20.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 19.89it/s]
(EngineCore_DP0 pid=556042) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 23.32it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 23.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 23.46it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 39/4096 [00:00<00:10, 381.18it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 416.25it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:09, 430.91it/s]
Adding requests:   4%|▍         | 174/4096 [00:00<00:08, 435.90it/s]
Adding requests:   5%|▌         | 220/4096 [00:00<00:08, 442.43it/s]
Adding requests:   7%|▋         | 268/4096 [00:00<00:08, 454.21it/s]
Adding requests:   8%|▊         | 314/4096 [00:00<00:08, 452.07it/s]
Adding requests:   9%|▉         | 360/4096 [00:00<00:08, 454.36it/s]
Adding requests:  10%|▉         | 406/4096 [00:00<00:08, 455.46it/s]
Adding requests:  11%|█         | 453/4096 [00:01<00:07, 456.66it/s]
Adding requests:  12%|█▏        | 500/4096 [00:01<00:07, 457.81it/s]
Adding requests:  13%|█▎        | 546/4096 [00:01<00:07, 453.57it/s]
Adding requests:  15%|█▍        | 595/4096 [00:01<00:07, 463.74it/s]
Adding requests:  16%|█▌        | 642/4096 [00:01<00:07, 464.10it/s]
Adding requests:  17%|█▋        | 690/4096 [00:01<00:07, 467.59it/s]
Adding requests:  18%|█▊        | 738/4096 [00:01<00:07, 470.21it/s]
Adding requests:  19%|█▉        | 786/4096 [00:01<00:07, 464.60it/s]
Adding requests:  20%|██        | 833/4096 [00:01<00:07, 454.18it/s]
Adding requests:  21%|██▏       | 879/4096 [00:01<00:07, 448.71it/s]
Adding requests:  23%|██▎       | 927/4096 [00:02<00:06, 455.51it/s]
Adding requests:  24%|██▍       | 974/4096 [00:02<00:06, 458.86it/s]
Adding requests:  25%|██▍       | 1022/4096 [00:02<00:06, 463.53it/s]
Adding requests:  26%|██▌       | 1069/4096 [00:02<00:06, 462.88it/s]
Adding requests:  27%|██▋       | 1116/4096 [00:02<00:06, 458.57it/s]
Adding requests:  28%|██▊       | 1165/4096 [00:02<00:06, 465.60it/s]
Adding requests:  30%|██▉       | 1215/4096 [00:02<00:06, 473.17it/s]
Adding requests:  31%|███       | 1263/4096 [00:02<00:06, 467.34it/s]
Adding requests:  32%|███▏      | 1310/4096 [00:02<00:06, 463.87it/s]
Adding requests:  33%|███▎      | 1359/4096 [00:02<00:05, 467.69it/s]
Adding requests:  34%|███▍      | 1409/4096 [00:03<00:05, 475.49it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:03<00:05, 474.12it/s]
Adding requests:  37%|███▋      | 1506/4096 [00:03<00:05, 476.66it/s]
Adding requests:  38%|███▊      | 1554/4096 [00:03<00:05, 476.46it/s]
Adding requests:  39%|███▉      | 1604/4096 [00:03<00:05, 482.90it/s]
Adding requests:  40%|████      | 1653/4096 [00:03<00:05, 480.13it/s]
Adding requests:  42%|████▏     | 1702/4096 [00:03<00:05, 477.09it/s]
Adding requests:  43%|████▎     | 1750/4096 [00:03<00:04, 476.93it/s]
Adding requests:  44%|████▍     | 1798/4096 [00:03<00:04, 476.88it/s]
Adding requests:  45%|████▌     | 1846/4096 [00:03<00:04, 476.93it/s]
Adding requests:  46%|████▌     | 1894/4096 [00:04<00:04, 473.97it/s]
Adding requests:  47%|████▋     | 1942/4096 [00:04<00:04, 464.98it/s]
Adding requests:  49%|████▊     | 1990/4096 [00:04<00:04, 466.81it/s]
Adding requests:  50%|████▉     | 2039/4096 [00:04<00:04, 471.58it/s]
Adding requests:  51%|█████     | 2088/4096 [00:04<00:04, 475.43it/s]
Adding requests:  52%|█████▏    | 2136/4096 [00:04<00:04, 470.96it/s]
Adding requests:  53%|█████▎    | 2184/4096 [00:04<00:04, 465.44it/s]
Adding requests:  55%|█████▍    | 2233/4096 [00:04<00:03, 471.40it/s]
Adding requests:  56%|█████▌    | 2281/4096 [00:04<00:03, 470.66it/s]
Adding requests:  57%|█████▋    | 2329/4096 [00:05<00:03, 473.16it/s]
Adding requests:  58%|█████▊    | 2377/4096 [00:05<00:03, 471.70it/s]
Adding requests:  59%|█████▉    | 2426/4096 [00:05<00:03, 475.75it/s]
Adding requests:  60%|██████    | 2474/4096 [00:05<00:03, 454.97it/s]
Adding requests:  62%|██████▏   | 2520/4096 [00:05<00:03, 456.05it/s]
Adding requests:  63%|██████▎   | 2569/4096 [00:05<00:03, 465.14it/s]
Adding requests:  64%|██████▍   | 2617/4096 [00:05<00:03, 466.72it/s]
Adding requests:  65%|██████▌   | 2666/4096 [00:05<00:03, 473.50it/s]
Adding requests:  66%|██████▋   | 2714/4096 [00:05<00:02, 468.09it/s]
Adding requests:  67%|██████▋   | 2762/4096 [00:05<00:02, 470.11it/s]
Adding requests:  69%|██████▊   | 2810/4096 [00:06<00:02, 466.97it/s]
Adding requests:  70%|██████▉   | 2858/4096 [00:06<00:02, 468.21it/s]
Adding requests:  71%|███████   | 2907/4096 [00:06<00:02, 472.11it/s]
Adding requests:  72%|███████▏  | 2955/4096 [00:06<00:02, 470.14it/s]
Adding requests:  73%|███████▎  | 3003/4096 [00:06<00:02, 471.36it/s]
Adding requests:  74%|███████▍  | 3051/4096 [00:06<00:02, 473.37it/s]
Adding requests:  76%|███████▌  | 3099/4096 [00:06<00:02, 460.29it/s]
Adding requests:  77%|███████▋  | 3147/4096 [00:06<00:02, 463.31it/s]
Adding requests:  78%|███████▊  | 3195/4096 [00:06<00:01, 467.67it/s]
Adding requests:  79%|███████▉  | 3244/4096 [00:06<00:01, 472.47it/s]
Adding requests:  80%|████████  | 3292/4096 [00:07<00:01, 474.17it/s]
Adding requests:  82%|████████▏ | 3341/4096 [00:07<00:01, 476.29it/s]
Adding requests:  83%|████████▎ | 3389/4096 [00:07<00:01, 476.29it/s]
Adding requests:  84%|████████▍ | 3438/4096 [00:07<00:01, 473.00it/s]
Adding requests:  85%|████████▌ | 3486/4096 [00:07<00:01, 465.16it/s]
Adding requests:  86%|████████▋ | 3534/4096 [00:07<00:01, 468.70it/s]
Adding requests:  87%|████████▋ | 3582/4096 [00:07<00:01, 469.70it/s]
Adding requests:  89%|████████▊ | 3629/4096 [00:07<00:00, 469.71it/s]
Adding requests:  90%|████████▉ | 3677/4096 [00:07<00:00, 470.81it/s]
Adding requests:  91%|█████████ | 3725/4096 [00:07<00:00, 473.36it/s]
Adding requests:  92%|█████████▏| 3774/4096 [00:08<00:00, 477.68it/s]
Adding requests:  93%|█████████▎| 3823/4096 [00:08<00:00, 479.08it/s]
Adding requests:  95%|█████████▍| 3873/4096 [00:08<00:00, 484.39it/s]
Adding requests:  96%|█████████▌| 3922/4096 [00:08<00:00, 482.56it/s]
Adding requests:  97%|█████████▋| 3971/4096 [00:08<00:00, 480.10it/s]
Adding requests:  98%|█████████▊| 4020/4096 [00:08<00:00, 480.48it/s]
Adding requests:  99%|█████████▉| 4069/4096 [00:08<00:00, 473.37it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 467.48it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 57427.79it/s, est. speed input: 58818054.61 toks/s, output: 57435.86 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 57343.45it/s, est. speed input: 58818054.61 toks/s, output: 57435.86 toks/s]
[rank0]:[W126 14:07:18.955784151 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 14:07:20
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:08:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=557647) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=557647) WARNING 01-26 14:08:17 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=557647) WARNING 01-26 14:08:24 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 468.28 requests/s, 479982.25 total tokens/s, 468.28 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 14:08:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:08:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:08:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:08:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:08:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:08:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:08:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:08:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:08:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:08:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:08:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:08:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:08:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:08:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:08:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:08:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:08:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:08:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:08:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:12] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:12] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:12] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:12] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:12] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=557647) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=557647) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.09it/s]
(EngineCore_DP0 pid=557647) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.09it/s]
(EngineCore_DP0 pid=557647) 
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=557647) [2026-01-26 14:08:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=557647) [rank0]:W0126 14:08:20.351000 557647 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=557647) [rank0]:W0126 14:08:20.420000 557647 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=557647) [rank0]:W0126 14:08:21.225000 557647 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=557647) [rank0]:W0126 14:08:21.322000 557647 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=557647) 2026-01-26 14:08:24,081 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=557647) 2026-01-26 14:08:24,105 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=557647) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:00, 17.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:00, 19.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 20.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 20.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 19.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:00<00:00, 19.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 18.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 19.34it/s]
(EngineCore_DP0 pid=557647) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00, 22.93it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 22.95it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 23.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 23.04it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 38/8192 [00:00<00:21, 376.42it/s]
Adding requests:   1%|          | 82/8192 [00:00<00:19, 411.63it/s]
Adding requests:   2%|▏         | 127/8192 [00:00<00:18, 425.26it/s]
Adding requests:   2%|▏         | 172/8192 [00:00<00:18, 432.19it/s]
Adding requests:   3%|▎         | 218/8192 [00:00<00:18, 439.67it/s]
Adding requests:   3%|▎         | 265/8192 [00:00<00:17, 449.34it/s]
Adding requests:   4%|▍         | 310/8192 [00:00<00:17, 449.12it/s]
Adding requests:   4%|▍         | 357/8192 [00:00<00:17, 453.31it/s]
Adding requests:   5%|▍         | 404/8192 [00:00<00:17, 457.68it/s]
Adding requests:   5%|▌         | 450/8192 [00:01<00:16, 456.65it/s]
Adding requests:   6%|▌         | 497/8192 [00:01<00:16, 459.78it/s]
Adding requests:   7%|▋         | 543/8192 [00:01<00:16, 452.99it/s]
Adding requests:   7%|▋         | 593/8192 [00:01<00:16, 463.90it/s]
Adding requests:   8%|▊         | 641/8192 [00:01<00:16, 466.86it/s]
Adding requests:   8%|▊         | 690/8192 [00:01<00:15, 473.33it/s]
Adding requests:   9%|▉         | 739/8192 [00:01<00:15, 476.61it/s]
Adding requests:  10%|▉         | 787/8192 [00:01<00:15, 472.87it/s]
Adding requests:  10%|█         | 835/8192 [00:01<00:15, 462.35it/s]
Adding requests:  11%|█         | 884/8192 [00:01<00:15, 468.11it/s]
Adding requests:  11%|█▏        | 933/8192 [00:02<00:15, 474.42it/s]
Adding requests:  12%|█▏        | 981/8192 [00:02<00:15, 473.72it/s]
Adding requests:  13%|█▎        | 1030/8192 [00:02<00:14, 478.22it/s]
Adding requests:  13%|█▎        | 1078/8192 [00:02<00:15, 472.58it/s]
Adding requests:  14%|█▎        | 1126/8192 [00:02<00:15, 470.85it/s]
Adding requests:  14%|█▍        | 1176/8192 [00:02<00:14, 477.18it/s]
Adding requests:  15%|█▍        | 1226/8192 [00:02<00:14, 482.78it/s]
Adding requests:  16%|█▌        | 1275/8192 [00:02<00:14, 477.95it/s]
Adding requests:  16%|█▌        | 1324/8192 [00:02<00:14, 481.18it/s]
Adding requests:  17%|█▋        | 1373/8192 [00:02<00:14, 482.64it/s]
Adding requests:  17%|█▋        | 1422/8192 [00:03<00:14, 482.85it/s]
Adding requests:  18%|█▊        | 1471/8192 [00:03<00:14, 475.52it/s]
Adding requests:  19%|█▊        | 1520/8192 [00:03<00:13, 479.14it/s]
Adding requests:  19%|█▉        | 1569/8192 [00:03<00:13, 479.66it/s]
Adding requests:  20%|█▉        | 1619/8192 [00:03<00:13, 483.97it/s]
Adding requests:  20%|██        | 1668/8192 [00:03<00:13, 480.18it/s]
Adding requests:  21%|██        | 1718/8192 [00:03<00:13, 483.63it/s]
Adding requests:  22%|██▏       | 1767/8192 [00:03<00:13, 479.47it/s]
Adding requests:  22%|██▏       | 1816/8192 [00:03<00:13, 481.07it/s]
Adding requests:  23%|██▎       | 1865/8192 [00:03<00:13, 478.48it/s]
Adding requests:  23%|██▎       | 1913/8192 [00:04<00:13, 478.06it/s]
Adding requests:  24%|██▍       | 1962/8192 [00:04<00:13, 479.14it/s]
Adding requests:  25%|██▍       | 2011/8192 [00:04<00:12, 481.85it/s]
Adding requests:  25%|██▌       | 2060/8192 [00:04<00:12, 482.64it/s]
Adding requests:  26%|██▌       | 2110/8192 [00:04<00:12, 486.47it/s]
Adding requests:  26%|██▋       | 2159/8192 [00:04<00:12, 475.31it/s]
Adding requests:  27%|██▋       | 2207/8192 [00:04<00:12, 472.36it/s]
Adding requests:  28%|██▊       | 2257/8192 [00:04<00:12, 479.15it/s]
Adding requests:  28%|██▊       | 2305/8192 [00:04<00:12, 472.30it/s]
Adding requests:  29%|██▊       | 2353/8192 [00:05<00:12, 473.16it/s]
Adding requests:  29%|██▉       | 2401/8192 [00:05<00:12, 471.69it/s]
Adding requests:  30%|██▉       | 2450/8192 [00:05<00:12, 474.76it/s]
Adding requests:  31%|███       | 2499/8192 [00:05<00:11, 478.22it/s]
Adding requests:  31%|███       | 2547/8192 [00:05<00:11, 478.11it/s]
Adding requests:  32%|███▏      | 2595/8192 [00:05<00:11, 467.66it/s]
Adding requests:  32%|███▏      | 2644/8192 [00:05<00:11, 472.44it/s]
Adding requests:  33%|███▎      | 2692/8192 [00:05<00:11, 471.01it/s]
Adding requests:  33%|███▎      | 2740/8192 [00:05<00:11, 470.89it/s]
Adding requests:  34%|███▍      | 2788/8192 [00:05<00:11, 469.38it/s]
Adding requests:  35%|███▍      | 2835/8192 [00:06<00:11, 467.87it/s]
Adding requests:  35%|███▌      | 2883/8192 [00:06<00:11, 471.01it/s]
Adding requests:  36%|███▌      | 2931/8192 [00:06<00:11, 469.63it/s]
Adding requests:  36%|███▋      | 2979/8192 [00:06<00:11, 471.96it/s]
Adding requests:  37%|███▋      | 3027/8192 [00:06<00:10, 471.14it/s]
Adding requests:  38%|███▊      | 3075/8192 [00:06<00:10, 468.67it/s]
Adding requests:  38%|███▊      | 3124/8192 [00:06<00:10, 473.20it/s]
Adding requests:  39%|███▊      | 3172/8192 [00:06<00:10, 469.88it/s]
Adding requests:  39%|███▉      | 3220/8192 [00:06<00:10, 471.25it/s]
Adding requests:  40%|███▉      | 3269/8192 [00:06<00:10, 473.56it/s]
Adding requests:  41%|████      | 3318/8192 [00:07<00:10, 475.65it/s]
Adding requests:  41%|████      | 3367/8192 [00:07<00:10, 479.52it/s]
Adding requests:  42%|████▏     | 3416/8192 [00:07<00:09, 479.62it/s]
Adding requests:  42%|████▏     | 3464/8192 [00:07<00:10, 471.49it/s]
Adding requests:  43%|████▎     | 3512/8192 [00:07<00:09, 472.90it/s]
Adding requests:  43%|████▎     | 3560/8192 [00:07<00:10, 461.89it/s]
Adding requests:  44%|████▍     | 3607/8192 [00:07<00:09, 459.53it/s]
Adding requests:  45%|████▍     | 3653/8192 [00:07<00:09, 458.50it/s]
Adding requests:  45%|████▌     | 3701/8192 [00:07<00:09, 464.11it/s]
Adding requests:  46%|████▌     | 3748/8192 [00:07<00:09, 451.25it/s]
Adding requests:  46%|████▋     | 3798/8192 [00:08<00:09, 463.70it/s]
Adding requests:  47%|████▋     | 3846/8192 [00:08<00:09, 467.40it/s]
Adding requests:  48%|████▊     | 3894/8192 [00:08<00:09, 469.00it/s]
Adding requests:  48%|████▊     | 3941/8192 [00:08<00:09, 468.76it/s]
Adding requests:  49%|████▊     | 3988/8192 [00:08<00:09, 466.50it/s]
Adding requests:  49%|████▉     | 4036/8192 [00:08<00:08, 467.61it/s]
Adding requests:  50%|████▉     | 4083/8192 [00:08<00:08, 467.23it/s]
Adding requests:  50%|█████     | 4131/8192 [00:08<00:08, 470.99it/s]
Adding requests:  51%|█████     | 4179/8192 [00:08<00:08, 473.66it/s]
Adding requests:  52%|█████▏    | 4227/8192 [00:08<00:08, 472.71it/s]
Adding requests:  52%|█████▏    | 4275/8192 [00:09<00:08, 473.61it/s]
Adding requests:  53%|█████▎    | 4323/8192 [00:09<00:08, 475.36it/s]
Adding requests:  53%|█████▎    | 4372/8192 [00:09<00:07, 479.19it/s]
Adding requests:  54%|█████▍    | 4420/8192 [00:09<00:07, 477.80it/s]
Adding requests:  55%|█████▍    | 4469/8192 [00:09<00:07, 480.63it/s]
Adding requests:  55%|█████▌    | 4518/8192 [00:09<00:07, 469.01it/s]
Adding requests:  56%|█████▌    | 4567/8192 [00:09<00:07, 473.57it/s]
Adding requests:  56%|█████▋    | 4615/8192 [00:09<00:07, 475.13it/s]
Adding requests:  57%|█████▋    | 4663/8192 [00:09<00:07, 476.11it/s]
Adding requests:  58%|█████▊    | 4711/8192 [00:10<00:07, 474.54it/s]
Adding requests:  58%|█████▊    | 4759/8192 [00:10<00:07, 474.76it/s]
Adding requests:  59%|█████▊    | 4807/8192 [00:10<00:07, 473.07it/s]
Adding requests:  59%|█████▉    | 4855/8192 [00:10<00:07, 474.75it/s]
Adding requests:  60%|█████▉    | 4903/8192 [00:10<00:07, 460.96it/s]
Adding requests:  60%|██████    | 4952/8192 [00:10<00:06, 467.61it/s]
Adding requests:  61%|██████    | 4999/8192 [00:10<00:06, 467.54it/s]
Adding requests:  62%|██████▏   | 5048/8192 [00:10<00:06, 472.74it/s]
Adding requests:  62%|██████▏   | 5097/8192 [00:10<00:06, 477.09it/s]
Adding requests:  63%|██████▎   | 5145/8192 [00:10<00:06, 476.25it/s]
Adding requests:  63%|██████▎   | 5193/8192 [00:11<00:06, 477.26it/s]
Adding requests:  64%|██████▍   | 5241/8192 [00:11<00:06, 473.48it/s]
Adding requests:  65%|██████▍   | 5289/8192 [00:11<00:06, 471.94it/s]
Adding requests:  65%|██████▌   | 5338/8192 [00:11<00:06, 474.34it/s]
Adding requests:  66%|██████▌   | 5386/8192 [00:11<00:05, 474.89it/s]
Adding requests:  66%|██████▋   | 5434/8192 [00:11<00:05, 475.31it/s]
Adding requests:  67%|██████▋   | 5482/8192 [00:11<00:05, 470.07it/s]
Adding requests:  68%|██████▊   | 5530/8192 [00:11<00:05, 469.69it/s]
Adding requests:  68%|██████▊   | 5578/8192 [00:11<00:05, 470.57it/s]
Adding requests:  69%|██████▊   | 5626/8192 [00:11<00:05, 466.73it/s]
Adding requests:  69%|██████▉   | 5673/8192 [00:12<00:05, 464.96it/s]
Adding requests:  70%|██████▉   | 5721/8192 [00:12<00:05, 468.80it/s]
Adding requests:  70%|███████   | 5770/8192 [00:12<00:05, 472.80it/s]
Adding requests:  71%|███████   | 5818/8192 [00:12<00:05, 464.88it/s]
Adding requests:  72%|███████▏  | 5866/8192 [00:12<00:04, 468.67it/s]
Adding requests:  72%|███████▏  | 5914/8192 [00:12<00:04, 471.14it/s]
Adding requests:  73%|███████▎  | 5962/8192 [00:12<00:04, 470.51it/s]
Adding requests:  73%|███████▎  | 6011/8192 [00:12<00:04, 474.61it/s]
Adding requests:  74%|███████▍  | 6059/8192 [00:12<00:04, 462.05it/s]
Adding requests:  75%|███████▍  | 6106/8192 [00:12<00:04, 453.64it/s]
Adding requests:  75%|███████▌  | 6153/8192 [00:13<00:04, 458.11it/s]
Adding requests:  76%|███████▌  | 6201/8192 [00:13<00:04, 463.84it/s]
Adding requests:  76%|███████▋  | 6250/8192 [00:13<00:04, 471.52it/s]
Adding requests:  77%|███████▋  | 6299/8192 [00:13<00:03, 475.08it/s]
Adding requests:  77%|███████▋  | 6348/8192 [00:13<00:03, 477.60it/s]
Adding requests:  78%|███████▊  | 6397/8192 [00:13<00:03, 479.82it/s]
Adding requests:  79%|███████▊  | 6447/8192 [00:13<00:03, 484.41it/s]
Adding requests:  79%|███████▉  | 6496/8192 [00:13<00:03, 484.20it/s]
Adding requests:  80%|███████▉  | 6545/8192 [00:13<00:03, 484.90it/s]
Adding requests:  80%|████████  | 6594/8192 [00:13<00:03, 480.75it/s]
Adding requests:  81%|████████  | 6643/8192 [00:14<00:03, 480.45it/s]
Adding requests:  82%|████████▏ | 6692/8192 [00:14<00:03, 478.11it/s]
Adding requests:  82%|████████▏ | 6740/8192 [00:14<00:03, 478.50it/s]
Adding requests:  83%|████████▎ | 6789/8192 [00:14<00:02, 480.17it/s]
Adding requests:  83%|████████▎ | 6838/8192 [00:14<00:02, 481.53it/s]
Adding requests:  84%|████████▍ | 6887/8192 [00:14<00:02, 482.86it/s]
Adding requests:  85%|████████▍ | 6937/8192 [00:14<00:02, 486.81it/s]
Adding requests:  85%|████████▌ | 6986/8192 [00:14<00:02, 480.89it/s]
Adding requests:  86%|████████▌ | 7035/8192 [00:14<00:02, 477.84it/s]
Adding requests:  86%|████████▋ | 7083/8192 [00:15<00:02, 476.97it/s]
Adding requests:  87%|████████▋ | 7132/8192 [00:15<00:02, 480.18it/s]
Adding requests:  88%|████████▊ | 7181/8192 [00:15<00:02, 472.97it/s]
Adding requests:  88%|████████▊ | 7229/8192 [00:15<00:02, 474.06it/s]
Adding requests:  89%|████████▉ | 7277/8192 [00:15<00:01, 467.06it/s]
Adding requests:  89%|████████▉ | 7326/8192 [00:15<00:01, 471.77it/s]
Adding requests:  90%|█████████ | 7374/8192 [00:15<00:01, 470.31it/s]
Adding requests:  91%|█████████ | 7424/8192 [00:15<00:01, 477.88it/s]
Adding requests:  91%|█████████ | 7473/8192 [00:15<00:01, 479.82it/s]
Adding requests:  92%|█████████▏| 7522/8192 [00:15<00:01, 478.53it/s]
Adding requests:  92%|█████████▏| 7570/8192 [00:16<00:01, 477.84it/s]
Adding requests:  93%|█████████▎| 7618/8192 [00:16<00:01, 474.56it/s]
Adding requests:  94%|█████████▎| 7668/8192 [00:16<00:01, 479.11it/s]
Adding requests:  94%|█████████▍| 7717/8192 [00:16<00:00, 481.02it/s]
Adding requests:  95%|█████████▍| 7766/8192 [00:16<00:00, 477.27it/s]
Adding requests:  95%|█████████▌| 7814/8192 [00:16<00:00, 476.13it/s]
Adding requests:  96%|█████████▌| 7862/8192 [00:16<00:00, 477.18it/s]
Adding requests:  97%|█████████▋| 7910/8192 [00:16<00:00, 473.03it/s]
Adding requests:  97%|█████████▋| 7958/8192 [00:16<00:00, 470.26it/s]
Adding requests:  98%|█████████▊| 8006/8192 [00:16<00:00, 470.86it/s]
Adding requests:  98%|█████████▊| 8054/8192 [00:17<00:00, 470.42it/s]
Adding requests:  99%|█████████▉| 8103/8192 [00:17<00:00, 475.42it/s]
Adding requests:  99%|█████████▉| 8151/8192 [00:17<00:00, 475.13it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 472.15it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  70%|███████   | 5751/8192 [00:00<00:00, 57506.08it/s, est. speed input: 58890296.16 toks/s, output: 57506.90 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 57506.08it/s, est. speed input: 59809497.09 toks/s, output: 58405.63 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 58358.41it/s, est. speed input: 59809497.09 toks/s, output: 58405.63 toks/s]
[rank0]:[W126 14:08:45.476232405 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 15:11:19
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:11:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=651978) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=651978) WARNING 01-26 15:11:41 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=651978) WARNING 01-26 15:11:51 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 34.98 requests/s, 17945.68 total tokens/s, 34.98 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 15:11:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:11:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:11:26] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:11:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:11:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:11:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:11:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:11:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:11:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:11:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:11:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:11:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:11:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:11:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:11:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:11:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:11:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:11:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:11:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:11:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:11:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:11:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:11:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:11:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:11:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:11:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:11:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:11:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=651978) [2026-01-26 15:11:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=651978) [2026-01-26 15:11:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=651978) [2026-01-26 15:11:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=651978) [2026-01-26 15:11:34] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=651978) [2026-01-26 15:11:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=651978) [2026-01-26 15:11:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=651978) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=651978) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.54it/s]
(EngineCore_DP0 pid=651978) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.54it/s]
(EngineCore_DP0 pid=651978) 
(EngineCore_DP0 pid=651978) [2026-01-26 15:11:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=651978) [2026-01-26 15:11:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=651978) [2026-01-26 15:11:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=651978) [2026-01-26 15:11:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=651978) [2026-01-26 15:11:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=651978) [2026-01-26 15:11:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=651978) [2026-01-26 15:11:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=651978) [2026-01-26 15:11:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=651978) 2026-01-26 15:11:51,541 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=651978) 2026-01-26 15:11:51,564 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=651978) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.59it/s]
(EngineCore_DP0 pid=651978) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.27it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  55%|█████▍    | 70/128 [00:00<00:00, 699.08it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 730.15it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:13,  9.53it/s, est. speed input: 4879.76 toks/s, output: 9.53 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:04, 25.87it/s, est. speed input: 12011.96 toks/s, output: 23.46 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 31.14it/s, est. speed input: 14396.71 toks/s, output: 28.12 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 33.60it/s, est. speed input: 15584.25 toks/s, output: 30.44 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 35.04it/s, est. speed input: 16316.63 toks/s, output: 31.87 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 35.92it/s, est. speed input: 16805.95 toks/s, output: 32.82 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 36.45it/s, est. speed input: 17151.07 toks/s, output: 33.50 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 36.81it/s, est. speed input: 17411.50 toks/s, output: 34.01 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 37.03it/s, est. speed input: 17611.28 toks/s, output: 34.40 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 37.18it/s, est. speed input: 17770.39 toks/s, output: 34.71 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 37.35it/s, est. speed input: 17911.23 toks/s, output: 34.98 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 37.42it/s, est. speed input: 18021.79 toks/s, output: 35.20 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 37.45it/s, est. speed input: 18113.46 toks/s, output: 35.38 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 37.50it/s, est. speed input: 18194.68 toks/s, output: 35.54 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 37.53it/s, est. speed input: 18265.27 toks/s, output: 35.67 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 37.60it/s, est. speed input: 18332.13 toks/s, output: 35.80 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 37.54it/s, est. speed input: 18380.17 toks/s, output: 35.90 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 37.63it/s, est. speed input: 18434.61 toks/s, output: 36.01 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 37.70it/s, est. speed input: 18484.41 toks/s, output: 36.10 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 37.68it/s, est. speed input: 18523.53 toks/s, output: 36.18 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 37.71it/s, est. speed input: 18562.75 toks/s, output: 36.26 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 37.65it/s, est. speed input: 18591.64 toks/s, output: 36.31 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 37.65it/s, est. speed input: 18621.55 toks/s, output: 36.37 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 37.66it/s, est. speed input: 18649.46 toks/s, output: 36.42 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 37.63it/s, est. speed input: 18672.45 toks/s, output: 36.47 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 37.64it/s, est. speed input: 18695.82 toks/s, output: 36.52 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 37.66it/s, est. speed input: 18718.67 toks/s, output: 36.56 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 37.69it/s, est. speed input: 18740.22 toks/s, output: 36.60 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 37.69it/s, est. speed input: 18759.61 toks/s, output: 36.64 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 37.69it/s, est. speed input: 18777.47 toks/s, output: 36.67 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 37.70it/s, est. speed input: 18794.83 toks/s, output: 36.71 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 37.69it/s, est. speed input: 18810.29 toks/s, output: 36.74 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.69it/s, est. speed input: 18817.68 toks/s, output: 36.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.75it/s, est. speed input: 18817.68 toks/s, output: 36.75 toks/s]
[rank0]:[W126 15:11:57.929345953 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 15:11:59
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:12:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=653198) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=653198) WARNING 01-26 15:12:22 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=653198) WARNING 01-26 15:12:31 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 35.86 requests/s, 36752.25 total tokens/s, 35.86 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 15:12:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:12:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:12:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:12:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:12:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:12:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:12:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:12:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:12:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:12:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:12:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:12:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:12:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:12:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:12:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:12:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:12:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:12:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=653198) [2026-01-26 15:12:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=653198) [2026-01-26 15:12:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=653198) [2026-01-26 15:12:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=653198) [2026-01-26 15:12:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=653198) [2026-01-26 15:12:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=653198) [2026-01-26 15:12:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=653198) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=653198) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=653198) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=653198) 
(EngineCore_DP0 pid=653198) [2026-01-26 15:12:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=653198) [2026-01-26 15:12:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=653198) [2026-01-26 15:12:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=653198) [2026-01-26 15:12:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=653198) [2026-01-26 15:12:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=653198) [2026-01-26 15:12:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=653198) [2026-01-26 15:12:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=653198) [2026-01-26 15:12:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=653198) 2026-01-26 15:12:31,578 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=653198) 2026-01-26 15:12:31,600 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=653198) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.63it/s]
(EngineCore_DP0 pid=653198) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.52it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|██▉       | 38/128 [00:00<00:00, 378.96it/s]
Adding requests:  65%|██████▍   | 83/128 [00:00<00:00, 418.83it/s]
Adding requests:  98%|█████████▊| 125/128 [00:00<00:00, 226.59it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 258.52it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:00, 143.27it/s, est. speed input: 146711.68 toks/s, output: 143.27 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 55.72it/s, est. speed input: 63214.07 toks/s, output: 61.73 toks/s]   
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 48.22it/s, est. speed input: 55415.65 toks/s, output: 54.12 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 44.88it/s, est. speed input: 52085.41 toks/s, output: 50.86 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 42.91it/s, est. speed input: 50156.99 toks/s, output: 48.98 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 41.56it/s, est. speed input: 48890.45 toks/s, output: 47.74 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 40.52it/s, est. speed input: 47881.15 toks/s, output: 46.76 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 39.72it/s, est. speed input: 47048.06 toks/s, output: 45.95 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 39.13it/s, est. speed input: 46355.51 toks/s, output: 45.27 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 38.73it/s, est. speed input: 45867.81 toks/s, output: 44.79 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 38.40it/s, est. speed input: 45433.09 toks/s, output: 44.37 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:01, 38.15it/s, est. speed input: 45047.03 toks/s, output: 43.99 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:01, 37.97it/s, est. speed input: 44703.60 toks/s, output: 43.66 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 37.79it/s, est. speed input: 44384.51 toks/s, output: 43.34 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 37.70it/s, est. speed input: 44101.69 toks/s, output: 43.07 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 37.70it/s, est. speed input: 43856.69 toks/s, output: 42.83 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 37.71it/s, est. speed input: 43634.56 toks/s, output: 42.61 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 37.69it/s, est. speed input: 43426.38 toks/s, output: 42.41 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 37.56it/s, est. speed input: 43217.21 toks/s, output: 42.20 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 37.56it/s, est. speed input: 43035.72 toks/s, output: 42.03 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 37.59it/s, est. speed input: 42872.53 toks/s, output: 41.87 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 37.65it/s, est. speed input: 42726.37 toks/s, output: 41.72 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.65it/s, est. speed input: 42643.30 toks/s, output: 41.64 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.64it/s, est. speed input: 42643.30 toks/s, output: 41.64 toks/s]
[rank0]:[W126 15:12:36.122950833 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 15:12:38
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:12:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=654311) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=654311) WARNING 01-26 15:13:01 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=654311) WARNING 01-26 15:13:11 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 66.96 requests/s, 68637.49 total tokens/s, 66.96 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 15:12:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:12:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:12:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:12:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:12:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:12:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:12:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:12:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:12:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:12:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:12:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:12:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:12:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:12:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:12:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:12:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:12:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:12:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:12:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=654311) [2026-01-26 15:12:54] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=654311) [2026-01-26 15:12:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=654311) [2026-01-26 15:12:54] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=654311) [2026-01-26 15:12:54] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=654311) [2026-01-26 15:12:54] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=654311) [2026-01-26 15:12:54] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=654311) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=654311) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.43it/s]
(EngineCore_DP0 pid=654311) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.43it/s]
(EngineCore_DP0 pid=654311) 
(EngineCore_DP0 pid=654311) [2026-01-26 15:12:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=654311) [2026-01-26 15:12:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=654311) [2026-01-26 15:12:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=654311) [2026-01-26 15:12:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=654311) [2026-01-26 15:12:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=654311) [2026-01-26 15:12:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=654311) [2026-01-26 15:12:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=654311) [2026-01-26 15:12:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=654311) 2026-01-26 15:13:11,149 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=654311) 2026-01-26 15:13:11,172 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=654311) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 12.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 12.40it/s]
(EngineCore_DP0 pid=654311) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.70it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.68it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:49,  5.17it/s]
Adding requests:  16%|█▌        | 40/256 [00:00<00:01, 167.99it/s]
Adding requests:  34%|███▎      | 86/256 [00:00<00:00, 275.77it/s]
Adding requests:  50%|█████     | 128/256 [00:00<00:00, 325.85it/s]
Adding requests:  66%|██████▋   | 170/256 [00:00<00:00, 355.12it/s]
Adding requests:  83%|████████▎ | 213/256 [00:00<00:00, 377.05it/s]
Adding requests:  99%|█████████▉| 253/256 [00:00<00:00, 383.25it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 316.18it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:00<00:00, 397.95it/s, est. speed input: 407527.30 toks/s, output: 397.95 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:00<00:01, 113.23it/s, est. speed input: 130633.42 toks/s, output: 127.57 toks/s]
Processed prompts:  41%|████      | 105/256 [00:00<00:01, 99.07it/s, est. speed input: 115245.09 toks/s, output: 112.54 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:01<00:01, 89.70it/s, est. speed input: 106641.53 toks/s, output: 104.14 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:01<00:01, 85.56it/s, est. speed input: 102647.89 toks/s, output: 100.24 toks/s]
Processed prompts:  56%|█████▌    | 143/256 [00:01<00:01, 84.46it/s, est. speed input: 100769.50 toks/s, output: 98.41 toks/s] 
Processed prompts:  60%|█████▉    | 153/256 [00:01<00:01, 81.69it/s, est. speed input: 98541.76 toks/s, output: 96.23 toks/s] 
Processed prompts:  63%|██████▎   | 162/256 [00:01<00:01, 77.56it/s, est. speed input: 96069.35 toks/s, output: 93.82 toks/s]
Processed prompts:  67%|██████▋   | 171/256 [00:01<00:01, 78.62it/s, est. speed input: 95358.00 toks/s, output: 93.12 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:01<00:01, 74.91it/s, est. speed input: 93419.78 toks/s, output: 91.23 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:02<00:00, 74.49it/s, est. speed input: 92453.14 toks/s, output: 90.29 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:02<00:00, 74.10it/s, est. speed input: 91568.17 toks/s, output: 89.42 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:02<00:00, 73.86it/s, est. speed input: 90780.50 toks/s, output: 88.65 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:02<00:00, 73.60it/s, est. speed input: 90048.43 toks/s, output: 87.94 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:02<00:00, 73.44it/s, est. speed input: 89385.72 toks/s, output: 87.29 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 73.38it/s, est. speed input: 88787.89 toks/s, output: 86.71 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 73.35it/s, est. speed input: 88239.69 toks/s, output: 86.17 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:02<00:00, 73.30it/s, est. speed input: 87728.49 toks/s, output: 85.67 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:02<00:00, 73.11it/s, est. speed input: 87232.56 toks/s, output: 85.19 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 73.11it/s, est. speed input: 87033.90 toks/s, output: 84.99 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 84.99it/s, est. speed input: 87033.90 toks/s, output: 84.99 toks/s]
[rank0]:[W126 15:13:17.245016749 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 15:13:18
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:13:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=655443) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=655443) WARNING 01-26 15:13:43 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=655443) WARNING 01-26 15:13:52 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 135.75 requests/s, 139147.56 total tokens/s, 135.75 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 15:13:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:13:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:13:28] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:13:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:13:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:13:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:13:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:13:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:13:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:13:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:13:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:13:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:13:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:13:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:13:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:13:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:13:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:13:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:13:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:13:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:13:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:13:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:13:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:13:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:13:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:13:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:13:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:13:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=655443) [2026-01-26 15:13:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=655443) [2026-01-26 15:13:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=655443) [2026-01-26 15:13:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=655443) [2026-01-26 15:13:35] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=655443) [2026-01-26 15:13:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=655443) [2026-01-26 15:13:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=655443) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=655443) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.45it/s]
(EngineCore_DP0 pid=655443) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.45it/s]
(EngineCore_DP0 pid=655443) 
(EngineCore_DP0 pid=655443) [2026-01-26 15:13:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=655443) [2026-01-26 15:13:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=655443) [2026-01-26 15:13:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=655443) [2026-01-26 15:13:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=655443) [2026-01-26 15:13:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=655443) [2026-01-26 15:13:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=655443) [2026-01-26 15:13:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=655443) [2026-01-26 15:13:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=655443) 2026-01-26 15:13:52,464 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=655443) 2026-01-26 15:13:52,488 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=655443) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.33it/s]
(EngineCore_DP0 pid=655443) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 17.58it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 17.29it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 30/512 [00:00<00:01, 296.93it/s]
Adding requests:  14%|█▍        | 71/512 [00:00<00:01, 361.21it/s]
Adding requests:  22%|██▏       | 114/512 [00:00<00:01, 390.34it/s]
Adding requests:  30%|███       | 154/512 [00:00<00:01, 328.59it/s]
Adding requests:  38%|███▊      | 195/512 [00:00<00:00, 354.50it/s]
Adding requests:  47%|████▋     | 241/512 [00:00<00:00, 386.06it/s]
Adding requests:  56%|█████▌    | 285/512 [00:00<00:00, 402.36it/s]
Adding requests:  64%|██████▍   | 329/512 [00:00<00:00, 411.04it/s]
Adding requests:  73%|███████▎  | 376/512 [00:00<00:00, 426.27it/s]
Adding requests:  82%|████████▏ | 422/512 [00:01<00:00, 436.01it/s]
Adding requests:  91%|█████████ | 467/512 [00:01<00:00, 439.60it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 439.79it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 405.44it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:00<00:00, 1524.75it/s, est. speed input: 1561448.91 toks/s, output: 1524.77 toks/s]
Processed prompts:  63%|██████▎   | 323/512 [00:01<00:00, 243.53it/s, est. speed input: 287518.26 toks/s, output: 280.78 toks/s]   
Processed prompts:  77%|███████▋  | 395/512 [00:01<00:00, 204.53it/s, est. speed input: 244925.41 toks/s, output: 239.18 toks/s]
Processed prompts:  86%|████████▌ | 441/512 [00:01<00:00, 191.60it/s, est. speed input: 231324.07 toks/s, output: 225.90 toks/s]
Processed prompts:  93%|█████████▎| 475/512 [00:02<00:00, 170.98it/s, est. speed input: 215964.04 toks/s, output: 210.90 toks/s]
Processed prompts:  98%|█████████▊| 501/512 [00:02<00:00, 168.67it/s, est. speed input: 212229.97 toks/s, output: 207.26 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 168.67it/s, est. speed input: 209098.24 toks/s, output: 204.20 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 204.19it/s, est. speed input: 209098.24 toks/s, output: 204.20 toks/s]
[rank0]:[W126 15:13:58.786749422 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 15:14:00
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:14:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=656594) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=656594) WARNING 01-26 15:14:27 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=656594) WARNING 01-26 15:14:36 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 240.96 requests/s, 246982.28 total tokens/s, 240.96 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 15:14:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:14:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:14:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:14:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:14:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:14:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:14:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:14:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:14:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:14:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:14:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:14:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:14:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:14:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:14:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:14:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:14:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:14:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:14:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:14:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:14:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:14:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:14:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:14:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:14:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:14:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:14:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:14:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=656594) [2026-01-26 15:14:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=656594) [2026-01-26 15:14:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=656594) [2026-01-26 15:14:19] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=656594) [2026-01-26 15:14:19] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=656594) [2026-01-26 15:14:19] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=656594) [2026-01-26 15:14:19] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=656594) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=656594) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.55it/s]
(EngineCore_DP0 pid=656594) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.55it/s]
(EngineCore_DP0 pid=656594) 
(EngineCore_DP0 pid=656594) [2026-01-26 15:14:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=656594) [2026-01-26 15:14:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=656594) [2026-01-26 15:14:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=656594) [2026-01-26 15:14:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=656594) [2026-01-26 15:14:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=656594) [2026-01-26 15:14:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=656594) [2026-01-26 15:14:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=656594) [2026-01-26 15:14:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=656594) 2026-01-26 15:14:36,551 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=656594) 2026-01-26 15:14:36,575 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=656594) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 15.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 15.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 14.65it/s]
(EngineCore_DP0 pid=656594) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 16.24it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.20it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 34/1024 [00:00<00:02, 333.88it/s]
Adding requests:   8%|▊         | 77/1024 [00:00<00:02, 389.74it/s]
Adding requests:  12%|█▏        | 122/1024 [00:00<00:02, 414.59it/s]
Adding requests:  16%|█▌        | 166/1024 [00:00<00:02, 421.60it/s]
Adding requests:  21%|██        | 211/1024 [00:00<00:01, 428.92it/s]
Adding requests:  25%|██▌       | 259/1024 [00:00<00:01, 445.16it/s]
Adding requests:  30%|██▉       | 304/1024 [00:00<00:01, 443.35it/s]
Adding requests:  34%|███▍      | 351/1024 [00:00<00:01, 450.74it/s]
Adding requests:  39%|███▉      | 398/1024 [00:00<00:01, 454.41it/s]
Adding requests:  43%|████▎     | 445/1024 [00:01<00:01, 456.77it/s]
Adding requests:  48%|████▊     | 492/1024 [00:01<00:01, 460.42it/s]
Adding requests:  53%|█████▎    | 539/1024 [00:01<00:01, 453.00it/s]
Adding requests:  57%|█████▋    | 588/1024 [00:01<00:00, 463.58it/s]
Adding requests:  62%|██████▏   | 636/1024 [00:01<00:00, 466.42it/s]
Adding requests:  67%|██████▋   | 685/1024 [00:01<00:00, 471.99it/s]
Adding requests:  72%|███████▏  | 734/1024 [00:01<00:00, 476.89it/s]
Adding requests:  76%|███████▋  | 782/1024 [00:01<00:00, 470.29it/s]
Adding requests:  81%|████████  | 830/1024 [00:01<00:00, 461.70it/s]
Adding requests:  86%|████████▌ | 878/1024 [00:01<00:00, 465.05it/s]
Adding requests:  91%|█████████ | 927/1024 [00:02<00:00, 470.84it/s]
Adding requests:  95%|█████████▌| 975/1024 [00:02<00:00, 471.85it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 474.84it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 456.28it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:00<00:00, 4458.37it/s, est. speed input: 4565616.03 toks/s, output: 4458.45 toks/s]
Processed prompts:  98%|█████████▊| 1000/1024 [00:01<00:00, 455.08it/s, est. speed input: 547741.67 toks/s, output: 534.90 toks/s]  
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 455.08it/s, est. speed input: 523196.65 toks/s, output: 510.93 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 510.90it/s, est. speed input: 523196.65 toks/s, output: 510.93 toks/s]
[rank0]:[W126 15:14:43.580383837 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 15:14:45
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:15:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=657835) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=657835) WARNING 01-26 15:15:16 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=657835) WARNING 01-26 15:15:26 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 266.62 requests/s, 273289.15 total tokens/s, 266.62 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 15:15:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:15:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:15:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:15:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:15:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:15:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:15:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:15:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:15:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:15:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:15:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:15:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:15:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:15:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:15:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:15:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:15:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:15:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:15:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:15:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:15:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:15:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:15:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:15:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:15:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:15:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:15:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:15:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=657835) [2026-01-26 15:15:09] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=657835) [2026-01-26 15:15:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=657835) [2026-01-26 15:15:09] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=657835) [2026-01-26 15:15:09] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=657835) [2026-01-26 15:15:09] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=657835) [2026-01-26 15:15:09] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=657835) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=657835) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.42it/s]
(EngineCore_DP0 pid=657835) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.42it/s]
(EngineCore_DP0 pid=657835) 
(EngineCore_DP0 pid=657835) [2026-01-26 15:15:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=657835) [2026-01-26 15:15:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=657835) [2026-01-26 15:15:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=657835) [2026-01-26 15:15:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=657835) [2026-01-26 15:15:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=657835) [2026-01-26 15:15:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=657835) [2026-01-26 15:15:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=657835) [2026-01-26 15:15:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=657835) 2026-01-26 15:15:26,344 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=657835) 2026-01-26 15:15:26,367 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=657835) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 15.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 17.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 17.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 16.28it/s]
(EngineCore_DP0 pid=657835) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 16.94it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 17.92it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 17.92it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 39/2048 [00:00<00:05, 381.58it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 418.48it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:04, 430.38it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:04, 434.82it/s]
Adding requests:  11%|█         | 220/2048 [00:00<00:04, 443.08it/s]
Adding requests:  13%|█▎        | 267/2048 [00:00<00:03, 451.90it/s]
Adding requests:  15%|█▌        | 313/2048 [00:00<00:03, 451.11it/s]
Adding requests:  18%|█▊        | 359/2048 [00:00<00:03, 451.75it/s]
Adding requests:  20%|█▉        | 406/2048 [00:00<00:03, 456.59it/s]
Adding requests:  22%|██▏       | 452/2048 [00:01<00:03, 452.68it/s]
Adding requests:  24%|██▍       | 498/2048 [00:01<00:03, 454.71it/s]
Adding requests:  27%|██▋       | 544/2048 [00:01<00:03, 448.36it/s]
Adding requests:  29%|██▉       | 593/2048 [00:01<00:03, 458.45it/s]
Adding requests:  31%|███       | 639/2048 [00:01<00:03, 455.67it/s]
Adding requests:  34%|███▎      | 687/2048 [00:01<00:02, 462.40it/s]
Adding requests:  36%|███▌      | 736/2048 [00:01<00:02, 468.04it/s]
Adding requests:  38%|███▊      | 783/2048 [00:01<00:02, 463.49it/s]
Adding requests:  41%|████      | 830/2048 [00:01<00:02, 454.85it/s]
Adding requests:  43%|████▎     | 876/2048 [00:01<00:02, 453.17it/s]
Adding requests:  45%|████▌     | 924/2048 [00:02<00:02, 458.46it/s]
Adding requests:  47%|████▋     | 971/2048 [00:02<00:02, 460.48it/s]
Adding requests:  50%|████▉     | 1019/2048 [00:02<00:02, 465.62it/s]
Adding requests:  52%|█████▏    | 1066/2048 [00:02<00:02, 466.37it/s]
Adding requests:  54%|█████▍    | 1113/2048 [00:02<00:02, 463.16it/s]
Adding requests:  57%|█████▋    | 1160/2048 [00:02<00:01, 447.45it/s]
Adding requests:  59%|█████▉    | 1208/2048 [00:02<00:01, 456.50it/s]
Adding requests:  61%|██████    | 1254/2048 [00:02<00:01, 453.44it/s]
Adding requests:  64%|██████▎   | 1301/2048 [00:02<00:01, 455.68it/s]
Adding requests:  66%|██████▌   | 1349/2048 [00:02<00:01, 460.84it/s]
Adding requests:  68%|██████▊   | 1398/2048 [00:03<00:01, 466.84it/s]
Adding requests:  71%|███████   | 1445/2048 [00:03<00:01, 464.61it/s]
Adding requests:  73%|███████▎  | 1493/2048 [00:03<00:01, 468.42it/s]
Adding requests:  75%|███████▌  | 1540/2048 [00:03<00:01, 468.11it/s]
Adding requests:  78%|███████▊  | 1589/2048 [00:03<00:00, 473.47it/s]
Adding requests:  80%|███████▉  | 1638/2048 [00:03<00:00, 475.74it/s]
Adding requests:  82%|████████▏ | 1686/2048 [00:03<00:00, 470.10it/s]
Adding requests:  85%|████████▍ | 1734/2048 [00:03<00:00, 457.50it/s]
Adding requests:  87%|████████▋ | 1780/2048 [00:03<00:00, 453.76it/s]
Adding requests:  89%|████████▉ | 1828/2048 [00:03<00:00, 458.87it/s]
Adding requests:  92%|█████████▏| 1875/2048 [00:04<00:00, 462.09it/s]
Adding requests:  94%|█████████▍| 1922/2048 [00:04<00:00, 462.23it/s]
Adding requests:  96%|█████████▌| 1969/2048 [00:04<00:00, 461.60it/s]
Adding requests:  98%|█████████▊| 2017/2048 [00:04<00:00, 466.48it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 458.43it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:00<00:00, 9156.54it/s, est. speed input: 9376840.28 toks/s, output: 9156.67 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 9156.54it/s, est. speed input: 652854.28 toks/s, output: 637.55 toks/s]  
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 637.53it/s, est. speed input: 652854.28 toks/s, output: 637.55 toks/s] 
[rank0]:[W126 15:15:36.696805732 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 15:15:38
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:16:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=659251) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=659251) WARNING 01-26 15:16:19 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=659251) WARNING 01-26 15:16:28 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 285.26 requests/s, 292390.50 total tokens/s, 285.26 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 15:16:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:16:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:16:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:16:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:16:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:16:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:16:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:16:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:16:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:16:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:16:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:16:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:16:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:16:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:16:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:16:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:16:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:16:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:16:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:16:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:16:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:16:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:16:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:16:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:16:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:16:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:16:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:16:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=659251) [2026-01-26 15:16:11] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=659251) [2026-01-26 15:16:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=659251) [2026-01-26 15:16:11] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=659251) [2026-01-26 15:16:11] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=659251) [2026-01-26 15:16:11] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=659251) [2026-01-26 15:16:11] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=659251) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=659251) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.55it/s]
(EngineCore_DP0 pid=659251) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.55it/s]
(EngineCore_DP0 pid=659251) 
(EngineCore_DP0 pid=659251) [2026-01-26 15:16:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=659251) [2026-01-26 15:16:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=659251) [2026-01-26 15:16:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=659251) [2026-01-26 15:16:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=659251) [2026-01-26 15:16:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=659251) [2026-01-26 15:16:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=659251) [2026-01-26 15:16:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=659251) [2026-01-26 15:16:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=659251) [rank0]:W0126 15:16:24.121000 659251 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=659251) [rank0]:W0126 15:16:24.209000 659251 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=659251) [rank0]:W0126 15:16:25.134000 659251 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=659251) [rank0]:W0126 15:16:25.258000 659251 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=659251) 2026-01-26 15:16:28,710 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=659251) 2026-01-26 15:16:28,736 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=659251) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00, 12.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 14.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 15.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 16.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 15.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 15.14it/s]
(EngineCore_DP0 pid=659251) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.70it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.89it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 17.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 17.18it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 39/4096 [00:00<00:10, 380.52it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 416.37it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:09, 431.04it/s]
Adding requests:   4%|▍         | 174/4096 [00:00<00:08, 436.49it/s]
Adding requests:   5%|▌         | 220/4096 [00:00<00:08, 443.06it/s]
Adding requests:   7%|▋         | 268/4096 [00:00<00:08, 455.21it/s]
Adding requests:   8%|▊         | 314/4096 [00:00<00:08, 453.29it/s]
Adding requests:   9%|▉         | 361/4096 [00:00<00:08, 456.00it/s]
Adding requests:  10%|▉         | 408/4096 [00:00<00:08, 459.61it/s]
Adding requests:  11%|█         | 455/4096 [00:01<00:07, 461.65it/s]
Adding requests:  12%|█▏        | 502/4096 [00:01<00:07, 460.34it/s]
Adding requests:  13%|█▎        | 549/4096 [00:01<00:07, 455.50it/s]
Adding requests:  15%|█▍        | 597/4096 [00:01<00:07, 461.30it/s]
Adding requests:  16%|█▌        | 645/4096 [00:01<00:07, 466.26it/s]
Adding requests:  17%|█▋        | 694/4096 [00:01<00:07, 472.43it/s]
Adding requests:  18%|█▊        | 742/4096 [00:01<00:07, 470.63it/s]
Adding requests:  19%|█▉        | 790/4096 [00:01<00:07, 463.48it/s]
Adding requests:  20%|██        | 837/4096 [00:01<00:07, 455.05it/s]
Adding requests:  22%|██▏       | 886/4096 [00:01<00:06, 463.07it/s]
Adding requests:  23%|██▎       | 933/4096 [00:02<00:06, 459.12it/s]
Adding requests:  24%|██▍       | 980/4096 [00:02<00:06, 461.01it/s]
Adding requests:  25%|██▌       | 1028/4096 [00:02<00:06, 465.62it/s]
Adding requests:  26%|██▌       | 1075/4096 [00:02<00:06, 463.24it/s]
Adding requests:  27%|██▋       | 1122/4096 [00:02<00:06, 462.28it/s]
Adding requests:  29%|██▊       | 1171/4096 [00:02<00:06, 469.41it/s]
Adding requests:  30%|██▉       | 1221/4096 [00:02<00:06, 477.19it/s]
Adding requests:  31%|███       | 1269/4096 [00:02<00:06, 470.49it/s]
Adding requests:  32%|███▏      | 1317/4096 [00:02<00:05, 471.32it/s]
Adding requests:  33%|███▎      | 1366/4096 [00:02<00:05, 474.14it/s]
Adding requests:  35%|███▍      | 1415/4096 [00:03<00:05, 476.05it/s]
Adding requests:  36%|███▌      | 1463/4096 [00:03<00:05, 476.21it/s]
Adding requests:  37%|███▋      | 1512/4096 [00:03<00:05, 477.76it/s]
Adding requests:  38%|███▊      | 1560/4096 [00:03<00:05, 476.88it/s]
Adding requests:  39%|███▉      | 1610/4096 [00:03<00:05, 481.10it/s]
Adding requests:  41%|████      | 1659/4096 [00:03<00:05, 476.96it/s]
Adding requests:  42%|████▏     | 1707/4096 [00:03<00:05, 475.61it/s]
Adding requests:  43%|████▎     | 1755/4096 [00:03<00:04, 474.31it/s]
Adding requests:  44%|████▍     | 1803/4096 [00:03<00:05, 457.24it/s]
Adding requests:  45%|████▌     | 1850/4096 [00:03<00:04, 458.32it/s]
Adding requests:  46%|████▋     | 1897/4096 [00:04<00:04, 461.17it/s]
Adding requests:  47%|████▋     | 1944/4096 [00:04<00:04, 463.22it/s]
Adding requests:  49%|████▊     | 1992/4096 [00:04<00:04, 465.20it/s]
Adding requests:  50%|████▉     | 2039/4096 [00:04<00:04, 457.38it/s]
Adding requests:  51%|█████     | 2088/4096 [00:04<00:04, 465.98it/s]
Adding requests:  52%|█████▏    | 2135/4096 [00:04<00:04, 463.01it/s]
Adding requests:  53%|█████▎    | 2182/4096 [00:04<00:04, 460.01it/s]
Adding requests:  54%|█████▍    | 2230/4096 [00:04<00:04, 465.12it/s]
Adding requests:  56%|█████▌    | 2278/4096 [00:04<00:03, 468.00it/s]
Adding requests:  57%|█████▋    | 2325/4096 [00:05<00:03, 468.26it/s]
Adding requests:  58%|█████▊    | 2373/4096 [00:05<00:03, 469.38it/s]
Adding requests:  59%|█████▉    | 2421/4096 [00:05<00:03, 471.14it/s]
Adding requests:  60%|██████    | 2469/4096 [00:05<00:03, 469.53it/s]
Adding requests:  61%|██████▏   | 2516/4096 [00:05<00:03, 468.69it/s]
Adding requests:  63%|██████▎   | 2565/4096 [00:05<00:03, 473.54it/s]
Adding requests:  64%|██████▍   | 2613/4096 [00:05<00:03, 470.41it/s]
Adding requests:  65%|██████▍   | 2661/4096 [00:05<00:03, 472.72it/s]
Adding requests:  66%|██████▌   | 2709/4096 [00:05<00:02, 465.47it/s]
Adding requests:  67%|██████▋   | 2756/4096 [00:05<00:02, 464.38it/s]
Adding requests:  68%|██████▊   | 2803/4096 [00:06<00:02, 462.23it/s]
Adding requests:  70%|██████▉   | 2850/4096 [00:06<00:02, 455.09it/s]
Adding requests:  71%|███████   | 2898/4096 [00:06<00:02, 460.71it/s]
Adding requests:  72%|███████▏  | 2945/4096 [00:06<00:02, 459.04it/s]
Adding requests:  73%|███████▎  | 2993/4096 [00:06<00:02, 463.99it/s]
Adding requests:  74%|███████▍  | 3040/4096 [00:06<00:02, 463.19it/s]
Adding requests:  75%|███████▌  | 3087/4096 [00:06<00:02, 463.01it/s]
Adding requests:  77%|███████▋  | 3135/4096 [00:06<00:02, 466.09it/s]
Adding requests:  78%|███████▊  | 3182/4096 [00:06<00:01, 466.60it/s]
Adding requests:  79%|███████▉  | 3229/4096 [00:06<00:01, 466.52it/s]
Adding requests:  80%|███████▉  | 3276/4096 [00:07<00:01, 457.20it/s]
Adding requests:  81%|████████  | 3324/4096 [00:07<00:01, 460.14it/s]
Adding requests:  82%|████████▏ | 3372/4096 [00:07<00:01, 465.53it/s]
Adding requests:  84%|████████▎ | 3421/4096 [00:07<00:01, 471.85it/s]
Adding requests:  85%|████████▍ | 3469/4096 [00:07<00:01, 463.44it/s]
Adding requests:  86%|████████▌ | 3517/4096 [00:07<00:01, 466.34it/s]
Adding requests:  87%|████████▋ | 3564/4096 [00:07<00:01, 465.47it/s]
Adding requests:  88%|████████▊ | 3611/4096 [00:07<00:01, 466.54it/s]
Adding requests:  89%|████████▉ | 3658/4096 [00:07<00:00, 464.69it/s]
Adding requests:  90%|█████████ | 3706/4096 [00:07<00:00, 468.77it/s]
Adding requests:  92%|█████████▏| 3754/4096 [00:08<00:00, 469.58it/s]
Adding requests:  93%|█████████▎| 3803/4096 [00:08<00:00, 473.69it/s]
Adding requests:  94%|█████████▍| 3852/4096 [00:08<00:00, 475.88it/s]
Adding requests:  95%|█████████▌| 3900/4096 [00:08<00:00, 475.56it/s]
Adding requests:  96%|█████████▋| 3948/4096 [00:08<00:00, 475.68it/s]
Adding requests:  98%|█████████▊| 3996/4096 [00:08<00:00, 470.35it/s]
Adding requests:  99%|█████████▊| 4044/4096 [00:08<00:00, 468.88it/s]
Adding requests: 100%|█████████▉| 4092/4096 [00:08<00:00, 472.05it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 465.23it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  61%|██████▏   | 2517/4096 [00:00<00:00, 14320.35it/s, est. speed input: 14664656.32 toks/s, output: 14320.51 toks/s]
Processed prompts:  96%|█████████▋| 3950/4096 [00:05<00:00, 638.29it/s, est. speed input: 799658.02 toks/s, output: 780.92 toks/s]      
Processed prompts: 100%|██████████| 4096/4096 [00:05<00:00, 638.29it/s, est. speed input: 755417.27 toks/s, output: 737.71 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:05<00:00, 737.70it/s, est. speed input: 755417.27 toks/s, output: 737.71 toks/s]
[rank0]:[W126 15:16:46.408629386 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 15:16:48
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:17:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=660993) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=660993) WARNING 01-26 15:17:47 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=660993) WARNING 01-26 15:17:56 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 289.81 requests/s, 297057.03 total tokens/s, 289.81 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 15:17:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:17:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:17:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:17:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:17:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:17:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:17:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:17:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:17:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:17:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:17:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:17:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:17:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:17:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:17:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:17:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:17:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:17:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:17:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:17:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:17:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:17:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:17:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:17:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:17:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:17:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:17:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:17:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=660993) [2026-01-26 15:17:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=660993) [2026-01-26 15:17:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=660993) [2026-01-26 15:17:39] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=660993) [2026-01-26 15:17:39] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=660993) [2026-01-26 15:17:39] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=660993) [2026-01-26 15:17:39] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=660993) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=660993) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.73it/s]
(EngineCore_DP0 pid=660993) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.73it/s]
(EngineCore_DP0 pid=660993) 
(EngineCore_DP0 pid=660993) [2026-01-26 15:17:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=660993) [2026-01-26 15:17:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=660993) [2026-01-26 15:17:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=660993) [2026-01-26 15:17:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=660993) [2026-01-26 15:17:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=660993) [2026-01-26 15:17:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=660993) [2026-01-26 15:17:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=660993) [2026-01-26 15:17:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=660993) [rank0]:W0126 15:17:51.825000 660993 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=660993) [rank0]:W0126 15:17:51.909000 660993 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=660993) [rank0]:W0126 15:17:52.837000 660993 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=660993) [rank0]:W0126 15:17:52.958000 660993 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=660993) 2026-01-26 15:17:56,316 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=660993) 2026-01-26 15:17:56,342 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=660993) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:05,  3.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:05,  3.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:02,  6.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01, 10.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:00, 11.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00, 10.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00, 12.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00, 13.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:01<00:00, 14.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 10.56it/s]
(EngineCore_DP0 pid=660993) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 12.96it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 14.89it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 15.76it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 16.75it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 17.33it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 16.59it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 38/8192 [00:00<00:21, 375.99it/s]
Adding requests:   1%|          | 82/8192 [00:00<00:19, 410.56it/s]
Adding requests:   2%|▏         | 126/8192 [00:00<00:19, 423.01it/s]
Adding requests:   2%|▏         | 170/8192 [00:00<00:18, 428.63it/s]
Adding requests:   3%|▎         | 214/8192 [00:00<00:18, 432.40it/s]
Adding requests:   3%|▎         | 262/8192 [00:00<00:17, 445.32it/s]
Adding requests:   4%|▎         | 307/8192 [00:00<00:17, 446.34it/s]
Adding requests:   4%|▍         | 354/8192 [00:00<00:17, 452.91it/s]
Adding requests:   5%|▍         | 400/8192 [00:00<00:17, 453.94it/s]
Adding requests:   5%|▌         | 447/8192 [00:01<00:16, 456.96it/s]
Adding requests:   6%|▌         | 494/8192 [00:01<00:16, 459.41it/s]
Adding requests:   7%|▋         | 540/8192 [00:01<00:16, 452.17it/s]
Adding requests:   7%|▋         | 589/8192 [00:01<00:16, 461.01it/s]
Adding requests:   8%|▊         | 636/8192 [00:01<00:16, 463.35it/s]
Adding requests:   8%|▊         | 685/8192 [00:01<00:15, 469.19it/s]
Adding requests:   9%|▉         | 734/8192 [00:01<00:15, 474.01it/s]
Adding requests:  10%|▉         | 782/8192 [00:01<00:15, 467.82it/s]
Adding requests:  10%|█         | 829/8192 [00:01<00:16, 459.91it/s]
Adding requests:  11%|█         | 877/8192 [00:01<00:15, 463.17it/s]
Adding requests:  11%|█▏        | 925/8192 [00:02<00:15, 467.73it/s]
Adding requests:  12%|█▏        | 973/8192 [00:02<00:15, 470.14it/s]
Adding requests:  12%|█▏        | 1021/8192 [00:02<00:15, 471.55it/s]
Adding requests:  13%|█▎        | 1069/8192 [00:02<00:15, 470.56it/s]
Adding requests:  14%|█▎        | 1117/8192 [00:02<00:15, 463.69it/s]
Adding requests:  14%|█▍        | 1167/8192 [00:02<00:14, 471.97it/s]
Adding requests:  15%|█▍        | 1216/8192 [00:02<00:14, 476.32it/s]
Adding requests:  15%|█▌        | 1264/8192 [00:02<00:14, 470.40it/s]
Adding requests:  16%|█▌        | 1312/8192 [00:02<00:14, 469.57it/s]
Adding requests:  17%|█▋        | 1360/8192 [00:02<00:14, 472.36it/s]
Adding requests:  17%|█▋        | 1410/8192 [00:03<00:14, 478.47it/s]
Adding requests:  18%|█▊        | 1458/8192 [00:03<00:14, 476.23it/s]
Adding requests:  18%|█▊        | 1506/8192 [00:03<00:14, 465.26it/s]
Adding requests:  19%|█▉        | 1554/8192 [00:03<00:14, 466.60it/s]
Adding requests:  20%|█▉        | 1604/8192 [00:03<00:13, 475.58it/s]
Adding requests:  20%|██        | 1652/8192 [00:03<00:13, 473.37it/s]
Adding requests:  21%|██        | 1700/8192 [00:03<00:13, 469.58it/s]
Adding requests:  21%|██▏       | 1748/8192 [00:03<00:13, 471.57it/s]
Adding requests:  22%|██▏       | 1796/8192 [00:03<00:13, 473.57it/s]
Adding requests:  23%|██▎       | 1844/8192 [00:03<00:13, 473.88it/s]
Adding requests:  23%|██▎       | 1892/8192 [00:04<00:13, 471.85it/s]
Adding requests:  24%|██▎       | 1940/8192 [00:04<00:13, 471.18it/s]
Adding requests:  24%|██▍       | 1988/8192 [00:04<00:13, 472.90it/s]
Adding requests:  25%|██▍       | 2036/8192 [00:04<00:12, 474.78it/s]
Adding requests:  25%|██▌       | 2085/8192 [00:04<00:12, 477.85it/s]
Adding requests:  26%|██▌       | 2133/8192 [00:04<00:12, 473.43it/s]
Adding requests:  27%|██▋       | 2181/8192 [00:04<00:12, 466.24it/s]
Adding requests:  27%|██▋       | 2228/8192 [00:04<00:12, 463.30it/s]
Adding requests:  28%|██▊       | 2276/8192 [00:04<00:12, 466.24it/s]
Adding requests:  28%|██▊       | 2324/8192 [00:05<00:12, 468.40it/s]
Adding requests:  29%|██▉       | 2372/8192 [00:05<00:12, 470.23it/s]
Adding requests:  30%|██▉       | 2420/8192 [00:05<00:12, 472.17it/s]
Adding requests:  30%|███       | 2468/8192 [00:05<00:12, 471.56it/s]
Adding requests:  31%|███       | 2516/8192 [00:05<00:12, 470.42it/s]
Adding requests:  31%|███▏      | 2565/8192 [00:05<00:11, 476.08it/s]
Adding requests:  32%|███▏      | 2613/8192 [00:05<00:11, 475.02it/s]
Adding requests:  32%|███▏      | 2661/8192 [00:05<00:11, 466.65it/s]
Adding requests:  33%|███▎      | 2708/8192 [00:05<00:11, 463.09it/s]
Adding requests:  34%|███▎      | 2755/8192 [00:05<00:11, 463.84it/s]
Adding requests:  34%|███▍      | 2802/8192 [00:06<00:11, 463.89it/s]
Adding requests:  35%|███▍      | 2850/8192 [00:06<00:11, 467.03it/s]
Adding requests:  35%|███▌      | 2898/8192 [00:06<00:11, 470.17it/s]
Adding requests:  36%|███▌      | 2946/8192 [00:06<00:11, 466.23it/s]
Adding requests:  37%|███▋      | 2994/8192 [00:06<00:11, 469.89it/s]
Adding requests:  37%|███▋      | 3042/8192 [00:06<00:10, 468.61it/s]
Adding requests:  38%|███▊      | 3090/8192 [00:06<00:10, 467.60it/s]
Adding requests:  38%|███▊      | 3138/8192 [00:06<00:10, 468.45it/s]
Adding requests:  39%|███▉      | 3186/8192 [00:06<00:10, 469.97it/s]
Adding requests:  39%|███▉      | 3234/8192 [00:06<00:10, 472.13it/s]
Adding requests:  40%|████      | 3282/8192 [00:07<00:10, 474.19it/s]
Adding requests:  41%|████      | 3330/8192 [00:07<00:10, 473.48it/s]
Adding requests:  41%|████      | 3379/8192 [00:07<00:10, 477.86it/s]
Adding requests:  42%|████▏     | 3428/8192 [00:07<00:09, 480.03it/s]
Adding requests:  42%|████▏     | 3477/8192 [00:07<00:10, 453.39it/s]
Adding requests:  43%|████▎     | 3525/8192 [00:07<00:10, 458.53it/s]
Adding requests:  44%|████▎     | 3572/8192 [00:07<00:10, 460.52it/s]
Adding requests:  44%|████▍     | 3619/8192 [00:07<00:09, 459.66it/s]
Adding requests:  45%|████▍     | 3666/8192 [00:07<00:09, 462.61it/s]
Adding requests:  45%|████▌     | 3714/8192 [00:07<00:09, 465.91it/s]
Adding requests:  46%|████▌     | 3761/8192 [00:08<00:09, 466.67it/s]
Adding requests:  47%|████▋     | 3810/8192 [00:08<00:09, 470.83it/s]
Adding requests:  47%|████▋     | 3860/8192 [00:08<00:09, 477.21it/s]
Adding requests:  48%|████▊     | 3908/8192 [00:08<00:09, 459.69it/s]
Adding requests:  48%|████▊     | 3956/8192 [00:08<00:09, 464.92it/s]
Adding requests:  49%|████▉     | 4003/8192 [00:08<00:09, 464.08it/s]
Adding requests:  49%|████▉     | 4050/8192 [00:08<00:08, 464.00it/s]
Adding requests:  50%|█████     | 4098/8192 [00:08<00:08, 466.51it/s]
Adding requests:  51%|█████     | 4145/8192 [00:08<00:08, 467.39it/s]
Adding requests:  51%|█████     | 4194/8192 [00:08<00:08, 472.82it/s]
Adding requests:  52%|█████▏    | 4242/8192 [00:09<00:08, 472.02it/s]
Adding requests:  52%|█████▏    | 4290/8192 [00:09<00:08, 469.85it/s]
Adding requests:  53%|█████▎    | 4339/8192 [00:09<00:08, 475.79it/s]
Adding requests:  54%|█████▎    | 4388/8192 [00:09<00:07, 478.75it/s]
Adding requests:  54%|█████▍    | 4436/8192 [00:09<00:07, 476.82it/s]
Adding requests:  55%|█████▍    | 4484/8192 [00:09<00:07, 472.97it/s]
Adding requests:  55%|█████▌    | 4532/8192 [00:09<00:07, 469.46it/s]
Adding requests:  56%|█████▌    | 4580/8192 [00:09<00:07, 471.36it/s]
Adding requests:  56%|█████▋    | 4628/8192 [00:09<00:07, 473.37it/s]
Adding requests:  57%|█████▋    | 4676/8192 [00:10<00:07, 472.89it/s]
Adding requests:  58%|█████▊    | 4724/8192 [00:10<00:07, 473.17it/s]
Adding requests:  58%|█████▊    | 4772/8192 [00:10<00:07, 474.01it/s]
Adding requests:  59%|█████▉    | 4820/8192 [00:10<00:07, 472.71it/s]
Adding requests:  59%|█████▉    | 4868/8192 [00:10<00:07, 470.73it/s]
Adding requests:  60%|██████    | 4916/8192 [00:10<00:06, 469.45it/s]
Adding requests:  61%|██████    | 4963/8192 [00:10<00:06, 468.28it/s]
Adding requests:  61%|██████    | 5010/8192 [00:10<00:06, 466.95it/s]
Adding requests:  62%|██████▏   | 5057/8192 [00:10<00:06, 467.28it/s]
Adding requests:  62%|██████▏   | 5106/8192 [00:10<00:06, 471.44it/s]
Adding requests:  63%|██████▎   | 5154/8192 [00:11<00:06, 456.82it/s]
Adding requests:  63%|██████▎   | 5201/8192 [00:11<00:06, 459.47it/s]
Adding requests:  64%|██████▍   | 5248/8192 [00:11<00:06, 456.60it/s]
Adding requests:  65%|██████▍   | 5295/8192 [00:11<00:06, 460.11it/s]
Adding requests:  65%|██████▌   | 5342/8192 [00:11<00:06, 462.40it/s]
Adding requests:  66%|██████▌   | 5389/8192 [00:11<00:06, 464.58it/s]
Adding requests:  66%|██████▋   | 5436/8192 [00:11<00:05, 465.74it/s]
Adding requests:  67%|██████▋   | 5483/8192 [00:11<00:05, 461.39it/s]
Adding requests:  68%|██████▊   | 5530/8192 [00:11<00:05, 462.55it/s]
Adding requests:  68%|██████▊   | 5577/8192 [00:11<00:05, 462.22it/s]
Adding requests:  69%|██████▊   | 5624/8192 [00:12<00:05, 461.26it/s]
Adding requests:  69%|██████▉   | 5671/8192 [00:12<00:05, 457.83it/s]
Adding requests:  70%|██████▉   | 5720/8192 [00:12<00:05, 464.72it/s]
Adding requests:  70%|███████   | 5768/8192 [00:12<00:05, 468.64it/s]
Adding requests:  71%|███████   | 5815/8192 [00:12<00:05, 463.69it/s]
Adding requests:  72%|███████▏  | 5862/8192 [00:12<00:05, 465.54it/s]
Adding requests:  72%|███████▏  | 5911/8192 [00:12<00:04, 472.03it/s]
Adding requests:  73%|███████▎  | 5959/8192 [00:12<00:04, 453.66it/s]
Adding requests:  73%|███████▎  | 6007/8192 [00:12<00:04, 460.95it/s]
Adding requests:  74%|███████▍  | 6056/8192 [00:12<00:04, 468.63it/s]
Adding requests:  74%|███████▍  | 6103/8192 [00:13<00:04, 467.25it/s]
Adding requests:  75%|███████▌  | 6151/8192 [00:13<00:04, 469.12it/s]
Adding requests:  76%|███████▌  | 6199/8192 [00:13<00:04, 470.95it/s]
Adding requests:  76%|███████▋  | 6249/8192 [00:13<00:04, 476.92it/s]
Adding requests:  77%|███████▋  | 6298/8192 [00:13<00:03, 478.55it/s]
Adding requests:  77%|███████▋  | 6347/8192 [00:13<00:03, 479.66it/s]
Adding requests:  78%|███████▊  | 6395/8192 [00:13<00:03, 467.69it/s]
Adding requests:  79%|███████▊  | 6444/8192 [00:13<00:03, 473.68it/s]
Adding requests:  79%|███████▉  | 6492/8192 [00:13<00:03, 475.12it/s]
Adding requests:  80%|███████▉  | 6541/8192 [00:14<00:03, 479.20it/s]
Adding requests:  80%|████████  | 6589/8192 [00:14<00:03, 478.10it/s]
Adding requests:  81%|████████  | 6637/8192 [00:14<00:03, 474.27it/s]
Adding requests:  82%|████████▏ | 6685/8192 [00:14<00:03, 474.27it/s]
Adding requests:  82%|████████▏ | 6733/8192 [00:14<00:03, 475.95it/s]
Adding requests:  83%|████████▎ | 6781/8192 [00:14<00:02, 475.23it/s]
Adding requests:  83%|████████▎ | 6830/8192 [00:14<00:02, 476.95it/s]
Adding requests:  84%|████████▍ | 6880/8192 [00:14<00:02, 479.54it/s]
Adding requests:  85%|████████▍ | 6929/8192 [00:14<00:02, 481.80it/s]
Adding requests:  85%|████████▌ | 6978/8192 [00:14<00:02, 481.86it/s]
Adding requests:  86%|████████▌ | 7027/8192 [00:15<00:02, 475.22it/s]
Adding requests:  86%|████████▋ | 7075/8192 [00:15<00:02, 474.62it/s]
Adding requests:  87%|████████▋ | 7124/8192 [00:15<00:02, 477.31it/s]
Adding requests:  88%|████████▊ | 7172/8192 [00:15<00:02, 473.49it/s]
Adding requests:  88%|████████▊ | 7220/8192 [00:15<00:02, 472.87it/s]
Adding requests:  89%|████████▊ | 7268/8192 [00:15<00:01, 474.95it/s]
Adding requests:  89%|████████▉ | 7316/8192 [00:15<00:01, 476.37it/s]
Adding requests:  90%|████████▉ | 7364/8192 [00:15<00:01, 473.78it/s]
Adding requests:  91%|█████████ | 7414/8192 [00:15<00:01, 479.74it/s]
Adding requests:  91%|█████████ | 7463/8192 [00:15<00:01, 480.68it/s]
Adding requests:  92%|█████████▏| 7512/8192 [00:16<00:01, 481.12it/s]
Adding requests:  92%|█████████▏| 7561/8192 [00:16<00:01, 478.84it/s]
Adding requests:  93%|█████████▎| 7609/8192 [00:16<00:01, 474.75it/s]
Adding requests:  93%|█████████▎| 7657/8192 [00:16<00:01, 467.85it/s]
Adding requests:  94%|█████████▍| 7705/8192 [00:16<00:01, 471.19it/s]
Adding requests:  95%|█████████▍| 7753/8192 [00:16<00:00, 469.96it/s]
Adding requests:  95%|█████████▌| 7801/8192 [00:16<00:00, 465.54it/s]
Adding requests:  96%|█████████▌| 7850/8192 [00:16<00:00, 471.31it/s]
Adding requests:  96%|█████████▋| 7898/8192 [00:16<00:00, 471.12it/s]
Adding requests:  97%|█████████▋| 7946/8192 [00:16<00:00, 466.42it/s]
Adding requests:  98%|█████████▊| 7993/8192 [00:17<00:00, 467.07it/s]
Adding requests:  98%|█████████▊| 8040/8192 [00:17<00:00, 464.88it/s]
Adding requests:  99%|█████████▊| 8089/8192 [00:17<00:00, 470.31it/s]
Adding requests:  99%|█████████▉| 8137/8192 [00:17<00:00, 470.98it/s]
Adding requests: 100%|█████████▉| 8186/8192 [00:17<00:00, 474.61it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 468.46it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  61%|██████▏   | 5025/8192 [00:00<00:00, 32794.87it/s, est. speed input: 33584298.63 toks/s, output: 32795.48 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:10<00:00, 32794.87it/s, est. speed input: 778463.59 toks/s, output: 760.22 toks/s]    
Processed prompts: 100%|██████████| 8192/8192 [00:10<00:00, 760.21it/s, est. speed input: 778463.59 toks/s, output: 760.22 toks/s]  
[rank0]:[W126 15:18:29.437233726 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 16:28:18
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:28:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=761512) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=761512) WARNING 01-26 16:28:42 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=761512) WARNING 01-26 16:28:52 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 38.29 requests/s, 19643.27 total tokens/s, 38.29 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 16:28:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:28:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:28:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:28:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:28:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:28:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:28:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:28:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:28:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:28:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:28:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:28:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:28:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:28:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:28:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:28:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:28:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:33] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:33] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:33] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:33] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:33] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=761512) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=761512) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=761512) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.21it/s]
(EngineCore_DP0 pid=761512) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.24it/s]
(EngineCore_DP0 pid=761512) 
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=761512) [2026-01-26 16:28:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=761512) 2026-01-26 16:28:52,170 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=761512) 2026-01-26 16:28:52,193 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=761512) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.16it/s]
(EngineCore_DP0 pid=761512) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.63it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 509.14it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 256.75it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 304.54it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:00, 138.29it/s, est. speed input: 70808.46 toks/s, output: 138.29 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 57.75it/s, est. speed input: 32508.32 toks/s, output: 63.49 toks/s]  
Processed prompts:  30%|██▉       | 38/128 [00:00<00:01, 50.22it/s, est. speed input: 28591.11 toks/s, output: 55.84 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 46.91it/s, est. speed input: 26933.09 toks/s, output: 52.60 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 45.03it/s, est. speed input: 25996.07 toks/s, output: 50.77 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 43.80it/s, est. speed input: 25395.62 toks/s, output: 49.60 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 42.79it/s, est. speed input: 24907.20 toks/s, output: 48.65 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 42.07it/s, est. speed input: 24515.20 toks/s, output: 47.88 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 41.51it/s, est. speed input: 24183.43 toks/s, output: 47.23 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 41.13it/s, est. speed input: 23906.82 toks/s, output: 46.69 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 40.89it/s, est. speed input: 23675.13 toks/s, output: 46.24 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:01, 40.68it/s, est. speed input: 23469.75 toks/s, output: 45.84 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 40.51it/s, est. speed input: 23286.71 toks/s, output: 45.48 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 40.43it/s, est. speed input: 23129.42 toks/s, output: 45.17 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 40.36it/s, est. speed input: 22988.35 toks/s, output: 44.90 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.29it/s, est. speed input: 22860.39 toks/s, output: 44.65 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 40.27it/s, est. speed input: 22747.08 toks/s, output: 44.43 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 40.26it/s, est. speed input: 22645.72 toks/s, output: 44.23 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 40.23it/s, est. speed input: 22551.65 toks/s, output: 44.05 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 40.18it/s, est. speed input: 22463.10 toks/s, output: 43.87 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.18it/s, est. speed input: 22432.72 toks/s, output: 43.81 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.81it/s, est. speed input: 22432.72 toks/s, output: 43.81 toks/s]
[rank0]:[W126 16:28:57.098468324 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 16:28:59
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:29:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=762726) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=762726) WARNING 01-26 16:29:23 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=762726) WARNING 01-26 16:29:32 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 39.82 requests/s, 40815.57 total tokens/s, 39.82 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 16:29:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:29:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:29:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:29:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:29:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:29:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:29:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:29:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:29:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:29:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:29:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:29:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:15] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:15] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:15] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:15] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:15] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=762726) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=762726) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.56it/s]
(EngineCore_DP0 pid=762726) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.30it/s]
(EngineCore_DP0 pid=762726) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.33it/s]
(EngineCore_DP0 pid=762726) 
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=762726) [2026-01-26 16:29:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=762726) 2026-01-26 16:29:32,827 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=762726) 2026-01-26 16:29:32,849 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=762726) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.96it/s]
(EngineCore_DP0 pid=762726) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.37it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  21%|██        | 27/128 [00:00<00:00, 267.23it/s]
Adding requests:  45%|████▍     | 57/128 [00:00<00:00, 282.00it/s]
Adding requests:  69%|██████▉   | 88/128 [00:00<00:00, 291.87it/s]
Adding requests:  95%|█████████▍| 121/128 [00:00<00:00, 303.03it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 296.79it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:00, 164.52it/s, est. speed input: 168477.99 toks/s, output: 164.52 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 61.53it/s, est. speed input: 70130.39 toks/s, output: 68.49 toks/s]   
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 52.87it/s, est. speed input: 61142.90 toks/s, output: 59.71 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:00<00:01, 49.24it/s, est. speed input: 57581.26 toks/s, output: 56.23 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 47.00it/s, est. speed input: 55464.16 toks/s, output: 54.16 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 45.30it/s, est. speed input: 53853.76 toks/s, output: 52.59 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 44.18it/s, est. speed input: 52769.71 toks/s, output: 51.53 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 43.23it/s, est. speed input: 51843.93 toks/s, output: 50.63 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 42.53it/s, est. speed input: 51070.09 toks/s, output: 49.87 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:01, 41.99it/s, est. speed input: 50398.54 toks/s, output: 49.22 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 41.61it/s, est. speed input: 49821.80 toks/s, output: 48.65 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 41.38it/s, est. speed input: 49328.33 toks/s, output: 48.17 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 41.15it/s, est. speed input: 48876.42 toks/s, output: 47.73 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.96it/s, est. speed input: 48469.50 toks/s, output: 47.33 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 40.82it/s, est. speed input: 48103.20 toks/s, output: 46.98 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 40.76it/s, est. speed input: 47781.28 toks/s, output: 46.66 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 40.73it/s, est. speed input: 47491.94 toks/s, output: 46.38 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 40.66it/s, est. speed input: 47218.40 toks/s, output: 46.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.66it/s, est. speed input: 47117.41 toks/s, output: 46.01 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 46.01it/s, est. speed input: 47117.41 toks/s, output: 46.01 toks/s]
[rank0]:[W126 16:29:37.955922062 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 16:29:39
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:29:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=763845) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=763845) WARNING 01-26 16:30:03 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=763845) WARNING 01-26 16:30:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 72.76 requests/s, 74576.67 total tokens/s, 72.76 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 16:29:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:29:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:29:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:29:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:29:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:29:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:29:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:29:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:29:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:29:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:29:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:29:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:29:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:29:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=763845) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=763845) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.64it/s]
(EngineCore_DP0 pid=763845) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.36it/s]
(EngineCore_DP0 pid=763845) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=763845) 
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=763845) [2026-01-26 16:29:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=763845) 2026-01-26 16:30:13,066 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=763845) 2026-01-26 16:30:13,089 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=763845) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.57it/s]
(EngineCore_DP0 pid=763845) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.87it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:50,  5.07it/s]
Adding requests:  11%|█         | 28/256 [00:00<00:01, 115.97it/s]
Adding requests:  24%|██▍       | 62/256 [00:00<00:00, 198.30it/s]
Adding requests:  36%|███▋      | 93/256 [00:00<00:00, 234.42it/s]
Adding requests:  49%|████▉     | 126/256 [00:00<00:00, 264.63it/s]
Adding requests:  62%|██████▎   | 160/256 [00:00<00:00, 286.58it/s]
Adding requests:  76%|███████▌  | 195/256 [00:00<00:00, 305.30it/s]
Adding requests:  90%|████████▉ | 230/256 [00:00<00:00, 318.92it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 258.96it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:00<00:00, 611.34it/s, est. speed input: 626054.67 toks/s, output: 611.35 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:00<00:01, 124.98it/s, est. speed input: 145323.35 toks/s, output: 141.92 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:01<00:00, 107.66it/s, est. speed input: 126422.87 toks/s, output: 123.46 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:01<00:00, 100.29it/s, est. speed input: 119071.75 toks/s, output: 116.28 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:01<00:00, 95.59it/s, est. speed input: 114745.95 toks/s, output: 112.06 toks/s] 
Processed prompts:  79%|███████▉  | 203/256 [00:01<00:00, 92.56it/s, est. speed input: 112037.73 toks/s, output: 109.41 toks/s]
Processed prompts:  84%|████████▍ | 215/256 [00:02<00:00, 89.78it/s, est. speed input: 109786.21 toks/s, output: 107.21 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:02<00:00, 85.89it/s, est. speed input: 107395.14 toks/s, output: 104.88 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 84.65it/s, est. speed input: 106021.00 toks/s, output: 103.54 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:02<00:00, 83.53it/s, est. speed input: 104770.26 toks/s, output: 102.31 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 82.75it/s, est. speed input: 103669.87 toks/s, output: 101.24 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 82.75it/s, est. speed input: 103669.87 toks/s, output: 101.24 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 101.24it/s, est. speed input: 103669.87 toks/s, output: 101.24 toks/s]
[rank0]:[W126 16:30:18.603620856 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 16:30:20
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:30:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=764978) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=764978) WARNING 01-26 16:30:45 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=764978) WARNING 01-26 16:30:55 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 136.05 requests/s, 139453.88 total tokens/s, 136.05 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 16:30:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:30:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:30:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:30:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:30:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:30:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:30:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:30:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:30:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:30:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:30:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:30:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:30:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:30:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:30:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:30:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:30:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=764978) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=764978) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.77it/s]
(EngineCore_DP0 pid=764978) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.41it/s]
(EngineCore_DP0 pid=764978) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.46it/s]
(EngineCore_DP0 pid=764978) 
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=764978) [2026-01-26 16:30:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=764978) 2026-01-26 16:30:55,819 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=764978) 2026-01-26 16:30:55,841 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=764978) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.30it/s]
(EngineCore_DP0 pid=764978) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 16.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 17.53it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 27/512 [00:00<00:01, 263.98it/s]
Adding requests:  12%|█▏        | 62/512 [00:00<00:01, 311.58it/s]
Adding requests:  18%|█▊        | 94/512 [00:00<00:01, 310.93it/s]
Adding requests:  25%|██▌       | 128/512 [00:00<00:01, 320.65it/s]
Adding requests:  32%|███▏      | 164/512 [00:00<00:01, 330.99it/s]
Adding requests:  39%|███▉      | 199/512 [00:00<00:00, 332.32it/s]
Adding requests:  46%|████▌     | 234/512 [00:00<00:00, 336.51it/s]
Adding requests:  53%|█████▎    | 269/512 [00:00<00:00, 338.56it/s]
Adding requests:  60%|█████▉    | 305/512 [00:00<00:00, 344.67it/s]
Adding requests:  67%|██████▋   | 343/512 [00:01<00:00, 353.13it/s]
Adding requests:  74%|███████▍  | 379/512 [00:01<00:00, 353.47it/s]
Adding requests:  82%|████████▏ | 418/512 [00:01<00:00, 363.18it/s]
Adding requests:  89%|████████▉ | 455/512 [00:01<00:00, 359.78it/s]
Adding requests:  97%|█████████▋| 495/512 [00:01<00:00, 370.75it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 346.39it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  41%|████      | 210/512 [00:00<00:00, 1757.39it/s, est. speed input: 1799675.02 toks/s, output: 1757.42 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:01<00:00, 238.74it/s, est. speed input: 284611.59 toks/s, output: 277.94 toks/s]   
Processed prompts:  91%|█████████ | 467/512 [00:01<00:00, 202.04it/s, est. speed input: 243772.18 toks/s, output: 238.06 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 202.04it/s, est. speed input: 229556.34 toks/s, output: 224.18 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 224.17it/s, est. speed input: 229556.34 toks/s, output: 224.18 toks/s]
[rank0]:[W126 16:31:01.911865543 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 16:31:03
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:31:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=766189) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=766189) WARNING 01-26 16:31:32 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=766189) WARNING 01-26 16:31:41 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 147.68 requests/s, 151372.08 total tokens/s, 147.68 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 16:31:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:31:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:31:16] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:31:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:31:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:31:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:31:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:31:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:31:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:31:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:31:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:31:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:31:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:31:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:31:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:31:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:31:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:24] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:24] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:24] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:24] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:24] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=766189) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=766189) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.76it/s]
(EngineCore_DP0 pid=766189) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.41it/s]
(EngineCore_DP0 pid=766189) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]
(EngineCore_DP0 pid=766189) 
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=766189) [2026-01-26 16:31:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=766189) 2026-01-26 16:31:41,911 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=766189) 2026-01-26 16:31:41,935 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=766189) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 17.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 16.16it/s]
(EngineCore_DP0 pid=766189) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 18.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.80it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.98it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 276.10it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 318.02it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:03, 300.35it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 309.68it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 321.84it/s]
Adding requests:  19%|█▉        | 199/1024 [00:00<00:02, 332.43it/s]
Adding requests:  23%|██▎       | 235/1024 [00:00<00:02, 339.86it/s]
Adding requests:  26%|██▋       | 270/1024 [00:00<00:02, 340.13it/s]
Adding requests:  30%|██▉       | 306/1024 [00:00<00:02, 345.06it/s]
Adding requests:  33%|███▎      | 343/1024 [00:01<00:01, 351.77it/s]
Adding requests:  37%|███▋      | 380/1024 [00:01<00:01, 355.36it/s]
Adding requests:  41%|████      | 418/1024 [00:01<00:01, 360.81it/s]
Adding requests:  44%|████▍     | 455/1024 [00:01<00:01, 358.08it/s]
Adding requests:  48%|████▊     | 495/1024 [00:01<00:01, 368.97it/s]
Adding requests:  52%|█████▏    | 532/1024 [00:01<00:01, 367.60it/s]
Adding requests:  56%|█████▌    | 569/1024 [00:01<00:01, 365.81it/s]
Adding requests:  59%|█████▉    | 606/1024 [00:01<00:01, 354.95it/s]
Adding requests:  63%|██████▎   | 642/1024 [00:01<00:01, 353.27it/s]
Adding requests:  66%|██████▌   | 678/1024 [00:01<00:01, 342.73it/s]
Adding requests:  70%|██████▉   | 715/1024 [00:02<00:00, 348.55it/s]
Adding requests:  73%|███████▎  | 750/1024 [00:02<00:00, 344.79it/s]
Adding requests:  77%|███████▋  | 786/1024 [00:02<00:00, 348.06it/s]
Adding requests:  80%|████████  | 821/1024 [00:02<00:00, 347.94it/s]
Adding requests:  84%|████████▍ | 859/1024 [00:02<00:00, 355.39it/s]
Adding requests:  88%|████████▊ | 896/1024 [00:02<00:00, 358.43it/s]
Adding requests:  91%|█████████ | 932/1024 [00:02<00:00, 351.58it/s]
Adding requests:  95%|█████████▍| 969/1024 [00:02<00:00, 355.78it/s]
Adding requests:  98%|█████████▊| 1005/1024 [00:02<00:00, 349.82it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 347.50it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:00<00:00, 2981.06it/s, est. speed input: 3052757.46 toks/s, output: 2981.10 toks/s]
Processed prompts:  72%|███████▏  | 741/1024 [00:02<00:00, 293.56it/s, est. speed input: 358428.69 toks/s, output: 350.03 toks/s]   
Processed prompts:  85%|████████▌ | 873/1024 [00:02<00:00, 242.06it/s, est. speed input: 300899.15 toks/s, output: 293.85 toks/s]
Processed prompts:  93%|█████████▎| 952/1024 [00:03<00:00, 218.80it/s, est. speed input: 278252.38 toks/s, output: 271.73 toks/s]
Processed prompts:  98%|█████████▊| 1006/1024 [00:03<00:00, 204.04it/s, est. speed input: 265722.30 toks/s, output: 259.49 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:03<00:00, 204.04it/s, est. speed input: 263075.55 toks/s, output: 256.91 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:03<00:00, 256.90it/s, est. speed input: 263075.55 toks/s, output: 256.91 toks/s]
[rank0]:[W126 16:31:50.217091250 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 16:31:52
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:32:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=767522) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=767522) WARNING 01-26 16:32:27 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=767522) WARNING 01-26 16:32:37 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 156.73 requests/s, 160646.32 total tokens/s, 156.73 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 16:32:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:32:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:32:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:32:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:32:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:32:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:32:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:32:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:32:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:32:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:32:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:32:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:32:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:32:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:32:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:32:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:32:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:19] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:19] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:19] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:19] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=767522) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=767522) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.75it/s]
(EngineCore_DP0 pid=767522) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.40it/s]
(EngineCore_DP0 pid=767522) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.44it/s]
(EngineCore_DP0 pid=767522) 
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=767522) [2026-01-26 16:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=767522) [rank0]:W0126 16:32:32.810000 767522 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=767522) [rank0]:W0126 16:32:32.893000 767522 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=767522) [rank0]:W0126 16:32:33.863000 767522 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=767522) [rank0]:W0126 16:32:33.995000 767522 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=767522) 2026-01-26 16:32:37,676 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=767522) 2026-01-26 16:32:37,702 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=767522) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 15.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 16.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 17.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 16.14it/s]
(EngineCore_DP0 pid=767522) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 18.23it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.62it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.50it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 279.06it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 318.68it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 316.10it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:05, 323.35it/s]
Adding requests:   8%|▊         | 164/2048 [00:00<00:05, 331.22it/s]
Adding requests:  10%|▉         | 201/2048 [00:00<00:05, 342.45it/s]
Adding requests:  12%|█▏        | 238/2048 [00:00<00:05, 348.15it/s]
Adding requests:  13%|█▎        | 273/2048 [00:00<00:05, 345.30it/s]
Adding requests:  15%|█▌        | 310/2048 [00:00<00:04, 352.08it/s]
Adding requests:  17%|█▋        | 348/2048 [00:01<00:04, 358.29it/s]
Adding requests:  19%|█▉        | 386/2048 [00:01<00:04, 361.15it/s]
Adding requests:  21%|██        | 425/2048 [00:01<00:04, 367.86it/s]
Adding requests:  23%|██▎       | 462/2048 [00:01<00:04, 363.96it/s]
Adding requests:  25%|██▍       | 502/2048 [00:01<00:04, 371.55it/s]
Adding requests:  26%|██▋       | 541/2048 [00:01<00:04, 373.94it/s]
Adding requests:  28%|██▊       | 579/2048 [00:01<00:03, 372.26it/s]
Adding requests:  30%|███       | 617/2048 [00:01<00:04, 357.04it/s]
Adding requests:  32%|███▏      | 653/2048 [00:01<00:03, 351.20it/s]
Adding requests:  34%|███▎      | 691/2048 [00:01<00:03, 357.89it/s]
Adding requests:  35%|███▌      | 727/2048 [00:02<00:03, 351.82it/s]
Adding requests:  37%|███▋      | 763/2048 [00:02<00:03, 352.14it/s]
Adding requests:  39%|███▉      | 800/2048 [00:02<00:03, 354.42it/s]
Adding requests:  41%|████      | 837/2048 [00:02<00:03, 358.81it/s]
Adding requests:  43%|████▎     | 875/2048 [00:02<00:03, 361.62it/s]
Adding requests:  45%|████▍     | 912/2048 [00:02<00:03, 361.14it/s]
Adding requests:  46%|████▋     | 949/2048 [00:02<00:03, 356.37it/s]
Adding requests:  48%|████▊     | 986/2048 [00:02<00:02, 359.22it/s]
Adding requests:  50%|████▉     | 1022/2048 [00:02<00:02, 355.78it/s]
Adding requests:  52%|█████▏    | 1058/2048 [00:02<00:02, 353.76it/s]
Adding requests:  53%|█████▎    | 1094/2048 [00:03<00:02, 351.25it/s]
Adding requests:  55%|█████▌    | 1133/2048 [00:03<00:02, 359.40it/s]
Adding requests:  57%|█████▋    | 1169/2048 [00:03<00:02, 354.24it/s]
Adding requests:  59%|█████▉    | 1206/2048 [00:03<00:02, 357.85it/s]
Adding requests:  61%|██████    | 1243/2048 [00:03<00:02, 360.36it/s]
Adding requests:  62%|██████▎   | 1280/2048 [00:03<00:02, 355.54it/s]
Adding requests:  64%|██████▍   | 1316/2048 [00:03<00:02, 356.02it/s]
Adding requests:  66%|██████▌   | 1354/2048 [00:03<00:01, 361.24it/s]
Adding requests:  68%|██████▊   | 1391/2048 [00:03<00:01, 348.07it/s]
Adding requests:  70%|██████▉   | 1426/2048 [00:04<00:01, 346.57it/s]
Adding requests:  71%|███████▏  | 1463/2048 [00:04<00:01, 351.85it/s]
Adding requests:  73%|███████▎  | 1500/2048 [00:04<00:01, 356.60it/s]
Adding requests:  75%|███████▌  | 1536/2048 [00:04<00:01, 355.14it/s]
Adding requests:  77%|███████▋  | 1572/2048 [00:04<00:01, 350.45it/s]
Adding requests:  79%|███████▊  | 1608/2048 [00:04<00:01, 352.33it/s]
Adding requests:  80%|████████  | 1644/2048 [00:04<00:01, 334.26it/s]
Adding requests:  82%|████████▏ | 1678/2048 [00:04<00:01, 334.87it/s]
Adding requests:  84%|████████▎ | 1715/2048 [00:04<00:00, 344.87it/s]
Adding requests:  86%|████████▌ | 1752/2048 [00:04<00:00, 350.38it/s]
Adding requests:  87%|████████▋ | 1788/2048 [00:05<00:00, 352.69it/s]
Adding requests:  89%|████████▉ | 1824/2048 [00:05<00:00, 353.28it/s]
Adding requests:  91%|█████████ | 1861/2048 [00:05<00:00, 355.46it/s]
Adding requests:  93%|█████████▎| 1897/2048 [00:05<00:00, 355.71it/s]
Adding requests:  94%|█████████▍| 1935/2048 [00:05<00:00, 362.24it/s]
Adding requests:  96%|█████████▋| 1973/2048 [00:05<00:00, 364.21it/s]
Adding requests:  98%|█████████▊| 2010/2048 [00:05<00:00, 356.50it/s]
Adding requests: 100%|█████████▉| 2046/2048 [00:05<00:00, 352.92it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 353.29it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  44%|████▍     | 909/2048 [00:00<00:00, 6450.89it/s, est. speed input: 6606043.17 toks/s, output: 6450.98 toks/s]
Processed prompts:  76%|███████▌  | 1555/2048 [00:04<00:01, 308.72it/s, est. speed input: 379494.72 toks/s, output: 370.60 toks/s]  
Processed prompts:  89%|████████▉ | 1830/2048 [00:05<00:00, 253.98it/s, est. speed input: 317327.53 toks/s, output: 309.89 toks/s]
Processed prompts:  97%|█████████▋| 1987/2048 [00:06<00:00, 230.41it/s, est. speed input: 294146.49 toks/s, output: 287.25 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:07<00:00, 230.41it/s, est. speed input: 288520.68 toks/s, output: 281.76 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:07<00:00, 281.75it/s, est. speed input: 288520.68 toks/s, output: 281.76 toks/s]
[rank0]:[W126 16:32:53.481675395 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 16:32:55
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:33:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=769091) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=769091) WARNING 01-26 16:33:42 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=769091) WARNING 01-26 16:33:52 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 162.05 requests/s, 166103.86 total tokens/s, 162.05 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 16:33:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:33:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:33:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:33:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:33:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:33:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:33:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:33:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:33:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:33:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:33:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:33:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:33:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:33:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:33:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:33:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:33:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:34] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=769091) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=769091) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.75it/s]
(EngineCore_DP0 pid=769091) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.40it/s]
(EngineCore_DP0 pid=769091) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.44it/s]
(EngineCore_DP0 pid=769091) 
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=769091) [2026-01-26 16:33:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=769091) [rank0]:W0126 16:33:47.146000 769091 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=769091) [rank0]:W0126 16:33:47.228000 769091 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=769091) [rank0]:W0126 16:33:48.171000 769091 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=769091) [rank0]:W0126 16:33:48.298000 769091 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=769091) 2026-01-26 16:33:52,167 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=769091) 2026-01-26 16:33:52,192 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=769091) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 13.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 12.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 14.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 15.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.40it/s]
(EngineCore_DP0 pid=769091) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.90it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.11it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 18.33it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 18.28it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 276.55it/s]
Adding requests:   2%|▏         | 64/4096 [00:00<00:12, 319.59it/s]
Adding requests:   2%|▏         | 96/4096 [00:00<00:12, 316.97it/s]
Adding requests:   3%|▎         | 130/4096 [00:00<00:12, 322.98it/s]
Adding requests:   4%|▍         | 166/4096 [00:00<00:11, 332.79it/s]
Adding requests:   5%|▍         | 203/4096 [00:00<00:11, 343.27it/s]
Adding requests:   6%|▌         | 239/4096 [00:00<00:11, 347.29it/s]
Adding requests:   7%|▋         | 274/4096 [00:00<00:11, 346.04it/s]
Adding requests:   8%|▊         | 311/4096 [00:00<00:10, 352.05it/s]
Adding requests:   9%|▊         | 349/4096 [00:01<00:10, 357.57it/s]
Adding requests:   9%|▉         | 386/4096 [00:01<00:10, 361.19it/s]
Adding requests:  10%|█         | 425/4096 [00:01<00:09, 367.36it/s]
Adding requests:  11%|█▏        | 462/4096 [00:01<00:09, 364.60it/s]
Adding requests:  12%|█▏        | 502/4096 [00:01<00:09, 371.75it/s]
Adding requests:  13%|█▎        | 542/4096 [00:01<00:09, 378.29it/s]
Adding requests:  14%|█▍        | 580/4096 [00:01<00:09, 374.81it/s]
Adding requests:  15%|█▌        | 618/4096 [00:01<00:09, 365.80it/s]
Adding requests:  16%|█▌        | 655/4096 [00:01<00:09, 358.63it/s]
Adding requests:  17%|█▋        | 693/4096 [00:01<00:09, 362.81it/s]
Adding requests:  18%|█▊        | 730/4096 [00:02<00:09, 356.34it/s]
Adding requests:  19%|█▊        | 766/4096 [00:02<00:09, 356.95it/s]
Adding requests:  20%|█▉        | 802/4096 [00:02<00:09, 355.82it/s]
Adding requests:  21%|██        | 840/4096 [00:02<00:08, 362.03it/s]
Adding requests:  21%|██▏       | 877/4096 [00:02<00:08, 362.18it/s]
Adding requests:  22%|██▏       | 914/4096 [00:02<00:08, 363.16it/s]
Adding requests:  23%|██▎       | 951/4096 [00:02<00:08, 359.41it/s]
Adding requests:  24%|██▍       | 987/4096 [00:02<00:08, 357.98it/s]
Adding requests:  25%|██▍       | 1023/4096 [00:02<00:08, 354.28it/s]
Adding requests:  26%|██▌       | 1059/4096 [00:02<00:08, 352.44it/s]
Adding requests:  27%|██▋       | 1095/4096 [00:03<00:08, 350.17it/s]
Adding requests:  28%|██▊       | 1134/4096 [00:03<00:08, 359.54it/s]
Adding requests:  29%|██▊       | 1170/4096 [00:03<00:08, 352.72it/s]
Adding requests:  29%|██▉       | 1206/4096 [00:03<00:08, 351.38it/s]
Adding requests:  30%|███       | 1243/4096 [00:03<00:08, 355.40it/s]
Adding requests:  31%|███       | 1279/4096 [00:03<00:08, 351.68it/s]
Adding requests:  32%|███▏      | 1315/4096 [00:03<00:07, 352.23it/s]
Adding requests:  33%|███▎      | 1353/4096 [00:03<00:07, 357.12it/s]
Adding requests:  34%|███▍      | 1390/4096 [00:03<00:07, 359.99it/s]
Adding requests:  35%|███▍      | 1427/4096 [00:04<00:07, 354.81it/s]
Adding requests:  36%|███▌      | 1464/4096 [00:04<00:07, 357.53it/s]
Adding requests:  37%|███▋      | 1502/4096 [00:04<00:07, 361.49it/s]
Adding requests:  38%|███▊      | 1539/4096 [00:04<00:07, 361.10it/s]
Adding requests:  38%|███▊      | 1576/4096 [00:04<00:07, 353.58it/s]
Adding requests:  39%|███▉      | 1612/4096 [00:04<00:07, 354.35it/s]
Adding requests:  40%|████      | 1648/4096 [00:04<00:07, 344.66it/s]
Adding requests:  41%|████      | 1683/4096 [00:04<00:06, 345.01it/s]
Adding requests:  42%|████▏     | 1720/4096 [00:04<00:06, 350.02it/s]
Adding requests:  43%|████▎     | 1757/4096 [00:04<00:06, 353.15it/s]
Adding requests:  44%|████▍     | 1794/4096 [00:05<00:06, 355.99it/s]
Adding requests:  45%|████▍     | 1831/4096 [00:05<00:06, 358.39it/s]
Adding requests:  46%|████▌     | 1867/4096 [00:05<00:06, 356.67it/s]
Adding requests:  46%|████▋     | 1904/4096 [00:05<00:06, 358.44it/s]
Adding requests:  47%|████▋     | 1940/4096 [00:05<00:06, 352.67it/s]
Adding requests:  48%|████▊     | 1976/4096 [00:05<00:05, 353.42it/s]
Adding requests:  49%|████▉     | 2012/4096 [00:05<00:05, 351.88it/s]
Adding requests:  50%|█████     | 2048/4096 [00:05<00:05, 348.34it/s]
Adding requests:  51%|█████     | 2083/4096 [00:05<00:06, 330.98it/s]
Adding requests:  52%|█████▏    | 2121/4096 [00:05<00:05, 344.20it/s]
Adding requests:  53%|█████▎    | 2156/4096 [00:06<00:05, 342.57it/s]
Adding requests:  53%|█████▎    | 2191/4096 [00:06<00:05, 339.88it/s]
Adding requests:  54%|█████▍    | 2226/4096 [00:06<00:05, 342.44it/s]
Adding requests:  55%|█████▌    | 2263/4096 [00:06<00:05, 347.45it/s]
Adding requests:  56%|█████▌    | 2301/4096 [00:06<00:05, 356.69it/s]
Adding requests:  57%|█████▋    | 2338/4096 [00:06<00:04, 359.48it/s]
Adding requests:  58%|█████▊    | 2375/4096 [00:06<00:04, 360.67it/s]
Adding requests:  59%|█████▉    | 2413/4096 [00:06<00:04, 366.37it/s]
Adding requests:  60%|█████▉    | 2450/4096 [00:06<00:04, 365.13it/s]
Adding requests:  61%|██████    | 2487/4096 [00:07<00:04, 366.18it/s]
Adding requests:  62%|██████▏   | 2524/4096 [00:07<00:04, 366.08it/s]
Adding requests:  63%|██████▎   | 2565/4096 [00:07<00:04, 378.62it/s]
Adding requests:  64%|██████▎   | 2603/4096 [00:07<00:03, 376.36it/s]
Adding requests:  64%|██████▍   | 2641/4096 [00:07<00:03, 364.33it/s]
Adding requests:  65%|██████▌   | 2678/4096 [00:07<00:03, 357.59it/s]
Adding requests:  66%|██████▋   | 2714/4096 [00:07<00:03, 352.67it/s]
Adding requests:  67%|██████▋   | 2751/4096 [00:07<00:03, 357.42it/s]
Adding requests:  68%|██████▊   | 2791/4096 [00:07<00:03, 366.77it/s]
Adding requests:  69%|██████▉   | 2828/4096 [00:07<00:03, 367.52it/s]
Adding requests:  70%|██████▉   | 2865/4096 [00:08<00:03, 365.38it/s]
Adding requests:  71%|███████   | 2902/4096 [00:08<00:03, 365.37it/s]
Adding requests:  72%|███████▏  | 2940/4096 [00:08<00:03, 367.62it/s]
Adding requests:  73%|███████▎  | 2977/4096 [00:08<00:03, 367.28it/s]
Adding requests:  74%|███████▎  | 3015/4096 [00:08<00:02, 370.28it/s]
Adding requests:  75%|███████▍  | 3053/4096 [00:08<00:02, 371.83it/s]
Adding requests:  75%|███████▌  | 3091/4096 [00:08<00:02, 370.55it/s]
Adding requests:  76%|███████▋  | 3130/4096 [00:08<00:02, 374.34it/s]
Adding requests:  77%|███████▋  | 3168/4096 [00:08<00:02, 368.78it/s]
Adding requests:  78%|███████▊  | 3205/4096 [00:08<00:02, 364.83it/s]
Adding requests:  79%|███████▉  | 3243/4096 [00:09<00:02, 366.65it/s]
Adding requests:  80%|████████  | 3280/4096 [00:09<00:02, 359.88it/s]
Adding requests:  81%|████████  | 3317/4096 [00:09<00:02, 354.49it/s]
Adding requests:  82%|████████▏ | 3354/4096 [00:09<00:02, 356.92it/s]
Adding requests:  83%|████████▎ | 3390/4096 [00:09<00:02, 351.05it/s]
Adding requests:  84%|████████▎ | 3427/4096 [00:09<00:01, 355.06it/s]
Adding requests:  85%|████████▍ | 3464/4096 [00:09<00:01, 358.44it/s]
Adding requests:  85%|████████▌ | 3500/4096 [00:09<00:01, 357.67it/s]
Adding requests:  86%|████████▋ | 3540/4096 [00:09<00:01, 368.49it/s]
Adding requests:  87%|████████▋ | 3577/4096 [00:10<00:01, 366.80it/s]
Adding requests:  88%|████████▊ | 3614/4096 [00:10<00:01, 367.56it/s]
Adding requests:  89%|████████▉ | 3651/4096 [00:10<00:01, 367.36it/s]
Adding requests:  90%|█████████ | 3688/4096 [00:10<00:01, 357.66it/s]
Adding requests:  91%|█████████ | 3725/4096 [00:10<00:01, 361.08it/s]
Adding requests:  92%|█████████▏| 3762/4096 [00:10<00:00, 353.69it/s]
Adding requests:  93%|█████████▎| 3798/4096 [00:10<00:00, 342.89it/s]
Adding requests:  94%|█████████▎| 3833/4096 [00:10<00:00, 342.26it/s]
Adding requests:  94%|█████████▍| 3870/4096 [00:10<00:00, 349.78it/s]
Adding requests:  95%|█████████▌| 3906/4096 [00:10<00:00, 343.91it/s]
Adding requests:  96%|█████████▌| 3942/4096 [00:11<00:00, 343.77it/s]
Adding requests:  97%|█████████▋| 3978/4096 [00:11<00:00, 345.22it/s]
Adding requests:  98%|█████████▊| 4015/4096 [00:11<00:00, 350.00it/s]
Adding requests:  99%|█████████▉| 4051/4096 [00:11<00:00, 345.38it/s]
Adding requests: 100%|█████████▉| 4087/4096 [00:11<00:00, 347.40it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 356.25it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  45%|████▌     | 1862/4096 [00:00<00:00, 6647.97it/s, est. speed input: 6807716.76 toks/s, output: 6648.03 toks/s]
Processed prompts:  62%|██████▏   | 2527/4096 [00:04<00:03, 478.27it/s, est. speed input: 616154.44 toks/s, output: 601.71 toks/s]   
Processed prompts:  69%|██████▊   | 2811/4096 [00:05<00:03, 357.32it/s, est. speed input: 482178.67 toks/s, output: 470.88 toks/s]
Processed prompts:  73%|███████▎  | 2973/4096 [00:06<00:03, 314.06it/s, est. speed input: 439048.48 toks/s, output: 428.76 toks/s]
Processed prompts:  75%|███████▌  | 3078/4096 [00:07<00:03, 275.73it/s, est. speed input: 407990.06 toks/s, output: 398.43 toks/s]
Processed prompts:  77%|███████▋  | 3150/4096 [00:08<00:03, 263.24it/s, est. speed input: 397080.20 toks/s, output: 387.77 toks/s]
Processed prompts:  78%|███████▊  | 3204/4096 [00:08<00:03, 264.24it/s, est. speed input: 394356.29 toks/s, output: 385.11 toks/s]
Processed prompts:  79%|███████▉  | 3249/4096 [00:08<00:03, 234.46it/s, est. speed input: 381664.26 toks/s, output: 372.72 toks/s]
Processed prompts:  80%|████████  | 3284/4096 [00:08<00:03, 227.25it/s, est. speed input: 377178.37 toks/s, output: 368.34 toks/s]
Processed prompts:  81%|████████  | 3314/4096 [00:09<00:03, 215.77it/s, est. speed input: 372314.09 toks/s, output: 363.59 toks/s]
Processed prompts:  82%|████████▏ | 3339/4096 [00:09<00:03, 199.95it/s, est. speed input: 367131.61 toks/s, output: 358.53 toks/s]
Processed prompts:  82%|████████▏ | 3366/4096 [00:09<00:03, 186.95it/s, est. speed input: 362348.20 toks/s, output: 353.86 toks/s]
Processed prompts:  83%|████████▎ | 3398/4096 [00:09<00:03, 182.00it/s, est. speed input: 358459.10 toks/s, output: 350.06 toks/s]
Processed prompts:  84%|████████▎ | 3430/4096 [00:09<00:03, 177.05it/s, est. speed input: 354599.39 toks/s, output: 346.29 toks/s]
Processed prompts:  85%|████████▍ | 3462/4096 [00:10<00:03, 172.81it/s, est. speed input: 350849.68 toks/s, output: 342.63 toks/s]
Processed prompts:  85%|████████▌ | 3494/4096 [00:10<00:03, 170.06it/s, est. speed input: 347324.70 toks/s, output: 339.18 toks/s]
Processed prompts:  86%|████████▌ | 3526/4096 [00:10<00:03, 167.18it/s, est. speed input: 343820.21 toks/s, output: 335.76 toks/s]
Processed prompts:  87%|████████▋ | 3558/4096 [00:10<00:03, 167.51it/s, est. speed input: 340773.25 toks/s, output: 332.79 toks/s]
Processed prompts:  88%|████████▊ | 3590/4096 [00:10<00:03, 165.95it/s, est. speed input: 337605.27 toks/s, output: 329.69 toks/s]
Processed prompts:  88%|████████▊ | 3622/4096 [00:11<00:02, 165.86it/s, est. speed input: 334676.90 toks/s, output: 326.83 toks/s]
Processed prompts:  89%|████████▉ | 3654/4096 [00:11<00:02, 164.48it/s, est. speed input: 331692.94 toks/s, output: 323.92 toks/s]
Processed prompts:  90%|████████▉ | 3686/4096 [00:11<00:02, 166.45it/s, est. speed input: 329149.76 toks/s, output: 321.44 toks/s]
Processed prompts:  91%|█████████ | 3718/4096 [00:11<00:02, 166.72it/s, est. speed input: 326562.81 toks/s, output: 318.91 toks/s]
Processed prompts:  92%|█████████▏| 3750/4096 [00:11<00:02, 164.51it/s, est. speed input: 323801.05 toks/s, output: 316.21 toks/s]
Processed prompts:  92%|█████████▏| 3782/4096 [00:12<00:01, 162.97it/s, est. speed input: 321130.08 toks/s, output: 313.60 toks/s]
Processed prompts:  93%|█████████▎| 3814/4096 [00:12<00:01, 162.34it/s, est. speed input: 318592.55 toks/s, output: 311.13 toks/s]
Processed prompts:  94%|█████████▍| 3846/4096 [00:12<00:01, 161.65it/s, est. speed input: 316110.25 toks/s, output: 308.70 toks/s]
Processed prompts:  95%|█████████▍| 3878/4096 [00:12<00:01, 161.63it/s, est. speed input: 313752.25 toks/s, output: 306.40 toks/s]
Processed prompts:  95%|█████████▌| 3910/4096 [00:12<00:01, 164.13it/s, est. speed input: 311713.92 toks/s, output: 304.41 toks/s]
Processed prompts:  96%|█████████▌| 3942/4096 [00:13<00:00, 165.12it/s, est. speed input: 309658.35 toks/s, output: 302.40 toks/s]
Processed prompts:  97%|█████████▋| 3974/4096 [00:13<00:00, 163.78it/s, est. speed input: 307476.53 toks/s, output: 300.27 toks/s]
Processed prompts:  98%|█████████▊| 4006/4096 [00:13<00:00, 163.74it/s, est. speed input: 305439.40 toks/s, output: 298.28 toks/s]
Processed prompts:  99%|█████████▊| 4038/4096 [00:13<00:00, 166.92it/s, est. speed input: 303740.33 toks/s, output: 296.62 toks/s]
Processed prompts:  99%|█████████▉| 4070/4096 [00:13<00:00, 174.96it/s, est. speed input: 302539.91 toks/s, output: 295.45 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:13<00:00, 174.96it/s, est. speed input: 304465.43 toks/s, output: 297.33 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:13<00:00, 297.33it/s, est. speed input: 304465.43 toks/s, output: 297.33 toks/s]
[rank0]:[W126 16:34:20.790603803 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 16:34:22
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:35:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=771169) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=771169) WARNING 01-26 16:35:33 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     def forward(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     raise e
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/tmp/torchinductor_root/xo/cxohmxvb3s5i5om5eclj5heiwlbja3nfrkpkl67ukch2tdr36mj6.py", line 1090, in call
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     triton_poi_fused_mul_quant_slide_fp8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_slide_fp8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     torch.cuda.synchronize()
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=771169) ERROR 01-26 16:35:40 [core.py:866] 

STDERR:
[2026-01-26 16:35:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:35:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:35:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:35:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:35:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:35:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:35:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:35:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:35:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:35:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:35:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:35:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:35:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:35:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:25] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=771169) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=771169) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.77it/s]
(EngineCore_DP0 pid=771169) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.42it/s]
(EngineCore_DP0 pid=771169) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.46it/s]
(EngineCore_DP0 pid=771169) 
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=771169) [2026-01-26 16:35:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=771169) [rank0]:W0126 16:35:38.594000 771169 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=771169) [rank0]:W0126 16:35:38.677000 771169 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=771169) [rank0]:W0126 16:35:39.794000 771169 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=771169) [rank0]:W0126 16:35:39.919000 771169 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=771169) Process EngineCore_DP0:
(EngineCore_DP0 pid=771169) Traceback (most recent call last):
(EngineCore_DP0 pid=771169)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=771169)     self.run()
(EngineCore_DP0 pid=771169)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=771169)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=771169)     raise e
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=771169)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=771169)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=771169)     super().__init__(
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=771169)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=771169)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=771169)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=771169)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=771169)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=771169)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=771169)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=771169)     return func(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=771169)     return func(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=771169)     self.model_runner.profile_run()
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=771169)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=771169)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=771169)     return func(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=771169)     outputs = self.model(
(EngineCore_DP0 pid=771169)               ^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=771169)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=771169)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=771169)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=771169)     hidden_states = self.model(
(EngineCore_DP0 pid=771169)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=771169)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=771169)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=771169)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=771169)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=771169)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=771169)     def forward(
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=771169)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=771169)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=771169)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=771169)     raise e
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=771169)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=771169)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=771169)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=771169)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=771169)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=771169)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=771169)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=771169)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=771169)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=771169)     return compiled_fn(full_args)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=771169)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=771169)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=771169)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=771169)                             ^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=771169)     outs = compiled_fn(args)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=771169)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=771169)     return self.current_callable(inputs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=771169)     out = model(new_inputs)
(EngineCore_DP0 pid=771169)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/tmp/torchinductor_root/xo/cxohmxvb3s5i5om5eclj5heiwlbja3nfrkpkl67ukch2tdr36mj6.py", line 1090, in call
(EngineCore_DP0 pid=771169)     triton_poi_fused_mul_quant_slide_fp8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_slide_fp8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=771169)     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=771169)     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=771169)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=771169)     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=771169)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=771169)     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=771169)     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=771169)     torch.cuda.synchronize()
(EngineCore_DP0 pid=771169)   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=771169)     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=771169)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=771169) torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=771169) Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=771169) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=771169) For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=771169) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=771169) 
[rank0]:[W126 16:35:40.217689091 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-26 18:12:30
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:12:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=892978) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=892978) WARNING 01-26 18:12:58 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=892978) WARNING 01-26 18:13:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 23.14 requests/s, 11872.00 total tokens/s, 23.14 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 18:12:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:12:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:12:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:12:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:12:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:12:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:12:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:12:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:12:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:12:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:12:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:12:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:12:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:12:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:12:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:12:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:12:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:12:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:12:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:12:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:12:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:12:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:12:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:12:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:12:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:12:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:12:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:12:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=892978) [2026-01-26 18:12:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=892978) [2026-01-26 18:12:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=892978) [2026-01-26 18:12:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=892978) [2026-01-26 18:12:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=892978) [2026-01-26 18:12:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=892978) [2026-01-26 18:12:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=892978) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=892978) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.07it/s]
(EngineCore_DP0 pid=892978) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.71it/s]
(EngineCore_DP0 pid=892978) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.39it/s]
(EngineCore_DP0 pid=892978) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.27it/s]
(EngineCore_DP0 pid=892978) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.32it/s]
(EngineCore_DP0 pid=892978) 
(EngineCore_DP0 pid=892978) [2026-01-26 18:12:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=892978) [2026-01-26 18:12:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=892978) [2026-01-26 18:12:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=892978) [2026-01-26 18:12:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=892978) [2026-01-26 18:12:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=892978) [2026-01-26 18:12:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=892978) [2026-01-26 18:12:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=892978) [2026-01-26 18:12:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=892978) 2026-01-26 18:13:13,926 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=892978) 2026-01-26 18:13:13,966 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=892978) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.96it/s]
(EngineCore_DP0 pid=892978) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 10.16it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 513.42it/s]
Adding requests:  88%|████████▊ | 112/128 [00:00<00:00, 562.98it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 560.97it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:03, 37.44it/s, est. speed input: 19171.33 toks/s, output: 37.44 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 28.76it/s, est. speed input: 15317.04 toks/s, output: 29.92 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:04, 26.73it/s, est. speed input: 14391.10 toks/s, output: 28.11 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:04, 25.66it/s, est. speed input: 13891.08 toks/s, output: 27.13 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:04, 24.97it/s, est. speed input: 13560.87 toks/s, output: 26.49 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:04, 24.60it/s, est. speed input: 13350.71 toks/s, output: 26.08 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:04, 24.35it/s, est. speed input: 13193.43 toks/s, output: 25.77 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:04, 24.20it/s, est. speed input: 13077.87 toks/s, output: 25.54 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:04, 24.06it/s, est. speed input: 12980.26 toks/s, output: 25.35 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:03, 23.95it/s, est. speed input: 12898.93 toks/s, output: 25.19 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:03, 23.91it/s, est. speed input: 12837.55 toks/s, output: 25.07 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:03, 23.88it/s, est. speed input: 12784.84 toks/s, output: 24.97 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:03, 23.89it/s, est. speed input: 12745.27 toks/s, output: 24.89 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:03, 23.90it/s, est. speed input: 12710.52 toks/s, output: 24.83 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:03, 23.87it/s, est. speed input: 12676.75 toks/s, output: 24.76 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:03, 23.87it/s, est. speed input: 12648.94 toks/s, output: 24.70 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:02<00:03, 23.88it/s, est. speed input: 12625.42 toks/s, output: 24.66 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:02<00:02, 23.82it/s, est. speed input: 12597.84 toks/s, output: 24.61 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:02<00:02, 23.83it/s, est. speed input: 12578.51 toks/s, output: 24.57 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:02<00:02, 23.81it/s, est. speed input: 12558.11 toks/s, output: 24.53 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:02<00:02, 23.81it/s, est. speed input: 12541.05 toks/s, output: 24.49 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:02, 23.84it/s, est. speed input: 12527.79 toks/s, output: 24.47 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:02<00:02, 23.91it/s, est. speed input: 12519.29 toks/s, output: 24.45 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:03<00:02, 23.88it/s, est. speed input: 12505.92 toks/s, output: 24.43 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:03<00:02, 23.89it/s, est. speed input: 12495.70 toks/s, output: 24.41 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:03<00:01, 23.89it/s, est. speed input: 12485.52 toks/s, output: 24.39 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:03<00:01, 23.92it/s, est. speed input: 12477.94 toks/s, output: 24.37 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:03<00:01, 23.91it/s, est. speed input: 12469.52 toks/s, output: 24.35 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:03<00:01, 23.93it/s, est. speed input: 12462.77 toks/s, output: 24.34 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:03<00:01, 23.94it/s, est. speed input: 12456.46 toks/s, output: 24.33 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:03<00:01, 23.78it/s, est. speed input: 12441.35 toks/s, output: 24.30 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:04<00:01, 23.75it/s, est. speed input: 12431.48 toks/s, output: 24.28 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:04<00:01, 23.80it/s, est. speed input: 12426.10 toks/s, output: 24.27 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:04<00:00, 23.80it/s, est. speed input: 12418.88 toks/s, output: 24.26 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:04<00:00, 23.41it/s, est. speed input: 12392.82 toks/s, output: 24.20 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:04<00:00, 23.54it/s, est. speed input: 12387.84 toks/s, output: 24.19 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:04<00:00, 23.66it/s, est. speed input: 12384.35 toks/s, output: 24.19 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:04<00:00, 23.71it/s, est. speed input: 12379.59 toks/s, output: 24.18 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:04<00:00, 23.66it/s, est. speed input: 12371.38 toks/s, output: 24.16 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:05<00:00, 23.73it/s, est. speed input: 12368.04 toks/s, output: 24.16 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:05<00:00, 23.78it/s, est. speed input: 12364.81 toks/s, output: 24.15 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.78it/s, est. speed input: 12361.48 toks/s, output: 24.14 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 24.14it/s, est. speed input: 12361.48 toks/s, output: 24.14 toks/s]
[rank0]:[W126 18:13:22.244645955 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 18:13:24
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:13:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=894383) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=894383) WARNING 01-26 18:13:52 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=894383) WARNING 01-26 18:14:06 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 23.38 requests/s, 23960.31 total tokens/s, 23.38 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 18:13:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:13:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:13:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:13:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:13:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:13:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:13:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:13:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:13:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:13:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:13:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:13:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:13:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:13:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:13:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:13:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:13:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:13:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:13:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:13:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:13:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:13:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:13:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:13:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:13:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:13:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:13:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:13:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=894383) [2026-01-26 18:13:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=894383) [2026-01-26 18:13:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=894383) [2026-01-26 18:13:39] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=894383) [2026-01-26 18:13:39] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=894383) [2026-01-26 18:13:39] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=894383) [2026-01-26 18:13:39] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=894383) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=894383) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.13it/s]
(EngineCore_DP0 pid=894383) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.80it/s]
(EngineCore_DP0 pid=894383) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.49it/s]
(EngineCore_DP0 pid=894383) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.37it/s]
(EngineCore_DP0 pid=894383) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.41it/s]
(EngineCore_DP0 pid=894383) 
(EngineCore_DP0 pid=894383) [2026-01-26 18:13:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=894383) [2026-01-26 18:13:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=894383) [2026-01-26 18:13:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=894383) [2026-01-26 18:13:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=894383) [2026-01-26 18:13:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=894383) [2026-01-26 18:13:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=894383) [2026-01-26 18:13:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=894383) [2026-01-26 18:13:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=894383) 2026-01-26 18:14:06,406 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=894383) 2026-01-26 18:14:06,444 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=894383) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.89it/s]
(EngineCore_DP0 pid=894383) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 10.11it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 275.75it/s]
Adding requests:  49%|████▉     | 63/128 [00:00<00:00, 316.64it/s]
Adding requests:  74%|███████▍  | 95/128 [00:00<00:00, 310.59it/s]
Adding requests:  99%|█████████▉| 127/128 [00:00<00:00, 310.90it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 308.54it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:01, 93.84it/s, est. speed input: 96095.22 toks/s, output: 93.84 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 35.20it/s, est. speed input: 39967.91 toks/s, output: 39.03 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:03, 30.42it/s, est. speed input: 34972.13 toks/s, output: 34.15 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:03, 28.55it/s, est. speed input: 33121.61 toks/s, output: 32.35 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:03, 27.18it/s, est. speed input: 31816.95 toks/s, output: 31.07 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:03, 26.40it/s, est. speed input: 31076.58 toks/s, output: 30.35 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:03, 25.75it/s, est. speed input: 30465.99 toks/s, output: 29.75 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:03, 25.16it/s, est. speed input: 29928.44 toks/s, output: 29.23 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:03, 24.74it/s, est. speed input: 29481.83 toks/s, output: 28.79 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:03, 24.45it/s, est. speed input: 29108.46 toks/s, output: 28.43 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:03, 24.02it/s, est. speed input: 28722.34 toks/s, output: 28.05 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:02<00:03, 23.95it/s, est. speed input: 28450.38 toks/s, output: 27.78 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:02<00:02, 23.89it/s, est. speed input: 28204.59 toks/s, output: 27.54 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:02<00:02, 23.53it/s, est. speed input: 27918.06 toks/s, output: 27.26 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:02, 23.55it/s, est. speed input: 27718.23 toks/s, output: 27.07 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:02<00:02, 23.60it/s, est. speed input: 27548.04 toks/s, output: 26.90 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:02, 23.64it/s, est. speed input: 27392.68 toks/s, output: 26.75 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:02, 23.67it/s, est. speed input: 27253.37 toks/s, output: 26.61 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:02, 23.69it/s, est. speed input: 27124.16 toks/s, output: 26.49 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:03<00:02, 23.72it/s, est. speed input: 27009.17 toks/s, output: 26.38 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:03<00:01, 23.76it/s, est. speed input: 26906.99 toks/s, output: 26.28 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:03<00:01, 23.75it/s, est. speed input: 26806.41 toks/s, output: 26.18 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:03<00:01, 23.75it/s, est. speed input: 26714.64 toks/s, output: 26.09 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:03<00:01, 23.75it/s, est. speed input: 26628.93 toks/s, output: 26.00 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:03<00:01, 23.76it/s, est. speed input: 26550.86 toks/s, output: 25.93 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:03<00:01, 23.72it/s, est. speed input: 26471.68 toks/s, output: 25.85 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:01, 23.73it/s, est. speed input: 26401.87 toks/s, output: 25.78 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:04<00:01, 23.73it/s, est. speed input: 26336.24 toks/s, output: 25.72 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:04<00:00, 23.72it/s, est. speed input: 26273.55 toks/s, output: 25.66 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:04<00:00, 23.70it/s, est. speed input: 26212.49 toks/s, output: 25.60 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:04<00:00, 23.64it/s, est. speed input: 26151.24 toks/s, output: 25.54 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:04<00:00, 23.66it/s, est. speed input: 26099.01 toks/s, output: 25.49 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:04<00:00, 23.66it/s, est. speed input: 26048.16 toks/s, output: 25.44 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:04<00:00, 23.65it/s, est. speed input: 25999.36 toks/s, output: 25.39 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:04<00:00, 23.61it/s, est. speed input: 25949.91 toks/s, output: 25.34 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.62it/s, est. speed input: 25906.63 toks/s, output: 25.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.62it/s, est. speed input: 25906.63 toks/s, output: 25.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 25.30it/s, est. speed input: 25906.63 toks/s, output: 25.30 toks/s]
[rank0]:[W126 18:14:13.048464740 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 18:14:15
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:14:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=895648) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=895648) WARNING 01-26 18:14:44 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=895648) WARNING 01-26 18:14:59 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 48.21 requests/s, 49413.17 total tokens/s, 48.21 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 18:14:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:14:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:14:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:14:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:14:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:14:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:14:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:14:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:14:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:14:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:14:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:14:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:14:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:14:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:14:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:14:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:14:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:14:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:14:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:14:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:14:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:14:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:14:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:14:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:14:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:14:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:14:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:14:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=895648) [2026-01-26 18:14:31] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=895648) [2026-01-26 18:14:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=895648) [2026-01-26 18:14:31] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=895648) [2026-01-26 18:14:31] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=895648) [2026-01-26 18:14:31] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=895648) [2026-01-26 18:14:31] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=895648) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=895648) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.30it/s]
(EngineCore_DP0 pid=895648) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:00,  2.05it/s]
(EngineCore_DP0 pid=895648) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.61it/s]
(EngineCore_DP0 pid=895648) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.44it/s]
(EngineCore_DP0 pid=895648) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.51it/s]
(EngineCore_DP0 pid=895648) 
(EngineCore_DP0 pid=895648) [2026-01-26 18:14:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=895648) [2026-01-26 18:14:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=895648) [2026-01-26 18:14:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=895648) [2026-01-26 18:14:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=895648) [2026-01-26 18:14:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=895648) [2026-01-26 18:14:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=895648) [2026-01-26 18:14:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=895648) [2026-01-26 18:14:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=895648) 2026-01-26 18:14:59,324 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=895648) 2026-01-26 18:14:59,361 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=895648) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  9.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.41it/s]
(EngineCore_DP0 pid=895648) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.06it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█         | 27/256 [00:00<00:00, 260.64it/s]
Adding requests:  24%|██▍       | 62/256 [00:00<00:00, 308.47it/s]
Adding requests:  36%|███▋      | 93/256 [00:00<00:00, 308.71it/s]
Adding requests:  48%|████▊     | 124/256 [00:00<00:00, 137.05it/s]
Adding requests:  61%|██████    | 156/256 [00:00<00:00, 172.85it/s]
Adding requests:  75%|███████▍  | 191/256 [00:00<00:00, 211.61it/s]
Adding requests:  88%|████████▊ | 226/256 [00:01<00:00, 243.48it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 228.11it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  21%|██        | 54/256 [00:00<00:00, 465.39it/s, est. speed input: 476585.10 toks/s, output: 465.40 toks/s]
Processed prompts:  39%|███▉      | 101/256 [00:01<00:01, 84.36it/s, est. speed input: 99449.37 toks/s, output: 97.12 toks/s]  
Processed prompts:  48%|████▊     | 124/256 [00:01<00:01, 69.47it/s, est. speed input: 83431.13 toks/s, output: 81.48 toks/s]
Processed prompts:  54%|█████▍    | 139/256 [00:01<00:01, 65.34it/s, est. speed input: 78906.56 toks/s, output: 77.06 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:02<00:01, 60.74it/s, est. speed input: 75147.98 toks/s, output: 73.39 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:02<00:01, 59.85it/s, est. speed input: 73839.26 toks/s, output: 72.11 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:02<00:01, 57.79it/s, est. speed input: 72301.19 toks/s, output: 70.61 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:02<00:01, 54.56it/s, est. speed input: 70550.41 toks/s, output: 68.90 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:02<00:01, 53.63it/s, est. speed input: 69663.11 toks/s, output: 68.03 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:02<00:01, 52.71it/s, est. speed input: 68828.89 toks/s, output: 67.22 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:02<00:01, 52.04it/s, est. speed input: 68090.00 toks/s, output: 66.49 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:03<00:01, 51.46it/s, est. speed input: 67403.64 toks/s, output: 65.82 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:03<00:01, 49.81it/s, est. speed input: 66538.18 toks/s, output: 64.98 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:03<00:00, 49.75it/s, est. speed input: 65954.43 toks/s, output: 64.41 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:03<00:00, 49.73it/s, est. speed input: 65414.72 toks/s, output: 63.88 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:03<00:00, 49.72it/s, est. speed input: 64914.96 toks/s, output: 63.39 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:03<00:00, 49.72it/s, est. speed input: 64447.79 toks/s, output: 62.94 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:03<00:00, 49.70it/s, est. speed input: 64008.41 toks/s, output: 62.51 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:03<00:00, 49.69it/s, est. speed input: 63597.33 toks/s, output: 62.11 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:03<00:00, 49.75it/s, est. speed input: 63219.72 toks/s, output: 61.74 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:04<00:00, 49.78it/s, est. speed input: 62862.65 toks/s, output: 61.39 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 49.78it/s, est. speed input: 62618.74 toks/s, output: 61.15 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 61.15it/s, est. speed input: 62618.74 toks/s, output: 61.15 toks/s]
[rank0]:[W126 18:15:06.099856060 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 18:15:08
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:15:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=896938) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=896938) WARNING 01-26 18:15:39 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=896938) WARNING 01-26 18:15:54 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 75.43 requests/s, 77314.40 total tokens/s, 75.43 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 18:15:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:15:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:15:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:15:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:15:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:15:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:15:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:15:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:15:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:15:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:15:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:15:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:15:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:15:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:15:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:15:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:15:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:15:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:15:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:15:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:15:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:15:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:15:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:15:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:15:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:15:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:15:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:15:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=896938) [2026-01-26 18:15:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=896938) [2026-01-26 18:15:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=896938) [2026-01-26 18:15:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=896938) [2026-01-26 18:15:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=896938) [2026-01-26 18:15:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=896938) [2026-01-26 18:15:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=896938) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=896938) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.16it/s]
(EngineCore_DP0 pid=896938) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.88it/s]
(EngineCore_DP0 pid=896938) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.56it/s]
(EngineCore_DP0 pid=896938) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.39it/s]
(EngineCore_DP0 pid=896938) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.44it/s]
(EngineCore_DP0 pid=896938) 
(EngineCore_DP0 pid=896938) [2026-01-26 18:15:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=896938) [2026-01-26 18:15:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=896938) [2026-01-26 18:15:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=896938) [2026-01-26 18:15:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=896938) [2026-01-26 18:15:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=896938) [2026-01-26 18:15:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=896938) [2026-01-26 18:15:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=896938) [2026-01-26 18:15:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=896938) 2026-01-26 18:15:54,429 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=896938) 2026-01-26 18:15:54,467 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=896938) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  9.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 10.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.75it/s]
(EngineCore_DP0 pid=896938) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  8.31it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  9.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  9.56it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 28/512 [00:00<00:01, 274.54it/s]
Adding requests:  12%|█▏        | 63/512 [00:00<00:01, 315.25it/s]
Adding requests:  19%|█▊        | 95/512 [00:00<00:01, 312.85it/s]
Adding requests:  25%|██▌       | 129/512 [00:00<00:01, 319.40it/s]
Adding requests:  32%|███▏      | 164/512 [00:00<00:01, 328.42it/s]
Adding requests:  39%|███▉      | 200/512 [00:00<00:00, 338.40it/s]
Adding requests:  46%|████▌     | 235/512 [00:00<00:00, 341.52it/s]
Adding requests:  53%|█████▎    | 270/512 [00:00<00:00, 339.78it/s]
Adding requests:  60%|█████▉    | 306/512 [00:00<00:00, 344.26it/s]
Adding requests:  67%|██████▋   | 343/512 [00:01<00:00, 351.53it/s]
Adding requests:  74%|███████▍  | 379/512 [00:01<00:00, 351.54it/s]
Adding requests:  81%|████████▏ | 417/512 [00:01<00:00, 357.92it/s]
Adding requests:  88%|████████▊ | 453/512 [00:01<00:00, 352.91it/s]
Adding requests:  96%|█████████▌| 492/512 [00:01<00:00, 362.00it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 344.26it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:00<00:00, 953.14it/s, est. speed input: 976078.34 toks/s, output: 953.16 toks/s]
Processed prompts:  41%|████      | 210/512 [00:01<00:02, 131.00it/s, est. speed input: 156067.96 toks/s, output: 152.41 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:01<00:02, 110.25it/s, est. speed input: 133128.85 toks/s, output: 130.01 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:02<00:02, 101.55it/s, est. speed input: 124450.63 toks/s, output: 121.53 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:02<00:02, 96.27it/s, est. speed input: 119731.86 toks/s, output: 116.93 toks/s] 
Processed prompts:  62%|██████▏   | 318/512 [00:02<00:02, 93.09it/s, est. speed input: 116926.62 toks/s, output: 114.19 toks/s]
Processed prompts:  65%|██████▍   | 331/512 [00:02<00:01, 91.33it/s, est. speed input: 115211.88 toks/s, output: 112.51 toks/s]
Processed prompts:  67%|██████▋   | 343/512 [00:03<00:01, 88.20it/s, est. speed input: 113274.16 toks/s, output: 110.62 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:03<00:01, 84.14it/s, est. speed input: 111245.71 toks/s, output: 108.64 toks/s]
Processed prompts:  71%|███████   | 364/512 [00:03<00:01, 85.98it/s, est. speed input: 110799.74 toks/s, output: 108.20 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:03<00:01, 79.93it/s, est. speed input: 108757.33 toks/s, output: 106.21 toks/s]
Processed prompts:  75%|███████▍  | 383/512 [00:03<00:01, 81.07it/s, est. speed input: 108141.12 toks/s, output: 105.61 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:03<00:01, 82.20it/s, est. speed input: 107580.89 toks/s, output: 105.06 toks/s]
Processed prompts:  78%|███████▊  | 401/512 [00:03<00:01, 83.04it/s, est. speed input: 107036.10 toks/s, output: 104.53 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:03<00:01, 74.15it/s, est. speed input: 105123.24 toks/s, output: 102.66 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:04<00:01, 74.72it/s, est. speed input: 104439.19 toks/s, output: 101.99 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:04<00:01, 75.11it/s, est. speed input: 103779.43 toks/s, output: 101.35 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:04<00:00, 76.77it/s, est. speed input: 103021.29 toks/s, output: 100.61 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:04<00:00, 76.64it/s, est. speed input: 102434.49 toks/s, output: 100.03 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:04<00:00, 76.52it/s, est. speed input: 101873.01 toks/s, output: 99.49 toks/s] 
Processed prompts:  90%|█████████ | 462/512 [00:04<00:00, 76.23it/s, est. speed input: 101314.86 toks/s, output: 98.94 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:04<00:00, 76.26it/s, est. speed input: 100806.77 toks/s, output: 98.44 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:04<00:00, 76.21it/s, est. speed input: 100314.06 toks/s, output: 97.96 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:04<00:00, 76.00it/s, est. speed input: 99824.97 toks/s, output: 97.49 toks/s] 
Processed prompts:  96%|█████████▋| 494/512 [00:05<00:00, 76.02it/s, est. speed input: 99371.29 toks/s, output: 97.04 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:05<00:00, 76.15it/s, est. speed input: 98947.48 toks/s, output: 96.63 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:05<00:00, 76.40it/s, est. speed input: 98553.15 toks/s, output: 96.24 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 76.40it/s, est. speed input: 98936.22 toks/s, output: 96.62 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 96.62it/s, est. speed input: 98936.22 toks/s, output: 96.62 toks/s]
[rank0]:[W126 18:16:03.152686632 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 18:16:05
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:16:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=898315) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=898315) WARNING 01-26 18:16:39 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=898315) WARNING 01-26 18:16:55 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 78.08 requests/s, 80029.37 total tokens/s, 78.08 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 18:16:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:16:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:16:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:16:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:16:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:16:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:16:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:16:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:16:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:16:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:16:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:16:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:16:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:16:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:16:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:16:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:16:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:16:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:16:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:16:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:16:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:16:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:16:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:16:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:16:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:16:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:16:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:16:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=898315) [2026-01-26 18:16:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=898315) [2026-01-26 18:16:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=898315) [2026-01-26 18:16:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=898315) [2026-01-26 18:16:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=898315) [2026-01-26 18:16:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=898315) [2026-01-26 18:16:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=898315) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=898315) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.17it/s]
(EngineCore_DP0 pid=898315) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.92it/s]
(EngineCore_DP0 pid=898315) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.56it/s]
(EngineCore_DP0 pid=898315) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.42it/s]
(EngineCore_DP0 pid=898315) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.47it/s]
(EngineCore_DP0 pid=898315) 
(EngineCore_DP0 pid=898315) [2026-01-26 18:16:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=898315) [2026-01-26 18:16:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=898315) [2026-01-26 18:16:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=898315) [2026-01-26 18:16:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=898315) [2026-01-26 18:16:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=898315) [2026-01-26 18:16:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=898315) [2026-01-26 18:16:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=898315) [2026-01-26 18:16:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=898315) 2026-01-26 18:16:55,500 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=898315) 2026-01-26 18:16:55,539 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=898315) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 10.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 10.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 10.12it/s]
(EngineCore_DP0 pid=898315) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  9.55it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 10.45it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 10.58it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 274.45it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 316.83it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 314.74it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 320.30it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 328.88it/s]
Adding requests:  19%|█▉        | 199/1024 [00:00<00:02, 338.51it/s]
Adding requests:  23%|██▎       | 235/1024 [00:00<00:02, 344.02it/s]
Adding requests:  26%|██▋       | 270/1024 [00:00<00:02, 342.66it/s]
Adding requests:  30%|██▉       | 306/1024 [00:00<00:02, 347.39it/s]
Adding requests:  33%|███▎      | 343/1024 [00:01<00:01, 353.50it/s]
Adding requests:  37%|███▋      | 380/1024 [00:01<00:01, 356.98it/s]
Adding requests:  41%|████      | 419/1024 [00:01<00:01, 364.93it/s]
Adding requests:  45%|████▍     | 456/1024 [00:01<00:01, 357.65it/s]
Adding requests:  48%|████▊     | 496/1024 [00:01<00:01, 368.12it/s]
Adding requests:  52%|█████▏    | 535/1024 [00:01<00:01, 372.59it/s]
Adding requests:  56%|█████▌    | 573/1024 [00:01<00:01, 370.24it/s]
Adding requests:  60%|█████▉    | 611/1024 [00:01<00:01, 357.72it/s]
Adding requests:  63%|██████▎   | 647/1024 [00:01<00:01, 353.74it/s]
Adding requests:  67%|██████▋   | 683/1024 [00:01<00:00, 355.54it/s]
Adding requests:  70%|███████   | 719/1024 [00:02<00:00, 355.60it/s]
Adding requests:  74%|███████▎  | 755/1024 [00:02<00:00, 351.69it/s]
Adding requests:  77%|███████▋  | 791/1024 [00:02<00:00, 353.14it/s]
Adding requests:  81%|████████  | 828/1024 [00:02<00:00, 354.68it/s]
Adding requests:  84%|████████▍ | 865/1024 [00:02<00:00, 356.97it/s]
Adding requests:  88%|████████▊ | 902/1024 [00:02<00:00, 359.52it/s]
Adding requests:  92%|█████████▏| 938/1024 [00:02<00:00, 352.46it/s]
Adding requests:  95%|█████████▌| 974/1024 [00:02<00:00, 348.07it/s]
Adding requests:  99%|█████████▊| 1009/1024 [00:02<00:00, 338.05it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 348.02it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:00<00:00, 2222.46it/s, est. speed input: 2275959.55 toks/s, output: 2222.50 toks/s]
Processed prompts:  44%|████▍     | 449/1024 [00:02<00:04, 135.60it/s, est. speed input: 161793.86 toks/s, output: 158.00 toks/s]   
Processed prompts:  53%|█████▎    | 546/1024 [00:04<00:04, 110.86it/s, est. speed input: 134530.65 toks/s, output: 131.38 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:04<00:04, 103.20it/s, est. speed input: 126605.76 toks/s, output: 123.64 toks/s]
Processed prompts:  62%|██████▏   | 639/1024 [00:05<00:03, 101.30it/s, est. speed input: 124074.89 toks/s, output: 121.17 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:05<00:03, 94.59it/s, est. speed input: 119982.63 toks/s, output: 117.17 toks/s] 
Processed prompts:  67%|██████▋   | 686/1024 [00:05<00:03, 95.00it/s, est. speed input: 119305.65 toks/s, output: 116.51 toks/s]
Processed prompts:  69%|██████▊   | 703/1024 [00:06<00:03, 93.46it/s, est. speed input: 118209.98 toks/s, output: 115.44 toks/s]
Processed prompts:  70%|███████   | 718/1024 [00:06<00:03, 90.05it/s, est. speed input: 116821.82 toks/s, output: 114.08 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:06<00:03, 83.86it/s, est. speed input: 115035.31 toks/s, output: 112.34 toks/s]
Processed prompts:  72%|███████▏  | 741/1024 [00:06<00:03, 86.95it/s, est. speed input: 114986.51 toks/s, output: 112.29 toks/s]
Processed prompts:  73%|███████▎  | 752/1024 [00:06<00:03, 90.04it/s, est. speed input: 114912.71 toks/s, output: 112.22 toks/s]
Processed prompts:  75%|███████▍  | 763/1024 [00:06<00:03, 79.18it/s, est. speed input: 113144.31 toks/s, output: 110.49 toks/s]
Processed prompts:  75%|███████▌  | 772/1024 [00:07<00:03, 80.74it/s, est. speed input: 112808.33 toks/s, output: 110.16 toks/s]
Processed prompts:  76%|███████▋  | 781/1024 [00:07<00:02, 82.35it/s, est. speed input: 112496.10 toks/s, output: 109.86 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:07<00:03, 76.40it/s, est. speed input: 111272.47 toks/s, output: 108.66 toks/s]
Processed prompts:  78%|███████▊  | 803/1024 [00:07<00:02, 78.78it/s, est. speed input: 110976.31 toks/s, output: 108.38 toks/s]
Processed prompts:  79%|███████▉  | 812/1024 [00:07<00:02, 81.09it/s, est. speed input: 110707.82 toks/s, output: 108.11 toks/s]
Processed prompts:  80%|████████  | 821/1024 [00:07<00:02, 83.25it/s, est. speed input: 110462.91 toks/s, output: 107.87 toks/s]
Processed prompts:  81%|████████  | 830/1024 [00:07<00:02, 84.51it/s, est. speed input: 110194.21 toks/s, output: 107.61 toks/s]
Processed prompts:  82%|████████▏ | 839/1024 [00:07<00:02, 85.34it/s, est. speed input: 109924.01 toks/s, output: 107.35 toks/s]
Processed prompts:  83%|████████▎ | 848/1024 [00:07<00:02, 86.48it/s, est. speed input: 109691.37 toks/s, output: 107.12 toks/s]
Processed prompts:  84%|████████▎ | 857/1024 [00:08<00:01, 86.07it/s, est. speed input: 109393.83 toks/s, output: 106.83 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:08<00:02, 67.44it/s, est. speed input: 107813.17 toks/s, output: 105.29 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:08<00:02, 70.12it/s, est. speed input: 107477.71 toks/s, output: 104.96 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:08<00:01, 72.26it/s, est. speed input: 107148.58 toks/s, output: 104.64 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:08<00:01, 72.75it/s, est. speed input: 106751.92 toks/s, output: 104.25 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:08<00:01, 73.94it/s, est. speed input: 106416.96 toks/s, output: 103.92 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:08<00:01, 75.18it/s, est. speed input: 106111.23 toks/s, output: 103.62 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:08<00:01, 76.06it/s, est. speed input: 105810.53 toks/s, output: 103.33 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:08<00:01, 76.83it/s, est. speed input: 105524.55 toks/s, output: 103.05 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:09<00:01, 76.74it/s, est. speed input: 105210.58 toks/s, output: 102.74 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:09<00:01, 76.77it/s, est. speed input: 104909.02 toks/s, output: 102.45 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:09<00:01, 77.33it/s, est. speed input: 104641.60 toks/s, output: 102.19 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:09<00:00, 78.30it/s, est. speed input: 104146.33 toks/s, output: 101.71 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:09<00:00, 78.40it/s, est. speed input: 103895.83 toks/s, output: 101.46 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:09<00:00, 78.11it/s, est. speed input: 103631.16 toks/s, output: 101.20 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:09<00:00, 78.10it/s, est. speed input: 103382.63 toks/s, output: 100.96 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:09<00:00, 78.11it/s, est. speed input: 103140.14 toks/s, output: 100.72 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:09<00:00, 78.19it/s, est. speed input: 102905.88 toks/s, output: 100.49 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:10<00:00, 78.28it/s, est. speed input: 102678.08 toks/s, output: 100.27 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:10<00:00, 78.28it/s, est. speed input: 103095.66 toks/s, output: 100.68 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:10<00:00, 100.68it/s, est. speed input: 103095.66 toks/s, output: 100.68 toks/s]
[rank0]:[W126 18:17:11.583818439 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 18:17:13
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:17:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=899858) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=899858) WARNING 01-26 18:17:53 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=899858) WARNING 01-26 18:18:11 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 83.64 requests/s, 85734.93 total tokens/s, 83.64 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 18:17:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:17:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:17:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:17:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:17:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:17:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:17:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:17:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:17:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:17:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:17:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:17:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:17:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:17:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:17:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:17:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:17:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:17:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:17:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:17:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:17:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:17:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:17:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:17:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:17:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:17:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:17:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:17:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=899858) [2026-01-26 18:17:40] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=899858) [2026-01-26 18:17:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=899858) [2026-01-26 18:17:40] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=899858) [2026-01-26 18:17:40] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=899858) [2026-01-26 18:17:40] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=899858) [2026-01-26 18:17:40] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=899858) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=899858) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.28it/s]
(EngineCore_DP0 pid=899858) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:00,  2.04it/s]
(EngineCore_DP0 pid=899858) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.61it/s]
(EngineCore_DP0 pid=899858) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.44it/s]
(EngineCore_DP0 pid=899858) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.51it/s]
(EngineCore_DP0 pid=899858) 
(EngineCore_DP0 pid=899858) [2026-01-26 18:17:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=899858) [2026-01-26 18:17:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=899858) [2026-01-26 18:17:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=899858) [2026-01-26 18:17:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=899858) [2026-01-26 18:17:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=899858) [2026-01-26 18:17:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=899858) [2026-01-26 18:17:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=899858) [2026-01-26 18:17:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=899858) [rank0]:W0126 18:18:01.220000 899858 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=899858) [rank0]:W0126 18:18:01.289000 899858 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=899858) [rank0]:W0126 18:18:02.072000 899858 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=899858) [rank0]:W0126 18:18:02.179000 899858 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=899858) 2026-01-26 18:18:11,283 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=899858) 2026-01-26 18:18:11,333 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=899858) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  9.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  7.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  9.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.16it/s]
(EngineCore_DP0 pid=899858) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 10.86it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 11.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 11.14it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 275.18it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 314.74it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 311.88it/s]
Adding requests:   6%|▌         | 127/2048 [00:00<00:06, 312.89it/s]
Adding requests:   8%|▊         | 161/2048 [00:00<00:05, 322.17it/s]
Adding requests:  10%|▉         | 197/2048 [00:00<00:05, 334.13it/s]
Adding requests:  11%|█▏        | 233/2048 [00:00<00:05, 339.67it/s]
Adding requests:  13%|█▎        | 268/2048 [00:00<00:05, 339.41it/s]
Adding requests:  15%|█▍        | 304/2048 [00:00<00:05, 343.60it/s]
Adding requests:  17%|█▋        | 342/2048 [00:01<00:04, 351.51it/s]
Adding requests:  18%|█▊        | 378/2048 [00:01<00:04, 352.95it/s]
Adding requests:  20%|██        | 416/2048 [00:01<00:04, 360.34it/s]
Adding requests:  22%|██▏       | 453/2048 [00:01<00:04, 356.57it/s]
Adding requests:  24%|██▍       | 492/2048 [00:01<00:04, 365.98it/s]
Adding requests:  26%|██▌       | 531/2048 [00:01<00:04, 373.08it/s]
Adding requests:  28%|██▊       | 569/2048 [00:01<00:04, 369.20it/s]
Adding requests:  30%|██▉       | 606/2048 [00:01<00:04, 356.02it/s]
Adding requests:  31%|███▏      | 642/2048 [00:01<00:04, 349.91it/s]
Adding requests:  33%|███▎      | 678/2048 [00:01<00:03, 347.57it/s]
Adding requests:  35%|███▍      | 715/2048 [00:02<00:03, 353.05it/s]
Adding requests:  37%|███▋      | 751/2048 [00:02<00:03, 346.06it/s]
Adding requests:  38%|███▊      | 787/2048 [00:02<00:03, 349.69it/s]
Adding requests:  40%|████      | 823/2048 [00:02<00:03, 349.51it/s]
Adding requests:  42%|████▏     | 861/2048 [00:02<00:03, 355.74it/s]
Adding requests:  44%|████▍     | 898/2048 [00:02<00:03, 358.37it/s]
Adding requests:  46%|████▌     | 934/2048 [00:02<00:03, 350.16it/s]
Adding requests:  47%|████▋     | 970/2048 [00:02<00:03, 352.95it/s]
Adding requests:  49%|████▉     | 1006/2048 [00:02<00:02, 348.44it/s]
Adding requests:  51%|█████     | 1042/2048 [00:02<00:02, 349.69it/s]
Adding requests:  53%|█████▎    | 1077/2048 [00:03<00:02, 335.47it/s]
Adding requests:  54%|█████▍    | 1111/2048 [00:03<00:02, 336.51it/s]
Adding requests:  56%|█████▌    | 1148/2048 [00:03<00:02, 343.94it/s]
Adding requests:  58%|█████▊    | 1183/2048 [00:03<00:02, 344.90it/s]
Adding requests:  60%|█████▉    | 1220/2048 [00:03<00:02, 351.18it/s]
Adding requests:  61%|██████▏   | 1256/2048 [00:03<00:02, 350.18it/s]
Adding requests:  63%|██████▎   | 1292/2048 [00:03<00:02, 343.49it/s]
Adding requests:  65%|██████▍   | 1329/2048 [00:03<00:02, 348.09it/s]
Adding requests:  67%|██████▋   | 1366/2048 [00:03<00:01, 353.33it/s]
Adding requests:  68%|██████▊   | 1402/2048 [00:04<00:01, 349.78it/s]
Adding requests:  70%|███████   | 1438/2048 [00:04<00:01, 351.45it/s]
Adding requests:  72%|███████▏  | 1474/2048 [00:04<00:01, 349.56it/s]
Adding requests:  74%|███████▍  | 1512/2048 [00:04<00:01, 355.95it/s]
Adding requests:  76%|███████▌  | 1548/2048 [00:04<00:01, 352.40it/s]
Adding requests:  77%|███████▋  | 1584/2048 [00:04<00:01, 345.16it/s]
Adding requests:  79%|███████▉  | 1619/2048 [00:04<00:01, 340.15it/s]
Adding requests:  81%|████████  | 1654/2048 [00:04<00:01, 336.83it/s]
Adding requests:  82%|████████▏ | 1688/2048 [00:04<00:01, 336.51it/s]
Adding requests:  84%|████████▍ | 1724/2048 [00:04<00:00, 343.00it/s]
Adding requests:  86%|████████▌ | 1760/2048 [00:05<00:00, 346.13it/s]
Adding requests:  88%|████████▊ | 1795/2048 [00:05<00:00, 340.05it/s]
Adding requests:  89%|████████▉ | 1831/2048 [00:05<00:00, 345.10it/s]
Adding requests:  91%|█████████ | 1866/2048 [00:05<00:00, 345.56it/s]
Adding requests:  93%|█████████▎| 1901/2048 [00:05<00:00, 346.16it/s]
Adding requests:  95%|█████████▍| 1939/2048 [00:05<00:00, 354.70it/s]
Adding requests:  96%|█████████▋| 1975/2048 [00:05<00:00, 347.73it/s]
Adding requests:  98%|█████████▊| 2010/2048 [00:05<00:00, 344.10it/s]
Adding requests: 100%|█████████▉| 2045/2048 [00:05<00:00, 342.21it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 346.50it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:00<00:00, 4410.51it/s, est. speed input: 4516687.95 toks/s, output: 4410.60 toks/s]
Processed prompts:  45%|████▌     | 924/2048 [00:05<00:07, 149.12it/s, est. speed input: 179902.18 toks/s, output: 175.69 toks/s]   
Processed prompts:  54%|█████▍    | 1111/2048 [00:07<00:07, 123.29it/s, est. speed input: 150833.14 toks/s, output: 147.30 toks/s]
Processed prompts:  59%|█████▉    | 1217/2048 [00:08<00:07, 116.90it/s, est. speed input: 143622.94 toks/s, output: 140.26 toks/s]
Processed prompts:  63%|██████▎   | 1285/2048 [00:09<00:07, 107.43it/s, est. speed input: 136550.29 toks/s, output: 133.35 toks/s]
Processed prompts:  65%|██████▍   | 1331/2048 [00:10<00:06, 103.44it/s, est. speed input: 133531.84 toks/s, output: 130.40 toks/s]
Processed prompts:  67%|██████▋   | 1364/2048 [00:10<00:06, 101.21it/s, est. speed input: 131866.19 toks/s, output: 128.78 toks/s]
Processed prompts:  68%|██████▊   | 1389/2048 [00:10<00:06, 103.51it/s, est. speed input: 131868.14 toks/s, output: 128.78 toks/s]
Processed prompts:  69%|██████▉   | 1411/2048 [00:11<00:06, 94.78it/s, est. speed input: 129361.46 toks/s, output: 126.33 toks/s] 
Processed prompts:  70%|██████▉   | 1428/2048 [00:11<00:06, 94.12it/s, est. speed input: 128723.50 toks/s, output: 125.71 toks/s]
Processed prompts:  70%|███████   | 1443/2048 [00:11<00:06, 91.84it/s, est. speed input: 127921.84 toks/s, output: 124.92 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:11<00:06, 90.24it/s, est. speed input: 127241.03 toks/s, output: 124.26 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:11<00:06, 88.95it/s, est. speed input: 126575.95 toks/s, output: 123.61 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:12<00:06, 87.75it/s, est. speed input: 125926.79 toks/s, output: 122.98 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:12<00:06, 86.66it/s, est. speed input: 125291.67 toks/s, output: 122.35 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:12<00:06, 86.77it/s, est. speed input: 124760.73 toks/s, output: 121.84 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:12<00:05, 86.09it/s, est. speed input: 124183.45 toks/s, output: 121.27 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:12<00:05, 86.80it/s, est. speed input: 123716.81 toks/s, output: 120.82 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:13<00:05, 85.49it/s, est. speed input: 123129.85 toks/s, output: 120.24 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:13<00:05, 85.11it/s, est. speed input: 122599.60 toks/s, output: 119.73 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:13<00:05, 84.73it/s, est. speed input: 122077.54 toks/s, output: 119.22 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:13<00:05, 85.38it/s, est. speed input: 121631.47 toks/s, output: 118.78 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:13<00:04, 85.06it/s, est. speed input: 121146.76 toks/s, output: 118.31 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:14<00:04, 84.65it/s, est. speed input: 120662.77 toks/s, output: 117.83 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:14<00:04, 84.51it/s, est. speed input: 120201.30 toks/s, output: 117.38 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:14<00:04, 84.33it/s, est. speed input: 119746.55 toks/s, output: 116.94 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:14<00:04, 83.87it/s, est. speed input: 119283.37 toks/s, output: 116.49 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:14<00:03, 83.55it/s, est. speed input: 118832.28 toks/s, output: 116.05 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:14<00:03, 84.90it/s, est. speed input: 118486.77 toks/s, output: 115.71 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:15<00:03, 85.62it/s, est. speed input: 118135.47 toks/s, output: 115.37 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:15<00:03, 85.00it/s, est. speed input: 117729.03 toks/s, output: 114.97 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:15<00:03, 84.84it/s, est. speed input: 117347.49 toks/s, output: 114.60 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:15<00:03, 84.40it/s, est. speed input: 116957.17 toks/s, output: 114.22 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:15<00:02, 84.20it/s, est. speed input: 116582.08 toks/s, output: 113.85 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:16<00:02, 84.13it/s, est. speed input: 116219.71 toks/s, output: 113.50 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:16<00:02, 84.00it/s, est. speed input: 115861.23 toks/s, output: 113.15 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:16<00:02, 83.95it/s, est. speed input: 115513.82 toks/s, output: 112.81 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:16<00:02, 83.80it/s, est. speed input: 115167.57 toks/s, output: 112.47 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:16<00:01, 85.03it/s, est. speed input: 114897.83 toks/s, output: 112.20 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:17<00:01, 84.73it/s, est. speed input: 114575.55 toks/s, output: 111.89 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:17<00:01, 84.72it/s, est. speed input: 114270.09 toks/s, output: 111.59 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:17<00:01, 84.55it/s, est. speed input: 113963.51 toks/s, output: 111.29 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:17<00:01, 84.29it/s, est. speed input: 113656.40 toks/s, output: 110.99 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:17<00:00, 84.00it/s, est. speed input: 113350.82 toks/s, output: 110.69 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:17<00:00, 85.24it/s, est. speed input: 113119.49 toks/s, output: 110.47 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:18<00:00, 84.99it/s, est. speed input: 112841.07 toks/s, output: 110.20 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:18<00:00, 84.69it/s, est. speed input: 112562.55 toks/s, output: 109.92 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:18<00:00, 81.40it/s, est. speed input: 112145.34 toks/s, output: 109.52 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:18<00:00, 81.40it/s, est. speed input: 112915.99 toks/s, output: 110.27 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:18<00:00, 110.27it/s, est. speed input: 112915.99 toks/s, output: 110.27 toks/s]
[rank0]:[W126 18:18:39.363152914 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 18:18:41
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:19:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=901769) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=901769) WARNING 01-26 18:19:32 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=901769) WARNING 01-26 18:19:51 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 86.06 requests/s, 88210.00 total tokens/s, 86.06 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 18:19:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:19:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:19:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:19:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:19:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:19:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:19:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:19:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:19:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:19:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:19:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:19:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:19:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:19:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:19:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:19:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:19:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:19:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:19:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:19:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:19:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:19:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:19:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:19:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:19:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:19:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:19:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:19:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=901769) [2026-01-26 18:19:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=901769) [2026-01-26 18:19:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=901769) [2026-01-26 18:19:19] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=901769) [2026-01-26 18:19:19] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=901769) [2026-01-26 18:19:19] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=901769) [2026-01-26 18:19:19] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=901769) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=901769) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.30it/s]
(EngineCore_DP0 pid=901769) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:00,  2.06it/s]
(EngineCore_DP0 pid=901769) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.61it/s]
(EngineCore_DP0 pid=901769) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.44it/s]
(EngineCore_DP0 pid=901769) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.51it/s]
(EngineCore_DP0 pid=901769) 
(EngineCore_DP0 pid=901769) [2026-01-26 18:19:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=901769) [2026-01-26 18:19:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=901769) [2026-01-26 18:19:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=901769) [2026-01-26 18:19:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=901769) [2026-01-26 18:19:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=901769) [2026-01-26 18:19:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=901769) [2026-01-26 18:19:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=901769) [2026-01-26 18:19:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=901769) [rank0]:W0126 18:19:40.634000 901769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=901769) [rank0]:W0126 18:19:40.704000 901769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=901769) [rank0]:W0126 18:19:41.601000 901769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=901769) [rank0]:W0126 18:19:41.709000 901769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=901769) 2026-01-26 18:19:51,398 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=901769) 2026-01-26 18:19:51,481 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=901769) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  9.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  9.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  9.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 10.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 10.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.70it/s]
(EngineCore_DP0 pid=901769) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 10.44it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00, 10.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 10.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 10.75it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 275.82it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 317.88it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 315.25it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:12, 320.67it/s]
Adding requests:   4%|▍         | 164/4096 [00:00<00:11, 331.09it/s]
Adding requests:   5%|▍         | 201/4096 [00:00<00:11, 341.03it/s]
Adding requests:   6%|▌         | 237/4096 [00:00<00:11, 345.41it/s]
Adding requests:   7%|▋         | 272/4096 [00:00<00:11, 343.23it/s]
Adding requests:   8%|▊         | 309/4096 [00:00<00:10, 348.96it/s]
Adding requests:   8%|▊         | 346/4096 [00:01<00:10, 355.13it/s]
Adding requests:   9%|▉         | 383/4096 [00:01<00:10, 358.56it/s]
Adding requests:  10%|█         | 421/4096 [00:01<00:10, 363.66it/s]
Adding requests:  11%|█         | 458/4096 [00:01<00:10, 361.15it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:09, 369.56it/s]
Adding requests:  13%|█▎        | 537/4096 [00:01<00:09, 373.73it/s]
Adding requests:  14%|█▍        | 575/4096 [00:01<00:09, 360.44it/s]
Adding requests:  15%|█▍        | 612/4096 [00:01<00:09, 350.62it/s]
Adding requests:  16%|█▌        | 648/4096 [00:01<00:09, 348.78it/s]
Adding requests:  17%|█▋        | 685/4096 [00:01<00:09, 352.23it/s]
Adding requests:  18%|█▊        | 721/4096 [00:02<00:09, 351.98it/s]
Adding requests:  18%|█▊        | 757/4096 [00:02<00:09, 351.81it/s]
Adding requests:  19%|█▉        | 793/4096 [00:02<00:09, 349.97it/s]
Adding requests:  20%|██        | 831/4096 [00:02<00:09, 357.71it/s]
Adding requests:  21%|██        | 868/4096 [00:02<00:09, 358.27it/s]
Adding requests:  22%|██▏       | 904/4096 [00:02<00:08, 358.63it/s]
Adding requests:  23%|██▎       | 940/4096 [00:02<00:08, 352.19it/s]
Adding requests:  24%|██▍       | 976/4096 [00:02<00:08, 353.68it/s]
Adding requests:  25%|██▍       | 1012/4096 [00:02<00:08, 349.28it/s]
Adding requests:  26%|██▌       | 1048/4096 [00:02<00:08, 350.26it/s]
Adding requests:  26%|██▋       | 1084/4096 [00:03<00:08, 349.90it/s]
Adding requests:  27%|██▋       | 1120/4096 [00:03<00:08, 348.40it/s]
Adding requests:  28%|██▊       | 1156/4096 [00:03<00:08, 350.80it/s]
Adding requests:  29%|██▉       | 1192/4096 [00:03<00:08, 347.23it/s]
Adding requests:  30%|███       | 1230/4096 [00:03<00:08, 355.29it/s]
Adding requests:  31%|███       | 1266/4096 [00:03<00:07, 353.87it/s]
Adding requests:  32%|███▏      | 1302/4096 [00:03<00:07, 352.40it/s]
Adding requests:  33%|███▎      | 1338/4096 [00:03<00:07, 351.40it/s]
Adding requests:  34%|███▎      | 1376/4096 [00:03<00:07, 356.95it/s]
Adding requests:  34%|███▍      | 1412/4096 [00:04<00:07, 354.14it/s]
Adding requests:  35%|███▌      | 1448/4096 [00:04<00:07, 352.59it/s]
Adding requests:  36%|███▋      | 1486/4096 [00:04<00:07, 359.37it/s]
Adding requests:  37%|███▋      | 1523/4096 [00:04<00:07, 360.80it/s]
Adding requests:  38%|███▊      | 1560/4096 [00:04<00:07, 355.50it/s]
Adding requests:  39%|███▉      | 1596/4096 [00:04<00:07, 351.73it/s]
Adding requests:  40%|███▉      | 1632/4096 [00:04<00:07, 346.12it/s]
Adding requests:  41%|████      | 1667/4096 [00:04<00:07, 339.29it/s]
Adding requests:  42%|████▏     | 1704/4096 [00:04<00:06, 347.43it/s]
Adding requests:  42%|████▏     | 1740/4096 [00:04<00:06, 351.01it/s]
Adding requests:  43%|████▎     | 1778/4096 [00:05<00:06, 356.49it/s]
Adding requests:  44%|████▍     | 1814/4096 [00:05<00:06, 351.90it/s]
Adding requests:  45%|████▌     | 1850/4096 [00:05<00:06, 352.80it/s]
Adding requests:  46%|████▌     | 1887/4096 [00:05<00:06, 357.51it/s]
Adding requests:  47%|████▋     | 1924/4096 [00:05<00:06, 360.52it/s]
Adding requests:  48%|████▊     | 1961/4096 [00:05<00:05, 363.28it/s]
Adding requests:  49%|████▉     | 1998/4096 [00:05<00:05, 357.12it/s]
Adding requests:  50%|████▉     | 2034/4096 [00:05<00:05, 349.97it/s]
Adding requests:  51%|█████     | 2070/4096 [00:05<00:06, 336.44it/s]
Adding requests:  51%|█████▏    | 2107/4096 [00:06<00:05, 344.47it/s]
Adding requests:  52%|█████▏    | 2142/4096 [00:06<00:05, 345.30it/s]
Adding requests:  53%|█████▎    | 2177/4096 [00:06<00:05, 339.46it/s]
Adding requests:  54%|█████▍    | 2212/4096 [00:06<00:05, 339.49it/s]
Adding requests:  55%|█████▍    | 2248/4096 [00:06<00:05, 345.37it/s]
Adding requests:  56%|█████▌    | 2285/4096 [00:06<00:05, 351.03it/s]
Adding requests:  57%|█████▋    | 2322/4096 [00:06<00:04, 356.24it/s]
Adding requests:  58%|█████▊    | 2359/4096 [00:06<00:04, 358.65it/s]
Adding requests:  59%|█████▊    | 2397/4096 [00:06<00:04, 364.69it/s]
Adding requests:  59%|█████▉    | 2434/4096 [00:06<00:04, 364.05it/s]
Adding requests:  60%|██████    | 2471/4096 [00:07<00:04, 362.16it/s]
Adding requests:  61%|██████▏   | 2509/4096 [00:07<00:04, 364.68it/s]
Adding requests:  62%|██████▏   | 2548/4096 [00:07<00:04, 370.60it/s]
Adding requests:  63%|██████▎   | 2587/4096 [00:07<00:04, 374.10it/s]
Adding requests:  64%|██████▍   | 2625/4096 [00:07<00:04, 366.83it/s]
Adding requests:  65%|██████▍   | 2662/4096 [00:07<00:03, 359.88it/s]
Adding requests:  66%|██████▌   | 2699/4096 [00:07<00:03, 356.31it/s]
Adding requests:  67%|██████▋   | 2735/4096 [00:07<00:03, 354.78it/s]
Adding requests:  68%|██████▊   | 2774/4096 [00:07<00:03, 361.76it/s]
Adding requests:  69%|██████▊   | 2812/4096 [00:07<00:03, 364.75it/s]
Adding requests:  70%|██████▉   | 2850/4096 [00:08<00:03, 367.34it/s]
Adding requests:  70%|███████   | 2887/4096 [00:08<00:03, 363.07it/s]
Adding requests:  71%|███████▏  | 2924/4096 [00:08<00:03, 361.97it/s]
Adding requests:  72%|███████▏  | 2962/4096 [00:08<00:03, 367.20it/s]
Adding requests:  73%|███████▎  | 2999/4096 [00:08<00:03, 360.14it/s]
Adding requests:  74%|███████▍  | 3037/4096 [00:08<00:02, 363.75it/s]
Adding requests:  75%|███████▌  | 3075/4096 [00:08<00:02, 367.67it/s]
Adding requests:  76%|███████▌  | 3112/4096 [00:08<00:02, 366.62it/s]
Adding requests:  77%|███████▋  | 3150/4096 [00:08<00:02, 367.22it/s]
Adding requests:  78%|███████▊  | 3187/4096 [00:08<00:02, 363.24it/s]
Adding requests:  79%|███████▊  | 3224/4096 [00:09<00:02, 361.39it/s]
Adding requests:  80%|███████▉  | 3261/4096 [00:09<00:02, 361.82it/s]
Adding requests:  81%|████████  | 3298/4096 [00:09<00:02, 350.94it/s]
Adding requests:  81%|████████▏ | 3334/4096 [00:09<00:02, 350.58it/s]
Adding requests:  82%|████████▏ | 3370/4096 [00:09<00:02, 347.59it/s]
Adding requests:  83%|████████▎ | 3407/4096 [00:09<00:01, 351.40it/s]
Adding requests:  84%|████████▍ | 3445/4096 [00:09<00:01, 357.78it/s]
Adding requests:  85%|████████▍ | 3481/4096 [00:09<00:01, 356.93it/s]
Adding requests:  86%|████████▌ | 3518/4096 [00:09<00:01, 359.95it/s]
Adding requests:  87%|████████▋ | 3557/4096 [00:10<00:01, 368.18it/s]
Adding requests:  88%|████████▊ | 3594/4096 [00:10<00:01, 363.24it/s]
Adding requests:  89%|████████▊ | 3632/4096 [00:10<00:01, 366.33it/s]
Adding requests:  90%|████████▉ | 3669/4096 [00:10<00:01, 358.53it/s]
Adding requests:  90%|█████████ | 3705/4096 [00:10<00:01, 356.95it/s]
Adding requests:  91%|█████████▏| 3741/4096 [00:10<00:00, 356.53it/s]
Adding requests:  92%|█████████▏| 3777/4096 [00:10<00:00, 347.31it/s]
Adding requests:  93%|█████████▎| 3812/4096 [00:10<00:00, 337.21it/s]
Adding requests:  94%|█████████▍| 3848/4096 [00:10<00:00, 342.32it/s]
Adding requests:  95%|█████████▍| 3884/4096 [00:10<00:00, 346.36it/s]
Adding requests:  96%|█████████▌| 3919/4096 [00:11<00:00, 341.20it/s]
Adding requests:  97%|█████████▋| 3955/4096 [00:11<00:00, 345.99it/s]
Adding requests:  97%|█████████▋| 3990/4096 [00:11<00:00, 343.43it/s]
Adding requests:  98%|█████████▊| 4025/4096 [00:11<00:00, 345.16it/s]
Adding requests:  99%|█████████▉| 4060/4096 [00:11<00:00, 344.84it/s]
Adding requests: 100%|█████████▉| 4095/4096 [00:11<00:00, 344.40it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 353.67it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  24%|██▍       | 980/4096 [00:00<00:00, 3841.52it/s, est. speed input: 3933840.72 toks/s, output: 3841.56 toks/s]
Processed prompts:  33%|███▎      | 1365/4096 [00:04<00:11, 230.56it/s, est. speed input: 296030.98 toks/s, output: 289.09 toks/s]  
Processed prompts:  37%|███▋      | 1529/4096 [00:06<00:14, 179.01it/s, est. speed input: 238151.40 toks/s, output: 232.57 toks/s]
Processed prompts:  40%|███▉      | 1622/4096 [00:07<00:15, 156.78it/s, est. speed input: 216429.63 toks/s, output: 211.36 toks/s]
Processed prompts:  41%|████      | 1682/4096 [00:08<00:15, 157.22it/s, est. speed input: 214051.49 toks/s, output: 209.03 toks/s]
Processed prompts:  42%|████▏     | 1726/4096 [00:08<00:17, 135.41it/s, est. speed input: 201246.89 toks/s, output: 196.53 toks/s]
Processed prompts:  43%|████▎     | 1757/4096 [00:09<00:18, 128.33it/s, est. speed input: 196614.13 toks/s, output: 192.01 toks/s]
Processed prompts:  43%|████▎     | 1781/4096 [00:09<00:19, 117.66it/s, est. speed input: 191486.06 toks/s, output: 187.00 toks/s]
Processed prompts:  44%|████▍     | 1812/4096 [00:09<00:20, 111.04it/s, est. speed input: 187481.22 toks/s, output: 183.09 toks/s]
Processed prompts:  45%|████▌     | 1844/4096 [00:10<00:21, 105.17it/s, est. speed input: 183736.48 toks/s, output: 179.43 toks/s]
Processed prompts:  46%|████▌     | 1876/4096 [00:10<00:21, 101.59it/s, est. speed input: 180633.47 toks/s, output: 176.40 toks/s]
Processed prompts:  47%|████▋     | 1908/4096 [00:11<00:22, 97.78it/s, est. speed input: 177534.98 toks/s, output: 173.37 toks/s] 
Processed prompts:  47%|████▋     | 1940/4096 [00:11<00:22, 94.70it/s, est. speed input: 174618.15 toks/s, output: 170.53 toks/s]
Processed prompts:  48%|████▊     | 1972/4096 [00:11<00:22, 92.80it/s, est. speed input: 171981.66 toks/s, output: 167.95 toks/s]
Processed prompts:  49%|████▉     | 2004/4096 [00:12<00:22, 90.96it/s, est. speed input: 169420.66 toks/s, output: 165.45 toks/s]
Processed prompts:  50%|████▉     | 2036/4096 [00:12<00:23, 89.36it/s, est. speed input: 166963.46 toks/s, output: 163.05 toks/s]
Processed prompts:  50%|█████     | 2068/4096 [00:12<00:22, 88.91it/s, est. speed input: 164778.20 toks/s, output: 160.92 toks/s]
Processed prompts:  51%|█████▏    | 2100/4096 [00:13<00:22, 88.22it/s, est. speed input: 162647.83 toks/s, output: 158.84 toks/s]
Processed prompts:  52%|█████▏    | 2132/4096 [00:13<00:22, 87.67it/s, est. speed input: 160625.61 toks/s, output: 156.86 toks/s]
Processed prompts:  53%|█████▎    | 2164/4096 [00:13<00:22, 87.03it/s, est. speed input: 158668.42 toks/s, output: 154.95 toks/s]
Processed prompts:  54%|█████▎    | 2196/4096 [00:14<00:21, 87.12it/s, est. speed input: 156899.13 toks/s, output: 153.22 toks/s]
Processed prompts:  54%|█████▍    | 2228/4096 [00:14<00:21, 86.85it/s, est. speed input: 155166.80 toks/s, output: 151.53 toks/s]
Processed prompts:  55%|█████▌    | 2260/4096 [00:15<00:21, 86.39it/s, est. speed input: 153480.04 toks/s, output: 149.88 toks/s]
Processed prompts:  56%|█████▌    | 2292/4096 [00:15<00:20, 86.14it/s, est. speed input: 151885.61 toks/s, output: 148.33 toks/s]
Processed prompts:  57%|█████▋    | 2324/4096 [00:15<00:20, 86.10it/s, est. speed input: 150385.01 toks/s, output: 146.86 toks/s]
Processed prompts:  58%|█████▊    | 2356/4096 [00:16<00:20, 86.04it/s, est. speed input: 148949.71 toks/s, output: 145.46 toks/s]
Processed prompts:  58%|█████▊    | 2388/4096 [00:16<00:19, 86.16it/s, est. speed input: 147599.14 toks/s, output: 144.14 toks/s]
Processed prompts:  59%|█████▉    | 2420/4096 [00:16<00:19, 86.10it/s, est. speed input: 146289.82 toks/s, output: 142.86 toks/s]
Processed prompts:  60%|█████▉    | 2452/4096 [00:17<00:19, 86.13it/s, est. speed input: 145046.16 toks/s, output: 141.65 toks/s]
Processed prompts:  61%|██████    | 2484/4096 [00:17<00:18, 86.10it/s, est. speed input: 143848.24 toks/s, output: 140.48 toks/s]
Processed prompts:  61%|██████▏   | 2516/4096 [00:18<00:18, 86.65it/s, est. speed input: 142763.40 toks/s, output: 139.42 toks/s]
Processed prompts:  62%|██████▏   | 2548/4096 [00:18<00:17, 86.47it/s, est. speed input: 141660.75 toks/s, output: 138.34 toks/s]
Processed prompts:  63%|██████▎   | 2580/4096 [00:18<00:17, 87.26it/s, est. speed input: 140698.11 toks/s, output: 137.40 toks/s]
Processed prompts:  64%|██████▍   | 2612/4096 [00:19<00:17, 86.90it/s, est. speed input: 139677.25 toks/s, output: 136.40 toks/s]
Processed prompts:  65%|██████▍   | 2644/4096 [00:19<00:16, 86.66it/s, est. speed input: 138696.77 toks/s, output: 135.45 toks/s]
Processed prompts:  65%|██████▌   | 2676/4096 [00:19<00:16, 86.39it/s, est. speed input: 137742.77 toks/s, output: 134.51 toks/s]
Processed prompts:  66%|██████▌   | 2708/4096 [00:20<00:16, 86.26it/s, est. speed input: 136829.64 toks/s, output: 133.62 toks/s]
Processed prompts:  67%|██████▋   | 2740/4096 [00:20<00:15, 86.54it/s, est. speed input: 135984.35 toks/s, output: 132.80 toks/s]
Processed prompts:  68%|██████▊   | 2772/4096 [00:21<00:15, 86.44it/s, est. speed input: 135141.19 toks/s, output: 131.97 toks/s]
Processed prompts:  68%|██████▊   | 2804/4096 [00:21<00:15, 86.11it/s, est. speed input: 134303.71 toks/s, output: 131.16 toks/s]
Processed prompts:  69%|██████▉   | 2836/4096 [00:21<00:14, 86.16it/s, est. speed input: 133519.95 toks/s, output: 130.39 toks/s]
Processed prompts:  70%|███████   | 2868/4096 [00:22<00:14, 85.99it/s, est. speed input: 132744.67 toks/s, output: 129.63 toks/s]
Processed prompts:  71%|███████   | 2900/4096 [00:22<00:13, 87.61it/s, est. speed input: 132140.32 toks/s, output: 129.04 toks/s]
Processed prompts:  72%|███████▏  | 2932/4096 [00:22<00:13, 87.13it/s, est. speed input: 131423.18 toks/s, output: 128.34 toks/s]
Processed prompts:  72%|███████▏  | 2964/4096 [00:23<00:13, 86.76it/s, est. speed input: 130726.18 toks/s, output: 127.66 toks/s]
Processed prompts:  73%|███████▎  | 2996/4096 [00:23<00:12, 86.55it/s, est. speed input: 130054.86 toks/s, output: 127.01 toks/s]
Processed prompts:  74%|███████▍  | 3028/4096 [00:23<00:12, 86.26it/s, est. speed input: 129393.43 toks/s, output: 126.36 toks/s]
Processed prompts:  75%|███████▍  | 3060/4096 [00:24<00:12, 86.12it/s, est. speed input: 128756.71 toks/s, output: 125.74 toks/s]
Processed prompts:  75%|███████▌  | 3092/4096 [00:24<00:11, 86.09it/s, est. speed input: 128143.96 toks/s, output: 125.14 toks/s]
Processed prompts:  76%|███████▋  | 3124/4096 [00:25<00:11, 85.98it/s, est. speed input: 127543.14 toks/s, output: 124.55 toks/s]
Processed prompts:  77%|███████▋  | 3156/4096 [00:25<00:10, 85.99it/s, est. speed input: 126966.51 toks/s, output: 123.99 toks/s]
Processed prompts:  78%|███████▊  | 3188/4096 [00:25<00:10, 86.05it/s, est. speed input: 126410.30 toks/s, output: 123.45 toks/s]
Processed prompts:  79%|███████▊  | 3220/4096 [00:26<00:10, 85.91it/s, est. speed input: 125856.56 toks/s, output: 122.91 toks/s]
Processed prompts:  79%|███████▉  | 3252/4096 [00:26<00:09, 86.02it/s, est. speed input: 125333.14 toks/s, output: 122.40 toks/s]
Processed prompts:  80%|████████  | 3284/4096 [00:26<00:09, 85.95it/s, est. speed input: 124814.24 toks/s, output: 121.89 toks/s]
Processed prompts:  81%|████████  | 3316/4096 [00:27<00:09, 86.04it/s, est. speed input: 124318.66 toks/s, output: 121.40 toks/s]
Processed prompts:  82%|████████▏ | 3348/4096 [00:27<00:08, 85.99it/s, est. speed input: 123828.74 toks/s, output: 120.93 toks/s]
Processed prompts:  83%|████████▎ | 3380/4096 [00:28<00:08, 85.76it/s, est. speed input: 123339.87 toks/s, output: 120.45 toks/s]
Processed prompts:  83%|████████▎ | 3412/4096 [00:28<00:07, 85.63it/s, est. speed input: 122865.17 toks/s, output: 119.99 toks/s]
Processed prompts:  84%|████████▍ | 3444/4096 [00:28<00:07, 85.73it/s, est. speed input: 122415.29 toks/s, output: 119.55 toks/s]
Processed prompts:  85%|████████▍ | 3476/4096 [00:29<00:07, 85.81it/s, est. speed input: 121976.94 toks/s, output: 119.12 toks/s]
Processed prompts:  86%|████████▌ | 3508/4096 [00:29<00:06, 85.78it/s, est. speed input: 121544.93 toks/s, output: 118.70 toks/s]
Processed prompts:  86%|████████▋ | 3540/4096 [00:29<00:06, 86.33it/s, est. speed input: 121156.98 toks/s, output: 118.32 toks/s]
Processed prompts:  87%|████████▋ | 3572/4096 [00:30<00:06, 86.20it/s, est. speed input: 120748.52 toks/s, output: 117.92 toks/s]
Processed prompts:  88%|████████▊ | 3604/4096 [00:30<00:05, 86.16it/s, est. speed input: 120353.30 toks/s, output: 117.53 toks/s]
Processed prompts:  89%|████████▉ | 3636/4096 [00:31<00:05, 85.94it/s, est. speed input: 119956.74 toks/s, output: 117.15 toks/s]
Processed prompts:  90%|████████▉ | 3668/4096 [00:31<00:04, 86.54it/s, est. speed input: 119610.68 toks/s, output: 116.81 toks/s]
Processed prompts:  90%|█████████ | 3700/4096 [00:31<00:04, 86.31it/s, est. speed input: 119237.49 toks/s, output: 116.44 toks/s]
Processed prompts:  91%|█████████ | 3732/4096 [00:32<00:04, 86.08it/s, est. speed input: 118869.56 toks/s, output: 116.08 toks/s]
Processed prompts:  92%|█████████▏| 3764/4096 [00:32<00:03, 85.85it/s, est. speed input: 118506.28 toks/s, output: 115.73 toks/s]
Processed prompts:  93%|█████████▎| 3796/4096 [00:32<00:03, 85.85it/s, est. speed input: 118159.54 toks/s, output: 115.39 toks/s]
Processed prompts:  93%|█████████▎| 3828/4096 [00:33<00:03, 85.97it/s, est. speed input: 117826.51 toks/s, output: 115.06 toks/s]
Processed prompts:  94%|█████████▍| 3860/4096 [00:33<00:02, 85.77it/s, est. speed input: 117487.03 toks/s, output: 114.73 toks/s]
Processed prompts:  95%|█████████▌| 3892/4096 [00:34<00:02, 85.72it/s, est. speed input: 117159.43 toks/s, output: 114.41 toks/s]
Processed prompts:  96%|█████████▌| 3924/4096 [00:34<00:01, 86.84it/s, est. speed input: 116895.03 toks/s, output: 114.16 toks/s]
Processed prompts:  97%|█████████▋| 3956/4096 [00:34<00:01, 86.47it/s, est. speed input: 116580.68 toks/s, output: 113.85 toks/s]
Processed prompts:  97%|█████████▋| 3988/4096 [00:35<00:01, 86.92it/s, est. speed input: 116306.36 toks/s, output: 113.58 toks/s]
Processed prompts:  98%|█████████▊| 4020/4096 [00:35<00:00, 86.60it/s, est. speed input: 116008.36 toks/s, output: 113.29 toks/s]
Processed prompts:  99%|█████████▉| 4052/4096 [00:35<00:00, 86.44it/s, est. speed input: 115719.44 toks/s, output: 113.01 toks/s]
Processed prompts: 100%|█████████▉| 4084/4096 [00:36<00:00, 104.64it/s, est. speed input: 116129.17 toks/s, output: 113.41 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:36<00:00, 104.64it/s, est. speed input: 116469.74 toks/s, output: 113.74 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:36<00:00, 113.74it/s, est. speed input: 116469.74 toks/s, output: 113.74 toks/s]
[rank0]:[W126 18:20:43.234383534 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 18:20:45
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:21:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=904341) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=904341) WARNING 01-26 18:22:01 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=904341) WARNING 01-26 18:22:20 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 87.72 requests/s, 89910.74 total tokens/s, 87.72 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 18:21:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:21:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:21:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:21:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:21:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:21:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:21:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:21:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:21:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:21:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:21:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:21:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:21:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:21:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:21:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:21:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:21:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:21:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:21:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:21:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:21:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:21:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:21:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:21:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:21:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:21:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:21:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:21:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=904341) [2026-01-26 18:21:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=904341) [2026-01-26 18:21:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=904341) [2026-01-26 18:21:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=904341) [2026-01-26 18:21:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=904341) [2026-01-26 18:21:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=904341) [2026-01-26 18:21:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=904341) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=904341) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.31it/s]
(EngineCore_DP0 pid=904341) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:00,  2.07it/s]
(EngineCore_DP0 pid=904341) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.62it/s]
(EngineCore_DP0 pid=904341) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.45it/s]
(EngineCore_DP0 pid=904341) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.52it/s]
(EngineCore_DP0 pid=904341) 
(EngineCore_DP0 pid=904341) [2026-01-26 18:21:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=904341) [2026-01-26 18:21:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=904341) [2026-01-26 18:21:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=904341) [2026-01-26 18:21:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=904341) [2026-01-26 18:21:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=904341) [2026-01-26 18:21:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=904341) [2026-01-26 18:21:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=904341) [2026-01-26 18:21:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=904341) [rank0]:W0126 18:22:09.334000 904341 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=904341) [rank0]:W0126 18:22:09.404000 904341 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=904341) [rank0]:W0126 18:22:10.195000 904341 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=904341) [rank0]:W0126 18:22:10.303000 904341 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=904341) 2026-01-26 18:22:20,364 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=904341) 2026-01-26 18:22:20,534 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=904341) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:02,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:01,  9.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:01,  8.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:01,  9.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:01,  9.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  9.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  9.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:01<00:00,  9.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:01<00:00,  9.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.33it/s]
(EngineCore_DP0 pid=904341) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 10.87it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 11.10it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 11.16it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 11.18it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 10.85it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00, 10.95it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 27/8192 [00:00<00:30, 267.11it/s]
Adding requests:   1%|          | 61/8192 [00:00<00:26, 308.54it/s]
Adding requests:   1%|          | 93/8192 [00:00<00:26, 309.47it/s]
Adding requests:   2%|▏         | 126/8192 [00:00<00:25, 317.17it/s]
Adding requests:   2%|▏         | 160/8192 [00:00<00:24, 324.91it/s]
Adding requests:   2%|▏         | 196/8192 [00:00<00:23, 335.96it/s]
Adding requests:   3%|▎         | 231/8192 [00:00<00:23, 338.46it/s]
Adding requests:   3%|▎         | 265/8192 [00:00<00:23, 336.47it/s]
Adding requests:   4%|▎         | 301/8192 [00:00<00:23, 340.48it/s]
Adding requests:   4%|▍         | 337/8192 [00:01<00:22, 346.38it/s]
Adding requests:   5%|▍         | 373/8192 [00:01<00:22, 349.60it/s]
Adding requests:   5%|▌         | 410/8192 [00:01<00:22, 352.85it/s]
Adding requests:   5%|▌         | 446/8192 [00:01<00:22, 350.47it/s]
Adding requests:   6%|▌         | 484/8192 [00:01<00:21, 359.16it/s]
Adding requests:   6%|▋         | 523/8192 [00:01<00:20, 366.27it/s]
Adding requests:   7%|▋         | 560/8192 [00:01<00:20, 365.80it/s]
Adding requests:   7%|▋         | 597/8192 [00:01<00:21, 354.27it/s]
Adding requests:   8%|▊         | 633/8192 [00:01<00:21, 352.90it/s]
Adding requests:   8%|▊         | 669/8192 [00:01<00:21, 343.99it/s]
Adding requests:   9%|▊         | 706/8192 [00:02<00:21, 350.02it/s]
Adding requests:   9%|▉         | 742/8192 [00:02<00:21, 347.70it/s]
Adding requests:   9%|▉         | 777/8192 [00:02<00:21, 348.12it/s]
Adding requests:  10%|▉         | 812/8192 [00:02<00:21, 345.29it/s]
Adding requests:  10%|█         | 850/8192 [00:02<00:20, 353.71it/s]
Adding requests:  11%|█         | 887/8192 [00:02<00:20, 356.69it/s]
Adding requests:  11%|█▏        | 923/8192 [00:02<00:20, 349.51it/s]
Adding requests:  12%|█▏        | 960/8192 [00:02<00:20, 352.41it/s]
Adding requests:  12%|█▏        | 996/8192 [00:02<00:20, 347.49it/s]
Adding requests:  13%|█▎        | 1031/8192 [00:02<00:20, 346.96it/s]
Adding requests:  13%|█▎        | 1066/8192 [00:03<00:20, 345.98it/s]
Adding requests:  13%|█▎        | 1101/8192 [00:03<00:20, 344.36it/s]
Adding requests:  14%|█▍        | 1139/8192 [00:03<00:20, 351.83it/s]
Adding requests:  14%|█▍        | 1175/8192 [00:03<00:20, 347.39it/s]
Adding requests:  15%|█▍        | 1212/8192 [00:03<00:19, 350.65it/s]
Adding requests:  15%|█▌        | 1248/8192 [00:03<00:19, 351.40it/s]
Adding requests:  16%|█▌        | 1284/8192 [00:03<00:19, 345.42it/s]
Adding requests:  16%|█▌        | 1320/8192 [00:03<00:19, 346.27it/s]
Adding requests:  17%|█▋        | 1355/8192 [00:03<00:19, 346.22it/s]
Adding requests:  17%|█▋        | 1390/8192 [00:04<00:19, 343.28it/s]
Adding requests:  17%|█▋        | 1425/8192 [00:04<00:20, 334.59it/s]
Adding requests:  18%|█▊        | 1459/8192 [00:04<00:20, 335.03it/s]
Adding requests:  18%|█▊        | 1495/8192 [00:04<00:19, 340.25it/s]
Adding requests:  19%|█▊        | 1530/8192 [00:04<00:19, 336.64it/s]
Adding requests:  19%|█▉        | 1564/8192 [00:04<00:19, 333.99it/s]
Adding requests:  20%|█▉        | 1598/8192 [00:04<00:19, 333.79it/s]
Adding requests:  20%|█▉        | 1632/8192 [00:04<00:19, 333.87it/s]
Adding requests:  20%|██        | 1666/8192 [00:04<00:20, 323.30it/s]
Adding requests:  21%|██        | 1702/8192 [00:04<00:19, 333.64it/s]
Adding requests:  21%|██        | 1738/8192 [00:05<00:18, 340.30it/s]
Adding requests:  22%|██▏       | 1776/8192 [00:05<00:18, 350.52it/s]
Adding requests:  22%|██▏       | 1812/8192 [00:05<00:18, 349.56it/s]
Adding requests:  23%|██▎       | 1848/8192 [00:05<00:18, 351.26it/s]
Adding requests:  23%|██▎       | 1885/8192 [00:05<00:17, 354.79it/s]
Adding requests:  23%|██▎       | 1922/8192 [00:05<00:17, 358.00it/s]
Adding requests:  24%|██▍       | 1960/8192 [00:05<00:17, 361.02it/s]
Adding requests:  24%|██▍       | 1997/8192 [00:05<00:17, 354.69it/s]
Adding requests:  25%|██▍       | 2033/8192 [00:05<00:17, 342.55it/s]
Adding requests:  25%|██▌       | 2068/8192 [00:06<00:18, 339.85it/s]
Adding requests:  26%|██▌       | 2104/8192 [00:06<00:17, 344.24it/s]
Adding requests:  26%|██▌       | 2140/8192 [00:06<00:17, 346.68it/s]
Adding requests:  27%|██▋       | 2175/8192 [00:06<00:17, 340.96it/s]
Adding requests:  27%|██▋       | 2210/8192 [00:06<00:17, 340.64it/s]
Adding requests:  27%|██▋       | 2246/8192 [00:06<00:17, 345.09it/s]
Adding requests:  28%|██▊       | 2284/8192 [00:06<00:16, 351.81it/s]
Adding requests:  28%|██▊       | 2320/8192 [00:06<00:16, 353.86it/s]
Adding requests:  29%|██▉       | 2358/8192 [00:06<00:16, 359.84it/s]
Adding requests:  29%|██▉       | 2396/8192 [00:06<00:15, 363.85it/s]
Adding requests:  30%|██▉       | 2433/8192 [00:07<00:15, 364.86it/s]
Adding requests:  30%|███       | 2470/8192 [00:07<00:15, 361.07it/s]
Adding requests:  31%|███       | 2509/8192 [00:07<00:15, 366.11it/s]
Adding requests:  31%|███       | 2548/8192 [00:07<00:15, 370.72it/s]
Adding requests:  32%|███▏      | 2587/8192 [00:07<00:14, 375.72it/s]
Adding requests:  32%|███▏      | 2625/8192 [00:07<00:15, 368.40it/s]
Adding requests:  32%|███▏      | 2662/8192 [00:07<00:15, 361.21it/s]
Adding requests:  33%|███▎      | 2699/8192 [00:07<00:15, 356.86it/s]
Adding requests:  33%|███▎      | 2735/8192 [00:07<00:15, 356.61it/s]
Adding requests:  34%|███▍      | 2774/8192 [00:07<00:14, 363.58it/s]
Adding requests:  34%|███▍      | 2812/8192 [00:08<00:14, 366.11it/s]
Adding requests:  35%|███▍      | 2849/8192 [00:08<00:15, 356.05it/s]
Adding requests:  35%|███▌      | 2885/8192 [00:08<00:15, 352.12it/s]
Adding requests:  36%|███▌      | 2921/8192 [00:08<00:15, 341.23it/s]
Adding requests:  36%|███▌      | 2959/8192 [00:08<00:14, 350.38it/s]
Adding requests:  37%|███▋      | 2996/8192 [00:08<00:14, 353.62it/s]
Adding requests:  37%|███▋      | 3034/8192 [00:08<00:14, 360.92it/s]
Adding requests:  37%|███▋      | 3071/8192 [00:08<00:14, 361.83it/s]
Adding requests:  38%|███▊      | 3108/8192 [00:08<00:13, 364.15it/s]
Adding requests:  38%|███▊      | 3146/8192 [00:09<00:13, 367.08it/s]
Adding requests:  39%|███▉      | 3183/8192 [00:09<00:13, 360.50it/s]
Adding requests:  39%|███▉      | 3220/8192 [00:09<00:13, 360.80it/s]
Adding requests:  40%|███▉      | 3257/8192 [00:09<00:13, 362.63it/s]
Adding requests:  40%|████      | 3294/8192 [00:09<00:13, 349.91it/s]
Adding requests:  41%|████      | 3330/8192 [00:09<00:13, 348.53it/s]
Adding requests:  41%|████      | 3367/8192 [00:09<00:13, 353.64it/s]
Adding requests:  42%|████▏     | 3405/8192 [00:09<00:13, 358.52it/s]
Adding requests:  42%|████▏     | 3442/8192 [00:09<00:13, 360.92it/s]
Adding requests:  42%|████▏     | 3479/8192 [00:09<00:13, 361.84it/s]
Adding requests:  43%|████▎     | 3516/8192 [00:10<00:12, 360.88it/s]
Adding requests:  43%|████▎     | 3556/8192 [00:10<00:12, 371.54it/s]
Adding requests:  44%|████▍     | 3594/8192 [00:10<00:12, 364.80it/s]
Adding requests:  44%|████▍     | 3631/8192 [00:10<00:12, 366.19it/s]
Adding requests:  45%|████▍     | 3668/8192 [00:10<00:12, 359.34it/s]
Adding requests:  45%|████▌     | 3704/8192 [00:10<00:12, 356.69it/s]
Adding requests:  46%|████▌     | 3740/8192 [00:10<00:12, 355.32it/s]
Adding requests:  46%|████▌     | 3776/8192 [00:10<00:12, 346.32it/s]
Adding requests:  47%|████▋     | 3811/8192 [00:10<00:13, 334.34it/s]
Adding requests:  47%|████▋     | 3847/8192 [00:10<00:12, 338.86it/s]
Adding requests:  47%|████▋     | 3883/8192 [00:11<00:12, 342.61it/s]
Adding requests:  48%|████▊     | 3918/8192 [00:11<00:12, 337.23it/s]
Adding requests:  48%|████▊     | 3953/8192 [00:11<00:12, 339.47it/s]
Adding requests:  49%|████▊     | 3988/8192 [00:11<00:12, 340.41it/s]
Adding requests:  49%|████▉     | 4023/8192 [00:11<00:12, 341.74it/s]
Adding requests:  50%|████▉     | 4058/8192 [00:11<00:12, 340.29it/s]
Adding requests:  50%|████▉     | 4093/8192 [00:11<00:12, 340.60it/s]
Adding requests:  50%|█████     | 4129/8192 [00:11<00:11, 345.45it/s]
Adding requests:  51%|█████     | 4164/8192 [00:11<00:11, 345.52it/s]
Adding requests:  51%|█████▏    | 4200/8192 [00:12<00:11, 347.09it/s]
Adding requests:  52%|█████▏    | 4235/8192 [00:12<00:11, 346.78it/s]
Adding requests:  52%|█████▏    | 4270/8192 [00:12<00:11, 332.21it/s]
Adding requests:  53%|█████▎    | 4305/8192 [00:12<00:11, 336.53it/s]
Adding requests:  53%|█████▎    | 4340/8192 [00:12<00:11, 337.93it/s]
Adding requests:  53%|█████▎    | 4377/8192 [00:12<00:11, 346.19it/s]
Adding requests:  54%|█████▍    | 4412/8192 [00:12<00:10, 343.99it/s]
Adding requests:  54%|█████▍    | 4448/8192 [00:12<00:10, 347.52it/s]
Adding requests:  55%|█████▍    | 4484/8192 [00:12<00:10, 348.28it/s]
Adding requests:  55%|█████▌    | 4520/8192 [00:12<00:10, 349.50it/s]
Adding requests:  56%|█████▌    | 4556/8192 [00:13<00:10, 349.52it/s]
Adding requests:  56%|█████▌    | 4593/8192 [00:13<00:10, 352.48it/s]
Adding requests:  57%|█████▋    | 4629/8192 [00:13<00:10, 349.64it/s]
Adding requests:  57%|█████▋    | 4664/8192 [00:13<00:10, 343.38it/s]
Adding requests:  57%|█████▋    | 4699/8192 [00:13<00:10, 338.41it/s]
Adding requests:  58%|█████▊    | 4738/8192 [00:13<00:09, 350.98it/s]
Adding requests:  58%|█████▊    | 4774/8192 [00:13<00:09, 351.39it/s]
Adding requests:  59%|█████▊    | 4810/8192 [00:13<00:09, 348.32it/s]
Adding requests:  59%|█████▉    | 4845/8192 [00:13<00:09, 341.46it/s]
Adding requests:  60%|█████▉    | 4881/8192 [00:13<00:09, 346.80it/s]
Adding requests:  60%|██████    | 4916/8192 [00:14<00:09, 345.05it/s]
Adding requests:  60%|██████    | 4952/8192 [00:14<00:09, 347.36it/s]
Adding requests:  61%|██████    | 4989/8192 [00:14<00:09, 351.39it/s]
Adding requests:  61%|██████▏   | 5025/8192 [00:14<00:09, 342.05it/s]
Adding requests:  62%|██████▏   | 5060/8192 [00:14<00:09, 344.14it/s]
Adding requests:  62%|██████▏   | 5096/8192 [00:14<00:08, 346.65it/s]
Adding requests:  63%|██████▎   | 5132/8192 [00:14<00:08, 347.77it/s]
Adding requests:  63%|██████▎   | 5167/8192 [00:14<00:08, 345.49it/s]
Adding requests:  64%|██████▎   | 5202/8192 [00:14<00:08, 341.65it/s]
Adding requests:  64%|██████▍   | 5238/8192 [00:15<00:08, 344.69it/s]
Adding requests:  64%|██████▍   | 5274/8192 [00:15<00:08, 345.99it/s]
Adding requests:  65%|██████▍   | 5309/8192 [00:15<00:08, 345.66it/s]
Adding requests:  65%|██████▌   | 5345/8192 [00:15<00:08, 348.25it/s]
Adding requests:  66%|██████▌   | 5380/8192 [00:15<00:08, 348.46it/s]
Adding requests:  66%|██████▌   | 5415/8192 [00:15<00:08, 339.28it/s]
Adding requests:  67%|██████▋   | 5451/8192 [00:15<00:07, 342.96it/s]
Adding requests:  67%|██████▋   | 5488/8192 [00:15<00:07, 348.56it/s]
Adding requests:  67%|██████▋   | 5525/8192 [00:15<00:07, 353.72it/s]
Adding requests:  68%|██████▊   | 5561/8192 [00:15<00:07, 341.31it/s]
Adding requests:  68%|██████▊   | 5596/8192 [00:16<00:08, 324.30it/s]
Adding requests:  69%|██████▉   | 5632/8192 [00:16<00:07, 333.80it/s]
Adding requests:  69%|██████▉   | 5668/8192 [00:16<00:07, 338.40it/s]
Adding requests:  70%|██████▉   | 5703/8192 [00:16<00:07, 340.44it/s]
Adding requests:  70%|███████   | 5738/8192 [00:16<00:07, 339.49it/s]
Adding requests:  70%|███████   | 5775/8192 [00:16<00:06, 346.03it/s]
Adding requests:  71%|███████   | 5811/8192 [00:16<00:06, 348.50it/s]
Adding requests:  71%|███████▏  | 5847/8192 [00:16<00:06, 348.45it/s]
Adding requests:  72%|███████▏  | 5883/8192 [00:16<00:06, 350.18it/s]
Adding requests:  72%|███████▏  | 5920/8192 [00:16<00:06, 353.62it/s]
Adding requests:  73%|███████▎  | 5957/8192 [00:17<00:06, 358.33it/s]
Adding requests:  73%|███████▎  | 5994/8192 [00:17<00:06, 360.09it/s]
Adding requests:  74%|███████▎  | 6031/8192 [00:17<00:06, 357.19it/s]
Adding requests:  74%|███████▍  | 6067/8192 [00:17<00:06, 354.07it/s]
Adding requests:  74%|███████▍  | 6103/8192 [00:17<00:05, 349.81it/s]
Adding requests:  75%|███████▍  | 6140/8192 [00:17<00:05, 352.77it/s]
Adding requests:  75%|███████▌  | 6177/8192 [00:17<00:05, 354.82it/s]
Adding requests:  76%|███████▌  | 6213/8192 [00:17<00:05, 350.29it/s]
Adding requests:  76%|███████▋  | 6249/8192 [00:17<00:05, 349.56it/s]
Adding requests:  77%|███████▋  | 6284/8192 [00:18<00:05, 349.65it/s]
Adding requests:  77%|███████▋  | 6321/8192 [00:18<00:05, 355.21it/s]
Adding requests:  78%|███████▊  | 6358/8192 [00:18<00:05, 358.79it/s]
Adding requests:  78%|███████▊  | 6394/8192 [00:18<00:05, 356.03it/s]
Adding requests:  78%|███████▊  | 6430/8192 [00:18<00:05, 347.88it/s]
Adding requests:  79%|███████▉  | 6465/8192 [00:18<00:05, 343.98it/s]
Adding requests:  79%|███████▉  | 6500/8192 [00:18<00:04, 343.09it/s]
Adding requests:  80%|███████▉  | 6537/8192 [00:18<00:04, 348.09it/s]
Adding requests:  80%|████████  | 6572/8192 [00:18<00:04, 347.64it/s]
Adding requests:  81%|████████  | 6607/8192 [00:18<00:04, 346.63it/s]
Adding requests:  81%|████████  | 6645/8192 [00:19<00:04, 354.35it/s]
Adding requests:  82%|████████▏ | 6681/8192 [00:19<00:04, 348.36it/s]
Adding requests:  82%|████████▏ | 6717/8192 [00:19<00:04, 350.85it/s]
Adding requests:  82%|████████▏ | 6754/8192 [00:19<00:04, 356.07it/s]
Adding requests:  83%|████████▎ | 6790/8192 [00:19<00:03, 352.48it/s]
Adding requests:  83%|████████▎ | 6826/8192 [00:19<00:03, 345.94it/s]
Adding requests:  84%|████████▍ | 6862/8192 [00:19<00:03, 349.39it/s]
Adding requests:  84%|████████▍ | 6897/8192 [00:19<00:03, 340.12it/s]
Adding requests:  85%|████████▍ | 6934/8192 [00:19<00:03, 347.01it/s]
Adding requests:  85%|████████▌ | 6969/8192 [00:19<00:03, 342.61it/s]
Adding requests:  86%|████████▌ | 7006/8192 [00:20<00:03, 348.39it/s]
Adding requests:  86%|████████▌ | 7042/8192 [00:20<00:03, 349.47it/s]
Adding requests:  86%|████████▋ | 7077/8192 [00:20<00:03, 349.19it/s]
Adding requests:  87%|████████▋ | 7112/8192 [00:20<00:03, 344.80it/s]
Adding requests:  87%|████████▋ | 7149/8192 [00:20<00:02, 350.19it/s]
Adding requests:  88%|████████▊ | 7185/8192 [00:20<00:02, 352.76it/s]
Adding requests:  88%|████████▊ | 7223/8192 [00:20<00:02, 357.67it/s]
Adding requests:  89%|████████▊ | 7260/8192 [00:20<00:02, 358.35it/s]
Adding requests:  89%|████████▉ | 7297/8192 [00:20<00:02, 358.40it/s]
Adding requests:  90%|████████▉ | 7333/8192 [00:21<00:02, 354.68it/s]
Adding requests:  90%|████████▉ | 7370/8192 [00:21<00:02, 356.36it/s]
Adding requests:  90%|█████████ | 7406/8192 [00:21<00:02, 351.68it/s]
Adding requests:  91%|█████████ | 7442/8192 [00:21<00:02, 351.95it/s]
Adding requests:  91%|█████████▏| 7478/8192 [00:21<00:02, 346.38it/s]
Adding requests:  92%|█████████▏| 7515/8192 [00:21<00:01, 352.65it/s]
Adding requests:  92%|█████████▏| 7551/8192 [00:21<00:01, 349.47it/s]
Adding requests:  93%|█████████▎| 7586/8192 [00:21<00:01, 347.46it/s]
Adding requests:  93%|█████████▎| 7622/8192 [00:21<00:01, 350.65it/s]
Adding requests:  93%|█████████▎| 7659/8192 [00:21<00:01, 354.64it/s]
Adding requests:  94%|█████████▍| 7697/8192 [00:22<00:01, 361.35it/s]
Adding requests:  94%|█████████▍| 7734/8192 [00:22<00:01, 358.90it/s]
Adding requests:  95%|█████████▍| 7770/8192 [00:22<00:01, 353.63it/s]
Adding requests:  95%|█████████▌| 7806/8192 [00:22<00:01, 354.74it/s]
Adding requests:  96%|█████████▌| 7842/8192 [00:22<00:01, 346.95it/s]
Adding requests:  96%|█████████▌| 7877/8192 [00:22<00:00, 345.74it/s]
Adding requests:  97%|█████████▋| 7913/8192 [00:22<00:00, 348.17it/s]
Adding requests:  97%|█████████▋| 7952/8192 [00:22<00:00, 359.73it/s]
Adding requests:  98%|█████████▊| 7990/8192 [00:22<00:00, 363.38it/s]
Adding requests:  98%|█████████▊| 8027/8192 [00:22<00:00, 356.34it/s]
Adding requests:  98%|█████████▊| 8065/8192 [00:23<00:00, 362.42it/s]
Adding requests:  99%|█████████▉| 8102/8192 [00:23<00:00, 356.70it/s]
Adding requests:  99%|█████████▉| 8138/8192 [00:23<00:00, 352.23it/s]
Adding requests: 100%|█████████▉| 8174/8192 [00:23<00:00, 348.40it/s]
Adding requests: 100%|██████████| 8192/8192 [00:23<00:00, 349.21it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  24%|██▍       | 2003/8192 [00:00<00:00, 14877.08it/s, est. speed input: 15235073.62 toks/s, output: 14877.29 toks/s]
Processed prompts:  42%|████▏     | 3474/8192 [00:16<00:00, 14877.08it/s, est. speed input: 220429.45 toks/s, output: 215.26 toks/s]    
Processed prompts:  42%|████▏     | 3475/8192 [00:16<00:27, 170.76it/s, est. speed input: 210913.23 toks/s, output: 205.97 toks/s]  
Processed prompts:  43%|████▎     | 3539/8192 [00:17<00:28, 165.98it/s, est. speed input: 205942.92 toks/s, output: 201.12 toks/s]
Processed prompts:  51%|█████     | 4157/8192 [00:24<00:29, 135.34it/s, est. speed input: 176300.91 toks/s, output: 172.17 toks/s]
Processed prompts:  55%|█████▍    | 4497/8192 [00:27<00:29, 124.64it/s, est. speed input: 165690.79 toks/s, output: 161.81 toks/s]
Processed prompts:  57%|█████▋    | 4707/8192 [00:30<00:30, 112.84it/s, est. speed input: 156981.11 toks/s, output: 153.30 toks/s]
Processed prompts:  59%|█████▉    | 4844/8192 [00:32<00:30, 110.21it/s, est. speed input: 154215.62 toks/s, output: 150.60 toks/s]
Processed prompts:  60%|██████    | 4938/8192 [00:32<00:29, 111.96it/s, est. speed input: 153745.81 toks/s, output: 150.14 toks/s]
Processed prompts:  61%|██████    | 5006/8192 [00:33<00:29, 109.60it/s, est. speed input: 152422.72 toks/s, output: 148.85 toks/s]
Processed prompts:  62%|██████▏   | 5055/8192 [00:34<00:30, 103.68it/s, est. speed input: 150650.72 toks/s, output: 147.12 toks/s]
Processed prompts:  62%|██████▏   | 5091/8192 [00:35<00:32, 94.60it/s, est. speed input: 148561.92 toks/s, output: 145.08 toks/s] 
Processed prompts:  63%|██████▎   | 5139/8192 [00:35<00:34, 88.98it/s, est. speed input: 146893.49 toks/s, output: 143.45 toks/s]
Processed prompts:  64%|██████▎   | 5203/8192 [00:36<00:33, 88.94it/s, est. speed input: 145789.52 toks/s, output: 142.37 toks/s]
Processed prompts:  64%|██████▍   | 5267/8192 [00:37<00:32, 88.85it/s, est. speed input: 144720.53 toks/s, output: 141.33 toks/s]
Processed prompts:  65%|██████▌   | 5331/8192 [00:37<00:32, 88.54it/s, est. speed input: 143663.42 toks/s, output: 140.30 toks/s]
Processed prompts:  66%|██████▌   | 5395/8192 [00:38<00:31, 88.24it/s, est. speed input: 142640.07 toks/s, output: 139.30 toks/s]
Processed prompts:  67%|██████▋   | 5459/8192 [00:39<00:30, 88.34it/s, est. speed input: 141689.85 toks/s, output: 138.37 toks/s]
Processed prompts:  67%|██████▋   | 5523/8192 [00:40<00:30, 88.32it/s, est. speed input: 140763.69 toks/s, output: 137.46 toks/s]
Processed prompts:  68%|██████▊   | 5587/8192 [00:40<00:29, 87.99it/s, est. speed input: 139839.27 toks/s, output: 136.56 toks/s]
Processed prompts:  69%|██████▉   | 5651/8192 [00:41<00:28, 87.79it/s, est. speed input: 138951.66 toks/s, output: 135.69 toks/s]
Processed prompts:  70%|██████▉   | 5715/8192 [00:42<00:28, 87.63it/s, est. speed input: 138093.08 toks/s, output: 134.86 toks/s]
Processed prompts:  71%|███████   | 5779/8192 [00:43<00:27, 87.57it/s, est. speed input: 137268.42 toks/s, output: 134.05 toks/s]
Processed prompts:  71%|███████▏  | 5843/8192 [00:43<00:26, 87.48it/s, est. speed input: 136467.36 toks/s, output: 133.27 toks/s]
Processed prompts:  72%|███████▏  | 5907/8192 [00:44<00:26, 87.75it/s, est. speed input: 135720.73 toks/s, output: 132.54 toks/s]
Processed prompts:  73%|███████▎  | 5971/8192 [00:45<00:25, 88.11it/s, est. speed input: 135012.20 toks/s, output: 131.85 toks/s]
Processed prompts:  74%|███████▎  | 6035/8192 [00:46<00:24, 88.12it/s, est. speed input: 134305.39 toks/s, output: 131.16 toks/s]
Processed prompts:  74%|███████▍  | 6099/8192 [00:46<00:23, 87.80it/s, est. speed input: 133595.60 toks/s, output: 130.46 toks/s]
Processed prompts:  75%|███████▌  | 6163/8192 [00:47<00:23, 87.65it/s, est. speed input: 132913.21 toks/s, output: 129.80 toks/s]
Processed prompts:  76%|███████▌  | 6227/8192 [00:48<00:22, 87.85it/s, est. speed input: 132274.85 toks/s, output: 129.17 toks/s]
Processed prompts:  77%|███████▋  | 6291/8192 [00:48<00:21, 87.78it/s, est. speed input: 131639.81 toks/s, output: 128.55 toks/s]
Processed prompts:  78%|███████▊  | 6355/8192 [00:49<00:20, 87.64it/s, est. speed input: 131016.77 toks/s, output: 127.95 toks/s]
Processed prompts:  78%|███████▊  | 6419/8192 [00:50<00:20, 87.50it/s, est. speed input: 130408.25 toks/s, output: 127.35 toks/s]
Processed prompts:  79%|███████▉  | 6483/8192 [00:51<00:19, 87.57it/s, est. speed input: 129829.90 toks/s, output: 126.79 toks/s]
Processed prompts:  80%|███████▉  | 6547/8192 [00:51<00:18, 87.61it/s, est. speed input: 129266.44 toks/s, output: 126.24 toks/s]
Processed prompts:  81%|████████  | 6611/8192 [00:52<00:18, 87.61it/s, est. speed input: 128716.95 toks/s, output: 125.70 toks/s]
Processed prompts:  81%|████████▏ | 6675/8192 [00:53<00:17, 87.63it/s, est. speed input: 128184.18 toks/s, output: 125.18 toks/s]
Processed prompts:  82%|████████▏ | 6739/8192 [00:54<00:16, 87.60it/s, est. speed input: 127662.65 toks/s, output: 124.67 toks/s]
Processed prompts:  83%|████████▎ | 6803/8192 [00:54<00:15, 87.53it/s, est. speed input: 127151.65 toks/s, output: 124.17 toks/s]
Processed prompts:  84%|████████▍ | 6867/8192 [00:55<00:15, 87.38it/s, est. speed input: 126647.87 toks/s, output: 123.68 toks/s]
Processed prompts:  85%|████████▍ | 6931/8192 [00:56<00:14, 87.41it/s, est. speed input: 126166.03 toks/s, output: 123.21 toks/s]
Processed prompts:  85%|████████▌ | 6995/8192 [00:56<00:13, 87.47it/s, est. speed input: 125698.62 toks/s, output: 122.75 toks/s]
Processed prompts:  86%|████████▌ | 7059/8192 [00:57<00:12, 87.72it/s, est. speed input: 125255.48 toks/s, output: 122.32 toks/s]
Processed prompts:  87%|████████▋ | 7123/8192 [00:58<00:12, 87.89it/s, est. speed input: 124823.35 toks/s, output: 121.90 toks/s]
Processed prompts:  88%|████████▊ | 7187/8192 [00:59<00:11, 87.69it/s, est. speed input: 124382.87 toks/s, output: 121.47 toks/s]
Processed prompts:  89%|████████▊ | 7251/8192 [00:59<00:10, 87.88it/s, est. speed input: 123972.71 toks/s, output: 121.07 toks/s]
Processed prompts:  89%|████████▉ | 7315/8192 [01:00<00:09, 87.82it/s, est. speed input: 123561.04 toks/s, output: 120.67 toks/s]
Processed prompts:  90%|█████████ | 7379/8192 [01:01<00:09, 87.73it/s, est. speed input: 123156.42 toks/s, output: 120.27 toks/s]
Processed prompts:  91%|█████████ | 7443/8192 [01:02<00:08, 87.92it/s, est. speed input: 122775.11 toks/s, output: 119.90 toks/s]
Processed prompts:  92%|█████████▏| 7507/8192 [01:02<00:07, 87.69it/s, est. speed input: 122383.31 toks/s, output: 119.51 toks/s]
Processed prompts:  92%|█████████▏| 7571/8192 [01:03<00:07, 87.46it/s, est. speed input: 121996.46 toks/s, output: 119.14 toks/s]
Processed prompts:  93%|█████████▎| 7635/8192 [01:04<00:06, 87.67it/s, est. speed input: 121638.36 toks/s, output: 118.79 toks/s]
Processed prompts:  94%|█████████▍| 7699/8192 [01:05<00:05, 87.79it/s, est. speed input: 121286.98 toks/s, output: 118.44 toks/s]
Processed prompts:  95%|█████████▍| 7763/8192 [01:05<00:04, 87.62it/s, est. speed input: 120929.97 toks/s, output: 118.10 toks/s]
Processed prompts:  96%|█████████▌| 7827/8192 [01:06<00:04, 87.50it/s, est. speed input: 120581.12 toks/s, output: 117.75 toks/s]
Processed prompts:  96%|█████████▋| 7891/8192 [01:07<00:03, 87.42it/s, est. speed input: 120239.60 toks/s, output: 117.42 toks/s]
Processed prompts:  97%|█████████▋| 7955/8192 [01:07<00:02, 87.45it/s, est. speed input: 119910.19 toks/s, output: 117.10 toks/s]
Processed prompts:  98%|█████████▊| 8019/8192 [01:08<00:01, 87.42it/s, est. speed input: 119585.04 toks/s, output: 116.78 toks/s]
Processed prompts:  99%|█████████▊| 8083/8192 [01:09<00:01, 87.37it/s, est. speed input: 119265.35 toks/s, output: 116.47 toks/s]
Processed prompts:  99%|█████████▉| 8147/8192 [01:09<00:00, 95.35it/s, est. speed input: 119301.89 toks/s, output: 116.51 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:09<00:00, 95.35it/s, est. speed input: 119959.87 toks/s, output: 117.15 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:09<00:00, 117.15it/s, est. speed input: 119959.87 toks/s, output: 117.15 toks/s]
[rank0]:[W126 18:23:59.788770923 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-28 13:52:33
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/BitNet-2B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:52:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3259020) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3259020) WARNING 01-28 13:52:55 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3259020) WARNING 01-28 13:53:06 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 33.03 requests/s, 16942.80 total tokens/s, 33.03 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-28 13:52:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:52:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:52:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:52:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:52:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:52:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:52:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:52:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:52:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:52:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:52:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:52:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:52:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:52:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:52:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:52:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:52:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:52:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3259020) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3259020) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.87it/s]
(EngineCore_DP0 pid=3259020) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.87it/s]
(EngineCore_DP0 pid=3259020) 
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3259020) 2026-01-28 13:53:06,427 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3259020) 2026-01-28 13:53:06,453 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3259020) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.66it/s]
(EngineCore_DP0 pid=3259020) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.78it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  59%|█████▊    | 75/128 [00:00<00:00, 742.79it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 753.19it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:20,  6.15it/s, est. speed input: 3151.33 toks/s, output: 6.15 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:05, 20.75it/s, est. speed input: 9299.72 toks/s, output: 18.16 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 27.02it/s, est. speed input: 11923.64 toks/s, output: 23.29 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 30.35it/s, est. speed input: 13378.29 toks/s, output: 26.13 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 32.25it/s, est. speed input: 14286.59 toks/s, output: 27.90 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 33.39it/s, est. speed input: 14905.55 toks/s, output: 29.11 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 34.15it/s, est. speed input: 15362.67 toks/s, output: 30.00 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 34.68it/s, est. speed input: 15715.23 toks/s, output: 30.69 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 35.06it/s, est. speed input: 15996.95 toks/s, output: 31.24 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 35.34it/s, est. speed input: 16227.20 toks/s, output: 31.69 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 35.54it/s, est. speed input: 16419.25 toks/s, output: 32.07 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 35.66it/s, est. speed input: 16577.73 toks/s, output: 32.38 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 35.64it/s, est. speed input: 16700.77 toks/s, output: 32.62 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 35.68it/s, est. speed input: 16812.73 toks/s, output: 32.84 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 35.70it/s, est. speed input: 16909.60 toks/s, output: 33.03 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 35.77it/s, est. speed input: 16999.07 toks/s, output: 33.20 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 35.73it/s, est. speed input: 17071.41 toks/s, output: 33.34 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 35.77it/s, est. speed input: 17141.04 toks/s, output: 33.48 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 35.83it/s, est. speed input: 17206.12 toks/s, output: 33.61 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 35.87it/s, est. speed input: 17264.99 toks/s, output: 33.72 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 35.86it/s, est. speed input: 17315.45 toks/s, output: 33.82 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 35.85it/s, est. speed input: 17361.65 toks/s, output: 33.91 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 35.84it/s, est. speed input: 17403.21 toks/s, output: 33.99 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 35.85it/s, est. speed input: 17442.75 toks/s, output: 34.07 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 35.90it/s, est. speed input: 17481.61 toks/s, output: 34.14 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 35.90it/s, est. speed input: 17515.28 toks/s, output: 34.21 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 35.92it/s, est. speed input: 17548.08 toks/s, output: 34.27 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 35.89it/s, est. speed input: 17575.80 toks/s, output: 34.33 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 35.86it/s, est. speed input: 17601.18 toks/s, output: 34.38 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 35.84it/s, est. speed input: 17625.12 toks/s, output: 34.42 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 35.89it/s, est. speed input: 17650.87 toks/s, output: 34.47 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 35.92it/s, est. speed input: 17674.44 toks/s, output: 34.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.92it/s, est. speed input: 17690.16 toks/s, output: 34.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.55it/s, est. speed input: 17690.16 toks/s, output: 34.55 toks/s]
[rank0]:[W128 13:53:12.974650369 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-28 13:53:14
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/BitNet-2B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:53:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3260361) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3260361) WARNING 01-28 13:53:37 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3260361) WARNING 01-28 13:53:47 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 32.74 requests/s, 33560.56 total tokens/s, 32.74 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-28 13:53:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:53:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:53:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:53:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:53:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:53:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:53:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:53:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:53:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:53:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:53:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:53:28] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:53:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:28] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:28] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:53:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:53:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:53:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:53:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:53:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:29] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:29] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:29] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:29] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:29] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3260361) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3260361) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=3260361) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=3260361) 
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3260361) 2026-01-28 13:53:47,159 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3260361) 2026-01-28 13:53:47,186 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3260361) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.60it/s]
(EngineCore_DP0 pid=3260361) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.56it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  32%|███▏      | 41/128 [00:00<00:00, 400.64it/s]
Adding requests:  67%|██████▋   | 86/128 [00:00<00:00, 426.83it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 429.97it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:02, 46.36it/s, est. speed input: 47481.08 toks/s, output: 46.37 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:03, 39.07it/s, est. speed input: 40972.07 toks/s, output: 40.01 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:03, 37.46it/s, est. speed input: 39464.79 toks/s, output: 38.54 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:03, 36.64it/s, est. speed input: 38675.90 toks/s, output: 37.77 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 36.16it/s, est. speed input: 38185.40 toks/s, output: 37.29 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 35.86it/s, est. speed input: 37850.52 toks/s, output: 36.96 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 35.61it/s, est. speed input: 37581.51 toks/s, output: 36.70 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 35.44it/s, est. speed input: 37378.25 toks/s, output: 36.50 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:02, 35.36it/s, est. speed input: 37232.45 toks/s, output: 36.36 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 35.38it/s, est. speed input: 37137.00 toks/s, output: 36.27 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 35.36it/s, est. speed input: 37050.08 toks/s, output: 36.18 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 35.30it/s, est. speed input: 36964.56 toks/s, output: 36.10 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:02, 35.32it/s, est. speed input: 36907.32 toks/s, output: 36.04 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 35.31it/s, est. speed input: 36852.78 toks/s, output: 35.99 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 35.31it/s, est. speed input: 36807.22 toks/s, output: 35.94 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 34.78it/s, est. speed input: 36653.32 toks/s, output: 35.79 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 34.97it/s, est. speed input: 36630.96 toks/s, output: 35.77 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:01, 35.05it/s, est. speed input: 36601.38 toks/s, output: 35.74 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 35.17it/s, est. speed input: 36585.67 toks/s, output: 35.73 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 35.19it/s, est. speed input: 36560.60 toks/s, output: 35.70 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 35.20it/s, est. speed input: 36537.54 toks/s, output: 35.68 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:01, 35.03it/s, est. speed input: 36488.29 toks/s, output: 35.63 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 35.13it/s, est. speed input: 36477.34 toks/s, output: 35.62 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 35.12it/s, est. speed input: 36455.00 toks/s, output: 35.60 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 35.11it/s, est. speed input: 36434.55 toks/s, output: 35.58 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 35.09it/s, est. speed input: 36413.62 toks/s, output: 35.56 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:03<00:00, 35.02it/s, est. speed input: 36386.64 toks/s, output: 35.53 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:03<00:00, 35.11it/s, est. speed input: 36378.63 toks/s, output: 35.53 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 35.17it/s, est. speed input: 36371.19 toks/s, output: 35.52 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 35.18it/s, est. speed input: 36360.99 toks/s, output: 35.51 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 35.20it/s, est. speed input: 36352.10 toks/s, output: 35.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.20it/s, est. speed input: 36301.63 toks/s, output: 35.45 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.45it/s, est. speed input: 36301.63 toks/s, output: 35.45 toks/s]
[rank0]:[W128 13:53:52.048743598 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-28 13:53:54
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/BitNet-2B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:54:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3261597) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3261597) WARNING 01-28 13:54:17 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3261597) WARNING 01-28 13:54:27 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 67.98 requests/s, 69677.78 total tokens/s, 67.98 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-28 13:54:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:54:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:54:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:54:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:54:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:54:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:54:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:54:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:54:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:54:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:54:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:54:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:54:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:54:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:54:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:54:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:54:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:54:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:09] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:09] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:09] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:09] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:09] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3261597) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3261597) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.79it/s]
(EngineCore_DP0 pid=3261597) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.79it/s]
(EngineCore_DP0 pid=3261597) 
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3261597) 2026-01-28 13:54:27,490 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3261597) 2026-01-28 13:54:27,515 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3261597) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 15.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 13.52it/s]
(EngineCore_DP0 pid=3261597) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.87it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  16%|█▌        | 40/256 [00:00<00:00, 397.75it/s]
Adding requests:  33%|███▎      | 84/256 [00:00<00:00, 420.11it/s]
Adding requests:  50%|█████     | 129/256 [00:00<00:00, 430.72it/s]
Adding requests:  68%|██████▊   | 173/256 [00:00<00:00, 433.92it/s]
Adding requests:  85%|████████▌ | 218/256 [00:00<00:00, 439.47it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 432.90it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:00<00:00, 390.48it/s, est. speed input: 399874.99 toks/s, output: 390.48 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:00<00:01, 106.77it/s, est. speed input: 122701.56 toks/s, output: 119.82 toks/s]
Processed prompts:  39%|███▉      | 101/256 [00:00<00:01, 94.05it/s, est. speed input: 108734.09 toks/s, output: 106.19 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:01<00:01, 85.01it/s, est. speed input: 100501.67 toks/s, output: 98.15 toks/s] 
Processed prompts:  50%|█████     | 128/256 [00:01<00:01, 81.56it/s, est. speed input: 97018.98 toks/s, output: 94.74 toks/s] 
Processed prompts:  54%|█████▍    | 138/256 [00:01<00:01, 79.06it/s, est. speed input: 94665.48 toks/s, output: 92.45 toks/s]
Processed prompts:  57%|█████▋    | 147/256 [00:01<00:01, 79.15it/s, est. speed input: 93732.81 toks/s, output: 91.54 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:01<00:01, 74.60it/s, est. speed input: 91280.86 toks/s, output: 89.14 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:01<00:01, 73.66it/s, est. speed input: 90126.75 toks/s, output: 88.01 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:01, 72.85it/s, est. speed input: 89091.37 toks/s, output: 87.00 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:02<00:01, 72.29it/s, est. speed input: 88187.84 toks/s, output: 86.12 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:02<00:00, 71.84it/s, est. speed input: 87373.94 toks/s, output: 85.33 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:02<00:00, 71.15it/s, est. speed input: 86563.66 toks/s, output: 84.53 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:02<00:00, 70.90it/s, est. speed input: 85881.46 toks/s, output: 83.87 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:02<00:00, 70.76it/s, est. speed input: 85265.50 toks/s, output: 83.27 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:02<00:00, 70.80it/s, est. speed input: 84728.17 toks/s, output: 82.74 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 70.92it/s, est. speed input: 84250.25 toks/s, output: 82.28 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 69.74it/s, est. speed input: 83607.99 toks/s, output: 81.65 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:03<00:00, 70.04it/s, est. speed input: 83188.09 toks/s, output: 81.24 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:03<00:00, 70.19it/s, est. speed input: 82790.24 toks/s, output: 80.85 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 70.19it/s, est. speed input: 82607.97 toks/s, output: 80.67 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 80.67it/s, est. speed input: 82607.97 toks/s, output: 80.67 toks/s]
[rank0]:[W128 13:54:33.662184334 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-28 13:54:35
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/BitNet-2B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:54:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3262761) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3262761) WARNING 01-28 13:54:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3262761) WARNING 01-28 13:55:09 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 131.07 requests/s, 134351.42 total tokens/s, 131.07 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-28 13:54:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:54:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:54:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:54:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:54:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:54:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:54:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:54:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:54:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:54:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:54:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:54:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:54:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:54:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:54:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:54:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:54:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:54:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:51] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:51] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:51] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:51] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:51] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3262761) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3262761) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.96it/s]
(EngineCore_DP0 pid=3262761) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.96it/s]
(EngineCore_DP0 pid=3262761) 
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3262761) 2026-01-28 13:55:09,555 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3262761) 2026-01-28 13:55:09,581 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3262761) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 15.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.99it/s]
(EngineCore_DP0 pid=3262761) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 15.85it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 16.02it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 37/512 [00:00<00:01, 368.32it/s]
Adding requests:  16%|█▌        | 81/512 [00:00<00:01, 408.37it/s]
Adding requests:  25%|██▍       | 126/512 [00:00<00:00, 425.78it/s]
Adding requests:  33%|███▎      | 170/512 [00:00<00:00, 430.04it/s]
Adding requests:  42%|████▏     | 216/512 [00:00<00:00, 438.36it/s]
Adding requests:  51%|█████     | 260/512 [00:00<00:00, 431.75it/s]
Adding requests:  60%|█████▉    | 305/512 [00:00<00:00, 435.78it/s]
Adding requests:  69%|██████▉   | 352/512 [00:00<00:00, 445.66it/s]
Adding requests:  78%|███████▊  | 400/512 [00:00<00:00, 454.18it/s]
Adding requests:  87%|████████▋ | 447/512 [00:01<00:00, 457.16it/s]
Adding requests:  96%|█████████▋| 494/512 [00:01<00:00, 460.54it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 443.60it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:00<00:00, 1386.99it/s, est. speed input: 1420370.63 toks/s, output: 1387.01 toks/s]
Processed prompts:  56%|█████▌    | 285/512 [00:01<00:00, 229.73it/s, est. speed input: 269841.89 toks/s, output: 263.52 toks/s]   
Processed prompts:  69%|██████▊   | 351/512 [00:01<00:00, 190.04it/s, est. speed input: 227149.50 toks/s, output: 221.82 toks/s]
Processed prompts:  77%|███████▋  | 393/512 [00:01<00:00, 179.13it/s, est. speed input: 215263.93 toks/s, output: 210.22 toks/s]
Processed prompts:  83%|████████▎ | 424/512 [00:02<00:00, 169.15it/s, est. speed input: 206720.07 toks/s, output: 201.87 toks/s]
Processed prompts:  88%|████████▊ | 449/512 [00:02<00:00, 164.51it/s, est. speed input: 202297.56 toks/s, output: 197.56 toks/s]
Processed prompts:  92%|█████████▏| 471/512 [00:02<00:00, 154.84it/s, est. speed input: 196525.28 toks/s, output: 191.92 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:02<00:00, 150.17it/s, est. speed input: 193110.49 toks/s, output: 188.58 toks/s]
Processed prompts:  99%|█████████▉| 507/512 [00:02<00:00, 149.73it/s, est. speed input: 191341.94 toks/s, output: 186.86 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 149.73it/s, est. speed input: 190595.67 toks/s, output: 186.13 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 186.12it/s, est. speed input: 190595.67 toks/s, output: 186.13 toks/s]
[rank0]:[W128 13:55:15.958884660 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-28 13:55:17
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/BitNet-2B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:55:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3263943) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3263943) WARNING 01-28 13:55:43 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3263943) WARNING 01-28 13:55:53 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 246.67 requests/s, 252839.22 total tokens/s, 246.67 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-28 13:55:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:55:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:55:28] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:55:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:28] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:28] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:55:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:55:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:55:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:55:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:55:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:55:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:55:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:55:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:55:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:55:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:55:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:55:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:55:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:55:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:36] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:36] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:36] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:36] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:36] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:36] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3263943) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3263943) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=3263943) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=3263943) 
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3263943) 2026-01-28 13:55:53,684 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3263943) 2026-01-28 13:55:53,714 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3263943) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 14.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 15.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 13.63it/s]
(EngineCore_DP0 pid=3263943) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 13.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 13.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 13.18it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 41/1024 [00:00<00:02, 401.96it/s]
Adding requests:   8%|▊         | 86/1024 [00:00<00:02, 428.55it/s]
Adding requests:  13%|█▎        | 131/1024 [00:00<00:02, 437.16it/s]
Adding requests:  17%|█▋        | 176/1024 [00:00<00:01, 439.39it/s]
Adding requests:  22%|██▏       | 223/1024 [00:00<00:01, 448.41it/s]
Adding requests:  26%|██▋       | 271/1024 [00:00<00:01, 458.97it/s]
Adding requests:  31%|███       | 317/1024 [00:00<00:01, 455.14it/s]
Adding requests:  36%|███▌      | 366/1024 [00:00<00:01, 463.96it/s]
Adding requests:  40%|████      | 413/1024 [00:00<00:01, 464.02it/s]
Adding requests:  45%|████▌     | 461/1024 [00:01<00:01, 468.56it/s]
Adding requests:  50%|████▉     | 508/1024 [00:01<00:01, 466.46it/s]
Adding requests:  54%|█████▍    | 555/1024 [00:01<00:01, 465.25it/s]
Adding requests:  59%|█████▉    | 602/1024 [00:01<00:00, 466.64it/s]
Adding requests:  64%|██████▎   | 652/1024 [00:01<00:00, 475.43it/s]
Adding requests:  69%|██████▊   | 702/1024 [00:01<00:00, 482.15it/s]
Adding requests:  73%|███████▎  | 751/1024 [00:01<00:00, 480.60it/s]
Adding requests:  78%|███████▊  | 800/1024 [00:01<00:00, 476.73it/s]
Adding requests:  83%|████████▎ | 848/1024 [00:01<00:00, 467.51it/s]
Adding requests:  88%|████████▊ | 898/1024 [00:01<00:00, 475.58it/s]
Adding requests:  92%|█████████▏| 946/1024 [00:02<00:00, 475.99it/s]
Adding requests:  97%|█████████▋| 995/1024 [00:02<00:00, 477.87it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 466.92it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:00<00:00, 4527.15it/s, est. speed input: 4636102.75 toks/s, output: 4527.22 toks/s]
Processed prompts:  95%|█████████▌| 975/1024 [00:01<00:00, 470.97it/s, est. speed input: 563342.75 toks/s, output: 550.14 toks/s]   
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 470.97it/s, est. speed input: 535822.88 toks/s, output: 523.26 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 523.24it/s, est. speed input: 535822.88 toks/s, output: 523.26 toks/s]
[rank0]:[W128 13:56:00.558608128 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-28 13:56:02
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/BitNet-2B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:56:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3265208) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3265208) WARNING 01-28 13:56:33 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3265208) WARNING 01-28 13:56:43 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 312.03 requests/s, 319832.27 total tokens/s, 312.03 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-28 13:56:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:56:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:56:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:56:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:56:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:56:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:56:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:56:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:56:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:56:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:56:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:56:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:56:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:56:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:56:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:56:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:56:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:56:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:25] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3265208) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3265208) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.97it/s]
(EngineCore_DP0 pid=3265208) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.97it/s]
(EngineCore_DP0 pid=3265208) 
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3265208) 2026-01-28 13:56:43,069 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3265208) 2026-01-28 13:56:43,095 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3265208) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 13.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 13.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 13.27it/s]
(EngineCore_DP0 pid=3265208) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 16.08it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.44it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 41/2048 [00:00<00:05, 400.49it/s]
Adding requests:   4%|▍         | 86/2048 [00:00<00:04, 426.19it/s]
Adding requests:   6%|▋         | 131/2048 [00:00<00:04, 433.78it/s]
Adding requests:   9%|▊         | 175/2048 [00:00<00:04, 434.97it/s]
Adding requests:  11%|█         | 219/2048 [00:00<00:04, 421.38it/s]
Adding requests:  13%|█▎        | 262/2048 [00:00<00:04, 421.24it/s]
Adding requests:  15%|█▍        | 305/2048 [00:00<00:04, 419.60it/s]
Adding requests:  17%|█▋        | 347/2048 [00:00<00:04, 416.67it/s]
Adding requests:  19%|█▉        | 393/2048 [00:00<00:03, 427.24it/s]
Adding requests:  21%|██▏       | 439/2048 [00:01<00:03, 434.30it/s]
Adding requests:  24%|██▎       | 486/2048 [00:01<00:03, 443.92it/s]
Adding requests:  26%|██▌       | 531/2048 [00:01<00:03, 430.36it/s]
Adding requests:  28%|██▊       | 575/2048 [00:01<00:03, 432.13it/s]
Adding requests:  30%|███       | 622/2048 [00:01<00:03, 441.36it/s]
Adding requests:  33%|███▎      | 670/2048 [00:01<00:03, 451.01it/s]
Adding requests:  35%|███▌      | 719/2048 [00:01<00:02, 460.98it/s]
Adding requests:  37%|███▋      | 766/2048 [00:01<00:02, 463.27it/s]
Adding requests:  40%|███▉      | 813/2048 [00:01<00:02, 455.80it/s]
Adding requests:  42%|████▏     | 860/2048 [00:01<00:02, 457.74it/s]
Adding requests:  44%|████▍     | 909/2048 [00:02<00:02, 465.39it/s]
Adding requests:  47%|████▋     | 956/2048 [00:02<00:02, 466.56it/s]
Adding requests:  49%|████▉     | 1005/2048 [00:02<00:02, 470.40it/s]
Adding requests:  51%|█████▏    | 1054/2048 [00:02<00:02, 473.41it/s]
Adding requests:  54%|█████▍    | 1102/2048 [00:02<00:02, 471.29it/s]
Adding requests:  56%|█████▌    | 1150/2048 [00:02<00:01, 467.85it/s]
Adding requests:  59%|█████▊    | 1201/2048 [00:02<00:01, 478.90it/s]
Adding requests:  61%|██████    | 1249/2048 [00:02<00:01, 478.31it/s]
Adding requests:  63%|██████▎   | 1297/2048 [00:02<00:01, 475.26it/s]
Adding requests:  66%|██████▌   | 1346/2048 [00:02<00:01, 478.41it/s]
Adding requests:  68%|██████▊   | 1395/2048 [00:03<00:01, 481.38it/s]
Adding requests:  71%|███████   | 1444/2048 [00:03<00:01, 478.05it/s]
Adding requests:  73%|███████▎  | 1493/2048 [00:03<00:01, 481.55it/s]
Adding requests:  75%|███████▌  | 1542/2048 [00:03<00:01, 481.78it/s]
Adding requests:  78%|███████▊  | 1591/2048 [00:03<00:01, 452.75it/s]
Adding requests:  80%|████████  | 1640/2048 [00:03<00:00, 462.38it/s]
Adding requests:  82%|████████▏ | 1687/2048 [00:03<00:00, 461.61it/s]
Adding requests:  85%|████████▍ | 1736/2048 [00:03<00:00, 467.00it/s]
Adding requests:  87%|████████▋ | 1783/2048 [00:03<00:00, 461.87it/s]
Adding requests:  89%|████████▉ | 1832/2048 [00:04<00:00, 468.32it/s]
Adding requests:  92%|█████████▏| 1880/2048 [00:04<00:00, 469.56it/s]
Adding requests:  94%|█████████▍| 1928/2048 [00:04<00:00, 470.71it/s]
Adding requests:  96%|█████████▋| 1976/2048 [00:04<00:00, 472.20it/s]
Adding requests:  99%|█████████▉| 2024/2048 [00:04<00:00, 471.88it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 458.12it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:00<00:00, 10791.87it/s, est. speed input: 11051443.84 toks/s, output: 10792.01 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:02<00:00, 10791.87it/s, est. speed input: 1002721.89 toks/s, output: 979.22 toks/s]   
Processed prompts: 100%|██████████| 2048/2048 [00:02<00:00, 979.16it/s, est. speed input: 1002721.89 toks/s, output: 979.22 toks/s]  
[rank0]:[W128 13:56:52.328870732 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-28 13:56:53
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/BitNet-2B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:57:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3266600) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3266600) WARNING 01-28 13:57:34 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3266600) WARNING 01-28 13:57:44 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 328.17 requests/s, 336374.14 total tokens/s, 328.17 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-28 13:57:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:57:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:57:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:57:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:57:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:57:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:57:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:57:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:57:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:57:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:57:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:57:25] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:57:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:25] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:25] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:57:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:57:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:57:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:57:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:57:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3266600) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3266600) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.07it/s]
(EngineCore_DP0 pid=3266600) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.07it/s]
(EngineCore_DP0 pid=3266600) 
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3266600) [rank0]:W0128 13:57:39.385000 3266600 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3266600) [rank0]:W0128 13:57:39.469000 3266600 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3266600) [rank0]:W0128 13:57:40.575000 3266600 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3266600) [rank0]:W0128 13:57:40.703000 3266600 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3266600) 2026-01-28 13:57:44,680 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3266600) 2026-01-28 13:57:44,709 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3266600) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 13.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 14.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 14.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 13.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 13.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 13.58it/s]
(EngineCore_DP0 pid=3266600) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 15.63it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 15.91it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 15.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 15.96it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 40/4096 [00:00<00:10, 397.40it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 420.11it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:09, 428.94it/s]
Adding requests:   4%|▍         | 173/4096 [00:00<00:09, 432.43it/s]
Adding requests:   5%|▌         | 218/4096 [00:00<00:08, 437.16it/s]
Adding requests:   6%|▋         | 266/4096 [00:00<00:08, 449.33it/s]
Adding requests:   8%|▊         | 311/4096 [00:00<00:08, 447.89it/s]
Adding requests:   9%|▊         | 356/4096 [00:00<00:08, 442.60it/s]
Adding requests:  10%|▉         | 402/4096 [00:00<00:08, 447.65it/s]
Adding requests:  11%|█         | 449/4096 [00:01<00:08, 453.20it/s]
Adding requests:  12%|█▏        | 496/4096 [00:01<00:07, 455.45it/s]
Adding requests:  13%|█▎        | 542/4096 [00:01<00:07, 449.63it/s]
Adding requests:  14%|█▍        | 591/4096 [00:01<00:07, 460.70it/s]
Adding requests:  16%|█▌        | 638/4096 [00:01<00:07, 462.66it/s]
Adding requests:  17%|█▋        | 687/4096 [00:01<00:07, 468.88it/s]
Adding requests:  18%|█▊        | 736/4096 [00:01<00:07, 474.30it/s]
Adding requests:  19%|█▉        | 784/4096 [00:01<00:07, 469.84it/s]
Adding requests:  20%|██        | 832/4096 [00:01<00:07, 460.64it/s]
Adding requests:  21%|██▏       | 880/4096 [00:01<00:06, 464.99it/s]
Adding requests:  23%|██▎       | 927/4096 [00:02<00:06, 461.52it/s]
Adding requests:  24%|██▍       | 975/4096 [00:02<00:06, 465.82it/s]
Adding requests:  25%|██▍       | 1023/4096 [00:02<00:06, 469.43it/s]
Adding requests:  26%|██▌       | 1070/4096 [00:02<00:06, 466.73it/s]
Adding requests:  27%|██▋       | 1117/4096 [00:02<00:06, 435.49it/s]
Adding requests:  28%|██▊       | 1165/4096 [00:02<00:06, 446.70it/s]
Adding requests:  30%|██▉       | 1213/4096 [00:02<00:06, 456.24it/s]
Adding requests:  31%|███       | 1259/4096 [00:02<00:06, 450.19it/s]
Adding requests:  32%|███▏      | 1306/4096 [00:02<00:06, 453.07it/s]
Adding requests:  33%|███▎      | 1354/4096 [00:02<00:05, 458.14it/s]
Adding requests:  34%|███▍      | 1403/4096 [00:03<00:05, 464.54it/s]
Adding requests:  35%|███▌      | 1450/4096 [00:03<00:05, 463.62it/s]
Adding requests:  37%|███▋      | 1499/4096 [00:03<00:05, 469.93it/s]
Adding requests:  38%|███▊      | 1547/4096 [00:03<00:05, 469.37it/s]
Adding requests:  39%|███▉      | 1596/4096 [00:03<00:05, 474.98it/s]
Adding requests:  40%|████      | 1644/4096 [00:03<00:05, 475.39it/s]
Adding requests:  41%|████▏     | 1692/4096 [00:03<00:05, 470.56it/s]
Adding requests:  42%|████▏     | 1740/4096 [00:03<00:04, 471.99it/s]
Adding requests:  44%|████▎     | 1788/4096 [00:03<00:04, 468.47it/s]
Adding requests:  45%|████▍     | 1835/4096 [00:03<00:04, 468.71it/s]
Adding requests:  46%|████▌     | 1882/4096 [00:04<00:04, 468.35it/s]
Adding requests:  47%|████▋     | 1929/4096 [00:04<00:04, 467.24it/s]
Adding requests:  48%|████▊     | 1977/4096 [00:04<00:04, 468.59it/s]
Adding requests:  49%|████▉     | 2024/4096 [00:04<00:04, 458.87it/s]
Adding requests:  51%|█████     | 2072/4096 [00:04<00:04, 464.73it/s]
Adding requests:  52%|█████▏    | 2119/4096 [00:04<00:04, 463.90it/s]
Adding requests:  53%|█████▎    | 2166/4096 [00:04<00:04, 458.62it/s]
Adding requests:  54%|█████▍    | 2212/4096 [00:04<00:04, 456.47it/s]
Adding requests:  55%|█████▌    | 2260/4096 [00:04<00:03, 461.59it/s]
Adding requests:  56%|█████▋    | 2308/4096 [00:05<00:03, 466.03it/s]
Adding requests:  57%|█████▋    | 2355/4096 [00:05<00:03, 464.38it/s]
Adding requests:  59%|█████▊    | 2402/4096 [00:05<00:03, 463.65it/s]
Adding requests:  60%|█████▉    | 2450/4096 [00:05<00:03, 465.75it/s]
Adding requests:  61%|██████    | 2498/4096 [00:05<00:03, 469.90it/s]
Adding requests:  62%|██████▏   | 2545/4096 [00:05<00:03, 467.31it/s]
Adding requests:  63%|██████▎   | 2593/4096 [00:05<00:03, 470.48it/s]
Adding requests:  64%|██████▍   | 2641/4096 [00:05<00:03, 471.27it/s]
Adding requests:  66%|██████▌   | 2689/4096 [00:05<00:03, 467.54it/s]
Adding requests:  67%|██████▋   | 2736/4096 [00:05<00:02, 465.32it/s]
Adding requests:  68%|██████▊   | 2784/4096 [00:06<00:02, 468.20it/s]
Adding requests:  69%|██████▉   | 2831/4096 [00:06<00:02, 459.48it/s]
Adding requests:  70%|███████   | 2879/4096 [00:06<00:02, 464.46it/s]
Adding requests:  71%|███████▏  | 2926/4096 [00:06<00:02, 464.76it/s]
Adding requests:  73%|███████▎  | 2973/4096 [00:06<00:02, 465.26it/s]
Adding requests:  74%|███████▎  | 3020/4096 [00:06<00:02, 461.59it/s]
Adding requests:  75%|███████▍  | 3067/4096 [00:06<00:02, 458.52it/s]
Adding requests:  76%|███████▌  | 3115/4096 [00:06<00:02, 463.57it/s]
Adding requests:  77%|███████▋  | 3162/4096 [00:06<00:02, 443.89it/s]
Adding requests:  78%|███████▊  | 3209/4096 [00:06<00:01, 450.24it/s]
Adding requests:  79%|███████▉  | 3255/4096 [00:07<00:01, 442.71it/s]
Adding requests:  81%|████████  | 3303/4096 [00:07<00:01, 450.80it/s]
Adding requests:  82%|████████▏ | 3351/4096 [00:07<00:01, 458.20it/s]
Adding requests:  83%|████████▎ | 3397/4096 [00:07<00:01, 458.22it/s]
Adding requests:  84%|████████▍ | 3445/4096 [00:07<00:01, 460.55it/s]
Adding requests:  85%|████████▌ | 3492/4096 [00:07<00:01, 459.24it/s]
Adding requests:  86%|████████▋ | 3539/4096 [00:07<00:01, 462.16it/s]
Adding requests:  88%|████████▊ | 3586/4096 [00:07<00:01, 464.10it/s]
Adding requests:  89%|████████▊ | 3633/4096 [00:07<00:00, 463.12it/s]
Adding requests:  90%|████████▉ | 3681/4096 [00:07<00:00, 465.55it/s]
Adding requests:  91%|█████████ | 3729/4096 [00:08<00:00, 466.51it/s]
Adding requests:  92%|█████████▏| 3778/4096 [00:08<00:00, 473.28it/s]
Adding requests:  93%|█████████▎| 3826/4096 [00:08<00:00, 472.84it/s]
Adding requests:  95%|█████████▍| 3876/4096 [00:08<00:00, 478.19it/s]
Adding requests:  96%|█████████▌| 3924/4096 [00:08<00:00, 477.17it/s]
Adding requests:  97%|█████████▋| 3972/4096 [00:08<00:00, 474.64it/s]
Adding requests:  98%|█████████▊| 4020/4096 [00:08<00:00, 475.13it/s]
Adding requests:  99%|█████████▉| 4068/4096 [00:08<00:00, 467.07it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 461.69it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  72%|███████▏  | 2932/4096 [00:00<00:00, 15017.34it/s, est. speed input: 15378394.16 toks/s, output: 15017.49 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:03<00:00, 15017.34it/s, est. speed input: 1162668.43 toks/s, output: 1135.42 toks/s]  
Processed prompts: 100%|██████████| 4096/4096 [00:03<00:00, 1135.37it/s, est. speed input: 1162668.43 toks/s, output: 1135.42 toks/s] 
[rank0]:[W128 13:58:00.518425343 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

