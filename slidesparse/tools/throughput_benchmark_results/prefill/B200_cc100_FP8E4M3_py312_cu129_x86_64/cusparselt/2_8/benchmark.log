
========== M=512 ==========
Time: 2026-01-26 14:14:53
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:15:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=568383) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=568383) WARNING 01-26 14:15:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=568383) WARNING 01-26 14:15:19 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 40.67 requests/s, 20861.62 total tokens/s, 40.67 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 14:14:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:15:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:15:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:15:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:15:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:15:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:15:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:15:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:15:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:15:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:15:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:15:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:15:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:15:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:07] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=568383) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=568383) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.63it/s]
(EngineCore_DP0 pid=568383) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.62it/s]
(EngineCore_DP0 pid=568383) 
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=568383) [2026-01-26 14:15:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=568383) 2026-01-26 14:15:19,907 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=568383) 2026-01-26 14:15:19,929 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=568383) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.05it/s]
(EngineCore_DP0 pid=568383) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.30it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  48%|████▊     | 62/128 [00:00<00:00, 612.16it/s]
Adding requests:  97%|█████████▋| 124/128 [00:00<00:00, 616.64it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 617.07it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:04, 25.16it/s, est. speed input: 12881.63 toks/s, output: 25.16 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 36.07it/s, est. speed input: 17608.35 toks/s, output: 34.39 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:02, 39.73it/s, est. speed input: 19251.62 toks/s, output: 37.60 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 41.59it/s, est. speed input: 20117.84 toks/s, output: 39.29 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 42.50it/s, est. speed input: 20607.86 toks/s, output: 40.25 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 43.15it/s, est. speed input: 20958.84 toks/s, output: 40.93 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 43.68it/s, est. speed input: 21234.70 toks/s, output: 41.47 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:02, 44.01it/s, est. speed input: 21440.55 toks/s, output: 41.88 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:01, 44.22it/s, est. speed input: 21598.20 toks/s, output: 42.18 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:01, 44.22it/s, est. speed input: 21702.91 toks/s, output: 42.39 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 44.27it/s, est. speed input: 21794.33 toks/s, output: 42.57 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 44.27it/s, est. speed input: 21867.48 toks/s, output: 42.71 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 44.33it/s, est. speed input: 21936.74 toks/s, output: 42.85 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 44.38it/s, est. speed input: 21996.22 toks/s, output: 42.96 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 44.40it/s, est. speed input: 22046.59 toks/s, output: 43.06 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 44.36it/s, est. speed input: 22085.12 toks/s, output: 43.13 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:01, 44.34it/s, est. speed input: 22120.31 toks/s, output: 43.20 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:00, 44.35it/s, est. speed input: 22153.45 toks/s, output: 43.27 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 44.19it/s, est. speed input: 22168.71 toks/s, output: 43.30 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 44.21it/s, est. speed input: 22192.91 toks/s, output: 43.35 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 44.18it/s, est. speed input: 22211.80 toks/s, output: 43.38 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 44.21it/s, est. speed input: 22232.37 toks/s, output: 43.42 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 44.19it/s, est. speed input: 22248.43 toks/s, output: 43.45 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 44.20it/s, est. speed input: 22265.11 toks/s, output: 43.49 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 44.27it/s, est. speed input: 22284.06 toks/s, output: 43.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.30it/s, est. speed input: 22300.80 toks/s, output: 43.56 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.30it/s, est. speed input: 22300.80 toks/s, output: 43.56 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.55it/s, est. speed input: 22300.80 toks/s, output: 43.56 toks/s]
[rank0]:[W126 14:15:25.979421060 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 14:15:27
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:15:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=569501) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=569501) WARNING 01-26 14:15:47 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=569501) WARNING 01-26 14:15:54 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 41.09 requests/s, 42115.22 total tokens/s, 41.09 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 14:15:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:15:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:15:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:15:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:15:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:15:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:15:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:15:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:15:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:15:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:15:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:15:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:15:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:15:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:15:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:15:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:42] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:42] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:42] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:42] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:42] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=569501) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=569501) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=569501) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=569501) 
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=569501) [2026-01-26 14:15:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=569501) 2026-01-26 14:15:54,673 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=569501) 2026-01-26 14:15:54,695 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=569501) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 16.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 16.18it/s]
(EngineCore_DP0 pid=569501) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 18.71it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|██▉       | 38/128 [00:00<00:00, 378.55it/s]
Adding requests:  65%|██████▍   | 83/128 [00:00<00:00, 420.00it/s]
Adding requests:  98%|█████████▊| 126/128 [00:00<00:00, 219.39it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 250.73it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:00, 187.19it/s, est. speed input: 191697.68 toks/s, output: 187.20 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 64.27it/s, est. speed input: 73211.43 toks/s, output: 71.49 toks/s]   
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 55.60it/s, est. speed input: 64051.43 toks/s, output: 62.55 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:00<00:01, 51.90it/s, est. speed input: 60346.72 toks/s, output: 58.93 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 49.65it/s, est. speed input: 58149.51 toks/s, output: 56.79 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 48.11it/s, est. speed input: 56688.37 toks/s, output: 55.36 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 46.91it/s, est. speed input: 55515.99 toks/s, output: 54.21 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 45.47it/s, est. speed input: 54456.99 toks/s, output: 53.18 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 44.97it/s, est. speed input: 53769.78 toks/s, output: 52.51 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 44.51it/s, est. speed input: 53150.65 toks/s, output: 51.90 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 44.23it/s, est. speed input: 52624.42 toks/s, output: 51.39 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 43.70it/s, est. speed input: 52080.95 toks/s, output: 50.86 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 43.76it/s, est. speed input: 51699.81 toks/s, output: 50.49 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 43.71it/s, est. speed input: 51336.49 toks/s, output: 50.13 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 43.67it/s, est. speed input: 51008.19 toks/s, output: 49.81 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 43.40it/s, est. speed input: 50666.24 toks/s, output: 49.48 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 43.43it/s, est. speed input: 50393.89 toks/s, output: 49.21 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.43it/s, est. speed input: 50341.72 toks/s, output: 49.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 49.16it/s, est. speed input: 50341.72 toks/s, output: 49.16 toks/s]
[rank0]:[W126 14:15:59.094599117 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 14:16:01
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:16:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=570545) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=570545) WARNING 01-26 14:16:22 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=570545) WARNING 01-26 14:16:29 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 82.56 requests/s, 84620.47 total tokens/s, 82.56 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 14:16:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:16:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:16:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:16:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:16:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:16:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:16:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:16:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:16:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:16:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:16:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:16:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:16:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:16:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=570545) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=570545) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.83it/s]
(EngineCore_DP0 pid=570545) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.83it/s]
(EngineCore_DP0 pid=570545) 
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=570545) [2026-01-26 14:16:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=570545) 2026-01-26 14:16:29,013 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=570545) 2026-01-26 14:16:29,035 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=570545) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.14it/s]
(EngineCore_DP0 pid=570545) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 22.51it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  15%|█▌        | 39/256 [00:00<00:00, 382.74it/s]
Adding requests:  33%|███▎      | 84/256 [00:00<00:00, 419.81it/s]
Adding requests:  50%|█████     | 128/256 [00:00<00:00, 426.87it/s]
Adding requests:  67%|██████▋   | 171/256 [00:00<00:00, 425.51it/s]
Adding requests:  84%|████████▎ | 214/256 [00:00<00:00, 427.02it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 428.31it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:00<00:00, 433.95it/s, est. speed input: 444408.41 toks/s, output: 433.96 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:00<00:01, 133.93it/s, est. speed input: 154157.85 toks/s, output: 150.54 toks/s]
Processed prompts:  46%|████▌     | 117/256 [00:00<00:01, 117.22it/s, est. speed input: 136224.64 toks/s, output: 133.03 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:01<00:01, 106.23it/s, est. speed input: 126130.13 toks/s, output: 123.17 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:01<00:01, 101.14it/s, est. speed input: 121227.98 toks/s, output: 118.39 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:01<00:00, 97.61it/s, est. speed input: 117940.37 toks/s, output: 115.18 toks/s] 
Processed prompts:  67%|██████▋   | 171/256 [00:01<00:00, 97.07it/s, est. speed input: 116367.82 toks/s, output: 113.64 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:01<00:00, 92.11it/s, est. speed input: 113368.10 toks/s, output: 110.71 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:01<00:00, 90.67it/s, est. speed input: 111719.49 toks/s, output: 109.10 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:01<00:00, 89.59it/s, est. speed input: 110292.69 toks/s, output: 107.71 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:01<00:00, 88.70it/s, est. speed input: 109019.46 toks/s, output: 106.46 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:02<00:00, 87.96it/s, est. speed input: 107866.76 toks/s, output: 105.34 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:02<00:00, 87.33it/s, est. speed input: 106815.82 toks/s, output: 104.31 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:02<00:00, 87.06it/s, est. speed input: 105907.55 toks/s, output: 103.43 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:02<00:00, 86.86it/s, est. speed input: 105085.32 toks/s, output: 102.62 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 86.86it/s, est. speed input: 104779.68 toks/s, output: 102.32 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 102.32it/s, est. speed input: 104779.68 toks/s, output: 102.32 toks/s]
[rank0]:[W126 14:16:34.719046908 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 14:16:36
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:16:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=571601) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=571601) WARNING 01-26 14:16:58 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=571601) WARNING 01-26 14:17:04 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 157.69 requests/s, 161637.08 total tokens/s, 157.69 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 14:16:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:16:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:45] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:16:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:16:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:16:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:16:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:16:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:16:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:16:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:16:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:16:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:16:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:16:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:16:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:16:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:16:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=571601) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=571601) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.48it/s]
(EngineCore_DP0 pid=571601) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.48it/s]
(EngineCore_DP0 pid=571601) 
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=571601) [2026-01-26 14:16:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=571601) 2026-01-26 14:17:04,816 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=571601) 2026-01-26 14:17:04,838 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=571601) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 22.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 19.68it/s]
(EngineCore_DP0 pid=571601) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.25it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.23it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 39/512 [00:00<00:01, 380.84it/s]
Adding requests:  16%|█▋        | 84/512 [00:00<00:01, 418.74it/s]
Adding requests:  25%|██▌       | 129/512 [00:00<00:00, 430.77it/s]
Adding requests:  34%|███▍      | 174/512 [00:00<00:00, 435.41it/s]
Adding requests:  43%|████▎     | 220/512 [00:00<00:00, 443.90it/s]
Adding requests:  52%|█████▏    | 268/512 [00:00<00:00, 454.14it/s]
Adding requests:  61%|██████▏   | 314/512 [00:00<00:00, 452.20it/s]
Adding requests:  70%|███████   | 360/512 [00:00<00:00, 453.35it/s]
Adding requests:  80%|███████▉  | 408/512 [00:00<00:00, 459.02it/s]
Adding requests:  89%|████████▊ | 454/512 [00:01<00:00, 457.87it/s]
Adding requests:  98%|█████████▊| 500/512 [00:01<00:00, 457.39it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 449.18it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:00<00:00, 1775.49it/s, est. speed input: 1818229.30 toks/s, output: 1775.53 toks/s]
Processed prompts:  70%|███████   | 360/512 [00:01<00:00, 262.09it/s, est. speed input: 308234.28 toks/s, output: 301.01 toks/s]   
Processed prompts:  87%|████████▋ | 443/512 [00:01<00:00, 226.63it/s, est. speed input: 268445.95 toks/s, output: 262.15 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [00:02<00:00, 211.96it/s, est. speed input: 253525.20 toks/s, output: 247.58 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 211.96it/s, est. speed input: 248968.92 toks/s, output: 243.13 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 243.12it/s, est. speed input: 248968.92 toks/s, output: 243.13 toks/s]
[rank0]:[W126 14:17:10.700311667 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 14:17:12
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:17:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=572684) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=572684) WARNING 01-26 14:17:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=572684) WARNING 01-26 14:17:43 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 306.29 requests/s, 313949.15 total tokens/s, 306.29 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 14:17:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:17:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:17:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:17:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:17:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:17:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:17:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:17:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:17:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:17:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:17:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:17:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:17:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:17:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:17:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:17:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:17:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:17:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:17:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:31] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:31] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:31] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:31] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:31] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=572684) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=572684) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.62it/s]
(EngineCore_DP0 pid=572684) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.61it/s]
(EngineCore_DP0 pid=572684) 
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=572684) [2026-01-26 14:17:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=572684) 2026-01-26 14:17:43,045 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=572684) 2026-01-26 14:17:43,067 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=572684) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 19.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 18.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 19.08it/s]
(EngineCore_DP0 pid=572684) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 23.82it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 37/1024 [00:00<00:02, 363.92it/s]
Adding requests:   8%|▊         | 81/1024 [00:00<00:02, 407.84it/s]
Adding requests:  12%|█▏        | 126/1024 [00:00<00:02, 425.17it/s]
Adding requests:  17%|█▋        | 169/1024 [00:00<00:02, 408.73it/s]
Adding requests:  21%|██        | 213/1024 [00:00<00:01, 419.26it/s]
Adding requests:  25%|██▌       | 261/1024 [00:00<00:01, 437.63it/s]
Adding requests:  30%|██▉       | 305/1024 [00:00<00:01, 437.15it/s]
Adding requests:  34%|███▍      | 351/1024 [00:00<00:01, 442.80it/s]
Adding requests:  39%|███▉      | 397/1024 [00:00<00:01, 447.21it/s]
Adding requests:  43%|████▎     | 443/1024 [00:01<00:01, 450.80it/s]
Adding requests:  48%|████▊     | 489/1024 [00:01<00:01, 451.84it/s]
Adding requests:  52%|█████▏    | 535/1024 [00:01<00:01, 445.24it/s]
Adding requests:  57%|█████▋    | 583/1024 [00:01<00:00, 454.26it/s]
Adding requests:  62%|██████▏   | 631/1024 [00:01<00:00, 460.24it/s]
Adding requests:  66%|██████▋   | 679/1024 [00:01<00:00, 463.37it/s]
Adding requests:  71%|███████   | 728/1024 [00:01<00:00, 468.78it/s]
Adding requests:  76%|███████▌  | 775/1024 [00:01<00:00, 464.82it/s]
Adding requests:  80%|████████  | 822/1024 [00:01<00:00, 456.81it/s]
Adding requests:  85%|████████▍ | 869/1024 [00:01<00:00, 460.62it/s]
Adding requests:  90%|████████▉ | 918/1024 [00:02<00:00, 465.72it/s]
Adding requests:  94%|█████████▍| 966/1024 [00:02<00:00, 468.53it/s]
Adding requests:  99%|█████████▉| 1013/1024 [00:02<00:00, 466.95it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 450.90it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:00<00:00, 6899.63it/s, est. speed input: 7065707.81 toks/s, output: 6899.74 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 6899.63it/s, est. speed input: 979120.66 toks/s, output: 956.17 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 956.07it/s, est. speed input: 979120.66 toks/s, output: 956.17 toks/s] 
[rank0]:[W126 14:17:49.274160156 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 14:17:50
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:18:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=573845) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=573845) WARNING 01-26 14:18:20 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=573845) WARNING 01-26 14:18:26 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 455.29 requests/s, 466671.42 total tokens/s, 455.29 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 14:18:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:18:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:18:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:18:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:18:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:18:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:18:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:18:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:18:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:18:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:18:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:18:14] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:18:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:18:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:18:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:18:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:18:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:18:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:18:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:15] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:15] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:15] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:15] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:15] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=573845) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=573845) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=573845) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.64it/s]
(EngineCore_DP0 pid=573845) 
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=573845) [2026-01-26 14:18:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=573845) 2026-01-26 14:18:26,797 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=573845) 2026-01-26 14:18:26,821 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=573845) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 17.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 20.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 19.41it/s]
(EngineCore_DP0 pid=573845) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 23.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 23.06it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 39/2048 [00:00<00:05, 381.37it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 419.27it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:04, 432.15it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:04, 436.31it/s]
Adding requests:  11%|█         | 218/2048 [00:00<00:04, 433.70it/s]
Adding requests:  13%|█▎        | 266/2048 [00:00<00:03, 447.90it/s]
Adding requests:  15%|█▌        | 312/2048 [00:00<00:03, 449.12it/s]
Adding requests:  17%|█▋        | 358/2048 [00:00<00:03, 451.63it/s]
Adding requests:  20%|█▉        | 405/2048 [00:00<00:03, 457.13it/s]
Adding requests:  22%|██▏       | 452/2048 [00:01<00:03, 460.66it/s]
Adding requests:  24%|██▍       | 499/2048 [00:01<00:03, 461.17it/s]
Adding requests:  27%|██▋       | 546/2048 [00:01<00:03, 454.52it/s]
Adding requests:  29%|██▉       | 593/2048 [00:01<00:03, 457.92it/s]
Adding requests:  31%|███▏      | 641/2048 [00:01<00:03, 462.13it/s]
Adding requests:  34%|███▎      | 689/2048 [00:01<00:02, 467.17it/s]
Adding requests:  36%|███▌      | 738/2048 [00:01<00:02, 472.75it/s]
Adding requests:  38%|███▊      | 786/2048 [00:01<00:02, 467.99it/s]
Adding requests:  41%|████      | 833/2048 [00:01<00:02, 459.54it/s]
Adding requests:  43%|████▎     | 880/2048 [00:01<00:02, 462.00it/s]
Adding requests:  45%|████▌     | 928/2048 [00:02<00:02, 467.22it/s]
Adding requests:  48%|████▊     | 976/2048 [00:02<00:02, 470.82it/s]
Adding requests:  50%|█████     | 1024/2048 [00:02<00:02, 471.74it/s]
Adding requests:  52%|█████▏    | 1072/2048 [00:02<00:02, 465.40it/s]
Adding requests:  55%|█████▍    | 1119/2048 [00:02<00:02, 463.19it/s]
Adding requests:  57%|█████▋    | 1166/2048 [00:02<00:01, 456.34it/s]
Adding requests:  59%|█████▉    | 1212/2048 [00:02<00:01, 455.69it/s]
Adding requests:  61%|██████▏   | 1258/2048 [00:02<00:01, 452.68it/s]
Adding requests:  64%|██████▎   | 1305/2048 [00:02<00:01, 456.61it/s]
Adding requests:  66%|██████▌   | 1353/2048 [00:02<00:01, 460.92it/s]
Adding requests:  68%|██████▊   | 1401/2048 [00:03<00:01, 466.54it/s]
Adding requests:  71%|███████   | 1448/2048 [00:03<00:01, 465.53it/s]
Adding requests:  73%|███████▎  | 1497/2048 [00:03<00:01, 472.26it/s]
Adding requests:  75%|███████▌  | 1545/2048 [00:03<00:01, 472.22it/s]
Adding requests:  78%|███████▊  | 1594/2048 [00:03<00:00, 476.36it/s]
Adding requests:  80%|████████  | 1642/2048 [00:03<00:00, 465.33it/s]
Adding requests:  82%|████████▏ | 1689/2048 [00:03<00:00, 462.17it/s]
Adding requests:  85%|████████▍ | 1738/2048 [00:03<00:00, 468.13it/s]
Adding requests:  87%|████████▋ | 1785/2048 [00:03<00:00, 462.02it/s]
Adding requests:  90%|████████▉ | 1833/2048 [00:03<00:00, 466.24it/s]
Adding requests:  92%|█████████▏| 1881/2048 [00:04<00:00, 467.86it/s]
Adding requests:  94%|█████████▍| 1928/2048 [00:04<00:00, 467.68it/s]
Adding requests:  96%|█████████▋| 1975/2048 [00:04<00:00, 468.23it/s]
Adding requests:  99%|█████████▉| 2024/2048 [00:04<00:00, 473.69it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 461.38it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 35348.80it/s, est. speed input: 36207151.71 toks/s, output: 35355.20 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 35287.81it/s, est. speed input: 36207151.71 toks/s, output: 35355.20 toks/s]
[rank0]:[W126 14:18:33.087215972 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 14:18:35
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:19:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=575134) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=575134) WARNING 01-26 14:19:13 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=575134) WARNING 01-26 14:19:20 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 461.01 requests/s, 472539.37 total tokens/s, 461.01 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 14:19:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:19:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:19:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:19:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:19:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:19:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:19:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:19:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:19:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:19:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:19:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:19:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:19:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:19:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:19:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:19:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:19:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:19:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:19:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=575134) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=575134) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=575134) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=575134) 
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=575134) [2026-01-26 14:19:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=575134) [rank0]:W0126 14:19:17.238000 575134 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=575134) [rank0]:W0126 14:19:17.306000 575134 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=575134) [rank0]:W0126 14:19:18.116000 575134 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=575134) [rank0]:W0126 14:19:18.214000 575134 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=575134) 2026-01-26 14:19:20,635 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=575134) 2026-01-26 14:19:20,660 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=575134) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 19.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 20.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 22.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 21.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 21.30it/s]
(EngineCore_DP0 pid=575134) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 23.41it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 20.18it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 20.07it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 38/4096 [00:00<00:10, 378.85it/s]
Adding requests:   2%|▏         | 82/4096 [00:00<00:09, 413.46it/s]
Adding requests:   3%|▎         | 127/4096 [00:00<00:09, 429.19it/s]
Adding requests:   4%|▍         | 172/4096 [00:00<00:09, 434.88it/s]
Adding requests:   5%|▌         | 217/4096 [00:00<00:08, 439.75it/s]
Adding requests:   6%|▋         | 265/4096 [00:00<00:08, 452.53it/s]
Adding requests:   8%|▊         | 311/4096 [00:00<00:08, 451.60it/s]
Adding requests:   9%|▊         | 358/4096 [00:00<00:08, 454.85it/s]
Adding requests:  10%|▉         | 405/4096 [00:00<00:08, 457.97it/s]
Adding requests:  11%|█         | 451/4096 [00:01<00:08, 452.82it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:07, 456.19it/s]
Adding requests:  13%|█▎        | 544/4096 [00:01<00:07, 449.46it/s]
Adding requests:  14%|█▍        | 593/4096 [00:01<00:07, 458.88it/s]
Adding requests:  16%|█▌        | 640/4096 [00:01<00:07, 461.32it/s]
Adding requests:  17%|█▋        | 689/4096 [00:01<00:07, 466.91it/s]
Adding requests:  18%|█▊        | 738/4096 [00:01<00:07, 471.56it/s]
Adding requests:  19%|█▉        | 786/4096 [00:01<00:07, 467.58it/s]
Adding requests:  20%|██        | 833/4096 [00:01<00:07, 458.03it/s]
Adding requests:  21%|██▏       | 879/4096 [00:01<00:07, 454.09it/s]
Adding requests:  23%|██▎       | 927/4096 [00:02<00:06, 460.94it/s]
Adding requests:  24%|██▍       | 974/4096 [00:02<00:06, 463.33it/s]
Adding requests:  25%|██▍       | 1022/4096 [00:02<00:06, 466.73it/s]
Adding requests:  26%|██▌       | 1069/4096 [00:02<00:06, 465.04it/s]
Adding requests:  27%|██▋       | 1116/4096 [00:02<00:06, 460.27it/s]
Adding requests:  28%|██▊       | 1165/4096 [00:02<00:06, 467.36it/s]
Adding requests:  30%|██▉       | 1215/4096 [00:02<00:06, 474.46it/s]
Adding requests:  31%|███       | 1263/4096 [00:02<00:06, 468.05it/s]
Adding requests:  32%|███▏      | 1311/4096 [00:02<00:05, 468.55it/s]
Adding requests:  33%|███▎      | 1359/4096 [00:02<00:05, 471.24it/s]
Adding requests:  34%|███▍      | 1409/4096 [00:03<00:05, 477.47it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:03<00:05, 475.16it/s]
Adding requests:  37%|███▋      | 1506/4096 [00:03<00:05, 476.64it/s]
Adding requests:  38%|███▊      | 1554/4096 [00:03<00:05, 475.99it/s]
Adding requests:  39%|███▉      | 1604/4096 [00:03<00:05, 482.37it/s]
Adding requests:  40%|████      | 1653/4096 [00:03<00:05, 478.26it/s]
Adding requests:  42%|████▏     | 1701/4096 [00:03<00:05, 473.92it/s]
Adding requests:  43%|████▎     | 1749/4096 [00:03<00:05, 454.62it/s]
Adding requests:  44%|████▍     | 1797/4096 [00:03<00:04, 460.01it/s]
Adding requests:  45%|████▌     | 1845/4096 [00:03<00:04, 464.20it/s]
Adding requests:  46%|████▌     | 1892/4096 [00:04<00:04, 464.52it/s]
Adding requests:  47%|████▋     | 1939/4096 [00:04<00:04, 455.67it/s]
Adding requests:  49%|████▊     | 1987/4096 [00:04<00:04, 460.27it/s]
Adding requests:  50%|████▉     | 2036/4096 [00:04<00:04, 466.28it/s]
Adding requests:  51%|█████     | 2085/4096 [00:04<00:04, 471.69it/s]
Adding requests:  52%|█████▏    | 2133/4096 [00:04<00:04, 468.33it/s]
Adding requests:  53%|█████▎    | 2180/4096 [00:04<00:04, 453.31it/s]
Adding requests:  54%|█████▍    | 2228/4096 [00:04<00:04, 458.81it/s]
Adding requests:  56%|█████▌    | 2275/4096 [00:04<00:03, 461.86it/s]
Adding requests:  57%|█████▋    | 2323/4096 [00:05<00:03, 465.85it/s]
Adding requests:  58%|█████▊    | 2370/4096 [00:05<00:03, 466.13it/s]
Adding requests:  59%|█████▉    | 2417/4096 [00:05<00:03, 466.76it/s]
Adding requests:  60%|██████    | 2465/4096 [00:05<00:03, 469.65it/s]
Adding requests:  61%|██████▏   | 2512/4096 [00:05<00:03, 468.77it/s]
Adding requests:  63%|██████▎   | 2561/4096 [00:05<00:03, 472.91it/s]
Adding requests:  64%|██████▎   | 2609/4096 [00:05<00:03, 471.26it/s]
Adding requests:  65%|██████▍   | 2658/4096 [00:05<00:03, 476.07it/s]
Adding requests:  66%|██████▌   | 2706/4096 [00:05<00:02, 468.21it/s]
Adding requests:  67%|██████▋   | 2754/4096 [00:05<00:02, 468.04it/s]
Adding requests:  68%|██████▊   | 2801/4096 [00:06<00:02, 465.46it/s]
Adding requests:  70%|██████▉   | 2849/4096 [00:06<00:02, 468.28it/s]
Adding requests:  71%|███████   | 2896/4096 [00:06<00:02, 468.79it/s]
Adding requests:  72%|███████▏  | 2943/4096 [00:06<00:02, 464.81it/s]
Adding requests:  73%|███████▎  | 2991/4096 [00:06<00:02, 467.87it/s]
Adding requests:  74%|███████▍  | 3038/4096 [00:06<00:02, 466.32it/s]
Adding requests:  75%|███████▌  | 3085/4096 [00:06<00:02, 452.87it/s]
Adding requests:  76%|███████▋  | 3133/4096 [00:06<00:02, 459.63it/s]
Adding requests:  78%|███████▊  | 3180/4096 [00:06<00:01, 462.27it/s]
Adding requests:  79%|███████▉  | 3227/4096 [00:06<00:01, 462.48it/s]
Adding requests:  80%|███████▉  | 3275/4096 [00:07<00:01, 465.74it/s]
Adding requests:  81%|████████  | 3323/4096 [00:07<00:01, 467.92it/s]
Adding requests:  82%|████████▏ | 3371/4096 [00:07<00:01, 469.05it/s]
Adding requests:  83%|████████▎ | 3420/4096 [00:07<00:01, 474.01it/s]
Adding requests:  85%|████████▍ | 3468/4096 [00:07<00:01, 465.27it/s]
Adding requests:  86%|████████▌ | 3515/4096 [00:07<00:01, 466.13it/s]
Adding requests:  87%|████████▋ | 3562/4096 [00:07<00:01, 463.38it/s]
Adding requests:  88%|████████▊ | 3610/4096 [00:07<00:01, 465.55it/s]
Adding requests:  89%|████████▉ | 3657/4096 [00:07<00:00, 462.88it/s]
Adding requests:  90%|█████████ | 3705/4096 [00:07<00:00, 467.57it/s]
Adding requests:  92%|█████████▏| 3752/4096 [00:08<00:00, 466.30it/s]
Adding requests:  93%|█████████▎| 3801/4096 [00:08<00:00, 472.69it/s]
Adding requests:  94%|█████████▍| 3849/4096 [00:08<00:00, 474.41it/s]
Adding requests:  95%|█████████▌| 3897/4096 [00:08<00:00, 474.95it/s]
Adding requests:  96%|█████████▋| 3945/4096 [00:08<00:00, 474.36it/s]
Adding requests:  97%|█████████▋| 3993/4096 [00:08<00:00, 469.33it/s]
Adding requests:  99%|█████████▊| 4041/4096 [00:08<00:00, 469.61it/s]
Adding requests: 100%|█████████▉| 4088/4096 [00:08<00:00, 469.72it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 464.56it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 62167.52it/s, est. speed input: 63673823.66 toks/s, output: 62177.19 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 62063.98it/s, est. speed input: 63673823.66 toks/s, output: 62177.19 toks/s]
[rank0]:[W126 14:19:32.663273124 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 14:19:34
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:20:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=576744) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=576744) WARNING 01-26 14:20:31 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=576744) WARNING 01-26 14:20:38 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 468.88 requests/s, 480603.23 total tokens/s, 468.88 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 14:20:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:20:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:20:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:20:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:20:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:20:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:20:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:20:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:20:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:20:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:20:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:20:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:20:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:20:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:20:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:20:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:20:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:20:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:20:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=576744) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=576744) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.15it/s]
(EngineCore_DP0 pid=576744) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.15it/s]
(EngineCore_DP0 pid=576744) 
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=576744) [2026-01-26 14:20:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=576744) [rank0]:W0126 14:20:34.737000 576744 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=576744) [rank0]:W0126 14:20:34.805000 576744 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=576744) [rank0]:W0126 14:20:35.612000 576744 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=576744) [rank0]:W0126 14:20:35.710000 576744 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=576744) 2026-01-26 14:20:38,097 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=576744) 2026-01-26 14:20:38,121 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=576744) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:03,  5.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:02,  5.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:01,  9.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01, 12.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:00, 15.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:00<00:00, 17.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:00<00:00, 19.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:01<00:00, 20.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 16.14it/s]
(EngineCore_DP0 pid=576744) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00, 23.13it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 23.28it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 23.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 23.20it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 38/8192 [00:00<00:21, 374.20it/s]
Adding requests:   1%|          | 82/8192 [00:00<00:19, 408.26it/s]
Adding requests:   2%|▏         | 126/8192 [00:00<00:19, 421.93it/s]
Adding requests:   2%|▏         | 170/8192 [00:00<00:18, 428.41it/s]
Adding requests:   3%|▎         | 215/8192 [00:00<00:18, 435.03it/s]
Adding requests:   3%|▎         | 263/8192 [00:00<00:17, 447.87it/s]
Adding requests:   4%|▍         | 308/8192 [00:00<00:17, 448.01it/s]
Adding requests:   4%|▍         | 355/8192 [00:00<00:17, 454.83it/s]
Adding requests:   5%|▍         | 402/8192 [00:00<00:17, 457.58it/s]
Adding requests:   5%|▌         | 448/8192 [00:01<00:16, 456.92it/s]
Adding requests:   6%|▌         | 495/8192 [00:01<00:16, 460.05it/s]
Adding requests:   7%|▋         | 542/8192 [00:01<00:16, 452.01it/s]
Adding requests:   7%|▋         | 590/8192 [00:01<00:16, 459.49it/s]
Adding requests:   8%|▊         | 637/8192 [00:01<00:16, 461.41it/s]
Adding requests:   8%|▊         | 686/8192 [00:01<00:16, 468.97it/s]
Adding requests:   9%|▉         | 735/8192 [00:01<00:15, 473.59it/s]
Adding requests:  10%|▉         | 783/8192 [00:01<00:15, 468.87it/s]
Adding requests:  10%|█         | 830/8192 [00:01<00:16, 459.93it/s]
Adding requests:  11%|█         | 878/8192 [00:01<00:15, 463.64it/s]
Adding requests:  11%|█▏        | 927/8192 [00:02<00:15, 469.54it/s]
Adding requests:  12%|█▏        | 975/8192 [00:02<00:15, 471.19it/s]
Adding requests:  12%|█▎        | 1024/8192 [00:02<00:15, 474.59it/s]
Adding requests:  13%|█▎        | 1072/8192 [00:02<00:15, 468.59it/s]
Adding requests:  14%|█▎        | 1119/8192 [00:02<00:15, 467.81it/s]
Adding requests:  14%|█▍        | 1169/8192 [00:02<00:14, 474.07it/s]
Adding requests:  15%|█▍        | 1219/8192 [00:02<00:14, 481.33it/s]
Adding requests:  15%|█▌        | 1268/8192 [00:02<00:14, 474.87it/s]
Adding requests:  16%|█▌        | 1316/8192 [00:02<00:14, 475.25it/s]
Adding requests:  17%|█▋        | 1365/8192 [00:02<00:14, 479.26it/s]
Adding requests:  17%|█▋        | 1414/8192 [00:03<00:14, 482.01it/s]
Adding requests:  18%|█▊        | 1463/8192 [00:03<00:13, 481.33it/s]
Adding requests:  18%|█▊        | 1512/8192 [00:03<00:14, 470.92it/s]
Adding requests:  19%|█▉        | 1560/8192 [00:03<00:14, 472.61it/s]
Adding requests:  20%|█▉        | 1610/8192 [00:03<00:13, 478.45it/s]
Adding requests:  20%|██        | 1658/8192 [00:03<00:13, 475.69it/s]
Adding requests:  21%|██        | 1706/8192 [00:03<00:13, 475.22it/s]
Adding requests:  21%|██▏       | 1754/8192 [00:03<00:13, 475.40it/s]
Adding requests:  22%|██▏       | 1802/8192 [00:03<00:13, 475.89it/s]
Adding requests:  23%|██▎       | 1850/8192 [00:03<00:13, 476.19it/s]
Adding requests:  23%|██▎       | 1898/8192 [00:04<00:13, 476.65it/s]
Adding requests:  24%|██▍       | 1946/8192 [00:04<00:13, 476.20it/s]
Adding requests:  24%|██▍       | 1994/8192 [00:04<00:13, 475.39it/s]
Adding requests:  25%|██▍       | 2043/8192 [00:04<00:12, 479.09it/s]
Adding requests:  26%|██▌       | 2093/8192 [00:04<00:12, 483.24it/s]
Adding requests:  26%|██▌       | 2142/8192 [00:04<00:12, 474.92it/s]
Adding requests:  27%|██▋       | 2190/8192 [00:04<00:12, 472.34it/s]
Adding requests:  27%|██▋       | 2239/8192 [00:04<00:12, 475.45it/s]
Adding requests:  28%|██▊       | 2287/8192 [00:04<00:12, 474.33it/s]
Adding requests:  29%|██▊       | 2335/8192 [00:04<00:12, 474.33it/s]
Adding requests:  29%|██▉       | 2384/8192 [00:05<00:12, 476.91it/s]
Adding requests:  30%|██▉       | 2432/8192 [00:05<00:12, 477.54it/s]
Adding requests:  30%|███       | 2480/8192 [00:05<00:11, 476.51it/s]
Adding requests:  31%|███       | 2528/8192 [00:05<00:11, 475.15it/s]
Adding requests:  31%|███▏      | 2578/8192 [00:05<00:11, 481.27it/s]
Adding requests:  32%|███▏      | 2627/8192 [00:05<00:11, 467.79it/s]
Adding requests:  33%|███▎      | 2675/8192 [00:05<00:11, 471.15it/s]
Adding requests:  33%|███▎      | 2723/8192 [00:05<00:11, 471.05it/s]
Adding requests:  34%|███▍      | 2771/8192 [00:05<00:11, 472.47it/s]
Adding requests:  34%|███▍      | 2819/8192 [00:06<00:11, 468.26it/s]
Adding requests:  35%|███▍      | 2867/8192 [00:06<00:11, 469.84it/s]
Adding requests:  36%|███▌      | 2915/8192 [00:06<00:11, 471.91it/s]
Adding requests:  36%|███▌      | 2963/8192 [00:06<00:11, 469.05it/s]
Adding requests:  37%|███▋      | 3012/8192 [00:06<00:10, 472.77it/s]
Adding requests:  37%|███▋      | 3060/8192 [00:06<00:10, 470.89it/s]
Adding requests:  38%|███▊      | 3108/8192 [00:06<00:10, 471.34it/s]
Adding requests:  39%|███▊      | 3156/8192 [00:06<00:10, 467.19it/s]
Adding requests:  39%|███▉      | 3203/8192 [00:06<00:10, 467.16it/s]
Adding requests:  40%|███▉      | 3252/8192 [00:06<00:10, 473.53it/s]
Adding requests:  40%|████      | 3300/8192 [00:07<00:10, 474.56it/s]
Adding requests:  41%|████      | 3349/8192 [00:07<00:10, 478.37it/s]
Adding requests:  41%|████▏     | 3397/8192 [00:07<00:10, 476.12it/s]
Adding requests:  42%|████▏     | 3445/8192 [00:07<00:09, 475.75it/s]
Adding requests:  43%|████▎     | 3493/8192 [00:07<00:09, 470.83it/s]
Adding requests:  43%|████▎     | 3542/8192 [00:07<00:09, 473.70it/s]
Adding requests:  44%|████▍     | 3590/8192 [00:07<00:09, 472.25it/s]
Adding requests:  44%|████▍     | 3638/8192 [00:07<00:09, 469.53it/s]
Adding requests:  45%|████▍     | 3686/8192 [00:07<00:09, 472.17it/s]
Adding requests:  46%|████▌     | 3734/8192 [00:07<00:09, 470.19it/s]
Adding requests:  46%|████▌     | 3782/8192 [00:08<00:09, 466.94it/s]
Adding requests:  47%|████▋     | 3830/8192 [00:08<00:09, 469.54it/s]
Adding requests:  47%|████▋     | 3879/8192 [00:08<00:09, 474.21it/s]
Adding requests:  48%|████▊     | 3927/8192 [00:08<00:08, 474.23it/s]
Adding requests:  49%|████▊     | 3975/8192 [00:08<00:08, 474.01it/s]
Adding requests:  49%|████▉     | 4023/8192 [00:08<00:08, 475.12it/s]
Adding requests:  50%|████▉     | 4071/8192 [00:08<00:08, 469.35it/s]
Adding requests:  50%|█████     | 4120/8192 [00:08<00:08, 472.59it/s]
Adding requests:  51%|█████     | 4168/8192 [00:08<00:08, 474.70it/s]
Adding requests:  51%|█████▏    | 4217/8192 [00:08<00:08, 477.36it/s]
Adding requests:  52%|█████▏    | 4265/8192 [00:09<00:08, 476.38it/s]
Adding requests:  53%|█████▎    | 4313/8192 [00:09<00:08, 476.76it/s]
Adding requests:  53%|█████▎    | 4363/8192 [00:09<00:07, 482.16it/s]
Adding requests:  54%|█████▍    | 4413/8192 [00:09<00:07, 485.44it/s]
Adding requests:  54%|█████▍    | 4462/8192 [00:09<00:07, 482.66it/s]
Adding requests:  55%|█████▌    | 4511/8192 [00:09<00:07, 478.60it/s]
Adding requests:  56%|█████▌    | 4559/8192 [00:09<00:07, 474.35it/s]
Adding requests:  56%|█████▋    | 4609/8192 [00:09<00:07, 478.65it/s]
Adding requests:  57%|█████▋    | 4657/8192 [00:09<00:07, 478.92it/s]
Adding requests:  57%|█████▋    | 4705/8192 [00:09<00:07, 476.80it/s]
Adding requests:  58%|█████▊    | 4754/8192 [00:10<00:07, 478.51it/s]
Adding requests:  59%|█████▊    | 4802/8192 [00:10<00:07, 477.91it/s]
Adding requests:  59%|█████▉    | 4850/8192 [00:10<00:07, 476.00it/s]
Adding requests:  60%|█████▉    | 4898/8192 [00:10<00:06, 471.79it/s]
Adding requests:  60%|██████    | 4946/8192 [00:10<00:06, 465.26it/s]
Adding requests:  61%|██████    | 4994/8192 [00:10<00:06, 467.44it/s]
Adding requests:  62%|██████▏   | 5043/8192 [00:10<00:06, 471.89it/s]
Adding requests:  62%|██████▏   | 5093/8192 [00:10<00:06, 480.04it/s]
Adding requests:  63%|██████▎   | 5142/8192 [00:10<00:06, 476.93it/s]
Adding requests:  63%|██████▎   | 5191/8192 [00:11<00:06, 477.78it/s]
Adding requests:  64%|██████▍   | 5239/8192 [00:11<00:06, 474.74it/s]
Adding requests:  65%|██████▍   | 5287/8192 [00:11<00:06, 473.07it/s]
Adding requests:  65%|██████▌   | 5336/8192 [00:11<00:05, 477.03it/s]
Adding requests:  66%|██████▌   | 5384/8192 [00:11<00:05, 476.57it/s]
Adding requests:  66%|██████▋   | 5433/8192 [00:11<00:05, 478.64it/s]
Adding requests:  67%|██████▋   | 5481/8192 [00:11<00:05, 473.07it/s]
Adding requests:  67%|██████▋   | 5529/8192 [00:11<00:05, 471.05it/s]
Adding requests:  68%|██████▊   | 5577/8192 [00:11<00:05, 471.59it/s]
Adding requests:  69%|██████▊   | 5625/8192 [00:11<00:05, 468.45it/s]
Adding requests:  69%|██████▉   | 5672/8192 [00:12<00:05, 465.94it/s]
Adding requests:  70%|██████▉   | 5721/8192 [00:12<00:05, 471.24it/s]
Adding requests:  70%|███████   | 5770/8192 [00:12<00:05, 475.10it/s]
Adding requests:  71%|███████   | 5818/8192 [00:12<00:05, 466.94it/s]
Adding requests:  72%|███████▏  | 5866/8192 [00:12<00:04, 470.64it/s]
Adding requests:  72%|███████▏  | 5914/8192 [00:12<00:04, 473.38it/s]
Adding requests:  73%|███████▎  | 5962/8192 [00:12<00:04, 472.66it/s]
Adding requests:  73%|███████▎  | 6011/8192 [00:12<00:04, 476.64it/s]
Adding requests:  74%|███████▍  | 6060/8192 [00:12<00:04, 479.98it/s]
Adding requests:  75%|███████▍  | 6109/8192 [00:12<00:04, 466.54it/s]
Adding requests:  75%|███████▌  | 6156/8192 [00:13<00:04, 466.99it/s]
Adding requests:  76%|███████▌  | 6205/8192 [00:13<00:04, 472.85it/s]
Adding requests:  76%|███████▋  | 6255/8192 [00:13<00:04, 479.91it/s]
Adding requests:  77%|███████▋  | 6304/8192 [00:13<00:04, 464.75it/s]
Adding requests:  78%|███████▊  | 6353/8192 [00:13<00:03, 469.73it/s]
Adding requests:  78%|███████▊  | 6401/8192 [00:13<00:03, 471.93it/s]
Adding requests:  79%|███████▊  | 6451/8192 [00:13<00:03, 478.29it/s]
Adding requests:  79%|███████▉  | 6501/8192 [00:13<00:03, 481.28it/s]
Adding requests:  80%|███████▉  | 6550/8192 [00:13<00:03, 482.79it/s]
Adding requests:  81%|████████  | 6599/8192 [00:13<00:03, 480.85it/s]
Adding requests:  81%|████████  | 6648/8192 [00:14<00:03, 481.16it/s]
Adding requests:  82%|████████▏ | 6697/8192 [00:14<00:03, 479.51it/s]
Adding requests:  82%|████████▏ | 6745/8192 [00:14<00:03, 477.31it/s]
Adding requests:  83%|████████▎ | 6795/8192 [00:14<00:02, 483.16it/s]
Adding requests:  84%|████████▎ | 6844/8192 [00:14<00:02, 482.26it/s]
Adding requests:  84%|████████▍ | 6893/8192 [00:14<00:02, 484.31it/s]
Adding requests:  85%|████████▍ | 6943/8192 [00:14<00:02, 487.48it/s]
Adding requests:  85%|████████▌ | 6992/8192 [00:14<00:02, 484.42it/s]
Adding requests:  86%|████████▌ | 7041/8192 [00:14<00:02, 479.21it/s]
Adding requests:  87%|████████▋ | 7090/8192 [00:15<00:02, 479.28it/s]
Adding requests:  87%|████████▋ | 7139/8192 [00:15<00:02, 480.85it/s]
Adding requests:  88%|████████▊ | 7188/8192 [00:15<00:02, 476.63it/s]
Adding requests:  88%|████████▊ | 7236/8192 [00:15<00:02, 477.17it/s]
Adding requests:  89%|████████▉ | 7284/8192 [00:15<00:01, 470.45it/s]
Adding requests:  90%|████████▉ | 7333/8192 [00:15<00:01, 475.04it/s]
Adding requests:  90%|█████████ | 7381/8192 [00:15<00:01, 473.50it/s]
Adding requests:  91%|█████████ | 7432/8192 [00:15<00:01, 482.20it/s]
Adding requests:  91%|█████████▏| 7481/8192 [00:15<00:01, 482.13it/s]
Adding requests:  92%|█████████▏| 7530/8192 [00:15<00:01, 481.20it/s]
Adding requests:  93%|█████████▎| 7579/8192 [00:16<00:01, 479.13it/s]
Adding requests:  93%|█████████▎| 7627/8192 [00:16<00:01, 474.98it/s]
Adding requests:  94%|█████████▎| 7677/8192 [00:16<00:01, 481.08it/s]
Adding requests:  94%|█████████▍| 7726/8192 [00:16<00:00, 481.46it/s]
Adding requests:  95%|█████████▍| 7775/8192 [00:16<00:00, 476.41it/s]
Adding requests:  95%|█████████▌| 7823/8192 [00:16<00:00, 474.22it/s]
Adding requests:  96%|█████████▌| 7871/8192 [00:16<00:00, 460.15it/s]
Adding requests:  97%|█████████▋| 7918/8192 [00:16<00:00, 462.19it/s]
Adding requests:  97%|█████████▋| 7966/8192 [00:16<00:00, 465.97it/s]
Adding requests:  98%|█████████▊| 8013/8192 [00:16<00:00, 466.20it/s]
Adding requests:  98%|█████████▊| 8060/8192 [00:17<00:00, 466.19it/s]
Adding requests:  99%|█████████▉| 8109/8192 [00:17<00:00, 472.70it/s]
Adding requests: 100%|█████████▉| 8157/8192 [00:17<00:00, 472.95it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 472.60it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  74%|███████▍  | 6102/8192 [00:00<00:00, 61013.80it/s, est. speed input: 62482156.08 toks/s, output: 61014.97 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 61013.80it/s, est. speed input: 62500216.87 toks/s, output: 61032.98 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 60976.34it/s, est. speed input: 62500216.87 toks/s, output: 61032.98 toks/s]
[rank0]:[W126 14:20:59.644779059 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 15:25:54
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:26:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=673130) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=673130) WARNING 01-26 15:26:17 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=673130) WARNING 01-26 15:26:26 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 34.14 requests/s, 17514.98 total tokens/s, 34.14 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 15:26:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:26:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:26:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:26:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:26:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:26:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:26:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:26:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:26:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:26:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:26:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:26:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:26:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:26:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:26:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:26:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:26:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:26:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=673130) [2026-01-26 15:26:09] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=673130) [2026-01-26 15:26:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=673130) [2026-01-26 15:26:09] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=673130) [2026-01-26 15:26:09] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=673130) [2026-01-26 15:26:09] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=673130) [2026-01-26 15:26:09] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=673130) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=673130) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.05it/s]
(EngineCore_DP0 pid=673130) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.05it/s]
(EngineCore_DP0 pid=673130) 
(EngineCore_DP0 pid=673130) [2026-01-26 15:26:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=673130) [2026-01-26 15:26:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=673130) [2026-01-26 15:26:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=673130) [2026-01-26 15:26:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=673130) [2026-01-26 15:26:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=673130) [2026-01-26 15:26:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=673130) [2026-01-26 15:26:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=673130) [2026-01-26 15:26:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=673130) 2026-01-26 15:26:26,750 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=673130) 2026-01-26 15:26:26,774 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=673130) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.52it/s]
(EngineCore_DP0 pid=673130) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.25it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  53%|█████▎    | 68/128 [00:00<00:00, 678.25it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 339.38it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:01, 75.62it/s, est. speed input: 38718.86 toks/s, output: 75.62 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:02, 46.70it/s, est. speed input: 25366.16 toks/s, output: 49.54 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 42.21it/s, est. speed input: 23138.37 toks/s, output: 45.19 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 40.35it/s, est. speed input: 22203.42 toks/s, output: 43.37 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 39.25it/s, est. speed input: 21623.27 toks/s, output: 42.23 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 38.34it/s, est. speed input: 21167.37 toks/s, output: 41.34 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 37.89it/s, est. speed input: 20907.14 toks/s, output: 40.83 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 37.56it/s, est. speed input: 20699.30 toks/s, output: 40.43 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 37.30it/s, est. speed input: 20527.46 toks/s, output: 40.09 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 37.16it/s, est. speed input: 20390.06 toks/s, output: 39.82 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 37.02it/s, est. speed input: 20268.61 toks/s, output: 39.59 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 36.94it/s, est. speed input: 20165.91 toks/s, output: 39.39 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 36.91it/s, est. speed input: 20080.74 toks/s, output: 39.22 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 36.83it/s, est. speed input: 19999.17 toks/s, output: 39.06 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 36.82it/s, est. speed input: 19931.48 toks/s, output: 38.93 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 36.77it/s, est. speed input: 19867.85 toks/s, output: 38.80 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 36.78it/s, est. speed input: 19814.13 toks/s, output: 38.70 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 36.84it/s, est. speed input: 19770.99 toks/s, output: 38.62 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 36.85it/s, est. speed input: 19728.91 toks/s, output: 38.53 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 36.82it/s, est. speed input: 19687.79 toks/s, output: 38.45 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 36.81it/s, est. speed input: 19651.23 toks/s, output: 38.38 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 36.81it/s, est. speed input: 19618.21 toks/s, output: 38.32 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 36.74it/s, est. speed input: 19582.88 toks/s, output: 38.25 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 36.82it/s, est. speed input: 19558.59 toks/s, output: 38.20 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 36.76it/s, est. speed input: 19529.04 toks/s, output: 38.14 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 36.78it/s, est. speed input: 19505.04 toks/s, output: 38.10 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 36.75it/s, est. speed input: 19480.25 toks/s, output: 38.05 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 36.77it/s, est. speed input: 19459.17 toks/s, output: 38.01 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.77it/s, est. speed input: 19442.71 toks/s, output: 37.97 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.97it/s, est. speed input: 19442.71 toks/s, output: 37.97 toks/s]
[rank0]:[W126 15:26:33.299596960 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 15:26:34
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:26:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=674338) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=674338) WARNING 01-26 15:26:58 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=674338) WARNING 01-26 15:27:07 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 34.73 requests/s, 35602.66 total tokens/s, 34.73 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 15:26:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:26:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:26:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:26:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:26:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:26:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:26:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:26:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:26:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:26:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:26:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:26:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:26:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:26:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:26:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:26:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:26:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:26:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:26:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=674338) [2026-01-26 15:26:50] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=674338) [2026-01-26 15:26:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=674338) [2026-01-26 15:26:50] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=674338) [2026-01-26 15:26:50] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=674338) [2026-01-26 15:26:50] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=674338) [2026-01-26 15:26:50] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=674338) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=674338) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.11it/s]
(EngineCore_DP0 pid=674338) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.11it/s]
(EngineCore_DP0 pid=674338) 
(EngineCore_DP0 pid=674338) [2026-01-26 15:26:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=674338) [2026-01-26 15:26:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=674338) [2026-01-26 15:26:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=674338) [2026-01-26 15:26:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=674338) [2026-01-26 15:26:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=674338) [2026-01-26 15:26:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=674338) [2026-01-26 15:26:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=674338) [2026-01-26 15:26:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=674338) 2026-01-26 15:27:07,699 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=674338) 2026-01-26 15:27:07,722 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=674338) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.88it/s]
(EngineCore_DP0 pid=674338) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.00it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 7/128 [00:00<00:03, 30.98it/s]
Adding requests:  34%|███▎      | 43/128 [00:00<00:00, 157.45it/s]
Adding requests:  66%|██████▋   | 85/128 [00:00<00:00, 250.16it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 309.46it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 241.85it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:00, 147.08it/s, est. speed input: 150624.13 toks/s, output: 147.09 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 54.89it/s, est. speed input: 62447.67 toks/s, output: 60.98 toks/s]   
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 47.31it/s, est. speed input: 54525.10 toks/s, output: 53.25 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 43.88it/s, est. speed input: 51112.97 toks/s, output: 49.91 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 41.85it/s, est. speed input: 49129.67 toks/s, output: 47.98 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 40.53it/s, est. speed input: 47864.39 toks/s, output: 46.74 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 39.60it/s, est. speed input: 46893.52 toks/s, output: 45.79 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 38.77it/s, est. speed input: 46051.24 toks/s, output: 44.97 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 38.19it/s, est. speed input: 45460.40 toks/s, output: 44.39 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 37.70it/s, est. speed input: 44939.29 toks/s, output: 43.89 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 37.29it/s, est. speed input: 44470.70 toks/s, output: 43.43 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:01, 36.98it/s, est. speed input: 44054.07 toks/s, output: 43.02 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 36.82it/s, est. speed input: 43698.32 toks/s, output: 42.67 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 36.71it/s, est. speed input: 43379.14 toks/s, output: 42.36 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 36.65it/s, est. speed input: 43092.88 toks/s, output: 42.08 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 36.60it/s, est. speed input: 42831.91 toks/s, output: 41.83 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 36.58it/s, est. speed input: 42596.56 toks/s, output: 41.60 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 36.55it/s, est. speed input: 42379.24 toks/s, output: 41.39 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 36.53it/s, est. speed input: 42178.86 toks/s, output: 41.19 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 36.61it/s, est. speed input: 42007.37 toks/s, output: 41.02 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 36.54it/s, est. speed input: 41830.03 toks/s, output: 40.85 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 36.45it/s, est. speed input: 41661.22 toks/s, output: 40.68 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.45it/s, est. speed input: 41547.34 toks/s, output: 40.57 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.57it/s, est. speed input: 41547.34 toks/s, output: 40.57 toks/s]
[rank0]:[W126 15:27:13.439466079 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 15:27:14
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:27:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=675468) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=675468) WARNING 01-26 15:27:38 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=675468) WARNING 01-26 15:27:48 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 67.51 requests/s, 69199.36 total tokens/s, 67.51 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 15:27:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:27:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:27:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:27:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:27:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:27:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:27:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:27:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:27:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:27:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:27:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:27:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:27:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:27:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:27:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:27:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:27:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:27:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:27:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:27:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:27:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:27:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:27:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:27:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:27:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:27:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:27:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:27:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=675468) [2026-01-26 15:27:30] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=675468) [2026-01-26 15:27:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=675468) [2026-01-26 15:27:30] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=675468) [2026-01-26 15:27:30] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=675468) [2026-01-26 15:27:30] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=675468) [2026-01-26 15:27:30] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=675468) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=675468) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.04it/s]
(EngineCore_DP0 pid=675468) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.04it/s]
(EngineCore_DP0 pid=675468) 
(EngineCore_DP0 pid=675468) [2026-01-26 15:27:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=675468) [2026-01-26 15:27:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=675468) [2026-01-26 15:27:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=675468) [2026-01-26 15:27:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=675468) [2026-01-26 15:27:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=675468) [2026-01-26 15:27:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=675468) [2026-01-26 15:27:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=675468) [2026-01-26 15:27:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=675468) 2026-01-26 15:27:48,241 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=675468) 2026-01-26 15:27:48,263 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=675468) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 17.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.87it/s]
(EngineCore_DP0 pid=675468) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.01it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<01:06,  3.85it/s]
Adding requests:  15%|█▌        | 39/256 [00:00<00:01, 136.91it/s]
Adding requests:  30%|███       | 77/256 [00:00<00:00, 218.45it/s]
Adding requests:  47%|████▋     | 121/256 [00:00<00:00, 289.19it/s]
Adding requests:  65%|██████▍   | 166/256 [00:00<00:00, 338.24it/s]
Adding requests:  82%|████████▏ | 211/256 [00:00<00:00, 370.63it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 297.39it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:00<00:00, 435.52it/s, est. speed input: 445998.68 toks/s, output: 435.52 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:00<00:01, 114.24it/s, est. speed input: 131541.31 toks/s, output: 128.46 toks/s]
Processed prompts:  43%|████▎     | 111/256 [00:00<00:01, 100.11it/s, est. speed input: 116046.15 toks/s, output: 113.33 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:01<00:01, 92.81it/s, est. speed input: 108979.25 toks/s, output: 106.42 toks/s] 
Processed prompts:  55%|█████▍    | 140/256 [00:01<00:01, 86.42it/s, est. speed input: 103727.64 toks/s, output: 101.30 toks/s]
Processed prompts:  59%|█████▉    | 151/256 [00:01<00:01, 85.58it/s, est. speed input: 102029.27 toks/s, output: 99.64 toks/s] 
Processed prompts:  63%|██████▎   | 161/256 [00:01<00:01, 83.13it/s, est. speed input: 99993.31 toks/s, output: 97.65 toks/s] 
Processed prompts:  66%|██████▋   | 170/256 [00:01<00:01, 79.21it/s, est. speed input: 97675.46 toks/s, output: 95.39 toks/s]
Processed prompts:  70%|██████▉   | 179/256 [00:01<00:00, 80.25it/s, est. speed input: 97004.73 toks/s, output: 94.73 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:02<00:00, 76.60it/s, est. speed input: 95158.07 toks/s, output: 92.93 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:02<00:00, 76.16it/s, est. speed input: 94225.93 toks/s, output: 92.02 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:02<00:00, 76.00it/s, est. speed input: 93427.07 toks/s, output: 91.24 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:02<00:00, 75.63it/s, est. speed input: 92648.67 toks/s, output: 90.48 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:02<00:00, 75.44it/s, est. speed input: 91956.87 toks/s, output: 89.80 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 75.37it/s, est. speed input: 91334.45 toks/s, output: 89.19 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 75.19it/s, est. speed input: 90740.31 toks/s, output: 88.61 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:02<00:00, 75.17it/s, est. speed input: 90208.20 toks/s, output: 88.09 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:02<00:00, 75.12it/s, est. speed input: 89711.06 toks/s, output: 87.61 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 75.12it/s, est. speed input: 89480.80 toks/s, output: 87.38 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 87.38it/s, est. speed input: 89480.80 toks/s, output: 87.38 toks/s]
[rank0]:[W126 15:27:53.201266107 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 15:27:55
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:28:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=676611) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=676611) WARNING 01-26 15:28:20 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=676611) WARNING 01-26 15:28:30 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 137.47 requests/s, 140906.69 total tokens/s, 137.47 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 15:28:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:28:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:28:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:28:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:28:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:28:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:28:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:28:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:28:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:28:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:28:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:28:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:28:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:28:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:28:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:28:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:28:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:28:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=676611) [2026-01-26 15:28:12] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=676611) [2026-01-26 15:28:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=676611) [2026-01-26 15:28:12] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=676611) [2026-01-26 15:28:12] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=676611) [2026-01-26 15:28:12] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=676611) [2026-01-26 15:28:12] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=676611) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=676611) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.13it/s]
(EngineCore_DP0 pid=676611) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.13it/s]
(EngineCore_DP0 pid=676611) 
(EngineCore_DP0 pid=676611) [2026-01-26 15:28:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=676611) [2026-01-26 15:28:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=676611) [2026-01-26 15:28:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=676611) [2026-01-26 15:28:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=676611) [2026-01-26 15:28:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=676611) [2026-01-26 15:28:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=676611) [2026-01-26 15:28:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=676611) [2026-01-26 15:28:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=676611) 2026-01-26 15:28:30,117 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=676611) 2026-01-26 15:28:30,141 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=676611) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.32it/s]
(EngineCore_DP0 pid=676611) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 15.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 16.20it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|▋         | 33/512 [00:00<00:01, 324.87it/s]
Adding requests:  15%|█▍        | 76/512 [00:00<00:01, 386.18it/s]
Adding requests:  24%|██▎       | 121/512 [00:00<00:00, 411.97it/s]
Adding requests:  32%|███▏      | 163/512 [00:00<00:00, 365.22it/s]
Adding requests:  40%|████      | 207/512 [00:00<00:00, 389.73it/s]
Adding requests:  50%|████▉     | 254/512 [00:00<00:00, 414.32it/s]
Adding requests:  58%|█████▊    | 299/512 [00:00<00:00, 421.40it/s]
Adding requests:  67%|██████▋   | 344/512 [00:00<00:00, 429.74it/s]
Adding requests:  76%|███████▋  | 391/512 [00:00<00:00, 441.28it/s]
Adding requests:  85%|████████▌ | 437/512 [00:01<00:00, 445.46it/s]
Adding requests:  95%|█████████▍| 484/512 [00:01<00:00, 451.68it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 423.66it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:00<00:00, 1562.22it/s, est. speed input: 1599825.03 toks/s, output: 1562.25 toks/s]
Processed prompts:  62%|██████▏   | 319/512 [00:01<00:00, 238.48it/s, est. speed input: 280405.31 toks/s, output: 273.83 toks/s]   
Processed prompts:  77%|███████▋  | 393/512 [00:01<00:00, 201.30it/s, est. speed input: 239659.81 toks/s, output: 234.04 toks/s]
Processed prompts:  86%|████████▌ | 440/512 [00:02<00:00, 186.31it/s, est. speed input: 224711.27 toks/s, output: 219.44 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:02<00:00, 174.09it/s, est. speed input: 214635.98 toks/s, output: 209.60 toks/s]
Processed prompts:  98%|█████████▊| 501/512 [00:02<00:00, 172.63it/s, est. speed input: 211605.45 toks/s, output: 206.65 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 172.63it/s, est. speed input: 208503.26 toks/s, output: 203.62 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 203.61it/s, est. speed input: 208503.26 toks/s, output: 203.62 toks/s]
[rank0]:[W126 15:28:36.530687354 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 15:28:38
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:28:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=677774) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=677774) WARNING 01-26 15:29:04 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=677774) WARNING 01-26 15:29:14 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 208.41 requests/s, 213619.21 total tokens/s, 208.41 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 15:28:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:28:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:28:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:28:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:28:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:28:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:28:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:28:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:28:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:28:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:28:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:28:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:28:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:28:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:28:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:28:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:28:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:28:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:28:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=677774) [2026-01-26 15:28:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=677774) [2026-01-26 15:28:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=677774) [2026-01-26 15:28:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=677774) [2026-01-26 15:28:57] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=677774) [2026-01-26 15:28:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=677774) [2026-01-26 15:28:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=677774) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=677774) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=677774) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=677774) 
(EngineCore_DP0 pid=677774) [2026-01-26 15:28:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=677774) [2026-01-26 15:28:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=677774) [2026-01-26 15:28:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=677774) [2026-01-26 15:28:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=677774) [2026-01-26 15:28:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=677774) [2026-01-26 15:28:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=677774) [2026-01-26 15:28:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=677774) [2026-01-26 15:28:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=677774) 2026-01-26 15:29:14,280 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=677774) 2026-01-26 15:29:14,304 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=677774) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 17.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 18.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 16.31it/s]
(EngineCore_DP0 pid=677774) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 14.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.00it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 38/1024 [00:00<00:02, 374.25it/s]
Adding requests:   8%|▊         | 82/1024 [00:00<00:02, 412.18it/s]
Adding requests:  12%|█▏        | 127/1024 [00:00<00:02, 427.48it/s]
Adding requests:  17%|█▋        | 171/1024 [00:00<00:01, 430.82it/s]
Adding requests:  21%|██        | 216/1024 [00:00<00:01, 435.27it/s]
Adding requests:  26%|██▌       | 263/1024 [00:00<00:01, 446.09it/s]
Adding requests:  30%|███       | 308/1024 [00:00<00:01, 444.65it/s]
Adding requests:  35%|███▍      | 355/1024 [00:00<00:01, 450.34it/s]
Adding requests:  39%|███▉      | 401/1024 [00:00<00:01, 449.56it/s]
Adding requests:  44%|████▎     | 447/1024 [00:01<00:01, 451.62it/s]
Adding requests:  48%|████▊     | 493/1024 [00:01<00:01, 454.03it/s]
Adding requests:  53%|█████▎    | 539/1024 [00:01<00:01, 445.13it/s]
Adding requests:  57%|█████▋    | 587/1024 [00:01<00:00, 455.35it/s]
Adding requests:  62%|██████▏   | 634/1024 [00:01<00:00, 457.76it/s]
Adding requests:  67%|██████▋   | 682/1024 [00:01<00:00, 463.20it/s]
Adding requests:  71%|███████▏  | 730/1024 [00:01<00:00, 465.94it/s]
Adding requests:  76%|███████▌  | 777/1024 [00:01<00:00, 460.26it/s]
Adding requests:  80%|████████  | 824/1024 [00:01<00:00, 452.61it/s]
Adding requests:  85%|████████▌ | 871/1024 [00:01<00:00, 455.03it/s]
Adding requests:  90%|████████▉ | 919/1024 [00:02<00:00, 461.11it/s]
Adding requests:  94%|█████████▍| 966/1024 [00:02<00:00, 461.86it/s]
Adding requests:  99%|█████████▉| 1013/1024 [00:02<00:00, 463.18it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 451.87it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:00<00:00, 3818.64it/s, est. speed input: 3910504.65 toks/s, output: 3818.70 toks/s]
Processed prompts:  84%|████████▍ | 864/1024 [00:01<00:00, 387.59it/s, est. speed input: 467139.17 toks/s, output: 456.19 toks/s]   
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 387.59it/s, est. speed input: 396285.81 toks/s, output: 387.00 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 386.98it/s, est. speed input: 396285.81 toks/s, output: 387.00 toks/s]
[rank0]:[W126 15:29:21.904948564 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 15:29:23
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:29:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=679012) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=679012) WARNING 01-26 15:29:55 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=679012) WARNING 01-26 15:30:04 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 225.66 requests/s, 231301.48 total tokens/s, 225.66 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 15:29:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:29:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:29:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:29:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:29:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:29:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:29:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:29:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:29:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:29:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:29:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:29:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:29:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:29:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:29:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:29:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:29:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:29:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:29:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:29:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:29:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:29:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:29:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:29:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:29:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:29:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:29:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:29:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=679012) [2026-01-26 15:29:47] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=679012) [2026-01-26 15:29:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=679012) [2026-01-26 15:29:47] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=679012) [2026-01-26 15:29:47] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=679012) [2026-01-26 15:29:47] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=679012) [2026-01-26 15:29:47] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=679012) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=679012) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=679012) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=679012) 
(EngineCore_DP0 pid=679012) [2026-01-26 15:29:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=679012) [2026-01-26 15:29:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=679012) [2026-01-26 15:29:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=679012) [2026-01-26 15:29:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=679012) [2026-01-26 15:29:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=679012) [2026-01-26 15:29:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=679012) [2026-01-26 15:29:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=679012) [2026-01-26 15:29:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=679012) 2026-01-26 15:30:04,808 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=679012) 2026-01-26 15:30:04,832 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=679012) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 16.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 17.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 15.22it/s]
(EngineCore_DP0 pid=679012) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 14.07it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 15.59it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 15.88it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 39/2048 [00:00<00:05, 380.54it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 418.19it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:04, 430.81it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:04, 434.44it/s]
Adding requests:  11%|█         | 220/2048 [00:00<00:04, 443.29it/s]
Adding requests:  13%|█▎        | 268/2048 [00:00<00:03, 454.20it/s]
Adding requests:  15%|█▌        | 314/2048 [00:00<00:03, 453.29it/s]
Adding requests:  18%|█▊        | 360/2048 [00:00<00:03, 454.37it/s]
Adding requests:  20%|█▉        | 408/2048 [00:00<00:03, 460.22it/s]
Adding requests:  22%|██▏       | 455/2048 [00:01<00:03, 462.07it/s]
Adding requests:  25%|██▍       | 502/2048 [00:01<00:03, 462.29it/s]
Adding requests:  27%|██▋       | 549/2048 [00:01<00:03, 456.66it/s]
Adding requests:  29%|██▉       | 598/2048 [00:01<00:03, 463.92it/s]
Adding requests:  31%|███▏      | 645/2048 [00:01<00:03, 463.14it/s]
Adding requests:  34%|███▍      | 694/2048 [00:01<00:02, 470.99it/s]
Adding requests:  36%|███▌      | 742/2048 [00:01<00:02, 468.80it/s]
Adding requests:  39%|███▊      | 789/2048 [00:01<00:02, 468.33it/s]
Adding requests:  41%|████      | 836/2048 [00:01<00:02, 458.35it/s]
Adding requests:  43%|████▎     | 885/2048 [00:01<00:02, 465.59it/s]
Adding requests:  46%|████▌     | 932/2048 [00:02<00:02, 463.43it/s]
Adding requests:  48%|████▊     | 980/2048 [00:02<00:02, 464.35it/s]
Adding requests:  50%|█████     | 1029/2048 [00:02<00:02, 469.80it/s]
Adding requests:  53%|█████▎    | 1076/2048 [00:02<00:02, 464.60it/s]
Adding requests:  55%|█████▍    | 1123/2048 [00:02<00:01, 464.32it/s]
Adding requests:  57%|█████▋    | 1172/2048 [00:02<00:01, 469.82it/s]
Adding requests:  60%|█████▉    | 1222/2048 [00:02<00:01, 476.30it/s]
Adding requests:  62%|██████▏   | 1270/2048 [00:02<00:01, 471.41it/s]
Adding requests:  64%|██████▍   | 1319/2048 [00:02<00:01, 473.89it/s]
Adding requests:  67%|██████▋   | 1367/2048 [00:02<00:01, 468.78it/s]
Adding requests:  69%|██████▉   | 1416/2048 [00:03<00:01, 471.36it/s]
Adding requests:  72%|███████▏  | 1465/2048 [00:03<00:01, 474.79it/s]
Adding requests:  74%|███████▍  | 1514/2048 [00:03<00:01, 477.88it/s]
Adding requests:  76%|███████▋  | 1562/2048 [00:03<00:01, 476.67it/s]
Adding requests:  79%|███████▊  | 1611/2048 [00:03<00:00, 480.01it/s]
Adding requests:  81%|████████  | 1660/2048 [00:03<00:00, 478.24it/s]
Adding requests:  83%|████████▎ | 1708/2048 [00:03<00:00, 476.22it/s]
Adding requests:  86%|████████▌ | 1756/2048 [00:03<00:00, 464.13it/s]
Adding requests:  88%|████████▊ | 1803/2048 [00:03<00:00, 464.14it/s]
Adding requests:  90%|█████████ | 1850/2048 [00:03<00:00, 465.21it/s]
Adding requests:  93%|█████████▎| 1897/2048 [00:04<00:00, 465.29it/s]
Adding requests:  95%|█████████▍| 1944/2048 [00:04<00:00, 464.66it/s]
Adding requests:  97%|█████████▋| 1991/2048 [00:04<00:00, 464.84it/s]
Adding requests: 100%|█████████▉| 2039/2048 [00:04<00:00, 467.47it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 464.26it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:00<00:00, 8488.32it/s, est. speed input: 8692654.84 toks/s, output: 8488.47 toks/s]
Processed prompts:  90%|████████▉ | 1843/2048 [00:03<00:00, 408.18it/s, est. speed input: 494072.49 toks/s, output: 482.49 toks/s]  
Processed prompts: 100%|██████████| 2048/2048 [00:04<00:00, 408.18it/s, est. speed input: 449776.47 toks/s, output: 439.23 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:04<00:00, 439.22it/s, est. speed input: 449776.47 toks/s, output: 439.23 toks/s]
[rank0]:[W126 15:30:16.743523358 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 15:30:18
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:30:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=680455) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=680455) WARNING 01-26 15:30:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=680455) WARNING 01-26 15:31:09 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 234.90 requests/s, 240770.12 total tokens/s, 234.90 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 15:30:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:30:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:30:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:30:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:30:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:30:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:30:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:30:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:30:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:30:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:30:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:30:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:30:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:30:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:30:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:30:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:30:50] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:30:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:30:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:30:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:30:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:30:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:30:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:30:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:30:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:30:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:30:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:30:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=680455) [2026-01-26 15:30:51] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=680455) [2026-01-26 15:30:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=680455) [2026-01-26 15:30:51] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=680455) [2026-01-26 15:30:51] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=680455) [2026-01-26 15:30:51] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=680455) [2026-01-26 15:30:51] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=680455) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=680455) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=680455) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=680455) 
(EngineCore_DP0 pid=680455) [2026-01-26 15:30:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=680455) [2026-01-26 15:30:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=680455) [2026-01-26 15:30:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=680455) [2026-01-26 15:30:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=680455) [2026-01-26 15:30:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=680455) [2026-01-26 15:30:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=680455) [2026-01-26 15:30:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=680455) [2026-01-26 15:30:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=680455) [rank0]:W0126 15:31:04.355000 680455 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=680455) [rank0]:W0126 15:31:04.446000 680455 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=680455) [rank0]:W0126 15:31:05.384000 680455 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=680455) [rank0]:W0126 15:31:05.511000 680455 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=680455) 2026-01-26 15:31:08,939 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=680455) 2026-01-26 15:31:08,965 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=680455) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 15.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 14.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 15.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 16.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 15.24it/s]
(EngineCore_DP0 pid=680455) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.74it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.89it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 18.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 18.07it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 39/4096 [00:00<00:10, 380.52it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 416.84it/s]
Adding requests:   3%|▎         | 130/4096 [00:00<00:09, 433.64it/s]
Adding requests:   4%|▍         | 175/4096 [00:00<00:08, 436.82it/s]
Adding requests:   5%|▌         | 221/4096 [00:00<00:08, 444.48it/s]
Adding requests:   7%|▋         | 268/4096 [00:00<00:08, 452.28it/s]
Adding requests:   8%|▊         | 314/4096 [00:00<00:08, 449.55it/s]
Adding requests:   9%|▉         | 361/4096 [00:00<00:08, 453.52it/s]
Adding requests:  10%|▉         | 408/4096 [00:00<00:08, 458.18it/s]
Adding requests:  11%|█         | 455/4096 [00:01<00:07, 460.97it/s]
Adding requests:  12%|█▏        | 502/4096 [00:01<00:07, 459.71it/s]
Adding requests:  13%|█▎        | 548/4096 [00:01<00:07, 454.64it/s]
Adding requests:  15%|█▍        | 596/4096 [00:01<00:07, 460.89it/s]
Adding requests:  16%|█▌        | 644/4096 [00:01<00:07, 466.14it/s]
Adding requests:  17%|█▋        | 693/4096 [00:01<00:07, 472.81it/s]
Adding requests:  18%|█▊        | 741/4096 [00:01<00:07, 471.77it/s]
Adding requests:  19%|█▉        | 789/4096 [00:01<00:07, 470.45it/s]
Adding requests:  20%|██        | 837/4096 [00:01<00:07, 461.05it/s]
Adding requests:  22%|██▏       | 886/4096 [00:01<00:06, 467.21it/s]
Adding requests:  23%|██▎       | 935/4096 [00:02<00:06, 471.37it/s]
Adding requests:  24%|██▍       | 983/4096 [00:02<00:06, 465.03it/s]
Adding requests:  25%|██▌       | 1032/4096 [00:02<00:06, 468.98it/s]
Adding requests:  26%|██▋       | 1079/4096 [00:02<00:06, 467.11it/s]
Adding requests:  27%|██▋       | 1126/4096 [00:02<00:06, 464.88it/s]
Adding requests:  29%|██▊       | 1175/4096 [00:02<00:06, 472.01it/s]
Adding requests:  30%|██▉       | 1223/4096 [00:02<00:06, 472.29it/s]
Adding requests:  31%|███       | 1271/4096 [00:02<00:06, 468.85it/s]
Adding requests:  32%|███▏      | 1319/4096 [00:02<00:05, 471.18it/s]
Adding requests:  33%|███▎      | 1368/4096 [00:02<00:05, 474.37it/s]
Adding requests:  35%|███▍      | 1416/4096 [00:03<00:05, 475.31it/s]
Adding requests:  36%|███▌      | 1465/4096 [00:03<00:05, 478.47it/s]
Adding requests:  37%|███▋      | 1514/4096 [00:03<00:05, 479.64it/s]
Adding requests:  38%|███▊      | 1562/4096 [00:03<00:05, 479.36it/s]
Adding requests:  39%|███▉      | 1611/4096 [00:03<00:05, 481.73it/s]
Adding requests:  41%|████      | 1660/4096 [00:03<00:05, 480.89it/s]
Adding requests:  42%|████▏     | 1709/4096 [00:03<00:04, 478.01it/s]
Adding requests:  43%|████▎     | 1757/4096 [00:03<00:04, 477.56it/s]
Adding requests:  44%|████▍     | 1805/4096 [00:03<00:04, 476.82it/s]
Adding requests:  45%|████▌     | 1853/4096 [00:03<00:04, 477.62it/s]
Adding requests:  46%|████▋     | 1902/4096 [00:04<00:04, 476.68it/s]
Adding requests:  48%|████▊     | 1950/4096 [00:04<00:04, 476.83it/s]
Adding requests:  49%|████▉     | 1998/4096 [00:04<00:04, 476.45it/s]
Adding requests:  50%|████▉     | 2047/4096 [00:04<00:04, 478.36it/s]
Adding requests:  51%|█████     | 2095/4096 [00:04<00:04, 470.49it/s]
Adding requests:  52%|█████▏    | 2143/4096 [00:04<00:04, 467.81it/s]
Adding requests:  53%|█████▎    | 2190/4096 [00:04<00:04, 464.98it/s]
Adding requests:  55%|█████▍    | 2239/4096 [00:04<00:03, 471.20it/s]
Adding requests:  56%|█████▌    | 2287/4096 [00:04<00:03, 469.60it/s]
Adding requests:  57%|█████▋    | 2335/4096 [00:05<00:03, 462.93it/s]
Adding requests:  58%|█████▊    | 2382/4096 [00:05<00:03, 453.17it/s]
Adding requests:  59%|█████▉    | 2430/4096 [00:05<00:03, 459.56it/s]
Adding requests:  60%|██████    | 2477/4096 [00:05<00:03, 461.49it/s]
Adding requests:  62%|██████▏   | 2524/4096 [00:05<00:03, 463.43it/s]
Adding requests:  63%|██████▎   | 2573/4096 [00:05<00:03, 469.80it/s]
Adding requests:  64%|██████▍   | 2621/4096 [00:05<00:03, 469.28it/s]
Adding requests:  65%|██████▌   | 2670/4096 [00:05<00:03, 474.48it/s]
Adding requests:  66%|██████▋   | 2718/4096 [00:05<00:02, 468.83it/s]
Adding requests:  68%|██████▊   | 2766/4096 [00:05<00:02, 470.98it/s]
Adding requests:  69%|██████▊   | 2814/4096 [00:06<00:02, 466.87it/s]
Adding requests:  70%|██████▉   | 2861/4096 [00:06<00:02, 467.78it/s]
Adding requests:  71%|███████   | 2909/4096 [00:06<00:02, 471.25it/s]
Adding requests:  72%|███████▏  | 2957/4096 [00:06<00:02, 467.61it/s]
Adding requests:  73%|███████▎  | 3005/4096 [00:06<00:02, 470.61it/s]
Adding requests:  75%|███████▍  | 3053/4096 [00:06<00:02, 470.80it/s]
Adding requests:  76%|███████▌  | 3101/4096 [00:06<00:02, 470.32it/s]
Adding requests:  77%|███████▋  | 3149/4096 [00:06<00:02, 468.17it/s]
Adding requests:  78%|███████▊  | 3197/4096 [00:06<00:01, 470.71it/s]
Adding requests:  79%|███████▉  | 3246/4096 [00:06<00:01, 473.91it/s]
Adding requests:  80%|████████  | 3294/4096 [00:07<00:01, 474.35it/s]
Adding requests:  82%|████████▏ | 3342/4096 [00:07<00:01, 460.64it/s]
Adding requests:  83%|████████▎ | 3390/4096 [00:07<00:01, 462.62it/s]
Adding requests:  84%|████████▍ | 3438/4096 [00:07<00:01, 467.55it/s]
Adding requests:  85%|████████▌ | 3485/4096 [00:07<00:01, 456.00it/s]
Adding requests:  86%|████████▋ | 3533/4096 [00:07<00:01, 460.43it/s]
Adding requests:  87%|████████▋ | 3580/4096 [00:07<00:01, 462.50it/s]
Adding requests:  89%|████████▊ | 3627/4096 [00:07<00:01, 462.77it/s]
Adding requests:  90%|████████▉ | 3675/4096 [00:07<00:00, 465.55it/s]
Adding requests:  91%|█████████ | 3722/4096 [00:07<00:00, 465.94it/s]
Adding requests:  92%|█████████▏| 3771/4096 [00:08<00:00, 471.28it/s]
Adding requests:  93%|█████████▎| 3820/4096 [00:08<00:00, 474.44it/s]
Adding requests:  94%|█████████▍| 3870/4096 [00:08<00:00, 479.52it/s]
Adding requests:  96%|█████████▌| 3918/4096 [00:08<00:00, 477.54it/s]
Adding requests:  97%|█████████▋| 3966/4096 [00:08<00:00, 476.45it/s]
Adding requests:  98%|█████████▊| 4014/4096 [00:08<00:00, 475.21it/s]
Adding requests:  99%|█████████▉| 4062/4096 [00:08<00:00, 468.42it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 467.52it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  50%|█████     | 2068/4096 [00:00<00:00, 9780.54it/s, est. speed input: 10015631.67 toks/s, output: 9780.61 toks/s]
Processed prompts:  74%|███████▍  | 3047/4096 [00:04<00:01, 576.73it/s, est. speed input: 730545.61 toks/s, output: 713.42 toks/s]    
Processed prompts:  85%|████████▍ | 3464/4096 [00:06<00:01, 450.04it/s, est. speed input: 588579.81 toks/s, output: 574.78 toks/s]
Processed prompts:  90%|█████████ | 3702/4096 [00:07<00:01, 393.46it/s, est. speed input: 533570.60 toks/s, output: 521.06 toks/s]
Processed prompts:  94%|█████████▍| 3855/4096 [00:07<00:00, 376.64it/s, est. speed input: 516183.60 toks/s, output: 504.09 toks/s]
Processed prompts:  97%|█████████▋| 3963/4096 [00:08<00:00, 344.81it/s, est. speed input: 495291.86 toks/s, output: 483.68 toks/s]
Processed prompts:  99%|█████████▊| 4041/4096 [00:08<00:00, 339.28it/s, est. speed input: 489220.90 toks/s, output: 477.75 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:08<00:00, 339.28it/s, est. speed input: 483538.46 toks/s, output: 472.21 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:08<00:00, 472.20it/s, est. speed input: 483538.46 toks/s, output: 472.21 toks/s]
[rank0]:[W126 15:31:29.720828396 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 15:31:31
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:32:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=682247) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=682247) WARNING 01-26 15:32:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=682247) WARNING 01-26 15:32:40 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 239.02 requests/s, 244999.20 total tokens/s, 239.02 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 15:32:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:32:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:32:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:32:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:32:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:32:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:32:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:32:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:32:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:32:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:32:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:32:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:32:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:32:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:32:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:32:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:32:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:32:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:32:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:32:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:32:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:32:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:32:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:32:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:32:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:32:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:32:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:32:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=682247) [2026-01-26 15:32:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=682247) [2026-01-26 15:32:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=682247) [2026-01-26 15:32:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=682247) [2026-01-26 15:32:23] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=682247) [2026-01-26 15:32:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=682247) [2026-01-26 15:32:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=682247) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=682247) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=682247) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=682247) 
(EngineCore_DP0 pid=682247) [2026-01-26 15:32:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=682247) [2026-01-26 15:32:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=682247) [2026-01-26 15:32:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=682247) [2026-01-26 15:32:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=682247) [2026-01-26 15:32:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=682247) [2026-01-26 15:32:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=682247) [2026-01-26 15:32:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=682247) [2026-01-26 15:32:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=682247) [rank0]:W0126 15:32:35.789000 682247 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=682247) [rank0]:W0126 15:32:35.877000 682247 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=682247) [rank0]:W0126 15:32:36.811000 682247 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=682247) [rank0]:W0126 15:32:36.936000 682247 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=682247) 2026-01-26 15:32:40,287 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=682247) 2026-01-26 15:32:40,313 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=682247) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:05,  3.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:05,  3.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:02,  6.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  9.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01, 10.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:00, 11.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00, 12.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00, 13.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00, 14.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:01<00:00, 15.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 11.11it/s]
(EngineCore_DP0 pid=682247) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 17.55it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 18.00it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 18.13it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 16.63it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 16.36it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 16.87it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 38/8192 [00:00<00:21, 377.39it/s]
Adding requests:   1%|          | 82/8192 [00:00<00:19, 411.81it/s]
Adding requests:   2%|▏         | 126/8192 [00:00<00:19, 423.29it/s]
Adding requests:   2%|▏         | 170/8192 [00:00<00:18, 428.37it/s]
Adding requests:   3%|▎         | 215/8192 [00:00<00:18, 434.87it/s]
Adding requests:   3%|▎         | 263/8192 [00:00<00:17, 447.89it/s]
Adding requests:   4%|▍         | 308/8192 [00:00<00:17, 447.67it/s]
Adding requests:   4%|▍         | 353/8192 [00:00<00:17, 442.57it/s]
Adding requests:   5%|▍         | 398/8192 [00:00<00:17, 442.50it/s]
Adding requests:   5%|▌         | 444/8192 [00:01<00:17, 446.78it/s]
Adding requests:   6%|▌         | 489/8192 [00:01<00:17, 446.56it/s]
Adding requests:   7%|▋         | 534/8192 [00:01<00:17, 442.04it/s]
Adding requests:   7%|▋         | 582/8192 [00:01<00:16, 452.42it/s]
Adding requests:   8%|▊         | 630/8192 [00:01<00:16, 458.33it/s]
Adding requests:   8%|▊         | 678/8192 [00:01<00:16, 463.02it/s]
Adding requests:   9%|▉         | 727/8192 [00:01<00:15, 468.40it/s]
Adding requests:   9%|▉         | 774/8192 [00:01<00:15, 464.63it/s]
Adding requests:  10%|█         | 821/8192 [00:01<00:16, 456.77it/s]
Adding requests:  11%|█         | 868/8192 [00:01<00:15, 459.36it/s]
Adding requests:  11%|█         | 917/8192 [00:02<00:15, 465.61it/s]
Adding requests:  12%|█▏        | 964/8192 [00:02<00:15, 466.86it/s]
Adding requests:  12%|█▏        | 1012/8192 [00:02<00:15, 468.38it/s]
Adding requests:  13%|█▎        | 1059/8192 [00:02<00:15, 466.98it/s]
Adding requests:  14%|█▎        | 1106/8192 [00:02<00:15, 463.97it/s]
Adding requests:  14%|█▍        | 1153/8192 [00:02<00:15, 462.47it/s]
Adding requests:  15%|█▍        | 1204/8192 [00:02<00:14, 474.51it/s]
Adding requests:  15%|█▌        | 1252/8192 [00:02<00:14, 468.94it/s]
Adding requests:  16%|█▌        | 1299/8192 [00:02<00:14, 467.20it/s]
Adding requests:  16%|█▋        | 1347/8192 [00:02<00:14, 469.80it/s]
Adding requests:  17%|█▋        | 1394/8192 [00:03<00:14, 460.41it/s]
Adding requests:  18%|█▊        | 1441/8192 [00:03<00:14, 462.72it/s]
Adding requests:  18%|█▊        | 1490/8192 [00:03<00:14, 469.06it/s]
Adding requests:  19%|█▉        | 1538/8192 [00:03<00:14, 470.96it/s]
Adding requests:  19%|█▉        | 1586/8192 [00:03<00:14, 463.07it/s]
Adding requests:  20%|█▉        | 1636/8192 [00:03<00:13, 471.40it/s]
Adding requests:  21%|██        | 1684/8192 [00:03<00:13, 467.81it/s]
Adding requests:  21%|██        | 1733/8192 [00:03<00:13, 472.22it/s]
Adding requests:  22%|██▏       | 1781/8192 [00:03<00:13, 467.59it/s]
Adding requests:  22%|██▏       | 1830/8192 [00:03<00:13, 471.29it/s]
Adding requests:  23%|██▎       | 1878/8192 [00:04<00:13, 472.00it/s]
Adding requests:  24%|██▎       | 1926/8192 [00:04<00:13, 472.03it/s]
Adding requests:  24%|██▍       | 1974/8192 [00:04<00:13, 471.56it/s]
Adding requests:  25%|██▍       | 2023/8192 [00:04<00:12, 475.52it/s]
Adding requests:  25%|██▌       | 2072/8192 [00:04<00:12, 477.72it/s]
Adding requests:  26%|██▌       | 2120/8192 [00:04<00:12, 474.85it/s]
Adding requests:  26%|██▋       | 2168/8192 [00:04<00:12, 468.70it/s]
Adding requests:  27%|██▋       | 2215/8192 [00:04<00:12, 467.58it/s]
Adding requests:  28%|██▊       | 2263/8192 [00:04<00:12, 468.92it/s]
Adding requests:  28%|██▊       | 2312/8192 [00:05<00:12, 474.23it/s]
Adding requests:  29%|██▉       | 2360/8192 [00:05<00:12, 471.65it/s]
Adding requests:  29%|██▉       | 2408/8192 [00:05<00:12, 469.42it/s]
Adding requests:  30%|██▉       | 2456/8192 [00:05<00:12, 470.12it/s]
Adding requests:  31%|███       | 2504/8192 [00:05<00:12, 468.10it/s]
Adding requests:  31%|███       | 2553/8192 [00:05<00:11, 473.69it/s]
Adding requests:  32%|███▏      | 2601/8192 [00:05<00:11, 471.79it/s]
Adding requests:  32%|███▏      | 2650/8192 [00:05<00:11, 474.62it/s]
Adding requests:  33%|███▎      | 2698/8192 [00:05<00:11, 470.70it/s]
Adding requests:  34%|███▎      | 2746/8192 [00:05<00:11, 470.83it/s]
Adding requests:  34%|███▍      | 2794/8192 [00:06<00:11, 456.14it/s]
Adding requests:  35%|███▍      | 2841/8192 [00:06<00:11, 458.77it/s]
Adding requests:  35%|███▌      | 2890/8192 [00:06<00:11, 465.92it/s]
Adding requests:  36%|███▌      | 2937/8192 [00:06<00:11, 460.65it/s]
Adding requests:  36%|███▋      | 2985/8192 [00:06<00:11, 466.00it/s]
Adding requests:  37%|███▋      | 3032/8192 [00:06<00:11, 463.88it/s]
Adding requests:  38%|███▊      | 3079/8192 [00:06<00:11, 462.88it/s]
Adding requests:  38%|███▊      | 3127/8192 [00:06<00:10, 466.99it/s]
Adding requests:  39%|███▊      | 3174/8192 [00:06<00:10, 465.64it/s]
Adding requests:  39%|███▉      | 3221/8192 [00:06<00:10, 465.62it/s]
Adding requests:  40%|███▉      | 3269/8192 [00:07<00:10, 469.21it/s]
Adding requests:  40%|████      | 3317/8192 [00:07<00:10, 471.17it/s]
Adding requests:  41%|████      | 3366/8192 [00:07<00:10, 474.01it/s]
Adding requests:  42%|████▏     | 3414/8192 [00:07<00:10, 473.82it/s]
Adding requests:  42%|████▏     | 3462/8192 [00:07<00:10, 465.49it/s]
Adding requests:  43%|████▎     | 3509/8192 [00:07<00:10, 466.68it/s]
Adding requests:  43%|████▎     | 3556/8192 [00:07<00:09, 466.84it/s]
Adding requests:  44%|████▍     | 3603/8192 [00:07<00:09, 467.12it/s]
Adding requests:  45%|████▍     | 3650/8192 [00:07<00:09, 465.30it/s]
Adding requests:  45%|████▌     | 3698/8192 [00:07<00:09, 467.23it/s]
Adding requests:  46%|████▌     | 3745/8192 [00:08<00:09, 463.65it/s]
Adding requests:  46%|████▋     | 3795/8192 [00:08<00:09, 472.83it/s]
Adding requests:  47%|████▋     | 3843/8192 [00:08<00:09, 474.71it/s]
Adding requests:  47%|████▋     | 3891/8192 [00:08<00:09, 472.68it/s]
Adding requests:  48%|████▊     | 3939/8192 [00:08<00:08, 472.74it/s]
Adding requests:  49%|████▊     | 3987/8192 [00:08<00:08, 468.71it/s]
Adding requests:  49%|████▉     | 4035/8192 [00:08<00:08, 470.48it/s]
Adding requests:  50%|████▉     | 4083/8192 [00:08<00:09, 455.78it/s]
Adding requests:  50%|█████     | 4132/8192 [00:08<00:08, 463.65it/s]
Adding requests:  51%|█████     | 4179/8192 [00:09<00:08, 465.47it/s]
Adding requests:  52%|█████▏    | 4227/8192 [00:09<00:08, 466.74it/s]
Adding requests:  52%|█████▏    | 4274/8192 [00:09<00:08, 466.17it/s]
Adding requests:  53%|█████▎    | 4322/8192 [00:09<00:08, 468.84it/s]
Adding requests:  53%|█████▎    | 4371/8192 [00:09<00:08, 473.95it/s]
Adding requests:  54%|█████▍    | 4419/8192 [00:09<00:08, 467.49it/s]
Adding requests:  55%|█████▍    | 4468/8192 [00:09<00:07, 471.22it/s]
Adding requests:  55%|█████▌    | 4516/8192 [00:09<00:07, 463.56it/s]
Adding requests:  56%|█████▌    | 4564/8192 [00:09<00:07, 465.16it/s]
Adding requests:  56%|█████▋    | 4613/8192 [00:09<00:07, 470.41it/s]
Adding requests:  57%|█████▋    | 4661/8192 [00:10<00:07, 468.94it/s]
Adding requests:  57%|█████▋    | 4708/8192 [00:10<00:07, 468.16it/s]
Adding requests:  58%|█████▊    | 4756/8192 [00:10<00:07, 470.14it/s]
Adding requests:  59%|█████▊    | 4804/8192 [00:10<00:07, 469.15it/s]
Adding requests:  59%|█████▉    | 4851/8192 [00:10<00:07, 468.67it/s]
Adding requests:  60%|█████▉    | 4898/8192 [00:10<00:07, 464.85it/s]
Adding requests:  60%|██████    | 4946/8192 [00:10<00:06, 469.09it/s]
Adding requests:  61%|██████    | 4993/8192 [00:10<00:06, 468.96it/s]
Adding requests:  62%|██████▏   | 5041/8192 [00:10<00:06, 471.76it/s]
Adding requests:  62%|██████▏   | 5089/8192 [00:10<00:06, 472.73it/s]
Adding requests:  63%|██████▎   | 5137/8192 [00:11<00:06, 453.53it/s]
Adding requests:  63%|██████▎   | 5184/8192 [00:11<00:06, 456.63it/s]
Adding requests:  64%|██████▍   | 5231/8192 [00:11<00:06, 457.28it/s]
Adding requests:  64%|██████▍   | 5278/8192 [00:11<00:06, 458.83it/s]
Adding requests:  65%|██████▌   | 5327/8192 [00:11<00:06, 466.94it/s]
Adding requests:  66%|██████▌   | 5374/8192 [00:11<00:06, 453.86it/s]
Adding requests:  66%|██████▌   | 5422/8192 [00:11<00:06, 459.12it/s]
Adding requests:  67%|██████▋   | 5468/8192 [00:11<00:05, 456.75it/s]
Adding requests:  67%|██████▋   | 5514/8192 [00:11<00:05, 456.53it/s]
Adding requests:  68%|██████▊   | 5561/8192 [00:11<00:05, 458.32it/s]
Adding requests:  68%|██████▊   | 5607/8192 [00:12<00:05, 446.23it/s]
Adding requests:  69%|██████▉   | 5652/8192 [00:12<00:05, 447.18it/s]
Adding requests:  70%|██████▉   | 5697/8192 [00:12<00:05, 445.92it/s]
Adding requests:  70%|███████   | 5744/8192 [00:12<00:05, 452.61it/s]
Adding requests:  71%|███████   | 5791/8192 [00:12<00:05, 456.78it/s]
Adding requests:  71%|███████▏  | 5837/8192 [00:12<00:05, 451.40it/s]
Adding requests:  72%|███████▏  | 5886/8192 [00:12<00:04, 462.00it/s]
Adding requests:  72%|███████▏  | 5934/8192 [00:12<00:04, 464.75it/s]
Adding requests:  73%|███████▎  | 5981/8192 [00:12<00:04, 464.73it/s]
Adding requests:  74%|███████▎  | 6030/8192 [00:12<00:04, 470.21it/s]
Adding requests:  74%|███████▍  | 6078/8192 [00:13<00:04, 472.12it/s]
Adding requests:  75%|███████▍  | 6126/8192 [00:13<00:04, 474.07it/s]
Adding requests:  75%|███████▌  | 6174/8192 [00:13<00:04, 473.21it/s]
Adding requests:  76%|███████▌  | 6224/8192 [00:13<00:04, 480.62it/s]
Adding requests:  77%|███████▋  | 6273/8192 [00:13<00:03, 483.28it/s]
Adding requests:  77%|███████▋  | 6322/8192 [00:13<00:03, 484.34it/s]
Adding requests:  78%|███████▊  | 6371/8192 [00:13<00:03, 483.46it/s]
Adding requests:  78%|███████▊  | 6420/8192 [00:13<00:03, 482.50it/s]
Adding requests:  79%|███████▉  | 6470/8192 [00:13<00:03, 484.66it/s]
Adding requests:  80%|███████▉  | 6521/8192 [00:14<00:03, 489.65it/s]
Adding requests:  80%|████████  | 6570/8192 [00:14<00:03, 487.19it/s]
Adding requests:  81%|████████  | 6619/8192 [00:14<00:03, 467.26it/s]
Adding requests:  81%|████████▏ | 6666/8192 [00:14<00:03, 468.02it/s]
Adding requests:  82%|████████▏ | 6714/8192 [00:14<00:03, 470.60it/s]
Adding requests:  83%|████████▎ | 6762/8192 [00:14<00:03, 471.51it/s]
Adding requests:  83%|████████▎ | 6811/8192 [00:14<00:02, 475.46it/s]
Adding requests:  84%|████████▎ | 6860/8192 [00:14<00:02, 478.36it/s]
Adding requests:  84%|████████▍ | 6909/8192 [00:14<00:02, 480.76it/s]
Adding requests:  85%|████████▍ | 6958/8192 [00:14<00:02, 480.75it/s]
Adding requests:  86%|████████▌ | 7007/8192 [00:15<00:02, 477.28it/s]
Adding requests:  86%|████████▌ | 7055/8192 [00:15<00:02, 476.02it/s]
Adding requests:  87%|████████▋ | 7103/8192 [00:15<00:02, 466.44it/s]
Adding requests:  87%|████████▋ | 7150/8192 [00:15<00:02, 466.60it/s]
Adding requests:  88%|████████▊ | 7198/8192 [00:15<00:02, 468.00it/s]
Adding requests:  88%|████████▊ | 7245/8192 [00:15<00:02, 460.00it/s]
Adding requests:  89%|████████▉ | 7295/8192 [00:15<00:01, 469.14it/s]
Adding requests:  90%|████████▉ | 7343/8192 [00:15<00:01, 471.42it/s]
Adding requests:  90%|█████████ | 7391/8192 [00:15<00:01, 471.97it/s]
Adding requests:  91%|█████████ | 7442/8192 [00:15<00:01, 482.77it/s]
Adding requests:  91%|█████████▏| 7491/8192 [00:16<00:01, 480.99it/s]
Adding requests:  92%|█████████▏| 7540/8192 [00:16<00:01, 482.22it/s]
Adding requests:  93%|█████████▎| 7589/8192 [00:16<00:01, 480.24it/s]
Adding requests:  93%|█████████▎| 7638/8192 [00:16<00:01, 478.98it/s]
Adding requests:  94%|█████████▍| 7687/8192 [00:16<00:01, 482.16it/s]
Adding requests:  94%|█████████▍| 7736/8192 [00:16<00:00, 479.98it/s]
Adding requests:  95%|█████████▌| 7785/8192 [00:16<00:00, 476.61it/s]
Adding requests:  96%|█████████▌| 7833/8192 [00:16<00:00, 477.04it/s]
Adding requests:  96%|█████████▌| 7881/8192 [00:16<00:00, 457.32it/s]
Adding requests:  97%|█████████▋| 7928/8192 [00:16<00:00, 460.06it/s]
Adding requests:  97%|█████████▋| 7975/8192 [00:17<00:00, 461.49it/s]
Adding requests:  98%|█████████▊| 8022/8192 [00:17<00:00, 462.57it/s]
Adding requests:  99%|█████████▊| 8070/8192 [00:17<00:00, 467.51it/s]
Adding requests:  99%|█████████▉| 8119/8192 [00:17<00:00, 472.50it/s]
Adding requests: 100%|█████████▉| 8167/8192 [00:17<00:00, 472.92it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 466.51it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  51%|█████     | 4151/8192 [00:00<00:00, 28121.50it/s, est. speed input: 28797999.06 toks/s, output: 28121.91 toks/s]
Processed prompts:  85%|████████▌ | 6964/8192 [00:11<00:02, 494.82it/s, est. speed input: 614672.90 toks/s, output: 600.27 toks/s]      
Processed prompts:  85%|████████▌ | 6989/8192 [00:11<00:02, 482.08it/s, est. speed input: 603076.47 toks/s, output: 588.94 toks/s]
Processed prompts: 100%|█████████▉| 8153/8192 [00:16<00:00, 374.51it/s, est. speed input: 501265.33 toks/s, output: 489.52 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:16<00:00, 374.51it/s, est. speed input: 502040.45 toks/s, output: 490.27 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:16<00:00, 490.27it/s, est. speed input: 502040.45 toks/s, output: 490.27 toks/s]
[rank0]:[W126 15:33:19.655238429 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 16:43:20
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:43:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=782950) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=782950) WARNING 01-26 16:43:43 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=782950) WARNING 01-26 16:43:53 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 37.68 requests/s, 19330.87 total tokens/s, 37.68 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 16:43:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:43:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:43:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:43:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:43:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:43:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:43:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:43:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:43:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:43:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:43:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:43:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:43:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:43:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:43:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:43:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:43:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:34] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=782950) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=782950) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.21it/s]
(EngineCore_DP0 pid=782950) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.05s/it]
(EngineCore_DP0 pid=782950) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.02s/it]
(EngineCore_DP0 pid=782950) 
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=782950) [2026-01-26 16:43:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=782950) 2026-01-26 16:43:53,524 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=782950) 2026-01-26 16:43:53,547 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=782950) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.26it/s]
(EngineCore_DP0 pid=782950) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.80it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 509.87it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 262.65it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 309.80it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:00, 122.75it/s, est. speed input: 62849.13 toks/s, output: 122.75 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:01, 56.74it/s, est. speed input: 31800.17 toks/s, output: 62.11 toks/s]  
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 49.72it/s, est. speed input: 28241.48 toks/s, output: 55.16 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 46.60it/s, est. speed input: 26730.66 toks/s, output: 52.21 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 44.44it/s, est. speed input: 25696.33 toks/s, output: 50.19 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 43.15it/s, est. speed input: 25064.26 toks/s, output: 48.95 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 42.16it/s, est. speed input: 24560.70 toks/s, output: 47.97 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 41.45it/s, est. speed input: 24157.72 toks/s, output: 47.18 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 40.88it/s, est. speed input: 23814.83 toks/s, output: 46.51 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 40.52it/s, est. speed input: 23534.99 toks/s, output: 45.97 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 40.22it/s, est. speed input: 23291.21 toks/s, output: 45.49 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:01, 39.94it/s, est. speed input: 23070.92 toks/s, output: 45.06 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:01, 39.76it/s, est. speed input: 22914.67 toks/s, output: 44.76 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 39.71it/s, est. speed input: 22783.41 toks/s, output: 44.50 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 39.59it/s, est. speed input: 22656.94 toks/s, output: 44.25 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 39.52it/s, est. speed input: 22543.11 toks/s, output: 44.03 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 39.51it/s, est. speed input: 22442.99 toks/s, output: 43.83 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 39.53it/s, est. speed input: 22352.62 toks/s, output: 43.66 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 39.56it/s, est. speed input: 22271.01 toks/s, output: 43.50 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 39.58it/s, est. speed input: 22195.84 toks/s, output: 43.35 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 39.46it/s, est. speed input: 22116.98 toks/s, output: 43.20 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 39.51it/s, est. speed input: 22052.15 toks/s, output: 43.07 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 39.46it/s, est. speed input: 21986.40 toks/s, output: 42.94 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 39.46it/s, est. speed input: 21972.91 toks/s, output: 42.92 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 42.91it/s, est. speed input: 21972.91 toks/s, output: 42.92 toks/s]
[rank0]:[W126 16:43:59.516333881 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 16:44:01
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:44:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=784136) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=784136) WARNING 01-26 16:44:24 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=784136) WARNING 01-26 16:44:34 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 39.80 requests/s, 40793.30 total tokens/s, 39.80 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 16:44:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:44:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:44:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:44:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:44:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:44:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:44:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:44:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:44:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:44:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:44:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:44:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:15] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:15] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:15] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:15] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:15] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=784136) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=784136) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.31it/s]
(EngineCore_DP0 pid=784136) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]
(EngineCore_DP0 pid=784136) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.06it/s]
(EngineCore_DP0 pid=784136) 
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=784136) [2026-01-26 16:44:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=784136) 2026-01-26 16:44:34,106 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=784136) 2026-01-26 16:44:34,129 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=784136) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.82it/s]
(EngineCore_DP0 pid=784136) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.20it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 273.27it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 319.45it/s]
Adding requests:  75%|███████▌  | 96/128 [00:00<00:00, 316.95it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 317.42it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:00, 176.40it/s, est. speed input: 180666.61 toks/s, output: 176.42 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 59.33it/s, est. speed input: 67468.40 toks/s, output: 65.89 toks/s]   
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 51.75it/s, est. speed input: 59438.96 toks/s, output: 58.05 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:00<00:01, 48.22it/s, est. speed input: 55879.79 toks/s, output: 54.57 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 46.26it/s, est. speed input: 54007.35 toks/s, output: 52.74 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 44.77it/s, est. speed input: 52581.29 toks/s, output: 51.35 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 43.81it/s, est. speed input: 51638.39 toks/s, output: 50.43 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 42.98it/s, est. speed input: 50824.10 toks/s, output: 49.63 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 42.34it/s, est. speed input: 50134.26 toks/s, output: 48.96 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:01, 41.86it/s, est. speed input: 49541.58 toks/s, output: 48.38 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 41.43it/s, est. speed input: 49005.88 toks/s, output: 47.86 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 41.21it/s, est. speed input: 48558.86 toks/s, output: 47.42 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 41.03it/s, est. speed input: 48158.44 toks/s, output: 47.03 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.93it/s, est. speed input: 47805.73 toks/s, output: 46.69 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 40.83it/s, est. speed input: 47484.64 toks/s, output: 46.37 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 40.73it/s, est. speed input: 47190.63 toks/s, output: 46.08 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 40.70it/s, est. speed input: 46929.15 toks/s, output: 45.83 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 40.70it/s, est. speed input: 46696.04 toks/s, output: 45.60 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.70it/s, est. speed input: 46612.95 toks/s, output: 45.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.52it/s, est. speed input: 46612.95 toks/s, output: 45.52 toks/s]
[rank0]:[W126 16:44:38.212888621 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 16:44:40
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:44:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=785250) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=785250) WARNING 01-26 16:45:05 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=785250) WARNING 01-26 16:45:15 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 74.01 requests/s, 75860.26 total tokens/s, 74.01 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 16:44:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:44:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:44:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:44:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:44:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:44:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:44:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:44:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:44:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:44:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:44:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:44:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:44:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:44:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:56] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=785250) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=785250) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.08it/s]
(EngineCore_DP0 pid=785250) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.06s/it]
(EngineCore_DP0 pid=785250) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.04s/it]
(EngineCore_DP0 pid=785250) 
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=785250) [2026-01-26 16:44:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=785250) 2026-01-26 16:45:14,979 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=785250) 2026-01-26 16:45:15,001 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=785250) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.68it/s]
(EngineCore_DP0 pid=785250) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.92it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:48,  5.29it/s]
Adding requests:  11%|█▏        | 29/256 [00:00<00:01, 123.48it/s]
Adding requests:  25%|██▌       | 64/256 [00:00<00:00, 209.61it/s]
Adding requests:  38%|███▊      | 96/256 [00:00<00:00, 246.18it/s]
Adding requests:  51%|█████     | 130/256 [00:00<00:00, 275.64it/s]
Adding requests:  64%|██████▍   | 164/256 [00:00<00:00, 295.97it/s]
Adding requests:  78%|███████▊  | 199/256 [00:00<00:00, 311.86it/s]
Adding requests:  91%|█████████▏| 234/256 [00:00<00:00, 321.97it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 265.38it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:00<00:00, 598.14it/s, est. speed input: 612532.23 toks/s, output: 598.15 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:00<00:01, 127.78it/s, est. speed input: 148677.45 toks/s, output: 145.19 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:01<00:00, 109.58it/s, est. speed input: 128835.61 toks/s, output: 125.82 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:00, 101.92it/s, est. speed input: 121175.07 toks/s, output: 118.33 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:01<00:00, 97.12it/s, est. speed input: 116720.09 toks/s, output: 113.98 toks/s] 
Processed prompts:  79%|███████▊  | 201/256 [00:01<00:00, 95.36it/s, est. speed input: 114580.86 toks/s, output: 111.90 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:01<00:00, 90.64it/s, est. speed input: 111556.69 toks/s, output: 108.94 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:02<00:00, 86.89it/s, est. speed input: 109127.99 toks/s, output: 106.57 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:02<00:00, 85.73it/s, est. speed input: 107724.02 toks/s, output: 105.20 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:02<00:00, 84.75it/s, est. speed input: 106467.43 toks/s, output: 103.97 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:02<00:00, 84.02it/s, est. speed input: 105347.84 toks/s, output: 102.88 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 84.02it/s, est. speed input: 105142.45 toks/s, output: 102.68 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 102.67it/s, est. speed input: 105142.45 toks/s, output: 102.68 toks/s]
[rank0]:[W126 16:45:20.491756672 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 16:45:21
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:45:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=786395) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=786395) WARNING 01-26 16:45:48 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=786395) WARNING 01-26 16:45:58 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 111.11 requests/s, 113883.59 total tokens/s, 111.11 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 16:45:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:45:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:45:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:45:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:45:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:45:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:45:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:45:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:45:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:45:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:45:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:45:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:45:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:45:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:45:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:45:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:45:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:39] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:39] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:39] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:39] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=786395) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=786395) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.18it/s]
(EngineCore_DP0 pid=786395) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.02s/it]
(EngineCore_DP0 pid=786395) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
(EngineCore_DP0 pid=786395) 
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=786395) [2026-01-26 16:45:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=786395) 2026-01-26 16:45:58,096 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=786395) 2026-01-26 16:45:58,119 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=786395) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.43it/s]
(EngineCore_DP0 pid=786395) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 17.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 17.13it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 28/512 [00:00<00:01, 276.83it/s]
Adding requests:  12%|█▏        | 63/512 [00:00<00:01, 318.33it/s]
Adding requests:  19%|█▊        | 95/512 [00:00<00:01, 315.47it/s]
Adding requests:  25%|██▍       | 127/512 [00:00<00:01, 316.39it/s]
Adding requests:  32%|███▏      | 162/512 [00:00<00:01, 326.21it/s]
Adding requests:  39%|███▊      | 198/512 [00:00<00:00, 337.57it/s]
Adding requests:  46%|████▌     | 234/512 [00:00<00:00, 343.21it/s]
Adding requests:  53%|█████▎    | 269/512 [00:00<00:00, 343.55it/s]
Adding requests:  60%|█████▉    | 305/512 [00:00<00:00, 347.50it/s]
Adding requests:  67%|██████▋   | 343/512 [00:01<00:00, 353.83it/s]
Adding requests:  74%|███████▍  | 380/512 [00:01<00:00, 356.98it/s]
Adding requests:  82%|████████▏ | 419/512 [00:01<00:00, 365.37it/s]
Adding requests:  89%|████████▉ | 456/512 [00:01<00:00, 361.22it/s]
Adding requests:  97%|█████████▋| 496/512 [00:01<00:00, 371.42it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 348.68it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:00<00:00, 1633.62it/s, est. speed input: 1672939.58 toks/s, output: 1633.65 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:01<00:00, 186.39it/s, est. speed input: 220725.13 toks/s, output: 215.55 toks/s]   
Processed prompts:  80%|███████▉  | 408/512 [00:02<00:00, 159.69it/s, est. speed input: 190713.80 toks/s, output: 186.24 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:02<00:00, 146.69it/s, est. speed input: 178122.71 toks/s, output: 173.95 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:02<00:00, 139.72it/s, est. speed input: 171890.34 toks/s, output: 167.86 toks/s]
Processed prompts: 100%|█████████▉| 511/512 [00:03<00:00, 132.42it/s, est. speed input: 166717.75 toks/s, output: 162.81 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 132.42it/s, est. speed input: 167040.54 toks/s, output: 163.13 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 163.12it/s, est. speed input: 167040.54 toks/s, output: 163.13 toks/s]
[rank0]:[W126 16:46:05.276036441 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 16:46:06
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:46:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=787629) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=787629) WARNING 01-26 16:46:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=787629) WARNING 01-26 16:46:45 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 120.00 requests/s, 122998.03 total tokens/s, 120.00 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 16:46:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:46:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:46:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:46:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:46:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:46:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:46:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:46:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:46:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:46:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:46:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:46:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:46:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:46:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:46:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:46:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:46:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:27] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:27] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:27] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:27] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:27] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=787629) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=787629) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=787629) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.00it/s]
(EngineCore_DP0 pid=787629) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]
(EngineCore_DP0 pid=787629) 
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=787629) [2026-01-26 16:46:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=787629) 2026-01-26 16:46:45,786 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=787629) 2026-01-26 16:46:45,854 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=787629) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 12.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 14.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 13.39it/s]
(EngineCore_DP0 pid=787629) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 18.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.49it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.43it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 278.93it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:02, 320.48it/s]
Adding requests:   9%|▉         | 96/1024 [00:00<00:02, 316.79it/s]
Adding requests:  13%|█▎        | 129/1024 [00:00<00:02, 321.47it/s]
Adding requests:  16%|█▌        | 165/1024 [00:00<00:02, 331.81it/s]
Adding requests:  20%|█▉        | 202/1024 [00:00<00:02, 342.32it/s]
Adding requests:  23%|██▎       | 238/1024 [00:00<00:02, 347.15it/s]
Adding requests:  27%|██▋       | 273/1024 [00:00<00:02, 345.15it/s]
Adding requests:  30%|███       | 310/1024 [00:00<00:02, 351.54it/s]
Adding requests:  34%|███▍      | 347/1024 [00:01<00:01, 356.41it/s]
Adding requests:  38%|███▊      | 384/1024 [00:01<00:01, 360.15it/s]
Adding requests:  41%|████▏     | 423/1024 [00:01<00:01, 367.17it/s]
Adding requests:  45%|████▍     | 460/1024 [00:01<00:01, 363.26it/s]
Adding requests:  49%|████▉     | 500/1024 [00:01<00:01, 372.21it/s]
Adding requests:  53%|█████▎    | 540/1024 [00:01<00:01, 378.49it/s]
Adding requests:  56%|█████▋    | 578/1024 [00:01<00:01, 374.85it/s]
Adding requests:  60%|██████    | 616/1024 [00:01<00:01, 365.74it/s]
Adding requests:  64%|██████▍   | 653/1024 [00:01<00:01, 358.36it/s]
Adding requests:  67%|██████▋   | 691/1024 [00:01<00:00, 363.56it/s]
Adding requests:  71%|███████   | 728/1024 [00:02<00:00, 356.72it/s]
Adding requests:  75%|███████▍  | 764/1024 [00:02<00:00, 356.85it/s]
Adding requests:  78%|███████▊  | 801/1024 [00:02<00:00, 357.32it/s]
Adding requests:  82%|████████▏ | 839/1024 [00:02<00:00, 363.67it/s]
Adding requests:  86%|████████▌ | 876/1024 [00:02<00:00, 363.09it/s]
Adding requests:  89%|████████▉ | 913/1024 [00:02<00:00, 360.01it/s]
Adding requests:  93%|█████████▎| 950/1024 [00:02<00:00, 356.55it/s]
Adding requests:  96%|█████████▋| 987/1024 [00:02<00:00, 359.80it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 355.58it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 355.18it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:00<00:00, 2196.30it/s, est. speed input: 2249106.41 toks/s, output: 2196.32 toks/s]
Processed prompts:  56%|█████▌    | 574/1024 [00:01<00:01, 246.90it/s, est. speed input: 302505.50 toks/s, output: 295.41 toks/s]   
Processed prompts:  66%|██████▌   | 672/1024 [00:02<00:01, 200.60it/s, est. speed input: 251352.40 toks/s, output: 245.46 toks/s]
Processed prompts:  71%|███████▏  | 731/1024 [00:03<00:01, 177.21it/s, est. speed input: 229154.17 toks/s, output: 223.78 toks/s]
Processed prompts:  75%|███████▌  | 771/1024 [00:03<00:01, 166.55it/s, est. speed input: 219491.27 toks/s, output: 214.35 toks/s]
Processed prompts:  78%|███████▊  | 801/1024 [00:03<00:01, 164.95it/s, est. speed input: 216382.08 toks/s, output: 211.31 toks/s]
Processed prompts:  81%|████████  | 827/1024 [00:04<00:01, 150.87it/s, est. speed input: 208832.14 toks/s, output: 203.94 toks/s]
Processed prompts:  83%|████████▎ | 848/1024 [00:04<00:01, 151.93it/s, est. speed input: 207378.99 toks/s, output: 202.52 toks/s]
Processed prompts:  85%|████████▍ | 867/1024 [00:04<00:01, 139.72it/s, est. speed input: 202465.93 toks/s, output: 197.72 toks/s]
Processed prompts:  86%|████████▌ | 883/1024 [00:04<00:01, 136.44it/s, est. speed input: 200154.19 toks/s, output: 195.46 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:04<00:00, 131.92it/s, est. speed input: 197772.23 toks/s, output: 193.14 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:04<00:00, 129.47it/s, est. speed input: 195721.89 toks/s, output: 191.13 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:04<00:00, 127.20it/s, est. speed input: 193748.18 toks/s, output: 189.21 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:05<00:00, 125.51it/s, est. speed input: 191900.98 toks/s, output: 187.40 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:05<00:00, 124.13it/s, est. speed input: 190139.40 toks/s, output: 185.68 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:05<00:00, 123.08it/s, est. speed input: 188464.52 toks/s, output: 184.05 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:05<00:00, 122.58it/s, est. speed input: 186908.13 toks/s, output: 182.53 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:05<00:00, 122.19it/s, est. speed input: 185422.88 toks/s, output: 181.08 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 122.19it/s, est. speed input: 185616.09 toks/s, output: 181.27 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 181.26it/s, est. speed input: 185616.09 toks/s, output: 181.27 toks/s]
[rank0]:[W126 16:46:56.062125678 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 16:46:58
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:47:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=788983) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=788983) WARNING 01-26 16:47:34 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=788983) WARNING 01-26 16:47:44 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 124.81 requests/s, 127932.04 total tokens/s, 124.81 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 16:47:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:47:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:47:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:47:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:47:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:47:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:47:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:47:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:47:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:47:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:47:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:47:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:47:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:47:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:47:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:47:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:47:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:25] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=788983) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=788983) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.29it/s]
(EngineCore_DP0 pid=788983) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
(EngineCore_DP0 pid=788983) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
(EngineCore_DP0 pid=788983) 
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=788983) [2026-01-26 16:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=788983) [rank0]:W0126 16:47:39.578000 788983 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=788983) [rank0]:W0126 16:47:39.659000 788983 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=788983) [rank0]:W0126 16:47:40.604000 788983 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=788983) [rank0]:W0126 16:47:40.732000 788983 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=788983) 2026-01-26 16:47:44,176 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=788983) 2026-01-26 16:47:44,202 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=788983) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 15.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 16.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 17.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 15.95it/s]
(EngineCore_DP0 pid=788983) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 18.07it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 17.64it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 17.43it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 276.11it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 315.33it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 312.00it/s]
Adding requests:   6%|▋         | 128/2048 [00:00<00:06, 318.88it/s]
Adding requests:   8%|▊         | 163/2048 [00:00<00:05, 328.02it/s]
Adding requests:  10%|▉         | 199/2048 [00:00<00:05, 338.01it/s]
Adding requests:  11%|█▏        | 235/2048 [00:00<00:05, 344.50it/s]
Adding requests:  13%|█▎        | 270/2048 [00:00<00:05, 336.63it/s]
Adding requests:  15%|█▍        | 306/2048 [00:00<00:05, 342.35it/s]
Adding requests:  17%|█▋        | 343/2048 [00:01<00:04, 350.44it/s]
Adding requests:  19%|█▊        | 380/2048 [00:01<00:04, 353.38it/s]
Adding requests:  20%|██        | 419/2048 [00:01<00:04, 361.91it/s]
Adding requests:  22%|██▏       | 456/2048 [00:01<00:04, 357.92it/s]
Adding requests:  24%|██▍       | 496/2048 [00:01<00:04, 368.24it/s]
Adding requests:  26%|██▌       | 535/2048 [00:01<00:04, 372.25it/s]
Adding requests:  28%|██▊       | 573/2048 [00:01<00:03, 370.98it/s]
Adding requests:  30%|██▉       | 611/2048 [00:01<00:04, 356.81it/s]
Adding requests:  32%|███▏      | 647/2048 [00:01<00:04, 349.82it/s]
Adding requests:  33%|███▎      | 683/2048 [00:01<00:03, 352.58it/s]
Adding requests:  35%|███▌      | 719/2048 [00:02<00:03, 353.66it/s]
Adding requests:  37%|███▋      | 755/2048 [00:02<00:03, 350.25it/s]
Adding requests:  39%|███▊      | 791/2048 [00:02<00:03, 352.24it/s]
Adding requests:  40%|████      | 828/2048 [00:02<00:03, 354.11it/s]
Adding requests:  42%|████▏     | 865/2048 [00:02<00:03, 357.85it/s]
Adding requests:  44%|████▍     | 902/2048 [00:02<00:03, 361.01it/s]
Adding requests:  46%|████▌     | 939/2048 [00:02<00:03, 354.82it/s]
Adding requests:  48%|████▊     | 976/2048 [00:02<00:03, 355.90it/s]
Adding requests:  49%|████▉     | 1012/2048 [00:02<00:02, 350.67it/s]
Adding requests:  51%|█████     | 1048/2048 [00:03<00:02, 338.66it/s]
Adding requests:  53%|█████▎    | 1083/2048 [00:03<00:02, 341.40it/s]
Adding requests:  55%|█████▍    | 1118/2048 [00:03<00:02, 341.72it/s]
Adding requests:  56%|█████▋    | 1154/2048 [00:03<00:02, 345.83it/s]
Adding requests:  58%|█████▊    | 1189/2048 [00:03<00:02, 346.23it/s]
Adding requests:  60%|█████▉    | 1227/2048 [00:03<00:02, 354.95it/s]
Adding requests:  62%|██████▏   | 1263/2048 [00:03<00:02, 350.69it/s]
Adding requests:  63%|██████▎   | 1299/2048 [00:03<00:02, 348.17it/s]
Adding requests:  65%|██████▌   | 1335/2048 [00:03<00:02, 349.72it/s]
Adding requests:  67%|██████▋   | 1372/2048 [00:03<00:01, 354.71it/s]
Adding requests:  69%|██████▉   | 1408/2048 [00:04<00:01, 352.82it/s]
Adding requests:  71%|███████   | 1444/2048 [00:04<00:01, 353.64it/s]
Adding requests:  72%|███████▏  | 1480/2048 [00:04<00:01, 354.83it/s]
Adding requests:  74%|███████▍  | 1517/2048 [00:04<00:01, 357.74it/s]
Adding requests:  76%|███████▌  | 1553/2048 [00:04<00:01, 355.27it/s]
Adding requests:  78%|███████▊  | 1589/2048 [00:04<00:01, 350.13it/s]
Adding requests:  79%|███████▉  | 1625/2048 [00:04<00:01, 343.01it/s]
Adding requests:  81%|████████  | 1660/2048 [00:04<00:01, 337.56it/s]
Adding requests:  83%|████████▎ | 1695/2048 [00:04<00:01, 340.58it/s]
Adding requests:  85%|████████▍ | 1732/2048 [00:04<00:00, 348.87it/s]
Adding requests:  86%|████████▋ | 1767/2048 [00:05<00:00, 345.92it/s]
Adding requests:  88%|████████▊ | 1802/2048 [00:05<00:00, 346.03it/s]
Adding requests:  90%|████████▉ | 1837/2048 [00:05<00:00, 346.09it/s]
Adding requests:  91%|█████████▏| 1873/2048 [00:05<00:00, 348.95it/s]
Adding requests:  93%|█████████▎| 1909/2048 [00:05<00:00, 351.21it/s]
Adding requests:  95%|█████████▌| 1947/2048 [00:05<00:00, 358.07it/s]
Adding requests:  97%|█████████▋| 1983/2048 [00:05<00:00, 357.25it/s]
Adding requests:  99%|█████████▊| 2019/2048 [00:05<00:00, 346.17it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 349.13it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▌      | 734/2048 [00:00<00:00, 3547.85it/s, est. speed input: 3633133.18 toks/s, output: 3547.90 toks/s]
Processed prompts:  53%|█████▎    | 1089/2048 [00:03<00:03, 293.49it/s, est. speed input: 368974.62 toks/s, output: 360.33 toks/s]  
Processed prompts:  61%|██████    | 1243/2048 [00:04<00:03, 236.91it/s, est. speed input: 305437.34 toks/s, output: 298.28 toks/s]
Processed prompts:  65%|██████▌   | 1333/2048 [00:04<00:03, 206.85it/s, est. speed input: 276901.93 toks/s, output: 270.41 toks/s]
Processed prompts:  68%|██████▊   | 1392/2048 [00:05<00:03, 188.96it/s, est. speed input: 261948.87 toks/s, output: 255.81 toks/s]
Processed prompts:  70%|███████   | 1434/2048 [00:05<00:03, 185.78it/s, est. speed input: 257657.92 toks/s, output: 251.62 toks/s]
Processed prompts:  72%|███████▏  | 1468/2048 [00:05<00:03, 179.24it/s, est. speed input: 252977.28 toks/s, output: 247.05 toks/s]
Processed prompts:  73%|███████▎  | 1495/2048 [00:06<00:03, 166.47it/s, est. speed input: 247070.35 toks/s, output: 241.28 toks/s]
Processed prompts:  74%|███████▍  | 1517/2048 [00:06<00:03, 167.27it/s, est. speed input: 245683.89 toks/s, output: 239.93 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:06<00:03, 147.54it/s, est. speed input: 239639.56 toks/s, output: 234.02 toks/s]
Processed prompts:  76%|███████▌  | 1555/2048 [00:06<00:03, 145.52it/s, est. speed input: 237695.24 toks/s, output: 232.12 toks/s]
Processed prompts:  77%|███████▋  | 1571/2048 [00:06<00:03, 143.07it/s, est. speed input: 235840.55 toks/s, output: 230.31 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:06<00:03, 137.79it/s, est. speed input: 233696.94 toks/s, output: 228.22 toks/s]
Processed prompts:  78%|███████▊  | 1600/2048 [00:07<00:03, 130.97it/s, est. speed input: 231428.48 toks/s, output: 226.00 toks/s]
Processed prompts:  79%|███████▉  | 1614/2048 [00:07<00:03, 127.36it/s, est. speed input: 229519.53 toks/s, output: 224.14 toks/s]
Processed prompts:  80%|███████▉  | 1630/2048 [00:07<00:03, 126.39it/s, est. speed input: 227702.51 toks/s, output: 222.37 toks/s]
Processed prompts:  80%|████████  | 1646/2048 [00:07<00:03, 125.64it/s, est. speed input: 225948.34 toks/s, output: 220.65 toks/s]
Processed prompts:  81%|████████  | 1662/2048 [00:07<00:03, 125.03it/s, est. speed input: 224249.61 toks/s, output: 218.99 toks/s]
Processed prompts:  82%|████████▏ | 1678/2048 [00:07<00:02, 124.89it/s, est. speed input: 222639.30 toks/s, output: 217.42 toks/s]
Processed prompts:  83%|████████▎ | 1694/2048 [00:07<00:02, 125.33it/s, est. speed input: 221136.38 toks/s, output: 215.95 toks/s]
Processed prompts:  83%|████████▎ | 1710/2048 [00:07<00:02, 125.17it/s, est. speed input: 219634.80 toks/s, output: 214.49 toks/s]
Processed prompts:  84%|████████▍ | 1726/2048 [00:08<00:02, 125.30it/s, est. speed input: 218203.54 toks/s, output: 213.09 toks/s]
Processed prompts:  85%|████████▌ | 1742/2048 [00:08<00:02, 127.07it/s, est. speed input: 216967.34 toks/s, output: 211.88 toks/s]
Processed prompts:  86%|████████▌ | 1758/2048 [00:08<00:02, 128.55it/s, est. speed input: 215782.83 toks/s, output: 210.73 toks/s]
Processed prompts:  87%|████████▋ | 1774/2048 [00:08<00:02, 127.25it/s, est. speed input: 214438.40 toks/s, output: 209.41 toks/s]
Processed prompts:  87%|████████▋ | 1790/2048 [00:08<00:02, 128.04it/s, est. speed input: 213271.95 toks/s, output: 208.27 toks/s]
Processed prompts:  88%|████████▊ | 1806/2048 [00:08<00:01, 127.09it/s, est. speed input: 212018.95 toks/s, output: 207.05 toks/s]
Processed prompts:  89%|████████▉ | 1822/2048 [00:08<00:01, 126.16it/s, est. speed input: 210780.01 toks/s, output: 205.84 toks/s]
Processed prompts:  90%|████████▉ | 1838/2048 [00:08<00:01, 125.57it/s, est. speed input: 209580.91 toks/s, output: 204.67 toks/s]
Processed prompts:  91%|█████████ | 1854/2048 [00:09<00:01, 125.18it/s, est. speed input: 208417.48 toks/s, output: 203.53 toks/s]
Processed prompts:  91%|█████████▏| 1870/2048 [00:09<00:01, 125.08it/s, est. speed input: 207299.93 toks/s, output: 202.44 toks/s]
Processed prompts:  92%|█████████▏| 1886/2048 [00:09<00:01, 126.94it/s, est. speed input: 206355.23 toks/s, output: 201.52 toks/s]
Processed prompts:  93%|█████████▎| 1902/2048 [00:09<00:01, 126.91it/s, est. speed input: 205338.15 toks/s, output: 200.53 toks/s]
Processed prompts:  94%|█████████▎| 1918/2048 [00:09<00:01, 125.59it/s, est. speed input: 204255.58 toks/s, output: 199.47 toks/s]
Processed prompts:  94%|█████████▍| 1934/2048 [00:09<00:00, 127.45it/s, est. speed input: 203395.97 toks/s, output: 198.63 toks/s]
Processed prompts:  95%|█████████▌| 1950/2048 [00:09<00:00, 126.37it/s, est. speed input: 202394.45 toks/s, output: 197.65 toks/s]
Processed prompts:  96%|█████████▌| 1966/2048 [00:09<00:00, 125.71it/s, est. speed input: 201424.77 toks/s, output: 196.70 toks/s]
Processed prompts:  97%|█████████▋| 1982/2048 [00:10<00:00, 127.45it/s, est. speed input: 200625.99 toks/s, output: 195.92 toks/s]
Processed prompts:  98%|█████████▊| 1998/2048 [00:10<00:00, 126.44it/s, est. speed input: 199700.65 toks/s, output: 195.02 toks/s]
Processed prompts:  98%|█████████▊| 2014/2048 [00:10<00:00, 126.04it/s, est. speed input: 198818.60 toks/s, output: 194.16 toks/s]
Processed prompts:  99%|█████████▉| 2030/2048 [00:10<00:00, 126.55it/s, est. speed input: 198007.31 toks/s, output: 193.37 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:10<00:00, 126.55it/s, est. speed input: 198948.56 toks/s, output: 194.29 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:10<00:00, 194.28it/s, est. speed input: 198948.56 toks/s, output: 194.29 toks/s]
[rank0]:[W126 16:48:03.251837759 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 16:48:04
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:48:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=790596) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=790596) WARNING 01-26 16:48:52 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=790596) WARNING 01-26 16:49:02 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 127.18 requests/s, 130361.69 total tokens/s, 127.18 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 16:48:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:48:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:48:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:48:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:48:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:48:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:48:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:48:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:48:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:48:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:48:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:48:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:48:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:48:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:48:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:48:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:48:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:43] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:43] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:43] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:43] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:43] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=790596) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=790596) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.28it/s]
(EngineCore_DP0 pid=790596) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
(EngineCore_DP0 pid=790596) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
(EngineCore_DP0 pid=790596) 
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=790596) [2026-01-26 16:48:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=790596) [rank0]:W0126 16:48:57.420000 790596 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=790596) [rank0]:W0126 16:48:57.501000 790596 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=790596) [rank0]:W0126 16:48:58.462000 790596 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=790596) [rank0]:W0126 16:48:58.589000 790596 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=790596) 2026-01-26 16:49:02,127 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=790596) 2026-01-26 16:49:02,154 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=790596) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 15.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 15.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 16.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 17.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 16.00it/s]
(EngineCore_DP0 pid=790596) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.81it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.01it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 17.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 15.61it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 275.53it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 318.39it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 316.02it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:12, 321.81it/s]
Adding requests:   4%|▍         | 165/4096 [00:00<00:11, 331.90it/s]
Adding requests:   5%|▍         | 202/4096 [00:00<00:11, 342.34it/s]
Adding requests:   6%|▌         | 238/4096 [00:00<00:11, 346.96it/s]
Adding requests:   7%|▋         | 273/4096 [00:00<00:11, 344.93it/s]
Adding requests:   8%|▊         | 310/4096 [00:00<00:10, 351.04it/s]
Adding requests:   8%|▊         | 346/4096 [00:01<00:10, 351.52it/s]
Adding requests:   9%|▉         | 383/4096 [00:01<00:10, 356.54it/s]
Adding requests:  10%|█         | 422/4096 [00:01<00:10, 363.57it/s]
Adding requests:  11%|█         | 459/4096 [00:01<00:10, 361.65it/s]
Adding requests:  12%|█▏        | 499/4096 [00:01<00:09, 369.71it/s]
Adding requests:  13%|█▎        | 538/4096 [00:01<00:09, 375.27it/s]
Adding requests:  14%|█▍        | 576/4096 [00:01<00:09, 374.85it/s]
Adding requests:  15%|█▍        | 614/4096 [00:01<00:09, 363.93it/s]
Adding requests:  16%|█▌        | 651/4096 [00:01<00:09, 357.67it/s]
Adding requests:  17%|█▋        | 689/4096 [00:01<00:09, 361.70it/s]
Adding requests:  18%|█▊        | 726/4096 [00:02<00:09, 355.17it/s]
Adding requests:  19%|█▊        | 762/4096 [00:02<00:09, 354.92it/s]
Adding requests:  19%|█▉        | 798/4096 [00:02<00:09, 355.10it/s]
Adding requests:  20%|██        | 836/4096 [00:02<00:09, 361.60it/s]
Adding requests:  21%|██▏       | 874/4096 [00:02<00:08, 364.06it/s]
Adding requests:  22%|██▏       | 911/4096 [00:02<00:08, 363.08it/s]
Adding requests:  23%|██▎       | 948/4096 [00:02<00:08, 357.54it/s]
Adding requests:  24%|██▍       | 984/4096 [00:02<00:08, 353.52it/s]
Adding requests:  25%|██▍       | 1020/4096 [00:02<00:08, 351.86it/s]
Adding requests:  26%|██▌       | 1056/4096 [00:02<00:08, 349.74it/s]
Adding requests:  27%|██▋       | 1092/4096 [00:03<00:08, 350.64it/s]
Adding requests:  28%|██▊       | 1130/4096 [00:03<00:08, 357.59it/s]
Adding requests:  28%|██▊       | 1166/4096 [00:03<00:08, 354.03it/s]
Adding requests:  29%|██▉       | 1202/4096 [00:03<00:08, 354.47it/s]
Adding requests:  30%|███       | 1240/4096 [00:03<00:07, 360.43it/s]
Adding requests:  31%|███       | 1277/4096 [00:03<00:07, 354.93it/s]
Adding requests:  32%|███▏      | 1313/4096 [00:03<00:07, 348.52it/s]
Adding requests:  33%|███▎      | 1348/4096 [00:03<00:07, 347.08it/s]
Adding requests:  34%|███▍      | 1385/4096 [00:03<00:07, 352.06it/s]
Adding requests:  35%|███▍      | 1421/4096 [00:04<00:07, 351.33it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:04<00:07, 353.31it/s]
Adding requests:  36%|███▋      | 1495/4096 [00:04<00:07, 360.10it/s]
Adding requests:  37%|███▋      | 1532/4096 [00:04<00:07, 359.43it/s]
Adding requests:  38%|███▊      | 1568/4096 [00:04<00:07, 353.80it/s]
Adding requests:  39%|███▉      | 1604/4096 [00:04<00:07, 352.62it/s]
Adding requests:  40%|████      | 1640/4096 [00:04<00:07, 345.51it/s]
Adding requests:  41%|████      | 1675/4096 [00:04<00:07, 342.62it/s]
Adding requests:  42%|████▏     | 1712/4096 [00:04<00:06, 350.48it/s]
Adding requests:  43%|████▎     | 1748/4096 [00:04<00:06, 353.09it/s]
Adding requests:  44%|████▎     | 1785/4096 [00:05<00:06, 356.90it/s]
Adding requests:  44%|████▍     | 1821/4096 [00:05<00:06, 352.94it/s]
Adding requests:  45%|████▌     | 1859/4096 [00:05<00:06, 357.83it/s]
Adding requests:  46%|████▋     | 1895/4096 [00:05<00:06, 357.96it/s]
Adding requests:  47%|████▋     | 1933/4096 [00:05<00:05, 363.93it/s]
Adding requests:  48%|████▊     | 1970/4096 [00:05<00:05, 363.78it/s]
Adding requests:  49%|████▉     | 2007/4096 [00:05<00:05, 359.86it/s]
Adding requests:  50%|████▉     | 2044/4096 [00:05<00:05, 354.13it/s]
Adding requests:  51%|█████     | 2080/4096 [00:05<00:05, 345.19it/s]
Adding requests:  52%|█████▏    | 2115/4096 [00:05<00:05, 345.92it/s]
Adding requests:  53%|█████▎    | 2151/4096 [00:06<00:05, 347.76it/s]
Adding requests:  53%|█████▎    | 2186/4096 [00:06<00:05, 341.78it/s]
Adding requests:  54%|█████▍    | 2221/4096 [00:06<00:05, 343.38it/s]
Adding requests:  55%|█████▌    | 2258/4096 [00:06<00:05, 348.42it/s]
Adding requests:  56%|█████▌    | 2295/4096 [00:06<00:05, 354.56it/s]
Adding requests:  57%|█████▋    | 2331/4096 [00:06<00:05, 349.65it/s]
Adding requests:  58%|█████▊    | 2368/4096 [00:06<00:04, 354.33it/s]
Adding requests:  59%|█████▊    | 2406/4096 [00:06<00:04, 361.38it/s]
Adding requests:  60%|█████▉    | 2443/4096 [00:06<00:04, 363.36it/s]
Adding requests:  61%|██████    | 2480/4096 [00:07<00:04, 362.10it/s]
Adding requests:  61%|██████▏   | 2518/4096 [00:07<00:04, 365.66it/s]
Adding requests:  62%|██████▏   | 2558/4096 [00:07<00:04, 373.08it/s]
Adding requests:  63%|██████▎   | 2597/4096 [00:07<00:03, 377.83it/s]
Adding requests:  64%|██████▍   | 2635/4096 [00:07<00:04, 364.79it/s]
Adding requests:  65%|██████▌   | 2672/4096 [00:07<00:03, 359.49it/s]
Adding requests:  66%|██████▌   | 2709/4096 [00:07<00:03, 355.20it/s]
Adding requests:  67%|██████▋   | 2747/4096 [00:07<00:03, 360.48it/s]
Adding requests:  68%|██████▊   | 2785/4096 [00:07<00:03, 365.19it/s]
Adding requests:  69%|██████▉   | 2823/4096 [00:07<00:03, 367.43it/s]
Adding requests:  70%|██████▉   | 2860/4096 [00:08<00:03, 367.62it/s]
Adding requests:  71%|███████   | 2897/4096 [00:08<00:03, 364.93it/s]
Adding requests:  72%|███████▏  | 2935/4096 [00:08<00:03, 367.32it/s]
Adding requests:  73%|███████▎  | 2972/4096 [00:08<00:03, 367.25it/s]
Adding requests:  73%|███████▎  | 3010/4096 [00:08<00:02, 369.54it/s]
Adding requests:  74%|███████▍  | 3048/4096 [00:08<00:02, 369.67it/s]
Adding requests:  75%|███████▌  | 3087/4096 [00:08<00:02, 373.95it/s]
Adding requests:  76%|███████▋  | 3125/4096 [00:08<00:02, 375.20it/s]
Adding requests:  77%|███████▋  | 3163/4096 [00:08<00:02, 370.83it/s]
Adding requests:  78%|███████▊  | 3201/4096 [00:08<00:02, 363.42it/s]
Adding requests:  79%|███████▉  | 3239/4096 [00:09<00:02, 368.19it/s]
Adding requests:  80%|███████▉  | 3276/4096 [00:09<00:02, 362.30it/s]
Adding requests:  81%|████████  | 3313/4096 [00:09<00:02, 353.96it/s]
Adding requests:  82%|████████▏ | 3349/4096 [00:09<00:02, 355.59it/s]
Adding requests:  83%|████████▎ | 3387/4096 [00:09<00:01, 361.25it/s]
Adding requests:  84%|████████▎ | 3424/4096 [00:09<00:01, 350.67it/s]
Adding requests:  84%|████████▍ | 3461/4096 [00:09<00:01, 355.25it/s]
Adding requests:  85%|████████▌ | 3497/4096 [00:09<00:01, 354.41it/s]
Adding requests:  86%|████████▋ | 3537/4096 [00:09<00:01, 367.36it/s]
Adding requests:  87%|████████▋ | 3574/4096 [00:10<00:01, 365.34it/s]
Adding requests:  88%|████████▊ | 3612/4096 [00:10<00:01, 367.06it/s]
Adding requests:  89%|████████▉ | 3649/4096 [00:10<00:01, 367.64it/s]
Adding requests:  90%|████████▉ | 3686/4096 [00:10<00:01, 357.60it/s]
Adding requests:  91%|█████████ | 3724/4096 [00:10<00:01, 361.02it/s]
Adding requests:  92%|█████████▏| 3761/4096 [00:10<00:00, 353.51it/s]
Adding requests:  93%|█████████▎| 3797/4096 [00:10<00:00, 343.81it/s]
Adding requests:  94%|█████████▎| 3832/4096 [00:10<00:00, 342.48it/s]
Adding requests:  94%|█████████▍| 3869/4096 [00:10<00:00, 349.14it/s]
Adding requests:  95%|█████████▌| 3904/4096 [00:10<00:00, 345.87it/s]
Adding requests:  96%|█████████▌| 3939/4096 [00:11<00:00, 344.29it/s]
Adding requests:  97%|█████████▋| 3974/4096 [00:11<00:00, 344.75it/s]
Adding requests:  98%|█████████▊| 4010/4096 [00:11<00:00, 348.08it/s]
Adding requests:  99%|█████████▉| 4045/4096 [00:11<00:00, 346.27it/s]
Adding requests: 100%|█████████▉| 4080/4096 [00:11<00:00, 346.99it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 355.92it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▌      | 1454/4096 [00:00<00:00, 5995.85it/s, est. speed input: 6140005.02 toks/s, output: 5995.92 toks/s]
Processed prompts:  50%|█████     | 2054/4096 [00:04<00:05, 345.27it/s, est. speed input: 442018.22 toks/s, output: 431.66 toks/s]   
Processed prompts:  56%|█████▋    | 2309/4096 [00:06<00:06, 262.36it/s, est. speed input: 349413.86 toks/s, output: 341.22 toks/s]
Processed prompts:  60%|█████▉    | 2454/4096 [00:08<00:07, 225.64it/s, est. speed input: 313650.58 toks/s, output: 306.30 toks/s]
Processed prompts:  62%|██████▏   | 2546/4096 [00:08<00:07, 207.36it/s, est. speed input: 297587.23 toks/s, output: 290.61 toks/s]
Processed prompts:  64%|██████▎   | 2610/4096 [00:09<00:07, 196.02it/s, est. speed input: 288690.81 toks/s, output: 281.92 toks/s]
Processed prompts:  65%|██████▍   | 2657/4096 [00:09<00:07, 195.30it/s, est. speed input: 286176.26 toks/s, output: 279.47 toks/s]
Processed prompts:  66%|██████▌   | 2695/4096 [00:09<00:07, 190.24it/s, est. speed input: 282851.34 toks/s, output: 276.22 toks/s]
Processed prompts:  67%|██████▋   | 2726/4096 [00:10<00:07, 180.39it/s, est. speed input: 278884.40 toks/s, output: 272.35 toks/s]
Processed prompts:  67%|██████▋   | 2751/4096 [00:10<00:08, 166.50it/s, est. speed input: 274551.07 toks/s, output: 268.12 toks/s]
Processed prompts:  68%|██████▊   | 2772/4096 [00:10<00:08, 150.24it/s, est. speed input: 270050.30 toks/s, output: 263.72 toks/s]
Processed prompts:  68%|██████▊   | 2798/4096 [00:10<00:09, 139.85it/s, est. speed input: 266195.83 toks/s, output: 259.96 toks/s]
Processed prompts:  69%|██████▉   | 2830/4096 [00:11<00:09, 137.04it/s, est. speed input: 263130.59 toks/s, output: 256.96 toks/s]
Processed prompts:  70%|██████▉   | 2862/4096 [00:11<00:09, 133.99it/s, est. speed input: 260070.56 toks/s, output: 253.97 toks/s]
Processed prompts:  71%|███████   | 2894/4096 [00:11<00:08, 134.44it/s, est. speed input: 257590.12 toks/s, output: 251.55 toks/s]
Processed prompts:  71%|███████▏  | 2926/4096 [00:11<00:08, 132.84it/s, est. speed input: 254927.51 toks/s, output: 248.95 toks/s]
Processed prompts:  72%|███████▏  | 2958/4096 [00:12<00:08, 131.67it/s, est. speed input: 252376.56 toks/s, output: 246.46 toks/s]
Processed prompts:  73%|███████▎  | 2990/4096 [00:12<00:08, 130.66it/s, est. speed input: 249908.12 toks/s, output: 244.05 toks/s]
Processed prompts:  74%|███████▍  | 3022/4096 [00:12<00:08, 129.37it/s, est. speed input: 247464.28 toks/s, output: 241.66 toks/s]
Processed prompts:  75%|███████▍  | 3054/4096 [00:12<00:08, 127.81it/s, est. speed input: 245035.39 toks/s, output: 239.29 toks/s]
Processed prompts:  75%|███████▌  | 3086/4096 [00:13<00:07, 127.87it/s, est. speed input: 242846.67 toks/s, output: 237.15 toks/s]
Processed prompts:  76%|███████▌  | 3118/4096 [00:13<00:07, 127.32it/s, est. speed input: 240668.09 toks/s, output: 235.03 toks/s]
Processed prompts:  77%|███████▋  | 3150/4096 [00:13<00:07, 128.15it/s, est. speed input: 238714.69 toks/s, output: 233.12 toks/s]
Processed prompts:  78%|███████▊  | 3182/4096 [00:13<00:07, 127.87it/s, est. speed input: 236732.52 toks/s, output: 231.18 toks/s]
Processed prompts:  78%|███████▊  | 3214/4096 [00:14<00:06, 127.49it/s, est. speed input: 234801.77 toks/s, output: 229.30 toks/s]
Processed prompts:  79%|███████▉  | 3246/4096 [00:14<00:06, 127.02it/s, est. speed input: 232917.23 toks/s, output: 227.46 toks/s]
Processed prompts:  80%|████████  | 3278/4096 [00:14<00:06, 126.65it/s, est. speed input: 231093.28 toks/s, output: 225.68 toks/s]
Processed prompts:  81%|████████  | 3310/4096 [00:14<00:06, 126.62it/s, est. speed input: 229356.97 toks/s, output: 223.98 toks/s]
Processed prompts:  82%|████████▏ | 3342/4096 [00:15<00:05, 126.30it/s, est. speed input: 227648.54 toks/s, output: 222.31 toks/s]
Processed prompts:  82%|████████▏ | 3374/4096 [00:15<00:05, 125.76it/s, est. speed input: 225965.35 toks/s, output: 220.67 toks/s]
Processed prompts:  83%|████████▎ | 3406/4096 [00:15<00:05, 126.56it/s, est. speed input: 224451.61 toks/s, output: 219.19 toks/s]
Processed prompts:  84%|████████▍ | 3438/4096 [00:15<00:05, 126.81it/s, est. speed input: 222956.49 toks/s, output: 217.73 toks/s]
Processed prompts:  85%|████████▍ | 3470/4096 [00:16<00:04, 126.65it/s, est. speed input: 221476.98 toks/s, output: 216.29 toks/s]
Processed prompts:  85%|████████▌ | 3502/4096 [00:16<00:04, 126.73it/s, est. speed input: 220061.15 toks/s, output: 214.90 toks/s]
Processed prompts:  86%|████████▋ | 3534/4096 [00:16<00:04, 125.79it/s, est. speed input: 218600.10 toks/s, output: 213.48 toks/s]
Processed prompts:  87%|████████▋ | 3566/4096 [00:16<00:04, 127.53it/s, est. speed input: 217391.24 toks/s, output: 212.30 toks/s]
Processed prompts:  88%|████████▊ | 3598/4096 [00:17<00:03, 127.06it/s, est. speed input: 216074.33 toks/s, output: 211.01 toks/s]
Processed prompts:  89%|████████▊ | 3630/4096 [00:17<00:03, 127.47it/s, est. speed input: 214856.52 toks/s, output: 209.82 toks/s]
Processed prompts:  89%|████████▉ | 3662/4096 [00:17<00:03, 127.73it/s, est. speed input: 213671.31 toks/s, output: 208.66 toks/s]
Processed prompts:  90%|█████████ | 3694/4096 [00:17<00:03, 128.33it/s, est. speed input: 212551.08 toks/s, output: 207.57 toks/s]
Processed prompts:  91%|█████████ | 3726/4096 [00:18<00:02, 128.25it/s, est. speed input: 211424.35 toks/s, output: 206.47 toks/s]
Processed prompts:  92%|█████████▏| 3758/4096 [00:18<00:02, 127.79it/s, est. speed input: 210297.45 toks/s, output: 205.37 toks/s]
Processed prompts:  93%|█████████▎| 3790/4096 [00:18<00:02, 126.95it/s, est. speed input: 209162.45 toks/s, output: 204.26 toks/s]
Processed prompts:  93%|█████████▎| 3822/4096 [00:18<00:02, 126.14it/s, est. speed input: 208041.92 toks/s, output: 203.17 toks/s]
Processed prompts:  94%|█████████▍| 3854/4096 [00:19<00:01, 125.72it/s, est. speed input: 206962.04 toks/s, output: 202.11 toks/s]
Processed prompts:  95%|█████████▍| 3886/4096 [00:19<00:01, 124.97it/s, est. speed input: 205877.11 toks/s, output: 201.05 toks/s]
Processed prompts:  96%|█████████▌| 3918/4096 [00:19<00:01, 128.53it/s, est. speed input: 205106.39 toks/s, output: 200.30 toks/s]
Processed prompts:  96%|█████████▋| 3950/4096 [00:19<00:01, 129.00it/s, est. speed input: 204213.60 toks/s, output: 199.43 toks/s]
Processed prompts:  97%|█████████▋| 3982/4096 [00:20<00:00, 128.97it/s, est. speed input: 203319.51 toks/s, output: 198.55 toks/s]
Processed prompts:  98%|█████████▊| 4014/4096 [00:20<00:00, 128.03it/s, est. speed input: 202388.42 toks/s, output: 197.64 toks/s]
Processed prompts:  99%|█████████▉| 4046/4096 [00:20<00:00, 130.57it/s, est. speed input: 201680.69 toks/s, output: 196.95 toks/s]
Processed prompts: 100%|█████████▉| 4078/4096 [00:20<00:00, 147.34it/s, est. speed input: 201781.90 toks/s, output: 197.05 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:20<00:00, 147.34it/s, est. speed input: 202670.05 toks/s, output: 197.92 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:20<00:00, 197.92it/s, est. speed input: 202670.05 toks/s, output: 197.92 toks/s]
[rank0]:[W126 16:49:37.694390783 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 16:49:39
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:50:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=792769) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=792769) WARNING 01-26 16:50:51 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     def forward(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     raise e
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/tmp/torchinductor_root/m5/cm5haedjxfph5usng6bbkwlykhb54lapizg4zoxkgvftcobafk7o.py", line 1093, in call
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 8)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 221, in quant_slide_fp8_triton
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) ERROR 01-26 16:50:57 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered

STDERR:
[2026-01-26 16:50:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:50:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:50:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:50:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:50:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:50:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:50:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:50:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:50:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:50:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:50:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:50:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:50:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:50:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:50:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:50:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:50:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:42] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:42] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:42] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:42] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:42] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=792769) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=792769) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.28it/s]
(EngineCore_DP0 pid=792769) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
(EngineCore_DP0 pid=792769) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.04it/s]
(EngineCore_DP0 pid=792769) 
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=792769) [2026-01-26 16:50:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=792769) [rank0]:W0126 16:50:56.209000 792769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=792769) [rank0]:W0126 16:50:56.290000 792769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=792769) [rank0]:W0126 16:50:57.356000 792769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=792769) [rank0]:W0126 16:50:57.479000 792769 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=792769) Process EngineCore_DP0:
(EngineCore_DP0 pid=792769) Traceback (most recent call last):
(EngineCore_DP0 pid=792769)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=792769)     self.run()
(EngineCore_DP0 pid=792769)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=792769)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=792769)     raise e
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=792769)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=792769)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=792769)     super().__init__(
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=792769)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=792769)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=792769)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=792769)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=792769)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=792769)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=792769)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=792769)     return func(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=792769)     return func(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=792769)     self.model_runner.profile_run()
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=792769)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=792769)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=792769)     return func(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=792769)     outputs = self.model(
(EngineCore_DP0 pid=792769)               ^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=792769)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=792769)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=792769)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=792769)     hidden_states = self.model(
(EngineCore_DP0 pid=792769)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=792769)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=792769)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=792769)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=792769)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=792769)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=792769)     def forward(
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=792769)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=792769)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=792769)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=792769)     raise e
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=792769)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=792769)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=792769)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=792769)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=792769)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=792769)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=792769)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=792769)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=792769)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=792769)     return compiled_fn(full_args)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=792769)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=792769)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=792769)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=792769)                             ^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=792769)     outs = compiled_fn(args)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=792769)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=792769)     return self.current_callable(inputs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=792769)     out = model(new_inputs)
(EngineCore_DP0 pid=792769)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/tmp/torchinductor_root/m5/cm5haedjxfph5usng6bbkwlykhb54lapizg4zoxkgvftcobafk7o.py", line 1093, in call
(EngineCore_DP0 pid=792769)     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 8)
(EngineCore_DP0 pid=792769)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=792769)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=792769)     return fn(input, L)
(EngineCore_DP0 pid=792769)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 221, in quant_slide_fp8_triton
(EngineCore_DP0 pid=792769)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=792769)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=792769)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=792769)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=792769)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=792769)     self._init_handles()
(EngineCore_DP0 pid=792769)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=792769)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=792769)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=792769) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 16:50:58.576229254 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-26 18:37:16
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:37:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=922072) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=922072) WARNING 01-26 18:37:46 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=922072) WARNING 01-26 18:38:01 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 23.01 requests/s, 11803.85 total tokens/s, 23.01 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 18:37:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:37:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:37:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:37:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:37:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:37:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:37:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:37:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:37:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:37:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:37:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:37:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:37:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:37:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:37:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:37:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:37:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:37:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:37:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:37:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:37:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:37:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:37:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:37:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:37:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:37:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:37:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:37:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=922072) [2026-01-26 18:37:31] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=922072) [2026-01-26 18:37:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=922072) [2026-01-26 18:37:31] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=922072) [2026-01-26 18:37:31] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=922072) [2026-01-26 18:37:31] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=922072) [2026-01-26 18:37:31] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=922072) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=922072) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.29s/it]
(EngineCore_DP0 pid=922072) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
(EngineCore_DP0 pid=922072) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
(EngineCore_DP0 pid=922072) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.21s/it]
(EngineCore_DP0 pid=922072) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
(EngineCore_DP0 pid=922072) 
(EngineCore_DP0 pid=922072) [2026-01-26 18:37:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=922072) [2026-01-26 18:37:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=922072) [2026-01-26 18:37:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=922072) [2026-01-26 18:37:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=922072) [2026-01-26 18:37:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=922072) [2026-01-26 18:37:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=922072) [2026-01-26 18:37:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=922072) [2026-01-26 18:37:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=922072) 2026-01-26 18:38:01,917 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=922072) 2026-01-26 18:38:01,958 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=922072) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.85it/s]
(EngineCore_DP0 pid=922072) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.85it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 511.33it/s]
Adding requests:  86%|████████▌ | 110/128 [00:00<00:00, 548.74it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 549.46it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:03, 37.56it/s, est. speed input: 19233.86 toks/s, output: 37.56 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 28.76it/s, est. speed input: 15323.06 toks/s, output: 29.93 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:04, 26.67it/s, est. speed input: 14372.15 toks/s, output: 28.07 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:04, 25.63it/s, est. speed input: 13880.97 toks/s, output: 27.11 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:04, 25.02it/s, est. speed input: 13571.45 toks/s, output: 26.51 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:04, 24.62it/s, est. speed input: 13355.54 toks/s, output: 26.08 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:04, 24.38it/s, est. speed input: 13203.04 toks/s, output: 25.79 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:04, 24.23it/s, est. speed input: 13088.57 toks/s, output: 25.56 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:04, 24.02it/s, est. speed input: 12977.66 toks/s, output: 25.35 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:03, 23.95it/s, est. speed input: 12900.59 toks/s, output: 25.20 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:03, 23.92it/s, est. speed input: 12839.72 toks/s, output: 25.08 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:03, 23.88it/s, est. speed input: 12786.87 toks/s, output: 24.97 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:03, 23.91it/s, est. speed input: 12749.12 toks/s, output: 24.90 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:03, 23.76it/s, est. speed input: 12695.39 toks/s, output: 24.80 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:03, 23.78it/s, est. speed input: 12663.23 toks/s, output: 24.73 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:03, 23.80it/s, est. speed input: 12635.26 toks/s, output: 24.68 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:02<00:03, 23.57it/s, est. speed input: 12586.57 toks/s, output: 24.58 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:02<00:03, 23.66it/s, est. speed input: 12566.58 toks/s, output: 24.54 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:02<00:02, 23.70it/s, est. speed input: 12546.83 toks/s, output: 24.51 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:02<00:02, 23.44it/s, est. speed input: 12504.05 toks/s, output: 24.42 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:02<00:02, 23.54it/s, est. speed input: 12488.65 toks/s, output: 24.39 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:02, 23.61it/s, est. speed input: 12474.84 toks/s, output: 24.36 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:02<00:02, 23.67it/s, est. speed input: 12462.39 toks/s, output: 24.34 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:03<00:02, 23.37it/s, est. speed input: 12426.31 toks/s, output: 24.27 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:03<00:02, 23.43it/s, est. speed input: 12412.41 toks/s, output: 24.24 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:03<00:01, 23.54it/s, est. speed input: 12403.54 toks/s, output: 24.23 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:03<00:01, 23.34it/s, est. speed input: 12377.59 toks/s, output: 24.17 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:03<00:01, 23.47it/s, est. speed input: 12370.61 toks/s, output: 24.16 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:03<00:01, 23.55it/s, est. speed input: 12363.48 toks/s, output: 24.15 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:03<00:01, 23.51it/s, est. speed input: 12350.81 toks/s, output: 24.12 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:03<00:01, 23.55it/s, est. speed input: 12343.06 toks/s, output: 24.11 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:04<00:01, 23.60it/s, est. speed input: 12337.06 toks/s, output: 24.10 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:04<00:01, 23.61it/s, est. speed input: 12329.80 toks/s, output: 24.08 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:04<00:00, 23.68it/s, est. speed input: 12326.35 toks/s, output: 24.07 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:04<00:00, 23.68it/s, est. speed input: 12320.65 toks/s, output: 24.06 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:04<00:00, 23.76it/s, est. speed input: 12318.84 toks/s, output: 24.06 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:04<00:00, 23.76it/s, est. speed input: 12314.76 toks/s, output: 24.05 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:04<00:00, 23.83it/s, est. speed input: 12314.02 toks/s, output: 24.05 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:04<00:00, 23.61it/s, est. speed input: 12301.45 toks/s, output: 24.03 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:05<00:00, 23.70it/s, est. speed input: 12300.20 toks/s, output: 24.02 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:05<00:00, 23.78it/s, est. speed input: 12299.35 toks/s, output: 24.02 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.78it/s, est. speed input: 12298.63 toks/s, output: 24.02 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 24.02it/s, est. speed input: 12298.63 toks/s, output: 24.02 toks/s]
[rank0]:[W126 18:38:10.525814398 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 18:38:12
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:38:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=923489) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=923489) WARNING 01-26 18:38:42 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=923489) WARNING 01-26 18:38:56 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 23.66 requests/s, 24250.27 total tokens/s, 23.66 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 18:38:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:38:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:38:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:38:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:38:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:38:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:38:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:38:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:38:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:38:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:38:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:38:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:38:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:38:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:38:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:38:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:38:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:38:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:38:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:38:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:38:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:38:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:38:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:38:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:38:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:38:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:38:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:38:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=923489) [2026-01-26 18:38:27] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=923489) [2026-01-26 18:38:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=923489) [2026-01-26 18:38:27] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=923489) [2026-01-26 18:38:27] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=923489) [2026-01-26 18:38:27] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=923489) [2026-01-26 18:38:27] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=923489) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=923489) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.40s/it]
(EngineCore_DP0 pid=923489) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]
(EngineCore_DP0 pid=923489) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.05it/s]
(EngineCore_DP0 pid=923489) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
(EngineCore_DP0 pid=923489) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]
(EngineCore_DP0 pid=923489) 
(EngineCore_DP0 pid=923489) [2026-01-26 18:38:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=923489) [2026-01-26 18:38:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=923489) [2026-01-26 18:38:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=923489) [2026-01-26 18:38:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=923489) [2026-01-26 18:38:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=923489) [2026-01-26 18:38:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=923489) [2026-01-26 18:38:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=923489) [2026-01-26 18:38:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=923489) 2026-01-26 18:38:56,462 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=923489) 2026-01-26 18:38:56,501 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=923489) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.79it/s]
(EngineCore_DP0 pid=923489) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 10.10it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 273.55it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 317.60it/s]
Adding requests:  75%|███████▌  | 96/128 [00:00<00:00, 316.52it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 184.69it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 215.98it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:00, 141.33it/s, est. speed input: 144734.82 toks/s, output: 141.34 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 36.69it/s, est. speed input: 42260.82 toks/s, output: 41.27 toks/s]   
Processed prompts:  30%|██▉       | 38/128 [00:01<00:02, 31.61it/s, est. speed input: 36770.40 toks/s, output: 35.91 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 29.31it/s, est. speed input: 34454.89 toks/s, output: 33.65 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 28.15it/s, est. speed input: 33353.49 toks/s, output: 32.57 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 26.94it/s, est. speed input: 32363.90 toks/s, output: 31.61 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:02, 26.22it/s, est. speed input: 31661.63 toks/s, output: 30.92 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:02, 25.77it/s, est. speed input: 31212.42 toks/s, output: 30.48 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:02<00:02, 25.39it/s, est. speed input: 30823.44 toks/s, output: 30.10 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:02, 24.90it/s, est. speed input: 30427.71 toks/s, output: 29.71 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:02<00:02, 24.70it/s, est. speed input: 30119.79 toks/s, output: 29.41 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:02, 24.55it/s, est. speed input: 29845.26 toks/s, output: 29.15 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:02, 24.45it/s, est. speed input: 29599.54 toks/s, output: 28.91 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:02, 24.11it/s, est. speed input: 29323.76 toks/s, output: 28.64 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 24.11it/s, est. speed input: 29119.85 toks/s, output: 28.44 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 24.11it/s, est. speed input: 28931.93 toks/s, output: 28.25 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:03<00:01, 24.11it/s, est. speed input: 28759.44 toks/s, output: 28.09 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:03<00:01, 23.62it/s, est. speed input: 28523.03 toks/s, output: 27.85 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:03<00:01, 23.76it/s, est. speed input: 28377.68 toks/s, output: 27.71 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:03<00:01, 23.90it/s, est. speed input: 28250.69 toks/s, output: 27.59 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:03<00:01, 23.97it/s, est. speed input: 28126.50 toks/s, output: 27.47 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:01, 23.76it/s, est. speed input: 27977.75 toks/s, output: 27.32 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:03<00:01, 23.86it/s, est. speed input: 27869.90 toks/s, output: 27.22 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:03<00:00, 23.95it/s, est. speed input: 27772.17 toks/s, output: 27.12 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:04<00:00, 23.99it/s, est. speed input: 27676.91 toks/s, output: 27.03 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:04<00:00, 23.90it/s, est. speed input: 27573.99 toks/s, output: 26.93 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:04<00:00, 23.98it/s, est. speed input: 27492.95 toks/s, output: 26.85 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:04<00:00, 24.06it/s, est. speed input: 27418.52 toks/s, output: 26.78 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:04<00:00, 24.11it/s, est. speed input: 27347.99 toks/s, output: 26.71 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:04<00:00, 24.11it/s, est. speed input: 27277.29 toks/s, output: 26.64 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 24.13it/s, est. speed input: 27212.87 toks/s, output: 26.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 24.13it/s, est. speed input: 27212.87 toks/s, output: 26.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 26.57it/s, est. speed input: 27212.87 toks/s, output: 26.58 toks/s]
[rank0]:[W126 18:39:03.193826982 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 18:39:05
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:39:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=924775) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=924775) WARNING 01-26 18:39:35 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=924775) WARNING 01-26 18:39:50 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 45.48 requests/s, 46613.04 total tokens/s, 45.48 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 18:39:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:39:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:39:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:39:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:39:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:39:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:39:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:39:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:39:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:39:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:39:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:39:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:39:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:39:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:39:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:39:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:39:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:39:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:39:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:39:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:39:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:39:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:39:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:39:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:39:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:39:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:39:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:39:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=924775) [2026-01-26 18:39:21] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=924775) [2026-01-26 18:39:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=924775) [2026-01-26 18:39:21] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=924775) [2026-01-26 18:39:21] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=924775) [2026-01-26 18:39:21] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=924775) [2026-01-26 18:39:21] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=924775) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=924775) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.35s/it]
(EngineCore_DP0 pid=924775) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.34it/s]
(EngineCore_DP0 pid=924775) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.13it/s]
(EngineCore_DP0 pid=924775) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.00s/it]
(EngineCore_DP0 pid=924775) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.02it/s]
(EngineCore_DP0 pid=924775) 
(EngineCore_DP0 pid=924775) [2026-01-26 18:39:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=924775) [2026-01-26 18:39:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=924775) [2026-01-26 18:39:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=924775) [2026-01-26 18:39:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=924775) [2026-01-26 18:39:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=924775) [2026-01-26 18:39:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=924775) [2026-01-26 18:39:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=924775) [2026-01-26 18:39:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=924775) 2026-01-26 18:39:50,628 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=924775) 2026-01-26 18:39:50,666 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=924775) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.00it/s]
(EngineCore_DP0 pid=924775) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.03it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:49,  5.16it/s]
Adding requests:  11%|█▏        | 29/256 [00:00<00:01, 121.54it/s]
Adding requests:  25%|██▌       | 65/256 [00:00<00:00, 209.08it/s]
Adding requests:  38%|███▊      | 97/256 [00:00<00:00, 245.18it/s]
Adding requests:  51%|█████     | 130/256 [00:00<00:00, 272.52it/s]
Adding requests:  64%|██████▍   | 165/256 [00:00<00:00, 297.01it/s]
Adding requests:  79%|███████▉  | 202/256 [00:00<00:00, 316.71it/s]
Adding requests:  93%|█████████▎| 237/256 [00:00<00:00, 326.00it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 264.88it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:00<00:00, 314.22it/s, est. speed input: 321776.64 toks/s, output: 314.22 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:00<00:02, 77.53it/s, est. speed input: 90178.36 toks/s, output: 88.06 toks/s]   
Processed prompts:  33%|███▎      | 84/256 [00:01<00:02, 66.04it/s, est. speed input: 77892.78 toks/s, output: 76.07 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:01<00:02, 62.86it/s, est. speed input: 74294.96 toks/s, output: 72.55 toks/s]
Processed prompts:  41%|████      | 104/256 [00:01<00:02, 57.44it/s, est. speed input: 70121.11 toks/s, output: 68.48 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:01<00:02, 55.41it/s, est. speed input: 68152.16 toks/s, output: 66.55 toks/s]
Processed prompts:  46%|████▋     | 119/256 [00:01<00:02, 55.69it/s, est. speed input: 67468.90 toks/s, output: 65.89 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:01<00:02, 52.13it/s, est. speed input: 65501.56 toks/s, output: 63.97 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:02<00:02, 51.11it/s, est. speed input: 64506.65 toks/s, output: 62.99 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:02<00:02, 50.50it/s, est. speed input: 63688.50 toks/s, output: 62.20 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:02<00:02, 50.02it/s, est. speed input: 62957.92 toks/s, output: 61.48 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:02<00:02, 49.68it/s, est. speed input: 62306.22 toks/s, output: 60.85 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:02<00:02, 49.40it/s, est. speed input: 61714.02 toks/s, output: 60.27 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:02<00:01, 49.13it/s, est. speed input: 61161.49 toks/s, output: 59.73 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:02<00:01, 48.98it/s, est. speed input: 60665.49 toks/s, output: 59.24 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:02<00:01, 48.92it/s, est. speed input: 60220.79 toks/s, output: 58.81 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:03<00:01, 48.82it/s, est. speed input: 59800.94 toks/s, output: 58.40 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:03<00:01, 48.76it/s, est. speed input: 59415.51 toks/s, output: 58.02 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:03<00:01, 48.66it/s, est. speed input: 59049.38 toks/s, output: 57.67 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:03<00:01, 48.63it/s, est. speed input: 58715.72 toks/s, output: 57.34 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:03<00:01, 47.46it/s, est. speed input: 58241.55 toks/s, output: 56.88 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:03<00:00, 47.72it/s, est. speed input: 57949.88 toks/s, output: 56.59 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:03<00:00, 47.91it/s, est. speed input: 57676.85 toks/s, output: 56.32 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:03<00:00, 48.09it/s, est. speed input: 57426.82 toks/s, output: 56.08 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:04<00:00, 48.11it/s, est. speed input: 57179.54 toks/s, output: 55.84 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:04<00:00, 48.23it/s, est. speed input: 56958.53 toks/s, output: 55.62 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:04<00:00, 48.30it/s, est. speed input: 56749.00 toks/s, output: 55.42 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:04<00:00, 48.36it/s, est. speed input: 56552.30 toks/s, output: 55.23 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:04<00:00, 48.38it/s, est. speed input: 56363.34 toks/s, output: 55.04 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 48.38it/s, est. speed input: 56232.66 toks/s, output: 54.91 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 54.91it/s, est. speed input: 56232.66 toks/s, output: 54.91 toks/s]
[rank0]:[W126 18:39:58.699304472 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 18:40:00
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:40:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=926097) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=926097) WARNING 01-26 18:40:32 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=926097) WARNING 01-26 18:40:47 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 60.56 requests/s, 62075.76 total tokens/s, 60.56 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 18:40:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:40:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:40:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:40:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:40:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:40:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:40:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:40:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:40:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:40:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:40:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:40:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:40:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:40:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:40:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:40:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:40:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:40:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:40:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:40:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:40:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:40:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:40:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:40:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:40:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:40:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:40:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:40:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=926097) [2026-01-26 18:40:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=926097) [2026-01-26 18:40:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=926097) [2026-01-26 18:40:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=926097) [2026-01-26 18:40:18] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=926097) [2026-01-26 18:40:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=926097) [2026-01-26 18:40:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=926097) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=926097) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.38s/it]
(EngineCore_DP0 pid=926097) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
(EngineCore_DP0 pid=926097) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.13it/s]
(EngineCore_DP0 pid=926097) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.00it/s]
(EngineCore_DP0 pid=926097) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.02it/s]
(EngineCore_DP0 pid=926097) 
(EngineCore_DP0 pid=926097) [2026-01-26 18:40:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=926097) [2026-01-26 18:40:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=926097) [2026-01-26 18:40:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=926097) [2026-01-26 18:40:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=926097) [2026-01-26 18:40:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=926097) [2026-01-26 18:40:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=926097) [2026-01-26 18:40:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=926097) [2026-01-26 18:40:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=926097) 2026-01-26 18:40:47,504 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=926097) 2026-01-26 18:40:47,542 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=926097) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  9.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.20it/s]
(EngineCore_DP0 pid=926097) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 11.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 11.10it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 28/512 [00:00<00:01, 277.84it/s]
Adding requests:  12%|█▏        | 63/512 [00:00<00:01, 319.60it/s]
Adding requests:  19%|█▊        | 95/512 [00:00<00:01, 315.85it/s]
Adding requests:  25%|██▌       | 129/512 [00:00<00:01, 322.36it/s]
Adding requests:  32%|███▏      | 164/512 [00:00<00:01, 331.08it/s]
Adding requests:  39%|███▉      | 201/512 [00:00<00:00, 340.48it/s]
Adding requests:  46%|████▋     | 237/512 [00:00<00:00, 345.43it/s]
Adding requests:  53%|█████▎    | 272/512 [00:00<00:00, 342.66it/s]
Adding requests:  60%|██████    | 309/512 [00:00<00:00, 349.00it/s]
Adding requests:  68%|██████▊   | 347/512 [00:01<00:00, 355.87it/s]
Adding requests:  75%|███████▌  | 384/512 [00:01<00:00, 357.96it/s]
Adding requests:  82%|████████▏ | 422/512 [00:01<00:00, 364.54it/s]
Adding requests:  90%|████████▉ | 459/512 [00:01<00:00, 361.29it/s]
Adding requests:  97%|█████████▋| 498/512 [00:01<00:00, 369.62it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 349.54it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:00<00:00, 747.82it/s, est. speed input: 765817.32 toks/s, output: 747.84 toks/s]
Processed prompts:  32%|███▏      | 165/512 [00:01<00:03, 108.63it/s, est. speed input: 129328.64 toks/s, output: 126.30 toks/s]
Processed prompts:  39%|███▉      | 200/512 [00:01<00:03, 88.95it/s, est. speed input: 107981.27 toks/s, output: 105.45 toks/s] 
Processed prompts:  43%|████▎     | 222/512 [00:02<00:03, 80.09it/s, est. speed input: 99449.64 toks/s, output: 97.12 toks/s]  
Processed prompts:  46%|████▋     | 238/512 [00:02<00:03, 76.04it/s, est. speed input: 95592.94 toks/s, output: 93.35 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:02<00:03, 73.32it/s, est. speed input: 93229.33 toks/s, output: 91.04 toks/s]
Processed prompts:  51%|█████     | 260/512 [00:02<00:03, 73.75it/s, est. speed input: 92539.43 toks/s, output: 90.37 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:03<00:03, 68.23it/s, est. speed input: 89947.94 toks/s, output: 87.84 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:03<00:03, 66.85it/s, est. speed input: 88813.79 toks/s, output: 86.73 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:03<00:03, 65.81it/s, est. speed input: 87825.85 toks/s, output: 85.77 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:03<00:03, 64.56it/s, est. speed input: 86836.38 toks/s, output: 84.80 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:03<00:03, 63.57it/s, est. speed input: 85925.54 toks/s, output: 83.91 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:03<00:03, 64.11it/s, est. speed input: 85318.60 toks/s, output: 83.32 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:03<00:03, 63.34it/s, est. speed input: 84554.00 toks/s, output: 82.57 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:03<00:02, 62.61it/s, est. speed input: 83816.54 toks/s, output: 81.85 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:04<00:02, 61.74it/s, est. speed input: 83074.43 toks/s, output: 81.13 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:04<00:02, 61.48it/s, est. speed input: 82432.61 toks/s, output: 80.50 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:04<00:02, 61.45it/s, est. speed input: 81848.78 toks/s, output: 79.93 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:04<00:02, 61.38it/s, est. speed input: 81293.83 toks/s, output: 79.39 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:04<00:02, 61.47it/s, est. speed input: 80787.34 toks/s, output: 78.89 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:04<00:02, 61.37it/s, est. speed input: 80288.41 toks/s, output: 78.41 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:04<00:02, 61.15it/s, est. speed input: 79798.71 toks/s, output: 77.93 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:05<00:01, 61.22it/s, est. speed input: 79360.39 toks/s, output: 77.50 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:05<00:01, 60.79it/s, est. speed input: 78890.95 toks/s, output: 77.04 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:05<00:01, 60.86it/s, est. speed input: 78485.46 toks/s, output: 76.65 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:05<00:01, 60.95it/s, est. speed input: 78103.39 toks/s, output: 76.27 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:05<00:01, 61.00it/s, est. speed input: 77737.93 toks/s, output: 75.92 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:05<00:01, 61.07it/s, est. speed input: 77393.18 toks/s, output: 75.58 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:05<00:01, 62.66it/s, est. speed input: 77205.64 toks/s, output: 75.40 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:05<00:01, 61.81it/s, est. speed input: 76849.04 toks/s, output: 75.05 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:06<00:00, 61.50it/s, est. speed input: 76532.71 toks/s, output: 74.74 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:06<00:00, 61.40it/s, est. speed input: 76239.35 toks/s, output: 74.45 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:06<00:00, 61.31it/s, est. speed input: 75956.94 toks/s, output: 74.18 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:06<00:00, 61.16it/s, est. speed input: 75679.07 toks/s, output: 73.91 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:06<00:00, 60.93it/s, est. speed input: 75401.49 toks/s, output: 73.63 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:06<00:00, 61.04it/s, est. speed input: 75156.07 toks/s, output: 73.39 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:06<00:00, 61.25it/s, est. speed input: 74931.06 toks/s, output: 73.17 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:06<00:00, 61.64it/s, est. speed input: 74732.39 toks/s, output: 72.98 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 61.64it/s, est. speed input: 75024.66 toks/s, output: 73.27 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 73.27it/s, est. speed input: 75024.66 toks/s, output: 73.27 toks/s]
[rank0]:[W126 18:40:58.783910261 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 18:41:00
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:41:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=927516) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=927516) WARNING 01-26 18:41:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=927516) WARNING 01-26 18:41:51 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 64.50 requests/s, 66109.15 total tokens/s, 64.50 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 18:41:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:41:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:41:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:41:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:41:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:41:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:41:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:41:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:41:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:41:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:41:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:41:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:41:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:41:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:41:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:41:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:41:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:41:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:41:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:41:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:41:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:41:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:41:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:41:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:41:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:41:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:41:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:41:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=927516) [2026-01-26 18:41:21] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=927516) [2026-01-26 18:41:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=927516) [2026-01-26 18:41:21] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=927516) [2026-01-26 18:41:21] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=927516) [2026-01-26 18:41:21] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=927516) [2026-01-26 18:41:21] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=927516) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=927516) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.28s/it]
(EngineCore_DP0 pid=927516) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.35it/s]
(EngineCore_DP0 pid=927516) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.02it/s]
(EngineCore_DP0 pid=927516) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
(EngineCore_DP0 pid=927516) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]
(EngineCore_DP0 pid=927516) 
(EngineCore_DP0 pid=927516) [2026-01-26 18:41:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=927516) [2026-01-26 18:41:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=927516) [2026-01-26 18:41:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=927516) [2026-01-26 18:41:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=927516) [2026-01-26 18:41:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=927516) [2026-01-26 18:41:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=927516) [2026-01-26 18:41:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=927516) [2026-01-26 18:41:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=927516) 2026-01-26 18:41:51,759 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=927516) 2026-01-26 18:41:51,799 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=927516) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 10.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.35it/s]
(EngineCore_DP0 pid=927516) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 11.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 11.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 11.13it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 276.04it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 316.76it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 314.41it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 320.31it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 329.80it/s]
Adding requests:  20%|█▉        | 200/1024 [00:00<00:02, 340.93it/s]
Adding requests:  23%|██▎       | 236/1024 [00:00<00:02, 345.43it/s]
Adding requests:  26%|██▋       | 271/1024 [00:00<00:02, 343.64it/s]
Adding requests:  30%|██▉       | 306/1024 [00:00<00:02, 344.60it/s]
Adding requests:  33%|███▎      | 343/1024 [00:01<00:01, 351.81it/s]
Adding requests:  37%|███▋      | 380/1024 [00:01<00:01, 355.63it/s]
Adding requests:  41%|████      | 419/1024 [00:01<00:01, 364.79it/s]
Adding requests:  45%|████▍     | 456/1024 [00:01<00:01, 358.45it/s]
Adding requests:  48%|████▊     | 496/1024 [00:01<00:01, 369.20it/s]
Adding requests:  52%|█████▏    | 535/1024 [00:01<00:01, 373.38it/s]
Adding requests:  56%|█████▌    | 573/1024 [00:01<00:01, 371.18it/s]
Adding requests:  60%|█████▉    | 611/1024 [00:01<00:01, 358.88it/s]
Adding requests:  63%|██████▎   | 647/1024 [00:01<00:01, 354.92it/s]
Adding requests:  67%|██████▋   | 684/1024 [00:01<00:00, 357.03it/s]
Adding requests:  70%|███████   | 720/1024 [00:02<00:00, 355.03it/s]
Adding requests:  74%|███████▍  | 756/1024 [00:02<00:00, 352.66it/s]
Adding requests:  77%|███████▋  | 792/1024 [00:02<00:00, 352.93it/s]
Adding requests:  81%|████████  | 829/1024 [00:02<00:00, 357.69it/s]
Adding requests:  85%|████████▍ | 866/1024 [00:02<00:00, 358.97it/s]
Adding requests:  88%|████████▊ | 904/1024 [00:02<00:00, 363.36it/s]
Adding requests:  92%|█████████▏| 941/1024 [00:02<00:00, 355.62it/s]
Adding requests:  95%|█████████▌| 977/1024 [00:02<00:00, 355.15it/s]
Adding requests:  99%|█████████▉| 1013/1024 [00:02<00:00, 349.95it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 351.60it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:00<00:00, 1537.39it/s, est. speed input: 1574395.30 toks/s, output: 1537.43 toks/s]
Processed prompts:  33%|███▎      | 340/1024 [00:02<00:05, 117.64it/s, est. speed input: 141985.28 toks/s, output: 138.66 toks/s]   
Processed prompts:  40%|███▉      | 408/1024 [00:03<00:06, 99.39it/s, est. speed input: 121210.72 toks/s, output: 118.37 toks/s] 
Processed prompts:  44%|████▍     | 448/1024 [00:04<00:06, 91.05it/s, est. speed input: 113036.14 toks/s, output: 110.39 toks/s]
Processed prompts:  46%|████▋     | 475/1024 [00:04<00:06, 82.94it/s, est. speed input: 106798.95 toks/s, output: 104.30 toks/s]
Processed prompts:  48%|████▊     | 494/1024 [00:04<00:06, 82.09it/s, est. speed input: 105339.07 toks/s, output: 102.87 toks/s]
Processed prompts:  50%|████▉     | 510/1024 [00:05<00:06, 79.33it/s, est. speed input: 103452.25 toks/s, output: 101.03 toks/s]
Processed prompts:  51%|█████     | 523/1024 [00:05<00:06, 74.16it/s, est. speed input: 101098.02 toks/s, output: 98.73 toks/s] 
Processed prompts:  52%|█████▏    | 534/1024 [00:05<00:06, 75.89it/s, est. speed input: 100862.38 toks/s, output: 98.50 toks/s]
Processed prompts:  53%|█████▎    | 544/1024 [00:05<00:06, 76.58it/s, est. speed input: 100453.49 toks/s, output: 98.10 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:05<00:07, 65.98it/s, est. speed input: 97929.96 toks/s, output: 95.63 toks/s] 
Processed prompts:  55%|█████▍    | 562/1024 [00:05<00:07, 65.91it/s, est. speed input: 97293.60 toks/s, output: 95.01 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:06<00:06, 65.61it/s, est. speed input: 96648.72 toks/s, output: 94.38 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:06<00:06, 65.39it/s, est. speed input: 96036.22 toks/s, output: 93.79 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:06<00:06, 65.25it/s, est. speed input: 95453.40 toks/s, output: 93.22 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:06<00:06, 64.81it/s, est. speed input: 94857.26 toks/s, output: 92.63 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:06<00:06, 64.76it/s, est. speed input: 94313.46 toks/s, output: 92.10 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:06<00:06, 64.58it/s, est. speed input: 93776.54 toks/s, output: 91.58 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:06<00:06, 64.74it/s, est. speed input: 93287.33 toks/s, output: 91.10 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:06<00:06, 64.79it/s, est. speed input: 92808.83 toks/s, output: 90.63 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:07<00:06, 64.84it/s, est. speed input: 92348.50 toks/s, output: 90.18 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:07<00:05, 64.61it/s, est. speed input: 91881.04 toks/s, output: 89.73 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:07<00:05, 64.80it/s, est. speed input: 91458.87 toks/s, output: 89.32 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:07<00:05, 64.74it/s, est. speed input: 91035.74 toks/s, output: 88.90 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:07<00:05, 64.78it/s, est. speed input: 90632.81 toks/s, output: 88.51 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:07<00:05, 64.59it/s, est. speed input: 90225.46 toks/s, output: 88.11 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:07<00:05, 64.43it/s, est. speed input: 89829.78 toks/s, output: 87.72 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:07<00:05, 64.34it/s, est. speed input: 89448.35 toks/s, output: 87.35 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:08<00:05, 64.37it/s, est. speed input: 89085.23 toks/s, output: 87.00 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:08<00:04, 64.35it/s, est. speed input: 88730.47 toks/s, output: 86.65 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:08<00:04, 64.55it/s, est. speed input: 88401.10 toks/s, output: 86.33 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:08<00:04, 64.42it/s, est. speed input: 88062.76 toks/s, output: 86.00 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:08<00:04, 64.34it/s, est. speed input: 87735.62 toks/s, output: 85.68 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:08<00:04, 64.40it/s, est. speed input: 87425.27 toks/s, output: 85.38 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:08<00:04, 64.37it/s, est. speed input: 87119.00 toks/s, output: 85.08 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:08<00:04, 64.59it/s, est. speed input: 86836.15 toks/s, output: 84.80 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:09<00:04, 64.56it/s, est. speed input: 86550.34 toks/s, output: 84.52 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:09<00:03, 64.54it/s, est. speed input: 86271.96 toks/s, output: 84.25 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:09<00:03, 64.65it/s, est. speed input: 86008.51 toks/s, output: 83.99 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:09<00:03, 66.08it/s, est. speed input: 85829.20 toks/s, output: 83.82 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:09<00:03, 65.72it/s, est. speed input: 85577.71 toks/s, output: 83.57 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:09<00:03, 65.32it/s, est. speed input: 85324.38 toks/s, output: 83.32 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:09<00:03, 64.91it/s, est. speed input: 85070.36 toks/s, output: 83.08 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:09<00:03, 64.73it/s, est. speed input: 84828.17 toks/s, output: 82.84 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:09<00:03, 64.76it/s, est. speed input: 84600.51 toks/s, output: 82.62 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:10<00:02, 64.57it/s, est. speed input: 84367.30 toks/s, output: 82.39 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:10<00:02, 64.86it/s, est. speed input: 84161.81 toks/s, output: 82.19 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:10<00:02, 64.83it/s, est. speed input: 83949.15 toks/s, output: 81.98 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:10<00:02, 64.97it/s, est. speed input: 83749.86 toks/s, output: 81.79 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:10<00:02, 64.81it/s, est. speed input: 83541.80 toks/s, output: 81.58 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:10<00:02, 64.65it/s, est. speed input: 83336.34 toks/s, output: 81.38 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:10<00:02, 64.90it/s, est. speed input: 83153.52 toks/s, output: 81.20 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:10<00:02, 64.70it/s, est. speed input: 82956.62 toks/s, output: 81.01 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:11<00:01, 64.62it/s, est. speed input: 82767.11 toks/s, output: 80.83 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:11<00:01, 64.25it/s, est. speed input: 82566.62 toks/s, output: 80.63 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:11<00:01, 64.21it/s, est. speed input: 82380.95 toks/s, output: 80.45 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:11<00:01, 64.36it/s, est. speed input: 82207.65 toks/s, output: 80.28 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:11<00:01, 64.63it/s, est. speed input: 82045.11 toks/s, output: 80.12 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:11<00:01, 64.24it/s, est. speed input: 81860.41 toks/s, output: 79.94 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:11<00:01, 64.18it/s, est. speed input: 81688.96 toks/s, output: 79.77 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:11<00:01, 64.17it/s, est. speed input: 81522.35 toks/s, output: 79.61 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:12<00:00, 64.24it/s, est. speed input: 81362.71 toks/s, output: 79.46 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:12<00:00, 64.41it/s, est. speed input: 81211.14 toks/s, output: 79.31 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:12<00:00, 64.59it/s, est. speed input: 81065.41 toks/s, output: 79.17 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:12<00:00, 64.73it/s, est. speed input: 80922.86 toks/s, output: 79.03 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:12<00:00, 64.71it/s, est. speed input: 80778.50 toks/s, output: 78.89 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:12<00:00, 64.70it/s, est. speed input: 80636.92 toks/s, output: 78.75 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:12<00:00, 64.44it/s, est. speed input: 80488.19 toks/s, output: 78.60 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:12<00:00, 66.18it/s, est. speed input: 80416.87 toks/s, output: 78.53 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 66.18it/s, est. speed input: 80889.90 toks/s, output: 78.99 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 78.99it/s, est. speed input: 80889.90 toks/s, output: 78.99 toks/s]
[rank0]:[W126 18:42:10.590519026 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 18:42:12
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:42:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=929129) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=929129) WARNING 01-26 18:42:53 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=929129) WARNING 01-26 18:43:11 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 65.63 requests/s, 67268.27 total tokens/s, 65.63 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 18:42:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:42:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:42:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:42:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:42:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:42:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:42:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:42:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:42:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:42:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:42:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:42:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:42:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:42:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:42:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:42:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:42:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:42:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:42:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:42:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:42:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:42:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:42:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:42:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:42:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:42:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:42:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:42:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=929129) [2026-01-26 18:42:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=929129) [2026-01-26 18:42:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=929129) [2026-01-26 18:42:39] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=929129) [2026-01-26 18:42:39] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=929129) [2026-01-26 18:42:39] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=929129) [2026-01-26 18:42:39] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=929129) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=929129) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.39s/it]
(EngineCore_DP0 pid=929129) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
(EngineCore_DP0 pid=929129) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.04it/s]
(EngineCore_DP0 pid=929129) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
(EngineCore_DP0 pid=929129) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.06s/it]
(EngineCore_DP0 pid=929129) 
(EngineCore_DP0 pid=929129) [2026-01-26 18:42:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=929129) [2026-01-26 18:42:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=929129) [2026-01-26 18:42:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=929129) [2026-01-26 18:42:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=929129) [2026-01-26 18:42:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=929129) [2026-01-26 18:42:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=929129) [2026-01-26 18:42:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=929129) [2026-01-26 18:42:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=929129) [rank0]:W0126 18:43:01.124000 929129 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=929129) [rank0]:W0126 18:43:01.192000 929129 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=929129) [rank0]:W0126 18:43:01.982000 929129 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=929129) [rank0]:W0126 18:43:02.090000 929129 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=929129) 2026-01-26 18:43:10,858 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=929129) 2026-01-26 18:43:10,916 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=929129) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  9.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  9.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 10.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  9.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.61it/s]
(EngineCore_DP0 pid=929129) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 10.91it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 11.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 10.91it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 275.93it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 315.06it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 311.94it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:06, 318.58it/s]
Adding requests:   8%|▊         | 164/2048 [00:00<00:05, 328.06it/s]
Adding requests:  10%|▉         | 201/2048 [00:00<00:05, 338.67it/s]
Adding requests:  12%|█▏        | 237/2048 [00:00<00:05, 343.72it/s]
Adding requests:  13%|█▎        | 272/2048 [00:00<00:05, 336.41it/s]
Adding requests:  15%|█▌        | 308/2048 [00:00<00:05, 342.63it/s]
Adding requests:  17%|█▋        | 345/2048 [00:01<00:04, 350.18it/s]
Adding requests:  19%|█▊        | 381/2048 [00:01<00:04, 351.82it/s]
Adding requests:  20%|██        | 419/2048 [00:01<00:04, 359.93it/s]
Adding requests:  22%|██▏       | 456/2048 [00:01<00:04, 356.25it/s]
Adding requests:  24%|██▍       | 496/2048 [00:01<00:04, 366.50it/s]
Adding requests:  26%|██▌       | 535/2048 [00:01<00:04, 371.42it/s]
Adding requests:  28%|██▊       | 573/2048 [00:01<00:03, 369.24it/s]
Adding requests:  30%|██▉       | 610/2048 [00:01<00:04, 355.87it/s]
Adding requests:  32%|███▏      | 646/2048 [00:01<00:04, 348.74it/s]
Adding requests:  33%|███▎      | 682/2048 [00:01<00:03, 350.84it/s]
Adding requests:  35%|███▌      | 718/2048 [00:02<00:03, 352.33it/s]
Adding requests:  37%|███▋      | 754/2048 [00:02<00:03, 348.77it/s]
Adding requests:  39%|███▊      | 790/2048 [00:02<00:03, 350.54it/s]
Adding requests:  40%|████      | 826/2048 [00:02<00:03, 353.06it/s]
Adding requests:  42%|████▏     | 863/2048 [00:02<00:03, 356.21it/s]
Adding requests:  44%|████▍     | 900/2048 [00:02<00:03, 359.46it/s]
Adding requests:  46%|████▌     | 936/2048 [00:02<00:03, 351.23it/s]
Adding requests:  48%|████▊     | 973/2048 [00:02<00:03, 355.76it/s]
Adding requests:  49%|████▉     | 1009/2048 [00:02<00:02, 349.79it/s]
Adding requests:  51%|█████     | 1045/2048 [00:02<00:02, 349.51it/s]
Adding requests:  53%|█████▎    | 1081/2048 [00:03<00:02, 350.44it/s]
Adding requests:  55%|█████▍    | 1117/2048 [00:03<00:02, 347.56it/s]
Adding requests:  56%|█████▋    | 1153/2048 [00:03<00:02, 351.02it/s]
Adding requests:  58%|█████▊    | 1189/2048 [00:03<00:02, 349.65it/s]
Adding requests:  60%|█████▉    | 1224/2048 [00:03<00:02, 345.30it/s]
Adding requests:  61%|██████▏   | 1259/2048 [00:03<00:02, 343.43it/s]
Adding requests:  63%|██████▎   | 1294/2048 [00:03<00:02, 342.71it/s]
Adding requests:  65%|██████▍   | 1330/2048 [00:03<00:02, 346.53it/s]
Adding requests:  67%|██████▋   | 1367/2048 [00:03<00:01, 352.79it/s]
Adding requests:  69%|██████▊   | 1403/2048 [00:04<00:01, 350.68it/s]
Adding requests:  70%|███████   | 1439/2048 [00:04<00:01, 350.14it/s]
Adding requests:  72%|███████▏  | 1475/2048 [00:04<00:01, 349.21it/s]
Adding requests:  74%|███████▍  | 1513/2048 [00:04<00:01, 355.16it/s]
Adding requests:  76%|███████▌  | 1549/2048 [00:04<00:01, 352.45it/s]
Adding requests:  77%|███████▋  | 1585/2048 [00:04<00:01, 345.78it/s]
Adding requests:  79%|███████▉  | 1620/2048 [00:04<00:01, 342.34it/s]
Adding requests:  81%|████████  | 1655/2048 [00:04<00:01, 337.28it/s]
Adding requests:  83%|████████▎ | 1690/2048 [00:04<00:01, 339.67it/s]
Adding requests:  84%|████████▍ | 1727/2048 [00:04<00:00, 345.79it/s]
Adding requests:  86%|████████▌ | 1763/2048 [00:05<00:00, 348.94it/s]
Adding requests:  88%|████████▊ | 1798/2048 [00:05<00:00, 338.14it/s]
Adding requests:  90%|████████▉ | 1834/2048 [00:05<00:00, 343.19it/s]
Adding requests:  91%|█████████▏| 1869/2048 [00:05<00:00, 344.78it/s]
Adding requests:  93%|█████████▎| 1905/2048 [00:05<00:00, 348.37it/s]
Adding requests:  95%|█████████▍| 1944/2048 [00:05<00:00, 357.69it/s]
Adding requests:  97%|█████████▋| 1980/2048 [00:05<00:00, 357.91it/s]
Adding requests:  98%|█████████▊| 2016/2048 [00:05<00:00, 347.35it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 348.02it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:00<00:01, 1374.64it/s, est. speed input: 1407676.64 toks/s, output: 1374.65 toks/s]
Processed prompts:  26%|██▌       | 524/2048 [00:02<00:07, 191.32it/s, est. speed input: 241941.19 toks/s, output: 236.27 toks/s]   
Processed prompts:  29%|██▊       | 585/2048 [00:03<00:10, 140.33it/s, est. speed input: 187477.12 toks/s, output: 183.08 toks/s]
Processed prompts:  30%|███       | 621/2048 [00:03<00:11, 125.73it/s, est. speed input: 172686.45 toks/s, output: 168.64 toks/s]
Processed prompts:  32%|███▏      | 646/2048 [00:04<00:13, 107.57it/s, est. speed input: 158373.47 toks/s, output: 154.66 toks/s]
Processed prompts:  32%|███▏      | 664/2048 [00:04<00:13, 102.56it/s, est. speed input: 153753.13 toks/s, output: 150.15 toks/s]
Processed prompts:  33%|███▎      | 679/2048 [00:04<00:14, 95.39it/s, est. speed input: 148953.67 toks/s, output: 145.46 toks/s] 
Processed prompts:  34%|███▎      | 691/2048 [00:04<00:15, 86.12it/s, est. speed input: 143990.65 toks/s, output: 140.62 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:05<00:16, 80.80it/s, est. speed input: 140188.37 toks/s, output: 136.90 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:05<00:17, 76.98it/s, est. speed input: 136826.49 toks/s, output: 133.62 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:05<00:17, 74.03it/s, est. speed input: 133798.51 toks/s, output: 130.66 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:05<00:18, 71.74it/s, est. speed input: 131027.65 toks/s, output: 127.96 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:06<00:18, 70.02it/s, est. speed input: 128482.32 toks/s, output: 125.47 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:06<00:18, 70.07it/s, est. speed input: 126455.06 toks/s, output: 123.49 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:06<00:18, 68.64it/s, est. speed input: 124239.00 toks/s, output: 121.33 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:06<00:18, 67.70it/s, est. speed input: 122196.74 toks/s, output: 119.33 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:07<00:18, 67.15it/s, est. speed input: 120321.45 toks/s, output: 117.50 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:07<00:17, 67.03it/s, est. speed input: 118622.95 toks/s, output: 115.84 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:07<00:17, 66.44it/s, est. speed input: 116937.18 toks/s, output: 114.20 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:07<00:17, 65.96it/s, est. speed input: 115344.45 toks/s, output: 112.64 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:08<00:17, 65.67it/s, est. speed input: 113857.61 toks/s, output: 111.19 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:08<00:17, 65.81it/s, est. speed input: 112515.74 toks/s, output: 109.88 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:08<00:17, 65.67it/s, est. speed input: 111211.21 toks/s, output: 108.60 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:08<00:16, 65.61it/s, est. speed input: 109985.40 toks/s, output: 107.41 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:09<00:16, 65.45it/s, est. speed input: 108808.05 toks/s, output: 106.26 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:09<00:16, 65.39it/s, est. speed input: 107700.39 toks/s, output: 105.18 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:09<00:16, 65.36it/s, est. speed input: 106651.82 toks/s, output: 104.15 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:09<00:15, 65.39it/s, est. speed input: 105662.58 toks/s, output: 103.19 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:10<00:15, 65.27it/s, est. speed input: 104702.86 toks/s, output: 102.25 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:10<00:15, 65.66it/s, est. speed input: 103848.95 toks/s, output: 101.41 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:10<00:15, 65.85it/s, est. speed input: 103024.07 toks/s, output: 100.61 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:10<00:14, 65.68it/s, est. speed input: 102200.11 toks/s, output: 99.80 toks/s] 
Processed prompts:  53%|█████▎    | 1090/2048 [00:11<00:14, 65.60it/s, est. speed input: 101417.17 toks/s, output: 99.04 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:11<00:14, 65.46it/s, est. speed input: 100659.28 toks/s, output: 98.30 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:11<00:14, 65.24it/s, est. speed input: 99920.95 toks/s, output: 97.58 toks/s] 
Processed prompts:  56%|█████▌    | 1138/2048 [00:11<00:13, 65.46it/s, est. speed input: 99252.41 toks/s, output: 96.93 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:11<00:13, 65.32it/s, est. speed input: 98580.55 toks/s, output: 96.27 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:12<00:13, 65.18it/s, est. speed input: 97932.48 toks/s, output: 95.64 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:12<00:13, 65.37it/s, est. speed input: 97337.46 toks/s, output: 95.06 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:12<00:12, 66.21it/s, est. speed input: 96830.86 toks/s, output: 94.56 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:12<00:12, 65.85it/s, est. speed input: 96256.70 toks/s, output: 94.00 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:13<00:12, 65.70it/s, est. speed input: 95712.59 toks/s, output: 93.47 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:13<00:12, 65.51it/s, est. speed input: 95180.76 toks/s, output: 92.95 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:13<00:11, 65.33it/s, est. speed input: 94663.86 toks/s, output: 92.45 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:13<00:11, 65.36it/s, est. speed input: 94178.60 toks/s, output: 91.97 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:14<00:11, 65.61it/s, est. speed input: 93729.09 toks/s, output: 91.53 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:14<00:11, 65.67it/s, est. speed input: 93285.21 toks/s, output: 91.10 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:14<00:10, 65.55it/s, est. speed input: 92843.45 toks/s, output: 90.67 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:14<00:10, 65.48it/s, est. speed input: 92416.88 toks/s, output: 90.25 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:15<00:10, 65.36it/s, est. speed input: 91998.97 toks/s, output: 89.84 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:15<00:10, 65.58it/s, est. speed input: 91616.96 toks/s, output: 89.47 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:15<00:09, 65.45it/s, est. speed input: 91225.94 toks/s, output: 89.09 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:15<00:09, 65.29it/s, est. speed input: 90842.44 toks/s, output: 88.71 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:16<00:09, 65.08it/s, est. speed input: 90462.98 toks/s, output: 88.34 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:16<00:09, 65.16it/s, est. speed input: 90111.67 toks/s, output: 88.00 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:16<00:08, 66.34it/s, est. speed input: 89844.75 toks/s, output: 87.74 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:16<00:08, 66.03it/s, est. speed input: 89511.50 toks/s, output: 87.41 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:17<00:08, 65.74it/s, est. speed input: 89182.84 toks/s, output: 87.09 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:17<00:08, 65.64it/s, est. speed input: 88869.31 toks/s, output: 86.79 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:17<00:07, 66.68it/s, est. speed input: 88633.17 toks/s, output: 86.56 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:17<00:07, 66.21it/s, est. speed input: 88331.10 toks/s, output: 86.26 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:18<00:07, 67.00it/s, est. speed input: 88102.76 toks/s, output: 86.04 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:18<00:07, 66.76it/s, est. speed input: 87834.98 toks/s, output: 85.78 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:18<00:06, 66.26it/s, est. speed input: 87554.41 toks/s, output: 85.50 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:18<00:06, 65.92it/s, est. speed input: 87282.16 toks/s, output: 85.24 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:19<00:06, 66.99it/s, est. speed input: 87089.13 toks/s, output: 85.05 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:19<00:06, 66.39it/s, est. speed input: 86827.36 toks/s, output: 84.79 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:19<00:06, 65.94it/s, est. speed input: 86570.39 toks/s, output: 84.54 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:19<00:05, 65.68it/s, est. speed input: 86322.19 toks/s, output: 84.30 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:20<00:05, 65.53it/s, est. speed input: 86082.27 toks/s, output: 84.06 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:20<00:05, 65.56it/s, est. speed input: 85854.77 toks/s, output: 83.84 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:20<00:05, 65.41it/s, est. speed input: 85624.14 toks/s, output: 83.62 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:20<00:04, 66.66it/s, est. speed input: 85467.40 toks/s, output: 83.46 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:20<00:04, 67.32it/s, est. speed input: 85302.29 toks/s, output: 83.30 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:21<00:04, 66.67it/s, est. speed input: 85087.56 toks/s, output: 83.09 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:21<00:04, 66.24it/s, est. speed input: 84878.90 toks/s, output: 82.89 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:21<00:03, 66.16it/s, est. speed input: 84685.03 toks/s, output: 82.70 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:21<00:03, 66.06it/s, est. speed input: 84493.68 toks/s, output: 82.51 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:22<00:03, 66.03it/s, est. speed input: 84308.12 toks/s, output: 82.33 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:22<00:03, 66.16it/s, est. speed input: 84133.62 toks/s, output: 82.16 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:22<00:02, 65.89it/s, est. speed input: 83946.61 toks/s, output: 81.98 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:22<00:02, 65.81it/s, est. speed input: 83768.16 toks/s, output: 81.80 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:23<00:02, 66.74it/s, est. speed input: 83637.29 toks/s, output: 81.68 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:23<00:02, 66.32it/s, est. speed input: 83462.31 toks/s, output: 81.51 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:23<00:01, 65.85it/s, est. speed input: 83283.09 toks/s, output: 81.33 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:23<00:01, 65.76it/s, est. speed input: 83118.13 toks/s, output: 81.17 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:24<00:01, 65.87it/s, est. speed input: 82963.72 toks/s, output: 81.02 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:24<00:01, 65.67it/s, est. speed input: 82800.66 toks/s, output: 80.86 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:24<00:00, 66.71it/s, est. speed input: 82689.17 toks/s, output: 80.75 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:24<00:00, 66.49it/s, est. speed input: 82541.43 toks/s, output: 80.61 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:25<00:00, 66.16it/s, est. speed input: 82389.54 toks/s, output: 80.46 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:25<00:00, 66.42it/s, est. speed input: 82259.88 toks/s, output: 80.33 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:25<00:00, 66.42it/s, est. speed input: 82825.40 toks/s, output: 80.88 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:25<00:00, 80.88it/s, est. speed input: 82825.40 toks/s, output: 80.88 toks/s]
[rank0]:[W126 18:43:45.569640164 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 18:43:47
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:44:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=931139) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=931139) WARNING 01-26 18:44:40 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=931139) WARNING 01-26 18:44:59 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 66.89 requests/s, 68558.10 total tokens/s, 66.89 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 18:44:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:44:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:44:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:44:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:44:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:44:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:44:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:44:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:44:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:44:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:44:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:44:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:44:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:44:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:44:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:44:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:44:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:44:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:44:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:44:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:44:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:44:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:44:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:44:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:44:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:44:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:44:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:44:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=931139) [2026-01-26 18:44:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=931139) [2026-01-26 18:44:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=931139) [2026-01-26 18:44:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=931139) [2026-01-26 18:44:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=931139) [2026-01-26 18:44:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=931139) [2026-01-26 18:44:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=931139) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=931139) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]
(EngineCore_DP0 pid=931139) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
(EngineCore_DP0 pid=931139) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.07it/s]
(EngineCore_DP0 pid=931139) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.03s/it]
(EngineCore_DP0 pid=931139) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.00s/it]
(EngineCore_DP0 pid=931139) 
(EngineCore_DP0 pid=931139) [2026-01-26 18:44:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=931139) [2026-01-26 18:44:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=931139) [2026-01-26 18:44:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=931139) [2026-01-26 18:44:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=931139) [2026-01-26 18:44:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=931139) [2026-01-26 18:44:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=931139) [2026-01-26 18:44:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=931139) [2026-01-26 18:44:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=931139) [rank0]:W0126 18:44:48.582000 931139 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=931139) [rank0]:W0126 18:44:48.651000 931139 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=931139) [rank0]:W0126 18:44:49.563000 931139 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=931139) [rank0]:W0126 18:44:49.672000 931139 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=931139) 2026-01-26 18:44:59,230 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=931139) 2026-01-26 18:44:59,342 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=931139) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  8.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  9.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  9.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  9.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 10.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.36it/s]
(EngineCore_DP0 pid=931139) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  9.51it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 10.82it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00, 11.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 11.18it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 11.04it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 276.25it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 317.64it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 315.18it/s]
Adding requests:   3%|▎         | 128/4096 [00:00<00:12, 320.97it/s]
Adding requests:   4%|▍         | 163/4096 [00:00<00:11, 330.79it/s]
Adding requests:   5%|▍         | 199/4096 [00:00<00:11, 340.18it/s]
Adding requests:   6%|▌         | 235/4096 [00:00<00:11, 344.96it/s]
Adding requests:   7%|▋         | 270/4096 [00:00<00:11, 342.91it/s]
Adding requests:   7%|▋         | 305/4096 [00:00<00:11, 333.88it/s]
Adding requests:   8%|▊         | 342/4096 [00:01<00:10, 343.00it/s]
Adding requests:   9%|▉         | 379/4096 [00:01<00:10, 348.67it/s]
Adding requests:  10%|█         | 417/4096 [00:01<00:10, 357.42it/s]
Adding requests:  11%|█         | 453/4096 [00:01<00:10, 355.56it/s]
Adding requests:  12%|█▏        | 493/4096 [00:01<00:09, 366.44it/s]
Adding requests:  13%|█▎        | 532/4096 [00:01<00:09, 372.65it/s]
Adding requests:  14%|█▍        | 570/4096 [00:01<00:09, 370.80it/s]
Adding requests:  15%|█▍        | 608/4096 [00:01<00:09, 359.47it/s]
Adding requests:  16%|█▌        | 645/4096 [00:01<00:09, 355.65it/s]
Adding requests:  17%|█▋        | 681/4096 [00:01<00:09, 355.12it/s]
Adding requests:  18%|█▊        | 718/4096 [00:02<00:09, 356.71it/s]
Adding requests:  18%|█▊        | 754/4096 [00:02<00:09, 353.41it/s]
Adding requests:  19%|█▉        | 790/4096 [00:02<00:09, 354.75it/s]
Adding requests:  20%|██        | 826/4096 [00:02<00:09, 355.97it/s]
Adding requests:  21%|██        | 863/4096 [00:02<00:08, 360.03it/s]
Adding requests:  22%|██▏       | 900/4096 [00:02<00:08, 362.05it/s]
Adding requests:  23%|██▎       | 937/4096 [00:02<00:08, 351.74it/s]
Adding requests:  24%|██▍       | 974/4096 [00:02<00:08, 354.84it/s]
Adding requests:  25%|██▍       | 1010/4096 [00:02<00:08, 344.37it/s]
Adding requests:  26%|██▌       | 1046/4096 [00:02<00:08, 345.74it/s]
Adding requests:  26%|██▋       | 1082/4096 [00:03<00:08, 347.93it/s]
Adding requests:  27%|██▋       | 1117/4096 [00:03<00:08, 345.91it/s]
Adding requests:  28%|██▊       | 1153/4096 [00:03<00:08, 349.90it/s]
Adding requests:  29%|██▉       | 1189/4096 [00:03<00:08, 349.09it/s]
Adding requests:  30%|██▉       | 1227/4096 [00:03<00:08, 358.05it/s]
Adding requests:  31%|███       | 1263/4096 [00:03<00:07, 355.19it/s]
Adding requests:  32%|███▏      | 1299/4096 [00:03<00:07, 352.19it/s]
Adding requests:  33%|███▎      | 1335/4096 [00:03<00:07, 353.32it/s]
Adding requests:  33%|███▎      | 1372/4096 [00:03<00:07, 357.93it/s]
Adding requests:  34%|███▍      | 1408/4096 [00:04<00:07, 355.84it/s]
Adding requests:  35%|███▌      | 1444/4096 [00:04<00:07, 356.58it/s]
Adding requests:  36%|███▌      | 1480/4096 [00:04<00:07, 357.37it/s]
Adding requests:  37%|███▋      | 1517/4096 [00:04<00:07, 359.82it/s]
Adding requests:  38%|███▊      | 1553/4096 [00:04<00:07, 358.44it/s]
Adding requests:  39%|███▉      | 1589/4096 [00:04<00:07, 352.90it/s]
Adding requests:  40%|███▉      | 1625/4096 [00:04<00:07, 345.38it/s]
Adding requests:  41%|████      | 1660/4096 [00:04<00:07, 339.49it/s]
Adding requests:  41%|████▏     | 1696/4096 [00:04<00:06, 343.00it/s]
Adding requests:  42%|████▏     | 1733/4096 [00:04<00:06, 350.19it/s]
Adding requests:  43%|████▎     | 1771/4096 [00:05<00:06, 356.91it/s]
Adding requests:  44%|████▍     | 1807/4096 [00:05<00:06, 352.61it/s]
Adding requests:  45%|████▌     | 1844/4096 [00:05<00:06, 357.53it/s]
Adding requests:  46%|████▌     | 1880/4096 [00:05<00:06, 355.85it/s]
Adding requests:  47%|████▋     | 1917/4096 [00:05<00:06, 358.56it/s]
Adding requests:  48%|████▊     | 1955/4096 [00:05<00:05, 364.07it/s]
Adding requests:  49%|████▊     | 1992/4096 [00:05<00:05, 358.92it/s]
Adding requests:  50%|████▉     | 2028/4096 [00:05<00:05, 349.14it/s]
Adding requests:  50%|█████     | 2063/4096 [00:05<00:05, 348.54it/s]
Adding requests:  51%|█████     | 2098/4096 [00:05<00:05, 346.64it/s]
Adding requests:  52%|█████▏    | 2134/4096 [00:06<00:05, 350.32it/s]
Adding requests:  53%|█████▎    | 2170/4096 [00:06<00:05, 336.74it/s]
Adding requests:  54%|█████▍    | 2204/4096 [00:06<00:05, 335.87it/s]
Adding requests:  55%|█████▍    | 2241/4096 [00:06<00:05, 343.55it/s]
Adding requests:  56%|█████▌    | 2278/4096 [00:06<00:05, 349.32it/s]
Adding requests:  57%|█████▋    | 2315/4096 [00:06<00:05, 354.22it/s]
Adding requests:  57%|█████▋    | 2353/4096 [00:06<00:04, 360.13it/s]
Adding requests:  58%|█████▊    | 2391/4096 [00:06<00:04, 362.91it/s]
Adding requests:  59%|█████▉    | 2428/4096 [00:06<00:04, 363.98it/s]
Adding requests:  60%|██████    | 2465/4096 [00:07<00:04, 360.01it/s]
Adding requests:  61%|██████    | 2503/4096 [00:07<00:04, 363.72it/s]
Adding requests:  62%|██████▏   | 2541/4096 [00:07<00:04, 368.49it/s]
Adding requests:  63%|██████▎   | 2581/4096 [00:07<00:04, 376.62it/s]
Adding requests:  64%|██████▍   | 2619/4096 [00:07<00:03, 370.19it/s]
Adding requests:  65%|██████▍   | 2657/4096 [00:07<00:04, 355.18it/s]
Adding requests:  66%|██████▌   | 2693/4096 [00:07<00:03, 351.41it/s]
Adding requests:  67%|██████▋   | 2729/4096 [00:07<00:03, 350.41it/s]
Adding requests:  68%|██████▊   | 2768/4096 [00:07<00:03, 359.71it/s]
Adding requests:  69%|██████▊   | 2807/4096 [00:07<00:03, 365.64it/s]
Adding requests:  69%|██████▉   | 2844/4096 [00:08<00:03, 363.86it/s]
Adding requests:  70%|███████   | 2881/4096 [00:08<00:03, 359.59it/s]
Adding requests:  71%|███████   | 2918/4096 [00:08<00:03, 361.62it/s]
Adding requests:  72%|███████▏  | 2955/4096 [00:08<00:03, 363.95it/s]
Adding requests:  73%|███████▎  | 2992/4096 [00:08<00:03, 361.74it/s]
Adding requests:  74%|███████▍  | 3030/4096 [00:08<00:02, 366.49it/s]
Adding requests:  75%|███████▍  | 3067/4096 [00:08<00:02, 367.04it/s]
Adding requests:  76%|███████▌  | 3105/4096 [00:08<00:02, 368.10it/s]
Adding requests:  77%|███████▋  | 3143/4096 [00:08<00:02, 371.15it/s]
Adding requests:  78%|███████▊  | 3181/4096 [00:08<00:02, 362.13it/s]
Adding requests:  79%|███████▊  | 3218/4096 [00:09<00:02, 361.46it/s]
Adding requests:  79%|███████▉  | 3256/4096 [00:09<00:02, 364.24it/s]
Adding requests:  80%|████████  | 3293/4096 [00:09<00:02, 350.40it/s]
Adding requests:  81%|████████▏ | 3329/4096 [00:09<00:02, 348.79it/s]
Adding requests:  82%|████████▏ | 3364/4096 [00:09<00:02, 345.43it/s]
Adding requests:  83%|████████▎ | 3401/4096 [00:09<00:01, 349.48it/s]
Adding requests:  84%|████████▍ | 3437/4096 [00:09<00:01, 351.48it/s]
Adding requests:  85%|████████▍ | 3474/4096 [00:09<00:01, 356.59it/s]
Adding requests:  86%|████████▌ | 3510/4096 [00:09<00:01, 346.59it/s]
Adding requests:  87%|████████▋ | 3548/4096 [00:10<00:01, 353.43it/s]
Adding requests:  88%|████████▊ | 3585/4096 [00:10<00:01, 355.49it/s]
Adding requests:  88%|████████▊ | 3622/4096 [00:10<00:01, 358.38it/s]
Adding requests:  89%|████████▉ | 3658/4096 [00:10<00:01, 358.15it/s]
Adding requests:  90%|█████████ | 3694/4096 [00:10<00:01, 351.65it/s]
Adding requests:  91%|█████████ | 3731/4096 [00:10<00:01, 356.75it/s]
Adding requests:  92%|█████████▏| 3767/4096 [00:10<00:00, 347.80it/s]
Adding requests:  93%|█████████▎| 3802/4096 [00:10<00:00, 338.40it/s]
Adding requests:  94%|█████████▎| 3837/4096 [00:10<00:00, 341.63it/s]
Adding requests:  95%|█████████▍| 3873/4096 [00:10<00:00, 343.98it/s]
Adding requests:  95%|█████████▌| 3908/4096 [00:11<00:00, 341.62it/s]
Adding requests:  96%|█████████▋| 3943/4096 [00:11<00:00, 340.20it/s]
Adding requests:  97%|█████████▋| 3978/4096 [00:11<00:00, 341.15it/s]
Adding requests:  98%|█████████▊| 4015/4096 [00:11<00:00, 347.44it/s]
Adding requests:  99%|█████████▉| 4050/4096 [00:11<00:00, 343.86it/s]
Adding requests: 100%|█████████▉| 4085/4096 [00:11<00:00, 343.94it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 352.84it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 756/4096 [00:00<00:01, 2737.02it/s, est. speed input: 2802796.82 toks/s, output: 2737.04 toks/s]
Processed prompts:  25%|██▌       | 1030/4096 [00:04<00:15, 198.74it/s, est. speed input: 255725.94 toks/s, output: 249.73 toks/s]  
Processed prompts:  28%|██▊       | 1147/4096 [00:06<00:20, 142.97it/s, est. speed input: 194256.25 toks/s, output: 189.70 toks/s]
Processed prompts:  30%|██▉       | 1214/4096 [00:06<00:22, 126.93it/s, est. speed input: 177732.12 toks/s, output: 173.57 toks/s]
Processed prompts:  31%|███       | 1257/4096 [00:07<00:23, 121.83it/s, est. speed input: 172255.45 toks/s, output: 168.22 toks/s]
Processed prompts:  31%|███▏      | 1288/4096 [00:07<00:25, 112.29it/s, est. speed input: 165797.67 toks/s, output: 161.91 toks/s]
Processed prompts:  32%|███▏      | 1311/4096 [00:08<00:27, 100.01it/s, est. speed input: 159151.70 toks/s, output: 155.42 toks/s]
Processed prompts:  33%|███▎      | 1332/4096 [00:08<00:31, 87.84it/s, est. speed input: 152938.64 toks/s, output: 149.35 toks/s] 
Processed prompts:  33%|███▎      | 1364/4096 [00:09<00:32, 82.85it/s, est. speed input: 148599.07 toks/s, output: 145.12 toks/s]
Processed prompts:  34%|███▍      | 1396/4096 [00:09<00:34, 79.00it/s, est. speed input: 144767.39 toks/s, output: 141.37 toks/s]
Processed prompts:  35%|███▍      | 1428/4096 [00:10<00:35, 75.70it/s, est. speed input: 141206.20 toks/s, output: 137.90 toks/s]
Processed prompts:  36%|███▌      | 1460/4096 [00:10<00:35, 73.90it/s, est. speed input: 138177.53 toks/s, output: 134.94 toks/s]
Processed prompts:  36%|███▋      | 1492/4096 [00:11<00:36, 71.84it/s, est. speed input: 135205.04 toks/s, output: 132.04 toks/s]
Processed prompts:  37%|███▋      | 1524/4096 [00:11<00:36, 71.01it/s, est. speed input: 132651.44 toks/s, output: 129.54 toks/s]
Processed prompts:  38%|███▊      | 1556/4096 [00:12<00:36, 70.25it/s, est. speed input: 130256.01 toks/s, output: 127.20 toks/s]
Processed prompts:  39%|███▉      | 1588/4096 [00:12<00:36, 69.23it/s, est. speed input: 127925.53 toks/s, output: 124.93 toks/s]
Processed prompts:  40%|███▉      | 1620/4096 [00:13<00:35, 69.05it/s, est. speed input: 125883.51 toks/s, output: 122.93 toks/s]
Processed prompts:  40%|████      | 1652/4096 [00:13<00:35, 68.41it/s, est. speed input: 123875.05 toks/s, output: 120.97 toks/s]
Processed prompts:  41%|████      | 1684/4096 [00:14<00:35, 67.86it/s, est. speed input: 121981.22 toks/s, output: 119.12 toks/s]
Processed prompts:  42%|████▏     | 1716/4096 [00:14<00:34, 68.12it/s, est. speed input: 120336.98 toks/s, output: 117.52 toks/s]
Processed prompts:  43%|████▎     | 1748/4096 [00:15<00:34, 68.34it/s, est. speed input: 118800.21 toks/s, output: 116.02 toks/s]
Processed prompts:  43%|████▎     | 1780/4096 [00:15<00:34, 67.78it/s, est. speed input: 117230.81 toks/s, output: 114.48 toks/s]
Processed prompts:  44%|████▍     | 1812/4096 [00:16<00:33, 67.48it/s, est. speed input: 115770.56 toks/s, output: 113.06 toks/s]
Processed prompts:  45%|████▌     | 1844/4096 [00:16<00:33, 67.06it/s, est. speed input: 114360.34 toks/s, output: 111.68 toks/s]
Processed prompts:  46%|████▌     | 1876/4096 [00:16<00:32, 67.65it/s, est. speed input: 113169.08 toks/s, output: 110.52 toks/s]
Processed prompts:  47%|████▋     | 1908/4096 [00:17<00:32, 67.32it/s, est. speed input: 111930.55 toks/s, output: 109.31 toks/s]
Processed prompts:  47%|████▋     | 1940/4096 [00:17<00:32, 67.09it/s, est. speed input: 110756.53 toks/s, output: 108.16 toks/s]
Processed prompts:  48%|████▊     | 1972/4096 [00:18<00:31, 67.58it/s, est. speed input: 109736.45 toks/s, output: 107.16 toks/s]
Processed prompts:  49%|████▉     | 2004/4096 [00:18<00:31, 67.31it/s, est. speed input: 108682.57 toks/s, output: 106.14 toks/s]
Processed prompts:  50%|████▉     | 2036/4096 [00:19<00:30, 67.12it/s, est. speed input: 107681.25 toks/s, output: 105.16 toks/s]
Processed prompts:  50%|█████     | 2068/4096 [00:19<00:30, 67.56it/s, est. speed input: 106801.55 toks/s, output: 104.30 toks/s]
Processed prompts:  51%|█████▏    | 2100/4096 [00:20<00:29, 67.20it/s, est. speed input: 105878.97 toks/s, output: 103.40 toks/s]
Processed prompts:  52%|█████▏    | 2132/4096 [00:20<00:29, 67.17it/s, est. speed input: 105025.89 toks/s, output: 102.56 toks/s]
Processed prompts:  53%|█████▎    | 2164/4096 [00:21<00:28, 67.19it/s, est. speed input: 104216.56 toks/s, output: 101.77 toks/s]
Processed prompts:  54%|█████▎    | 2196/4096 [00:21<00:28, 67.54it/s, est. speed input: 103479.77 toks/s, output: 101.05 toks/s]
Processed prompts:  54%|█████▍    | 2228/4096 [00:22<00:27, 67.12it/s, est. speed input: 102702.29 toks/s, output: 100.30 toks/s]
Processed prompts:  55%|█████▌    | 2260/4096 [00:22<00:27, 67.12it/s, est. speed input: 101988.03 toks/s, output: 99.60 toks/s] 
Processed prompts:  56%|█████▌    | 2292/4096 [00:23<00:26, 66.95it/s, est. speed input: 101285.63 toks/s, output: 98.91 toks/s]
Processed prompts:  57%|█████▋    | 2324/4096 [00:23<00:26, 66.82it/s, est. speed input: 100610.88 toks/s, output: 98.25 toks/s]
Processed prompts:  58%|█████▊    | 2356/4096 [00:24<00:26, 66.84it/s, est. speed input: 99974.92 toks/s, output: 97.63 toks/s] 
Processed prompts:  58%|█████▊    | 2388/4096 [00:24<00:25, 66.77it/s, est. speed input: 99354.00 toks/s, output: 97.03 toks/s]
Processed prompts:  59%|█████▉    | 2420/4096 [00:25<00:25, 66.73it/s, est. speed input: 98759.21 toks/s, output: 96.44 toks/s]
Processed prompts:  60%|█████▉    | 2452/4096 [00:25<00:24, 66.70it/s, est. speed input: 98185.35 toks/s, output: 95.88 toks/s]
Processed prompts:  61%|██████    | 2484/4096 [00:26<00:24, 66.59it/s, est. speed input: 97624.88 toks/s, output: 95.34 toks/s]
Processed prompts:  61%|██████▏   | 2516/4096 [00:26<00:23, 67.34it/s, est. speed input: 97156.97 toks/s, output: 94.88 toks/s]
Processed prompts:  62%|██████▏   | 2548/4096 [00:26<00:22, 67.31it/s, est. speed input: 96657.84 toks/s, output: 94.39 toks/s]
Processed prompts:  63%|██████▎   | 2580/4096 [00:27<00:22, 67.77it/s, est. speed input: 96215.57 toks/s, output: 93.96 toks/s]
Processed prompts:  64%|██████▍   | 2612/4096 [00:27<00:22, 67.45it/s, est. speed input: 95736.34 toks/s, output: 93.49 toks/s]
Processed prompts:  65%|██████▍   | 2644/4096 [00:28<00:21, 67.36it/s, est. speed input: 95283.88 toks/s, output: 93.05 toks/s]
Processed prompts:  65%|██████▌   | 2676/4096 [00:28<00:21, 67.27it/s, est. speed input: 94844.74 toks/s, output: 92.62 toks/s]
Processed prompts:  66%|██████▌   | 2708/4096 [00:29<00:20, 67.03it/s, est. speed input: 94405.50 toks/s, output: 92.19 toks/s]
Processed prompts:  67%|██████▋   | 2740/4096 [00:29<00:20, 67.64it/s, est. speed input: 94038.92 toks/s, output: 91.83 toks/s]
Processed prompts:  68%|██████▊   | 2772/4096 [00:30<00:19, 67.39it/s, est. speed input: 93634.05 toks/s, output: 91.44 toks/s]
Processed prompts:  68%|██████▊   | 2804/4096 [00:30<00:19, 67.26it/s, est. speed input: 93245.05 toks/s, output: 91.06 toks/s]
Processed prompts:  69%|██████▉   | 2836/4096 [00:31<00:18, 67.14it/s, est. speed input: 92865.74 toks/s, output: 90.69 toks/s]
Processed prompts:  70%|███████   | 2868/4096 [00:31<00:18, 66.98it/s, est. speed input: 92492.50 toks/s, output: 90.32 toks/s]
Processed prompts:  71%|███████   | 2900/4096 [00:32<00:17, 67.73it/s, est. speed input: 92188.89 toks/s, output: 90.03 toks/s]
Processed prompts:  72%|███████▏  | 2932/4096 [00:32<00:17, 67.48it/s, est. speed input: 91842.33 toks/s, output: 89.69 toks/s]
Processed prompts:  72%|███████▏  | 2964/4096 [00:33<00:16, 67.17it/s, est. speed input: 91496.87 toks/s, output: 89.35 toks/s]
Processed prompts:  73%|███████▎  | 2996/4096 [00:33<00:16, 66.94it/s, est. speed input: 91160.83 toks/s, output: 89.02 toks/s]
Processed prompts:  74%|███████▍  | 3028/4096 [00:34<00:15, 66.90it/s, est. speed input: 90841.54 toks/s, output: 88.71 toks/s]
Processed prompts:  75%|███████▍  | 3060/4096 [00:34<00:15, 66.92it/s, est. speed input: 90534.24 toks/s, output: 88.41 toks/s]
Processed prompts:  75%|███████▌  | 3092/4096 [00:35<00:15, 66.78it/s, est. speed input: 90225.65 toks/s, output: 88.11 toks/s]
Processed prompts:  76%|███████▋  | 3124/4096 [00:35<00:14, 66.79it/s, est. speed input: 89932.21 toks/s, output: 87.82 toks/s]
Processed prompts:  77%|███████▋  | 3156/4096 [00:36<00:14, 66.67it/s, est. speed input: 89638.64 toks/s, output: 87.54 toks/s]
Processed prompts:  78%|███████▊  | 3188/4096 [00:36<00:13, 66.66it/s, est. speed input: 89357.31 toks/s, output: 87.26 toks/s]
Processed prompts:  79%|███████▊  | 3220/4096 [00:37<00:13, 66.71it/s, est. speed input: 89086.82 toks/s, output: 87.00 toks/s]
Processed prompts:  79%|███████▉  | 3252/4096 [00:37<00:12, 66.66it/s, est. speed input: 88818.04 toks/s, output: 86.74 toks/s]
Processed prompts:  80%|████████  | 3284/4096 [00:37<00:12, 66.72it/s, est. speed input: 88561.28 toks/s, output: 86.49 toks/s]
Processed prompts:  81%|████████  | 3316/4096 [00:38<00:11, 66.74it/s, est. speed input: 88310.05 toks/s, output: 86.24 toks/s]
Processed prompts:  82%|████████▏ | 3348/4096 [00:38<00:11, 66.74it/s, est. speed input: 88064.12 toks/s, output: 86.00 toks/s]
Processed prompts:  83%|████████▎ | 3380/4096 [00:39<00:10, 66.62it/s, est. speed input: 87817.91 toks/s, output: 85.76 toks/s]
Processed prompts:  83%|████████▎ | 3412/4096 [00:39<00:10, 66.72it/s, est. speed input: 87586.96 toks/s, output: 85.53 toks/s]
Processed prompts:  84%|████████▍ | 3444/4096 [00:40<00:09, 66.57it/s, est. speed input: 87350.47 toks/s, output: 85.30 toks/s]
Processed prompts:  85%|████████▍ | 3476/4096 [00:40<00:09, 66.59it/s, est. speed input: 87125.72 toks/s, output: 85.08 toks/s]
Processed prompts:  86%|████████▌ | 3508/4096 [00:41<00:08, 66.76it/s, est. speed input: 86914.37 toks/s, output: 84.88 toks/s]
Processed prompts:  86%|████████▋ | 3540/4096 [00:41<00:08, 67.42it/s, est. speed input: 86733.82 toks/s, output: 84.70 toks/s]
Processed prompts:  87%|████████▋ | 3572/4096 [00:42<00:07, 67.41it/s, est. speed input: 86534.82 toks/s, output: 84.51 toks/s]
Processed prompts:  88%|████████▊ | 3604/4096 [00:42<00:07, 67.32it/s, est. speed input: 86336.11 toks/s, output: 84.31 toks/s]
Processed prompts:  89%|████████▉ | 3636/4096 [00:43<00:06, 67.13it/s, est. speed input: 86135.63 toks/s, output: 84.12 toks/s]
Processed prompts:  90%|████████▉ | 3668/4096 [00:43<00:06, 67.54it/s, est. speed input: 85964.86 toks/s, output: 83.95 toks/s]
Processed prompts:  90%|█████████ | 3700/4096 [00:44<00:05, 67.27it/s, est. speed input: 85772.55 toks/s, output: 83.76 toks/s]
Processed prompts:  91%|█████████ | 3732/4096 [00:44<00:05, 67.18it/s, est. speed input: 85588.25 toks/s, output: 83.58 toks/s]
Processed prompts:  92%|█████████▏| 3764/4096 [00:45<00:04, 67.16it/s, est. speed input: 85409.99 toks/s, output: 83.41 toks/s]
Processed prompts:  93%|█████████▎| 3796/4096 [00:45<00:04, 66.96it/s, est. speed input: 85227.30 toks/s, output: 83.23 toks/s]
Processed prompts:  93%|█████████▎| 3828/4096 [00:46<00:04, 66.70it/s, est. speed input: 85043.02 toks/s, output: 83.05 toks/s]
Processed prompts:  94%|█████████▍| 3860/4096 [00:46<00:03, 66.68it/s, est. speed input: 84869.74 toks/s, output: 82.88 toks/s]
Processed prompts:  95%|█████████▌| 3892/4096 [00:47<00:03, 66.68it/s, est. speed input: 84700.71 toks/s, output: 82.72 toks/s]
Processed prompts:  96%|█████████▌| 3924/4096 [00:47<00:02, 67.41it/s, est. speed input: 84565.78 toks/s, output: 82.58 toks/s]
Processed prompts:  97%|█████████▋| 3956/4096 [00:47<00:02, 67.23it/s, est. speed input: 84404.60 toks/s, output: 82.43 toks/s]
Processed prompts:  97%|█████████▋| 3988/4096 [00:48<00:01, 67.65it/s, est. speed input: 84269.04 toks/s, output: 82.29 toks/s]
Processed prompts:  98%|█████████▊| 4020/4096 [00:48<00:01, 67.29it/s, est. speed input: 84109.37 toks/s, output: 82.14 toks/s]
Processed prompts:  99%|█████████▉| 4052/4096 [00:49<00:00, 67.22it/s, est. speed input: 83960.46 toks/s, output: 81.99 toks/s]
Processed prompts: 100%|█████████▉| 4084/4096 [00:49<00:00, 80.87it/s, est. speed input: 84268.38 toks/s, output: 82.29 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:49<00:00, 80.87it/s, est. speed input: 84515.65 toks/s, output: 82.53 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:49<00:00, 82.53it/s, est. speed input: 84515.65 toks/s, output: 82.53 toks/s]
[rank0]:[W126 18:46:04.851951551 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 18:46:06
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:47:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=933917) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=933917) WARNING 01-26 18:47:24 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=933917) WARNING 01-26 18:47:43 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 68.58 requests/s, 70295.95 total tokens/s, 68.58 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 18:47:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:47:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:47:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:47:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:47:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:47:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:47:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:47:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:47:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:47:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:47:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:47:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:47:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:47:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:47:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:47:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:47:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:47:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:47:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:47:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:47:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:47:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:47:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:47:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:47:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:47:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:47:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:47:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=933917) [2026-01-26 18:47:09] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=933917) [2026-01-26 18:47:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=933917) [2026-01-26 18:47:10] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=933917) [2026-01-26 18:47:10] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=933917) [2026-01-26 18:47:10] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=933917) [2026-01-26 18:47:10] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=933917) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=933917) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.39s/it]
(EngineCore_DP0 pid=933917) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]
(EngineCore_DP0 pid=933917) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.09it/s]
(EngineCore_DP0 pid=933917) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.02s/it]
(EngineCore_DP0 pid=933917) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.00it/s]
(EngineCore_DP0 pid=933917) 
(EngineCore_DP0 pid=933917) [2026-01-26 18:47:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=933917) [2026-01-26 18:47:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=933917) [2026-01-26 18:47:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=933917) [2026-01-26 18:47:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=933917) [2026-01-26 18:47:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=933917) [2026-01-26 18:47:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=933917) [2026-01-26 18:47:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=933917) [2026-01-26 18:47:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=933917) [rank0]:W0126 18:47:32.172000 933917 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=933917) [rank0]:W0126 18:47:32.240000 933917 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=933917) [rank0]:W0126 18:47:33.069000 933917 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=933917) [rank0]:W0126 18:47:33.177000 933917 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=933917) 2026-01-26 18:47:43,041 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=933917) 2026-01-26 18:47:43,238 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=933917) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:02,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:01,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:01,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  9.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:01,  9.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:01,  8.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  8.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  9.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  9.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:01<00:00,  8.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:01<00:00,  9.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.17it/s]
(EngineCore_DP0 pid=933917) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 10.78it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 11.01it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 11.08it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 10.64it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 10.78it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00, 10.86it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 27/8192 [00:00<00:30, 266.41it/s]
Adding requests:   1%|          | 61/8192 [00:00<00:26, 308.60it/s]
Adding requests:   1%|          | 93/8192 [00:00<00:26, 309.52it/s]
Adding requests:   2%|▏         | 126/8192 [00:00<00:25, 317.03it/s]
Adding requests:   2%|▏         | 160/8192 [00:00<00:24, 324.41it/s]
Adding requests:   2%|▏         | 194/8192 [00:00<00:24, 328.80it/s]
Adding requests:   3%|▎         | 227/8192 [00:00<00:24, 326.28it/s]
Adding requests:   3%|▎         | 260/8192 [00:00<00:25, 306.07it/s]
Adding requests:   4%|▎         | 295/8192 [00:00<00:24, 315.97it/s]
Adding requests:   4%|▍         | 331/8192 [00:01<00:24, 327.15it/s]
Adding requests:   4%|▍         | 364/8192 [00:01<00:24, 324.42it/s]
Adding requests:   5%|▍         | 400/8192 [00:01<00:23, 332.07it/s]
Adding requests:   5%|▌         | 436/8192 [00:01<00:22, 338.41it/s]
Adding requests:   6%|▌         | 472/8192 [00:01<00:22, 343.44it/s]
Adding requests:   6%|▌         | 509/8192 [00:01<00:21, 350.42it/s]
Adding requests:   7%|▋         | 546/8192 [00:01<00:21, 356.20it/s]
Adding requests:   7%|▋         | 582/8192 [00:01<00:21, 356.51it/s]
Adding requests:   8%|▊         | 618/8192 [00:01<00:21, 351.10it/s]
Adding requests:   8%|▊         | 654/8192 [00:01<00:21, 345.25it/s]
Adding requests:   8%|▊         | 691/8192 [00:02<00:21, 351.04it/s]
Adding requests:   9%|▉         | 727/8192 [00:02<00:21, 344.90it/s]
Adding requests:   9%|▉         | 762/8192 [00:02<00:21, 346.06it/s]
Adding requests:  10%|▉         | 798/8192 [00:02<00:21, 346.72it/s]
Adding requests:  10%|█         | 836/8192 [00:02<00:20, 353.35it/s]
Adding requests:  11%|█         | 873/8192 [00:02<00:20, 356.44it/s]
Adding requests:  11%|█         | 909/8192 [00:02<00:20, 356.20it/s]
Adding requests:  12%|█▏        | 945/8192 [00:02<00:20, 349.96it/s]
Adding requests:  12%|█▏        | 981/8192 [00:02<00:20, 351.12it/s]
Adding requests:  12%|█▏        | 1017/8192 [00:02<00:20, 346.54it/s]
Adding requests:  13%|█▎        | 1052/8192 [00:03<00:20, 345.94it/s]
Adding requests:  13%|█▎        | 1087/8192 [00:03<00:20, 345.10it/s]
Adding requests:  14%|█▎        | 1123/8192 [00:03<00:20, 347.86it/s]
Adding requests:  14%|█▍        | 1158/8192 [00:03<00:20, 347.25it/s]
Adding requests:  15%|█▍        | 1193/8192 [00:03<00:20, 342.80it/s]
Adding requests:  15%|█▌        | 1231/8192 [00:03<00:19, 350.88it/s]
Adding requests:  15%|█▌        | 1267/8192 [00:03<00:19, 350.78it/s]
Adding requests:  16%|█▌        | 1303/8192 [00:03<00:19, 347.73it/s]
Adding requests:  16%|█▋        | 1338/8192 [00:03<00:19, 347.68it/s]
Adding requests:  17%|█▋        | 1375/8192 [00:04<00:19, 353.40it/s]
Adding requests:  17%|█▋        | 1411/8192 [00:04<00:19, 350.87it/s]
Adding requests:  18%|█▊        | 1447/8192 [00:04<00:19, 349.34it/s]
Adding requests:  18%|█▊        | 1484/8192 [00:04<00:18, 354.44it/s]
Adding requests:  19%|█▊        | 1521/8192 [00:04<00:18, 357.81it/s]
Adding requests:  19%|█▉        | 1557/8192 [00:04<00:18, 351.64it/s]
Adding requests:  19%|█▉        | 1593/8192 [00:04<00:19, 347.12it/s]
Adding requests:  20%|█▉        | 1628/8192 [00:04<00:19, 341.93it/s]
Adding requests:  20%|██        | 1663/8192 [00:04<00:19, 327.25it/s]
Adding requests:  21%|██        | 1698/8192 [00:04<00:19, 332.37it/s]
Adding requests:  21%|██        | 1734/8192 [00:05<00:18, 339.99it/s]
Adding requests:  22%|██▏       | 1770/8192 [00:05<00:18, 344.67it/s]
Adding requests:  22%|██▏       | 1805/8192 [00:05<00:18, 338.84it/s]
Adding requests:  22%|██▏       | 1841/8192 [00:05<00:18, 343.12it/s]
Adding requests:  23%|██▎       | 1876/8192 [00:05<00:18, 334.64it/s]
Adding requests:  23%|██▎       | 1912/8192 [00:05<00:18, 341.40it/s]
Adding requests:  24%|██▍       | 1949/8192 [00:05<00:17, 348.04it/s]
Adding requests:  24%|██▍       | 1985/8192 [00:05<00:17, 348.26it/s]
Adding requests:  25%|██▍       | 2020/8192 [00:05<00:18, 339.82it/s]
Adding requests:  25%|██▌       | 2055/8192 [00:06<00:18, 340.00it/s]
Adding requests:  26%|██▌       | 2090/8192 [00:06<00:18, 334.32it/s]
Adding requests:  26%|██▌       | 2127/8192 [00:06<00:17, 344.46it/s]
Adding requests:  26%|██▋       | 2162/8192 [00:06<00:17, 341.02it/s]
Adding requests:  27%|██▋       | 2197/8192 [00:06<00:17, 336.97it/s]
Adding requests:  27%|██▋       | 2232/8192 [00:06<00:17, 338.89it/s]
Adding requests:  28%|██▊       | 2269/8192 [00:06<00:17, 346.09it/s]
Adding requests:  28%|██▊       | 2305/8192 [00:06<00:16, 348.93it/s]
Adding requests:  29%|██▊       | 2342/8192 [00:06<00:16, 354.31it/s]
Adding requests:  29%|██▉       | 2378/8192 [00:06<00:16, 342.41it/s]
Adding requests:  29%|██▉       | 2416/8192 [00:07<00:16, 352.48it/s]
Adding requests:  30%|██▉       | 2452/8192 [00:07<00:16, 351.51it/s]
Adding requests:  30%|███       | 2488/8192 [00:07<00:16, 352.11it/s]
Adding requests:  31%|███       | 2525/8192 [00:07<00:15, 354.51it/s]
Adding requests:  31%|███▏      | 2565/8192 [00:07<00:15, 367.70it/s]
Adding requests:  32%|███▏      | 2603/8192 [00:07<00:15, 366.98it/s]
Adding requests:  32%|███▏      | 2640/8192 [00:07<00:15, 355.00it/s]
Adding requests:  33%|███▎      | 2676/8192 [00:07<00:15, 355.11it/s]
Adding requests:  33%|███▎      | 2712/8192 [00:07<00:15, 349.84it/s]
Adding requests:  34%|███▎      | 2749/8192 [00:07<00:15, 354.85it/s]
Adding requests:  34%|███▍      | 2787/8192 [00:08<00:14, 361.12it/s]
Adding requests:  34%|███▍      | 2824/8192 [00:08<00:14, 363.28it/s]
Adding requests:  35%|███▍      | 2861/8192 [00:08<00:14, 362.07it/s]
Adding requests:  35%|███▌      | 2898/8192 [00:08<00:14, 358.28it/s]
Adding requests:  36%|███▌      | 2935/8192 [00:08<00:14, 361.46it/s]
Adding requests:  36%|███▋      | 2972/8192 [00:08<00:15, 345.68it/s]
Adding requests:  37%|███▋      | 3010/8192 [00:08<00:14, 354.10it/s]
Adding requests:  37%|███▋      | 3047/8192 [00:08<00:14, 357.62it/s]
Adding requests:  38%|███▊      | 3084/8192 [00:08<00:14, 361.02it/s]
Adding requests:  38%|███▊      | 3123/8192 [00:09<00:13, 366.16it/s]
Adding requests:  39%|███▊      | 3160/8192 [00:09<00:13, 360.42it/s]
Adding requests:  39%|███▉      | 3197/8192 [00:09<00:13, 357.22it/s]
Adding requests:  39%|███▉      | 3235/8192 [00:09<00:13, 361.22it/s]
Adding requests:  40%|███▉      | 3272/8192 [00:09<00:13, 356.29it/s]
Adding requests:  40%|████      | 3308/8192 [00:09<00:14, 345.74it/s]
Adding requests:  41%|████      | 3345/8192 [00:09<00:13, 350.78it/s]
Adding requests:  41%|████▏     | 3381/8192 [00:09<00:13, 352.91it/s]
Adding requests:  42%|████▏     | 3417/8192 [00:09<00:13, 354.44it/s]
Adding requests:  42%|████▏     | 3454/8192 [00:09<00:13, 357.62it/s]
Adding requests:  43%|████▎     | 3490/8192 [00:10<00:13, 353.44it/s]
Adding requests:  43%|████▎     | 3529/8192 [00:10<00:12, 362.48it/s]
Adding requests:  44%|████▎     | 3568/8192 [00:10<00:12, 367.26it/s]
Adding requests:  44%|████▍     | 3605/8192 [00:10<00:12, 360.94it/s]
Adding requests:  44%|████▍     | 3643/8192 [00:10<00:12, 364.78it/s]
Adding requests:  45%|████▍     | 3680/8192 [00:10<00:12, 355.12it/s]
Adding requests:  45%|████▌     | 3716/8192 [00:10<00:12, 355.66it/s]
Adding requests:  46%|████▌     | 3752/8192 [00:10<00:12, 347.98it/s]
Adding requests:  46%|████▌     | 3787/8192 [00:10<00:12, 339.69it/s]
Adding requests:  47%|████▋     | 3822/8192 [00:11<00:13, 335.48it/s]
Adding requests:  47%|████▋     | 3858/8192 [00:11<00:12, 340.28it/s]
Adding requests:  48%|████▊     | 3893/8192 [00:11<00:12, 340.94it/s]
Adding requests:  48%|████▊     | 3928/8192 [00:11<00:12, 338.53it/s]
Adding requests:  48%|████▊     | 3963/8192 [00:11<00:12, 341.00it/s]
Adding requests:  49%|████▉     | 3998/8192 [00:11<00:12, 339.71it/s]
Adding requests:  49%|████▉     | 4034/8192 [00:11<00:12, 342.60it/s]
Adding requests:  50%|████▉     | 4069/8192 [00:11<00:12, 342.76it/s]
Adding requests:  50%|█████     | 4104/8192 [00:11<00:11, 342.65it/s]
Adding requests:  51%|█████     | 4139/8192 [00:11<00:11, 344.59it/s]
Adding requests:  51%|█████     | 4176/8192 [00:12<00:11, 350.54it/s]
Adding requests:  51%|█████▏    | 4212/8192 [00:12<00:11, 344.68it/s]
Adding requests:  52%|█████▏    | 4249/8192 [00:12<00:11, 350.56it/s]
Adding requests:  52%|█████▏    | 4285/8192 [00:12<00:11, 337.48it/s]
Adding requests:  53%|█████▎    | 4320/8192 [00:12<00:11, 340.29it/s]
Adding requests:  53%|█████▎    | 4356/8192 [00:12<00:11, 345.44it/s]
Adding requests:  54%|█████▎    | 4391/8192 [00:12<00:11, 345.33it/s]
Adding requests:  54%|█████▍    | 4427/8192 [00:12<00:10, 348.30it/s]
Adding requests:  54%|█████▍    | 4463/8192 [00:12<00:10, 350.45it/s]
Adding requests:  55%|█████▍    | 4499/8192 [00:12<00:10, 350.15it/s]
Adding requests:  55%|█████▌    | 4536/8192 [00:13<00:10, 355.83it/s]
Adding requests:  56%|█████▌    | 4572/8192 [00:13<00:10, 349.81it/s]
Adding requests:  56%|█████▋    | 4608/8192 [00:13<00:10, 352.28it/s]
Adding requests:  57%|█████▋    | 4644/8192 [00:13<00:10, 347.56it/s]
Adding requests:  57%|█████▋    | 4679/8192 [00:13<00:10, 341.55it/s]
Adding requests:  58%|█████▊    | 4714/8192 [00:13<00:10, 340.84it/s]
Adding requests:  58%|█████▊    | 4752/8192 [00:13<00:09, 351.78it/s]
Adding requests:  58%|█████▊    | 4788/8192 [00:13<00:09, 346.63it/s]
Adding requests:  59%|█████▉    | 4823/8192 [00:13<00:09, 337.08it/s]
Adding requests:  59%|█████▉    | 4857/8192 [00:14<00:09, 336.13it/s]
Adding requests:  60%|█████▉    | 4892/8192 [00:14<00:09, 339.16it/s]
Adding requests:  60%|██████    | 4928/8192 [00:14<00:09, 344.03it/s]
Adding requests:  61%|██████    | 4964/8192 [00:14<00:09, 347.20it/s]
Adding requests:  61%|██████    | 4999/8192 [00:14<00:09, 346.98it/s]
Adding requests:  61%|██████▏   | 5037/8192 [00:14<00:08, 354.66it/s]
Adding requests:  62%|██████▏   | 5073/8192 [00:14<00:08, 353.25it/s]
Adding requests:  62%|██████▏   | 5109/8192 [00:14<00:08, 354.81it/s]
Adding requests:  63%|██████▎   | 5145/8192 [00:14<00:08, 353.55it/s]
Adding requests:  63%|██████▎   | 5181/8192 [00:14<00:08, 351.40it/s]
Adding requests:  64%|██████▎   | 5217/8192 [00:15<00:08, 346.87it/s]
Adding requests:  64%|██████▍   | 5252/8192 [00:15<00:08, 346.82it/s]
Adding requests:  65%|██████▍   | 5287/8192 [00:15<00:08, 345.53it/s]
Adding requests:  65%|██████▍   | 5322/8192 [00:15<00:08, 346.42it/s]
Adding requests:  65%|██████▌   | 5358/8192 [00:15<00:08, 349.57it/s]
Adding requests:  66%|██████▌   | 5393/8192 [00:15<00:08, 344.74it/s]
Adding requests:  66%|██████▋   | 5428/8192 [00:15<00:08, 341.07it/s]
Adding requests:  67%|██████▋   | 5466/8192 [00:15<00:07, 349.72it/s]
Adding requests:  67%|██████▋   | 5501/8192 [00:15<00:07, 340.22it/s]
Adding requests:  68%|██████▊   | 5537/8192 [00:15<00:07, 342.93it/s]
Adding requests:  68%|██████▊   | 5572/8192 [00:16<00:07, 342.90it/s]
Adding requests:  68%|██████▊   | 5607/8192 [00:16<00:07, 330.16it/s]
Adding requests:  69%|██████▉   | 5644/8192 [00:16<00:07, 339.25it/s]
Adding requests:  69%|██████▉   | 5679/8192 [00:16<00:07, 339.18it/s]
Adding requests:  70%|██████▉   | 5714/8192 [00:16<00:07, 342.07it/s]
Adding requests:  70%|███████   | 5749/8192 [00:16<00:07, 343.76it/s]
Adding requests:  71%|███████   | 5785/8192 [00:16<00:06, 347.44it/s]
Adding requests:  71%|███████   | 5822/8192 [00:16<00:06, 352.75it/s]
Adding requests:  72%|███████▏  | 5858/8192 [00:16<00:06, 348.74it/s]
Adding requests:  72%|███████▏  | 5896/8192 [00:17<00:06, 355.95it/s]
Adding requests:  72%|███████▏  | 5932/8192 [00:17<00:06, 356.13it/s]
Adding requests:  73%|███████▎  | 5969/8192 [00:17<00:06, 358.81it/s]
Adding requests:  73%|███████▎  | 6005/8192 [00:17<00:06, 357.94it/s]
Adding requests:  74%|███████▎  | 6041/8192 [00:17<00:05, 358.52it/s]
Adding requests:  74%|███████▍  | 6077/8192 [00:17<00:05, 354.53it/s]
Adding requests:  75%|███████▍  | 6113/8192 [00:17<00:05, 347.74it/s]
Adding requests:  75%|███████▌  | 6150/8192 [00:17<00:05, 353.13it/s]
Adding requests:  76%|███████▌  | 6186/8192 [00:17<00:05, 352.78it/s]
Adding requests:  76%|███████▌  | 6222/8192 [00:17<00:05, 349.31it/s]
Adding requests:  76%|███████▋  | 6258/8192 [00:18<00:05, 350.26it/s]
Adding requests:  77%|███████▋  | 6294/8192 [00:18<00:05, 350.06it/s]
Adding requests:  77%|███████▋  | 6332/8192 [00:18<00:05, 358.49it/s]
Adding requests:  78%|███████▊  | 6368/8192 [00:18<00:05, 353.08it/s]
Adding requests:  78%|███████▊  | 6404/8192 [00:18<00:05, 344.68it/s]
Adding requests:  79%|███████▊  | 6439/8192 [00:18<00:05, 339.33it/s]
Adding requests:  79%|███████▉  | 6473/8192 [00:18<00:05, 338.57it/s]
Adding requests:  79%|███████▉  | 6508/8192 [00:18<00:04, 341.57it/s]
Adding requests:  80%|███████▉  | 6544/8192 [00:18<00:04, 344.90it/s]
Adding requests:  80%|████████  | 6580/8192 [00:18<00:04, 347.73it/s]
Adding requests:  81%|████████  | 6615/8192 [00:19<00:04, 346.99it/s]
Adding requests:  81%|████████  | 6651/8192 [00:19<00:04, 350.03it/s]
Adding requests:  82%|████████▏ | 6687/8192 [00:19<00:04, 345.97it/s]
Adding requests:  82%|████████▏ | 6724/8192 [00:19<00:04, 350.44it/s]
Adding requests:  83%|████████▎ | 6760/8192 [00:19<00:04, 352.11it/s]
Adding requests:  83%|████████▎ | 6796/8192 [00:19<00:03, 351.56it/s]
Adding requests:  83%|████████▎ | 6832/8192 [00:19<00:03, 347.00it/s]
Adding requests:  84%|████████▍ | 6868/8192 [00:19<00:03, 350.19it/s]
Adding requests:  84%|████████▍ | 6904/8192 [00:19<00:03, 348.45it/s]
Adding requests:  85%|████████▍ | 6940/8192 [00:19<00:03, 341.06it/s]
Adding requests:  85%|████████▌ | 6975/8192 [00:20<00:03, 339.04it/s]
Adding requests:  86%|████████▌ | 7013/8192 [00:20<00:03, 348.94it/s]
Adding requests:  86%|████████▌ | 7048/8192 [00:20<00:03, 346.35it/s]
Adding requests:  86%|████████▋ | 7084/8192 [00:20<00:03, 347.63it/s]
Adding requests:  87%|████████▋ | 7119/8192 [00:20<00:03, 348.06it/s]
Adding requests:  87%|████████▋ | 7154/8192 [00:20<00:03, 345.28it/s]
Adding requests:  88%|████████▊ | 7191/8192 [00:20<00:02, 351.39it/s]
Adding requests:  88%|████████▊ | 7227/8192 [00:20<00:02, 353.82it/s]
Adding requests:  89%|████████▊ | 7264/8192 [00:20<00:02, 356.21it/s]
Adding requests:  89%|████████▉ | 7300/8192 [00:21<00:02, 356.13it/s]
Adding requests:  90%|████████▉ | 7336/8192 [00:21<00:02, 353.88it/s]
Adding requests:  90%|█████████ | 7373/8192 [00:21<00:02, 358.58it/s]
Adding requests:  90%|█████████ | 7409/8192 [00:21<00:02, 353.03it/s]
Adding requests:  91%|█████████ | 7445/8192 [00:21<00:02, 352.62it/s]
Adding requests:  91%|█████████▏| 7481/8192 [00:21<00:02, 346.41it/s]
Adding requests:  92%|█████████▏| 7516/8192 [00:21<00:01, 340.77it/s]
Adding requests:  92%|█████████▏| 7551/8192 [00:21<00:01, 341.61it/s]
Adding requests:  93%|█████████▎| 7586/8192 [00:21<00:01, 341.19it/s]
Adding requests:  93%|█████████▎| 7622/8192 [00:21<00:01, 344.88it/s]
Adding requests:  93%|█████████▎| 7659/8192 [00:22<00:01, 350.60it/s]
Adding requests:  94%|█████████▍| 7697/8192 [00:22<00:01, 359.06it/s]
Adding requests:  94%|█████████▍| 7733/8192 [00:22<00:01, 356.60it/s]
Adding requests:  95%|█████████▍| 7769/8192 [00:22<00:01, 353.09it/s]
Adding requests:  95%|█████████▌| 7805/8192 [00:22<00:01, 354.17it/s]
Adding requests:  96%|█████████▌| 7841/8192 [00:22<00:01, 345.81it/s]
Adding requests:  96%|█████████▌| 7876/8192 [00:22<00:00, 343.47it/s]
Adding requests:  97%|█████████▋| 7912/8192 [00:22<00:00, 346.25it/s]
Adding requests:  97%|█████████▋| 7951/8192 [00:22<00:00, 356.44it/s]
Adding requests:  98%|█████████▊| 7989/8192 [00:22<00:00, 362.29it/s]
Adding requests:  98%|█████████▊| 8026/8192 [00:23<00:00, 355.28it/s]
Adding requests:  98%|█████████▊| 8064/8192 [00:23<00:00, 359.27it/s]
Adding requests:  99%|█████████▉| 8100/8192 [00:23<00:00, 354.80it/s]
Adding requests:  99%|█████████▉| 8136/8192 [00:23<00:00, 351.67it/s]
Adding requests: 100%|█████████▉| 8172/8192 [00:23<00:00, 345.98it/s]
Adding requests: 100%|██████████| 8192/8192 [00:23<00:00, 347.59it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 1557/8192 [00:00<00:00, 13219.40it/s, est. speed input: 13537684.33 toks/s, output: 13219.65 toks/s]
Processed prompts:  33%|███▎      | 2708/8192 [00:16<00:00, 13219.40it/s, est. speed input: 174355.23 toks/s, output: 170.27 toks/s]    
Processed prompts:  33%|███▎      | 2709/8192 [00:16<00:41, 133.49it/s, est. speed input: 164824.85 toks/s, output: 160.96 toks/s]  
Processed prompts:  34%|███▍      | 2773/8192 [00:17<00:42, 128.72it/s, est. speed input: 159863.48 toks/s, output: 156.12 toks/s]
Processed prompts:  41%|████      | 3322/8192 [00:25<00:47, 103.58it/s, est. speed input: 134886.55 toks/s, output: 131.73 toks/s]
Processed prompts:  44%|████▍     | 3624/8192 [00:29<00:49, 92.21it/s, est. speed input: 124092.86 toks/s, output: 121.18 toks/s] 
Processed prompts:  46%|████▋     | 3809/8192 [00:32<00:50, 87.05it/s, est. speed input: 119283.74 toks/s, output: 116.49 toks/s]
Processed prompts:  48%|████▊     | 3930/8192 [00:34<00:51, 83.57it/s, est. speed input: 116452.68 toks/s, output: 113.72 toks/s]
Processed prompts:  49%|████▉     | 4012/8192 [00:35<00:49, 84.08it/s, est. speed input: 115782.63 toks/s, output: 113.07 toks/s]
Processed prompts:  50%|████▉     | 4070/8192 [00:36<00:50, 81.39it/s, est. speed input: 114475.50 toks/s, output: 111.79 toks/s]
Processed prompts:  50%|█████     | 4117/8192 [00:37<00:53, 76.56it/s, est. speed input: 112892.19 toks/s, output: 110.25 toks/s]
Processed prompts:  51%|█████     | 4181/8192 [00:38<00:53, 75.06it/s, est. speed input: 111839.90 toks/s, output: 109.22 toks/s]
Processed prompts:  52%|█████▏    | 4245/8192 [00:39<00:53, 73.70it/s, est. speed input: 110844.01 toks/s, output: 108.25 toks/s]
Processed prompts:  53%|█████▎    | 4309/8192 [00:40<00:53, 72.46it/s, est. speed input: 109887.51 toks/s, output: 107.31 toks/s]
Processed prompts:  53%|█████▎    | 4373/8192 [00:41<00:53, 71.56it/s, est. speed input: 108992.84 toks/s, output: 106.44 toks/s]
Processed prompts:  54%|█████▍    | 4437/8192 [00:42<00:52, 71.03it/s, est. speed input: 108163.03 toks/s, output: 105.63 toks/s]
Processed prompts:  55%|█████▍    | 4501/8192 [00:42<00:52, 70.50it/s, est. speed input: 107355.00 toks/s, output: 104.84 toks/s]
Processed prompts:  56%|█████▌    | 4565/8192 [00:43<00:51, 70.13it/s, est. speed input: 106584.72 toks/s, output: 104.09 toks/s]
Processed prompts:  57%|█████▋    | 4629/8192 [00:44<00:51, 69.51it/s, est. speed input: 105807.02 toks/s, output: 103.33 toks/s]
Processed prompts:  57%|█████▋    | 4693/8192 [00:45<00:50, 69.11it/s, est. speed input: 105066.96 toks/s, output: 102.60 toks/s]
Processed prompts:  58%|█████▊    | 4757/8192 [00:46<00:49, 69.06it/s, est. speed input: 104380.88 toks/s, output: 101.93 toks/s]
Processed prompts:  59%|█████▉    | 4821/8192 [00:47<00:49, 68.76it/s, est. speed input: 103695.42 toks/s, output: 101.27 toks/s]
Processed prompts:  60%|█████▉    | 4885/8192 [00:48<00:47, 69.01it/s, est. speed input: 103081.27 toks/s, output: 100.67 toks/s]
Processed prompts:  60%|██████    | 4949/8192 [00:49<00:47, 68.98it/s, est. speed input: 102470.49 toks/s, output: 100.07 toks/s]
Processed prompts:  61%|██████    | 5013/8192 [00:50<00:46, 68.88it/s, est. speed input: 101875.69 toks/s, output: 99.49 toks/s] 
Processed prompts:  62%|██████▏   | 5077/8192 [00:51<00:45, 68.66it/s, est. speed input: 101288.47 toks/s, output: 98.91 toks/s]
Processed prompts:  63%|██████▎   | 5141/8192 [00:52<00:44, 68.60it/s, est. speed input: 100730.22 toks/s, output: 98.37 toks/s]
Processed prompts:  64%|██████▎   | 5205/8192 [00:53<00:43, 68.80it/s, est. speed input: 100212.70 toks/s, output: 97.86 toks/s]
Processed prompts:  64%|██████▍   | 5269/8192 [00:54<00:42, 69.05it/s, est. speed input: 99722.21 toks/s, output: 97.38 toks/s] 
Processed prompts:  65%|██████▌   | 5333/8192 [00:55<00:41, 68.77it/s, est. speed input: 99210.81 toks/s, output: 96.89 toks/s]
Processed prompts:  66%|██████▌   | 5397/8192 [00:55<00:40, 68.50it/s, est. speed input: 98710.82 toks/s, output: 96.40 toks/s]
Processed prompts:  67%|██████▋   | 5461/8192 [00:56<00:39, 68.75it/s, est. speed input: 98260.97 toks/s, output: 95.96 toks/s]
Processed prompts:  67%|██████▋   | 5525/8192 [00:57<00:38, 68.81it/s, est. speed input: 97817.53 toks/s, output: 95.52 toks/s]
Processed prompts:  68%|██████▊   | 5589/8192 [00:58<00:37, 68.74it/s, est. speed input: 97379.30 toks/s, output: 95.10 toks/s]
Processed prompts:  69%|██████▉   | 5653/8192 [00:59<00:36, 68.73it/s, est. speed input: 96957.93 toks/s, output: 94.69 toks/s]
Processed prompts:  70%|██████▉   | 5717/8192 [01:00<00:36, 68.62it/s, est. speed input: 96541.56 toks/s, output: 94.28 toks/s]
Processed prompts:  71%|███████   | 5781/8192 [01:01<00:35, 68.48it/s, est. speed input: 96133.59 toks/s, output: 93.88 toks/s]
Processed prompts:  71%|███████▏  | 5845/8192 [01:02<00:34, 68.53it/s, est. speed input: 95748.31 toks/s, output: 93.50 toks/s]
Processed prompts:  72%|███████▏  | 5909/8192 [01:03<00:33, 68.79it/s, est. speed input: 95389.62 toks/s, output: 93.15 toks/s]
Processed prompts:  73%|███████▎  | 5973/8192 [01:04<00:32, 68.96it/s, est. speed input: 95039.92 toks/s, output: 92.81 toks/s]
Processed prompts:  74%|███████▎  | 6037/8192 [01:05<00:31, 68.84it/s, est. speed input: 94685.10 toks/s, output: 92.47 toks/s]
Processed prompts:  74%|███████▍  | 6101/8192 [01:06<00:30, 68.63it/s, est. speed input: 94331.87 toks/s, output: 92.12 toks/s]
Processed prompts:  75%|███████▌  | 6165/8192 [01:07<00:29, 68.45it/s, est. speed input: 93986.51 toks/s, output: 91.78 toks/s]
Processed prompts:  76%|███████▌  | 6229/8192 [01:08<00:28, 68.59it/s, est. speed input: 93667.05 toks/s, output: 91.47 toks/s]
Processed prompts:  77%|███████▋  | 6293/8192 [01:09<00:27, 68.49it/s, est. speed input: 93344.31 toks/s, output: 91.16 toks/s]
Processed prompts:  78%|███████▊  | 6357/8192 [01:09<00:26, 68.51it/s, est. speed input: 93035.52 toks/s, output: 90.85 toks/s]
Processed prompts:  78%|███████▊  | 6421/8192 [01:10<00:25, 68.46it/s, est. speed input: 92731.29 toks/s, output: 90.56 toks/s]
Processed prompts:  79%|███████▉  | 6485/8192 [01:11<00:25, 68.27it/s, est. speed input: 92425.70 toks/s, output: 90.26 toks/s]
Processed prompts:  80%|███████▉  | 6549/8192 [01:12<00:24, 68.27it/s, est. speed input: 92135.70 toks/s, output: 89.98 toks/s]
Processed prompts:  81%|████████  | 6613/8192 [01:13<00:23, 68.27it/s, est. speed input: 91852.71 toks/s, output: 89.70 toks/s]
Processed prompts:  82%|████████▏ | 6677/8192 [01:14<00:22, 68.10it/s, est. speed input: 91567.63 toks/s, output: 89.42 toks/s]
Processed prompts:  82%|████████▏ | 6741/8192 [01:15<00:21, 68.20it/s, est. speed input: 91301.88 toks/s, output: 89.16 toks/s]
Processed prompts:  83%|████████▎ | 6805/8192 [01:16<00:20, 68.23it/s, est. speed input: 91040.64 toks/s, output: 88.91 toks/s]
Processed prompts:  84%|████████▍ | 6869/8192 [01:17<00:19, 68.22it/s, est. speed input: 90783.63 toks/s, output: 88.66 toks/s]
Processed prompts:  85%|████████▍ | 6933/8192 [01:18<00:18, 68.20it/s, est. speed input: 90531.99 toks/s, output: 88.41 toks/s]
Processed prompts:  85%|████████▌ | 6997/8192 [01:19<00:17, 68.24it/s, est. speed input: 90289.71 toks/s, output: 88.17 toks/s]
Processed prompts:  86%|████████▌ | 7061/8192 [01:20<00:16, 68.54it/s, est. speed input: 90066.23 toks/s, output: 87.96 toks/s]
Processed prompts:  87%|████████▋ | 7125/8192 [01:21<00:15, 68.75it/s, est. speed input: 89848.11 toks/s, output: 87.74 toks/s]
Processed prompts:  88%|████████▊ | 7189/8192 [01:22<00:14, 68.61it/s, est. speed input: 89620.99 toks/s, output: 87.52 toks/s]
Processed prompts:  89%|████████▊ | 7253/8192 [01:23<00:13, 68.74it/s, est. speed input: 89409.60 toks/s, output: 87.31 toks/s]
Processed prompts:  89%|████████▉ | 7317/8192 [01:24<00:12, 68.58it/s, est. speed input: 89191.26 toks/s, output: 87.10 toks/s]
Processed prompts:  90%|█████████ | 7381/8192 [01:24<00:11, 68.53it/s, est. speed input: 88980.27 toks/s, output: 86.89 toks/s]
Processed prompts:  91%|█████████ | 7445/8192 [01:25<00:10, 68.71it/s, est. speed input: 88784.52 toks/s, output: 86.70 toks/s]
Processed prompts:  92%|█████████▏| 7509/8192 [01:26<00:09, 68.57it/s, est. speed input: 88580.12 toks/s, output: 86.50 toks/s]
Processed prompts:  92%|█████████▏| 7573/8192 [01:27<00:08, 68.87it/s, est. speed input: 88398.66 toks/s, output: 86.33 toks/s]
Processed prompts:  93%|█████████▎| 7637/8192 [01:28<00:08, 68.74it/s, est. speed input: 88205.17 toks/s, output: 86.14 toks/s]
Processed prompts:  94%|█████████▍| 7701/8192 [01:29<00:07, 68.82it/s, est. speed input: 88023.67 toks/s, output: 85.96 toks/s]
Processed prompts:  95%|█████████▍| 7765/8192 [01:30<00:06, 68.73it/s, est. speed input: 87839.58 toks/s, output: 85.78 toks/s]
Processed prompts:  96%|█████████▌| 7829/8192 [01:31<00:05, 68.40it/s, est. speed input: 87647.14 toks/s, output: 85.59 toks/s]
Processed prompts:  96%|█████████▋| 7893/8192 [01:32<00:04, 68.32it/s, est. speed input: 87465.35 toks/s, output: 85.42 toks/s]
Processed prompts:  97%|█████████▋| 7957/8192 [01:33<00:03, 68.32it/s, est. speed input: 87289.91 toks/s, output: 85.24 toks/s]
Processed prompts:  98%|█████████▊| 8021/8192 [01:34<00:02, 68.16it/s, est. speed input: 87111.07 toks/s, output: 85.07 toks/s]
Processed prompts:  99%|█████████▊| 8085/8192 [01:35<00:01, 68.16it/s, est. speed input: 86940.33 toks/s, output: 84.90 toks/s]
Processed prompts:  99%|█████████▉| 8149/8192 [01:35<00:00, 75.09it/s, est. speed input: 87034.39 toks/s, output: 84.99 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:35<00:00, 75.09it/s, est. speed input: 87493.16 toks/s, output: 85.44 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:35<00:00, 85.44it/s, est. speed input: 87493.16 toks/s, output: 85.44 toks/s]
[rank0]:[W126 18:49:48.770853763 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

