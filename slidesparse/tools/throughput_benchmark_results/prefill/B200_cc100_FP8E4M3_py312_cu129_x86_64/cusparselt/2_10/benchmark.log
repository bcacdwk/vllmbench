
========== M=16 ==========
Time: 2026-01-26 13:22:51
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:22:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=485567) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=485567) WARNING 01-26 13:23:10 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=485567) WARNING 01-26 13:23:17 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 44.56 requests/s, 757.47 total tokens/s, 44.56 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-26 13:22:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:22:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:22:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:22:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:22:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:22:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:22:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:22:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:22:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:22:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:22:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:22:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:22:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:22:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:23:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:23:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:23:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:23:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:23:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:23:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:23:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:23:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:23:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:23:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:23:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:23:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:23:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:23:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=485567) [2026-01-26 13:23:05] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=485567) [2026-01-26 13:23:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=485567) [2026-01-26 13:23:05] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=485567) [2026-01-26 13:23:05] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=485567) [2026-01-26 13:23:05] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=485567) [2026-01-26 13:23:05] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=485567) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=485567) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.50it/s]
(EngineCore_DP0 pid=485567) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.49it/s]
(EngineCore_DP0 pid=485567) 
(EngineCore_DP0 pid=485567) [2026-01-26 13:23:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=485567) [2026-01-26 13:23:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=485567) [2026-01-26 13:23:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=485567) [2026-01-26 13:23:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=485567) [2026-01-26 13:23:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=485567) [2026-01-26 13:23:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=485567) [2026-01-26 13:23:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=485567) [2026-01-26 13:23:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=485567) 2026-01-26 13:23:17,278 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=485567) 2026-01-26 13:23:17,298 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=485567) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.98it/s]
(EngineCore_DP0 pid=485567) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 21.40it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 440.78it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 520.54it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:00, 131.56it/s, est. speed input: 2105.07 toks/s, output: 131.56 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:01, 63.57it/s, est. speed input: 1108.90 toks/s, output: 69.31 toks/s]  
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 55.78it/s, est. speed input: 985.86 toks/s, output: 61.62 toks/s] 
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 52.30it/s, est. speed input: 932.74 toks/s, output: 58.30 toks/s]
Processed prompts:  41%|████      | 52/128 [00:00<00:01, 50.19it/s, est. speed input: 901.37 toks/s, output: 56.34 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 48.66it/s, est. speed input: 878.10 toks/s, output: 54.88 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 47.68it/s, est. speed input: 862.67 toks/s, output: 53.92 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 46.96it/s, est. speed input: 850.23 toks/s, output: 53.14 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 46.37it/s, est. speed input: 839.57 toks/s, output: 52.47 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 45.88it/s, est. speed input: 830.25 toks/s, output: 51.89 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:00, 45.58it/s, est. speed input: 822.44 toks/s, output: 51.40 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 44.97it/s, est. speed input: 814.06 toks/s, output: 50.88 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:01<00:00, 44.91it/s, est. speed input: 808.12 toks/s, output: 50.51 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:01<00:00, 44.40it/s, est. speed input: 801.26 toks/s, output: 50.08 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 44.56it/s, est. speed input: 796.83 toks/s, output: 49.80 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 44.78it/s, est. speed input: 793.19 toks/s, output: 49.57 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 44.59it/s, est. speed input: 788.92 toks/s, output: 49.31 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 44.73it/s, est. speed input: 785.76 toks/s, output: 49.11 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 44.78it/s, est. speed input: 782.78 toks/s, output: 48.92 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.79it/s, est. speed input: 779.98 toks/s, output: 48.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.79it/s, est. speed input: 779.98 toks/s, output: 48.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 48.75it/s, est. speed input: 779.98 toks/s, output: 48.75 toks/s]
[rank0]:[W126 13:23:22.703407913 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 13:23:24
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:23:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=486631) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=486631) WARNING 01-26 13:23:43 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=486631) WARNING 01-26 13:23:50 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 42.06 requests/s, 5425.48 total tokens/s, 42.06 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-26 13:23:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:23:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:23:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:23:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:23:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:23:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:23:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:23:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:23:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:23:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:23:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:23:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:23:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:23:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:23:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:23:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:23:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:23:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:23:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:23:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:23:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:23:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:23:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:23:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:23:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:23:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:23:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:23:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=486631) [2026-01-26 13:23:38] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=486631) [2026-01-26 13:23:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=486631) [2026-01-26 13:23:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=486631) [2026-01-26 13:23:38] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=486631) [2026-01-26 13:23:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=486631) [2026-01-26 13:23:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=486631) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=486631) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.68it/s]
(EngineCore_DP0 pid=486631) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.68it/s]
(EngineCore_DP0 pid=486631) 
(EngineCore_DP0 pid=486631) [2026-01-26 13:23:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=486631) [2026-01-26 13:23:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=486631) [2026-01-26 13:23:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=486631) [2026-01-26 13:23:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=486631) [2026-01-26 13:23:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=486631) [2026-01-26 13:23:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=486631) [2026-01-26 13:23:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=486631) [2026-01-26 13:23:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=486631) 2026-01-26 13:23:50,444 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=486631) 2026-01-26 13:23:50,467 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=486631) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 21.39it/s]
(EngineCore_DP0 pid=486631) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 21.14it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1971.54it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 17.00it/s, est. speed input: 2175.63 toks/s, output: 17.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:03, 32.63it/s, est. speed input: 3871.66 toks/s, output: 30.25 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:03, 37.87it/s, est. speed input: 4459.09 toks/s, output: 34.84 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 40.24it/s, est. speed input: 4746.91 toks/s, output: 37.08 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 41.52it/s, est. speed input: 4917.75 toks/s, output: 38.42 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 42.30it/s, est. speed input: 5032.62 toks/s, output: 39.32 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 42.90it/s, est. speed input: 5120.08 toks/s, output: 40.00 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 43.32it/s, est. speed input: 5186.85 toks/s, output: 40.52 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:01, 43.51it/s, est. speed input: 5235.11 toks/s, output: 40.90 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:01, 43.59it/s, est. speed input: 5272.10 toks/s, output: 41.19 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 43.69it/s, est. speed input: 5303.69 toks/s, output: 41.43 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 43.80it/s, est. speed input: 5331.61 toks/s, output: 41.65 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 43.88it/s, est. speed input: 5355.02 toks/s, output: 41.84 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 43.89it/s, est. speed input: 5374.24 toks/s, output: 41.99 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 43.92it/s, est. speed input: 5391.14 toks/s, output: 42.12 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 43.89it/s, est. speed input: 5404.93 toks/s, output: 42.23 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 43.86it/s, est. speed input: 5416.71 toks/s, output: 42.32 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:00, 43.96it/s, est. speed input: 5429.90 toks/s, output: 42.42 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 44.00it/s, est. speed input: 5441.03 toks/s, output: 42.51 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 44.01it/s, est. speed input: 5450.81 toks/s, output: 42.58 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 44.14it/s, est. speed input: 5462.13 toks/s, output: 42.67 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 44.16it/s, est. speed input: 5470.89 toks/s, output: 42.74 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 44.17it/s, est. speed input: 5478.87 toks/s, output: 42.80 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 44.18it/s, est. speed input: 5486.34 toks/s, output: 42.86 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 44.28it/s, est. speed input: 5494.65 toks/s, output: 42.93 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 44.28it/s, est. speed input: 5501.36 toks/s, output: 42.98 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.28it/s, est. speed input: 5502.67 toks/s, output: 42.99 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 42.99it/s, est. speed input: 5502.67 toks/s, output: 42.99 toks/s]
[rank0]:[W126 13:23:55.743511243 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 13:23:57
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:24:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=487674) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=487674) WARNING 01-26 13:24:16 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=487674) WARNING 01-26 13:24:23 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 41.01 requests/s, 10539.28 total tokens/s, 41.01 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-26 13:24:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:24:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:24:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:24:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:24:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:24:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:24:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:24:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:24:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:24:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:24:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:24:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:24:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:24:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:24:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:24:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 13:24:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 13:24:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:24:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:24:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:24:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:24:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 13:24:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 13:24:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:24:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:24:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:24:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:24:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=487674) [2026-01-26 13:24:11] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=487674) [2026-01-26 13:24:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=487674) [2026-01-26 13:24:11] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=487674) [2026-01-26 13:24:11] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=487674) [2026-01-26 13:24:11] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=487674) [2026-01-26 13:24:11] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=487674) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=487674) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.36it/s]
(EngineCore_DP0 pid=487674) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.36it/s]
(EngineCore_DP0 pid=487674) 
(EngineCore_DP0 pid=487674) [2026-01-26 13:24:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=487674) [2026-01-26 13:24:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=487674) [2026-01-26 13:24:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=487674) [2026-01-26 13:24:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=487674) [2026-01-26 13:24:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=487674) [2026-01-26 13:24:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=487674) [2026-01-26 13:24:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=487674) [2026-01-26 13:24:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=487674) 2026-01-26 13:24:23,802 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=487674) 2026-01-26 13:24:23,824 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=487674) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.91it/s]
(EngineCore_DP0 pid=487674) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.42it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  95%|█████████▌| 122/128 [00:00<00:00, 1214.62it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1218.08it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:19,  6.37it/s, est. speed input: 1631.56 toks/s, output: 6.37 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:04, 25.33it/s, est. speed input: 5644.32 toks/s, output: 22.05 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 33.38it/s, est. speed input: 7335.51 toks/s, output: 28.65 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:02, 37.54it/s, est. speed input: 8255.20 toks/s, output: 32.25 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 40.00it/s, est. speed input: 8838.88 toks/s, output: 34.53 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 41.57it/s, est. speed input: 9244.72 toks/s, output: 36.11 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 42.58it/s, est. speed input: 9540.38 toks/s, output: 37.27 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 43.24it/s, est. speed input: 9765.24 toks/s, output: 38.15 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:01, 43.71it/s, est. speed input: 9944.29 toks/s, output: 38.84 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:01, 44.01it/s, est. speed input: 10087.55 toks/s, output: 39.40 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 44.18it/s, est. speed input: 10203.51 toks/s, output: 39.86 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 44.29it/s, est. speed input: 10300.53 toks/s, output: 40.24 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 44.40it/s, est. speed input: 10384.53 toks/s, output: 40.56 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 44.45it/s, est. speed input: 10455.40 toks/s, output: 40.84 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 44.53it/s, est. speed input: 10519.69 toks/s, output: 41.09 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 44.20it/s, est. speed input: 10557.61 toks/s, output: 41.24 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 44.33it/s, est. speed input: 10607.37 toks/s, output: 41.43 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:00, 44.46it/s, est. speed input: 10653.46 toks/s, output: 41.61 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 44.20it/s, est. speed input: 10680.29 toks/s, output: 41.72 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 44.44it/s, est. speed input: 10720.90 toks/s, output: 41.88 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 44.42it/s, est. speed input: 10750.88 toks/s, output: 42.00 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 44.49it/s, est. speed input: 10781.25 toks/s, output: 42.11 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 44.46it/s, est. speed input: 10806.03 toks/s, output: 42.21 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 44.48it/s, est. speed input: 10830.38 toks/s, output: 42.31 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 43.90it/s, est. speed input: 10833.73 toks/s, output: 42.32 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 44.22it/s, est. speed input: 10859.24 toks/s, output: 42.42 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 44.22it/s, est. speed input: 10868.03 toks/s, output: 42.45 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 42.45it/s, est. speed input: 10868.03 toks/s, output: 42.45 toks/s]
[rank0]:[W126 13:24:28.175677489 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 14:21:01
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:21:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=578000) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=578000) WARNING 01-26 14:21:21 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=578000) WARNING 01-26 14:21:27 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 42.49 requests/s, 21796.23 total tokens/s, 42.49 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 14:21:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:21:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:21:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:21:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:21:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:21:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:21:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:21:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:21:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:21:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:21:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:21:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:21:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:21:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:16] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:16] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:16] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:16] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:16] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=578000) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=578000) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=578000) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=578000) 
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=578000) [2026-01-26 14:21:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=578000) 2026-01-26 14:21:27,899 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=578000) 2026-01-26 14:21:27,922 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=578000) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.41it/s]
(EngineCore_DP0 pid=578000) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.06it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  54%|█████▍    | 69/128 [00:00<00:00, 689.57it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 321.13it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:00, 182.58it/s, est. speed input: 93487.14 toks/s, output: 182.58 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 63.96it/s, est. speed input: 36387.31 toks/s, output: 71.07 toks/s]  
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 55.33it/s, est. speed input: 31842.49 toks/s, output: 62.19 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:00<00:01, 51.61it/s, est. speed input: 29996.47 toks/s, output: 58.59 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 49.34it/s, est. speed input: 28892.91 toks/s, output: 56.43 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 47.83it/s, est. speed input: 28171.44 toks/s, output: 55.02 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 46.62it/s, est. speed input: 27584.83 toks/s, output: 53.88 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 45.57it/s, est. speed input: 27132.31 toks/s, output: 52.99 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 45.04it/s, est. speed input: 26794.70 toks/s, output: 52.33 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 44.59it/s, est. speed input: 26495.61 toks/s, output: 51.75 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 43.87it/s, est. speed input: 26183.91 toks/s, output: 51.14 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 43.70it/s, est. speed input: 25952.28 toks/s, output: 50.69 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 43.68it/s, est. speed input: 25757.77 toks/s, output: 50.31 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 43.47it/s, est. speed input: 25562.66 toks/s, output: 49.93 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 43.39it/s, est. speed input: 25393.68 toks/s, output: 49.60 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 43.35it/s, est. speed input: 25241.56 toks/s, output: 49.30 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 43.36it/s, est. speed input: 25106.63 toks/s, output: 49.04 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.36it/s, est. speed input: 25081.65 toks/s, output: 48.99 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 48.99it/s, est. speed input: 25081.65 toks/s, output: 48.99 toks/s]
[rank0]:[W126 14:21:32.157709725 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 14:21:34
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:21:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=579010) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=579010) WARNING 01-26 14:21:54 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=579010) WARNING 01-26 14:22:00 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 41.57 requests/s, 42609.20 total tokens/s, 41.57 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 14:21:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:21:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:21:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:21:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:21:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:21:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:21:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:21:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:21:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:21:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:21:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:21:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:21:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:21:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:21:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:21:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:49] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:49] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:49] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:49] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:49] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=579010) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=579010) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=579010) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=579010) 
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=579010) [2026-01-26 14:21:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=579010) 2026-01-26 14:22:00,930 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=579010) 2026-01-26 14:22:00,950 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=579010) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 21.19it/s]
(EngineCore_DP0 pid=579010) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.96it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  29%|██▉       | 37/128 [00:00<00:00, 366.46it/s]
Adding requests:  63%|██████▎   | 81/128 [00:00<00:00, 405.46it/s]
Adding requests:  95%|█████████▌| 122/128 [00:00<00:00, 185.43it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 219.72it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:00, 192.73it/s, est. speed input: 197384.53 toks/s, output: 192.74 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 67.53it/s, est. speed input: 77204.21 toks/s, output: 75.39 toks/s]   
Processed prompts:  42%|████▏     | 54/128 [00:00<00:01, 58.25it/s, est. speed input: 67534.41 toks/s, output: 65.95 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:00<00:01, 54.20it/s, est. speed input: 63551.30 toks/s, output: 62.06 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 51.55it/s, est. speed input: 61070.48 toks/s, output: 59.64 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 49.48it/s, est. speed input: 59285.51 toks/s, output: 57.90 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 48.20it/s, est. speed input: 57993.68 toks/s, output: 56.63 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:00, 47.32it/s, est. speed input: 57086.13 toks/s, output: 55.75 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 46.33it/s, est. speed input: 56213.44 toks/s, output: 54.90 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 45.75it/s, est. speed input: 55510.61 toks/s, output: 54.21 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:01<00:00, 45.20it/s, est. speed input: 54860.11 toks/s, output: 53.57 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 44.93it/s, est. speed input: 54320.48 toks/s, output: 53.05 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 44.65it/s, est. speed input: 53819.12 toks/s, output: 52.56 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 44.53it/s, est. speed input: 53386.53 toks/s, output: 52.14 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 44.49it/s, est. speed input: 53003.18 toks/s, output: 51.76 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 44.41it/s, est. speed input: 52646.92 toks/s, output: 51.41 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.41it/s, est. speed input: 52524.28 toks/s, output: 51.29 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 51.29it/s, est. speed input: 52524.28 toks/s, output: 51.29 toks/s]
[rank0]:[W126 14:22:05.204275027 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 14:22:07
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:22:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=580056) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=580056) WARNING 01-26 14:22:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=580056) WARNING 01-26 14:22:35 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 82.77 requests/s, 84839.16 total tokens/s, 82.77 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 14:22:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:22:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:22:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:22:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:22:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:22:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:22:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:22:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:22:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:22:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:22:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:22:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:22:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:22:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:23] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=580056) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=580056) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=580056) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=580056) 
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=580056) [2026-01-26 14:22:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=580056) 2026-01-26 14:22:35,013 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=580056) 2026-01-26 14:22:35,034 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=580056) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.55it/s]
(EngineCore_DP0 pid=580056) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 22.73it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 37/256 [00:00<00:00, 362.67it/s]
Adding requests:  31%|███▏      | 80/256 [00:00<00:00, 399.61it/s]
Adding requests:  48%|████▊     | 124/256 [00:00<00:00, 416.52it/s]
Adding requests:  66%|██████▌   | 168/256 [00:00<00:00, 419.87it/s]
Adding requests:  82%|████████▏ | 211/256 [00:00<00:00, 423.19it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 423.17it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|██        | 52/256 [00:00<00:00, 423.15it/s, est. speed input: 433325.83 toks/s, output: 423.15 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:00<00:01, 139.18it/s, est. speed input: 160172.59 toks/s, output: 156.42 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:00<00:01, 116.20it/s, est. speed input: 136651.03 toks/s, output: 133.45 toks/s]
Processed prompts:  53%|█████▎    | 135/256 [00:01<00:01, 109.46it/s, est. speed input: 129351.25 toks/s, output: 126.32 toks/s]
Processed prompts:  58%|█████▊    | 149/256 [00:01<00:01, 103.33it/s, est. speed input: 123890.42 toks/s, output: 120.99 toks/s]
Processed prompts:  63%|██████▎   | 161/256 [00:01<00:00, 98.56it/s, est. speed input: 119956.54 toks/s, output: 117.14 toks/s] 
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:00, 93.64it/s, est. speed input: 116398.08 toks/s, output: 113.67 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:01<00:00, 91.90it/s, est. speed input: 114394.84 toks/s, output: 111.71 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:01<00:00, 90.16it/s, est. speed input: 112551.91 toks/s, output: 109.91 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:01<00:00, 89.25it/s, est. speed input: 111079.12 toks/s, output: 108.48 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:01<00:00, 88.65it/s, est. speed input: 109803.43 toks/s, output: 107.23 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:02<00:00, 87.73it/s, est. speed input: 108553.60 toks/s, output: 106.01 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:02<00:00, 87.43it/s, est. speed input: 107519.67 toks/s, output: 105.00 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:02<00:00, 87.05it/s, est. speed input: 106553.83 toks/s, output: 104.06 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:02<00:00, 86.84it/s, est. speed input: 105692.50 toks/s, output: 103.22 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 86.84it/s, est. speed input: 105411.71 toks/s, output: 102.94 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 102.94it/s, est. speed input: 105411.71 toks/s, output: 102.94 toks/s]
[rank0]:[W126 14:22:40.604037886 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 14:22:42
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:22:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=581109) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=581109) WARNING 01-26 14:23:04 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=581109) WARNING 01-26 14:23:10 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 164.64 requests/s, 168758.05 total tokens/s, 164.64 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 14:22:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:22:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:22:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:22:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:22:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:22:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:22:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:22:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:22:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:22:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:22:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:22:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:22:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:22:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:22:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:22:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=581109) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=581109) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=581109) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=581109) 
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=581109) [2026-01-26 14:22:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=581109) 2026-01-26 14:23:10,842 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=581109) 2026-01-26 14:23:10,864 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=581109) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 17.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 16.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 16.83it/s]
(EngineCore_DP0 pid=581109) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 22.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 22.93it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 34/512 [00:00<00:01, 335.57it/s]
Adding requests:  15%|█▌        | 78/512 [00:00<00:01, 395.11it/s]
Adding requests:  24%|██▍       | 123/512 [00:00<00:00, 417.18it/s]
Adding requests:  33%|███▎      | 168/512 [00:00<00:00, 424.69it/s]
Adding requests:  42%|████▏     | 214/512 [00:00<00:00, 435.92it/s]
Adding requests:  51%|█████     | 262/512 [00:00<00:00, 447.80it/s]
Adding requests:  60%|██████    | 308/512 [00:00<00:00, 449.23it/s]
Adding requests:  69%|██████▉   | 355/512 [00:00<00:00, 454.70it/s]
Adding requests:  79%|███████▊  | 402/512 [00:00<00:00, 457.01it/s]
Adding requests:  88%|████████▊ | 449/512 [00:01<00:00, 459.18it/s]
Adding requests:  97%|█████████▋| 496/512 [00:01<00:00, 460.86it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 444.62it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:00<00:00, 1733.03it/s, est. speed input: 1774741.28 toks/s, output: 1733.06 toks/s]
Processed prompts:  71%|███████   | 364/512 [00:01<00:00, 289.36it/s, est. speed input: 340761.42 toks/s, output: 332.77 toks/s]   
Processed prompts:  87%|████████▋ | 446/512 [00:01<00:00, 243.31it/s, est. speed input: 290626.61 toks/s, output: 283.81 toks/s]
Processed prompts:  97%|█████████▋| 499/512 [00:01<00:00, 224.12it/s, est. speed input: 271645.15 toks/s, output: 265.28 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 224.12it/s, est. speed input: 267900.34 toks/s, output: 261.62 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 261.61it/s, est. speed input: 267900.34 toks/s, output: 261.62 toks/s]
[rank0]:[W126 14:23:16.720984053 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 14:23:18
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:23:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=582204) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=582204) WARNING 01-26 14:23:42 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=582204) WARNING 01-26 14:23:48 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 290.04 requests/s, 297288.96 total tokens/s, 290.04 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 14:23:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:23:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:23:29] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:23:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:23:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:23:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:23:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:23:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:23:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:23:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:23:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:23:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:23:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:23:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:23:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:23:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:23:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:23:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:23:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=582204) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=582204) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=582204) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=582204) 
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=582204) [2026-01-26 14:23:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=582204) 2026-01-26 14:23:48,858 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=582204) 2026-01-26 14:23:48,923 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=582204) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 22.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 20.53it/s]
(EngineCore_DP0 pid=582204) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 22.99it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 38/1024 [00:00<00:02, 377.78it/s]
Adding requests:   8%|▊         | 80/1024 [00:00<00:02, 397.59it/s]
Adding requests:  12%|█▏        | 125/1024 [00:00<00:02, 419.55it/s]
Adding requests:  17%|█▋        | 169/1024 [00:00<00:02, 427.27it/s]
Adding requests:  21%|██        | 214/1024 [00:00<00:01, 434.14it/s]
Adding requests:  26%|██▌       | 262/1024 [00:00<00:01, 447.01it/s]
Adding requests:  30%|██▉       | 307/1024 [00:00<00:01, 432.96it/s]
Adding requests:  34%|███▍      | 352/1024 [00:00<00:01, 437.23it/s]
Adding requests:  39%|███▉      | 398/1024 [00:00<00:01, 442.45it/s]
Adding requests:  43%|████▎     | 444/1024 [00:01<00:01, 447.19it/s]
Adding requests:  48%|████▊     | 491/1024 [00:01<00:01, 452.37it/s]
Adding requests:  52%|█████▏    | 537/1024 [00:01<00:01, 445.86it/s]
Adding requests:  57%|█████▋    | 585/1024 [00:01<00:00, 453.73it/s]
Adding requests:  62%|██████▏   | 633/1024 [00:01<00:00, 458.70it/s]
Adding requests:  67%|██████▋   | 682/1024 [00:01<00:00, 465.90it/s]
Adding requests:  71%|███████▏  | 730/1024 [00:01<00:00, 469.47it/s]
Adding requests:  76%|███████▌  | 777/1024 [00:01<00:00, 462.33it/s]
Adding requests:  80%|████████  | 824/1024 [00:01<00:00, 455.91it/s]
Adding requests:  85%|████████▌ | 871/1024 [00:01<00:00, 459.28it/s]
Adding requests:  90%|████████▉ | 920/1024 [00:02<00:00, 466.08it/s]
Adding requests:  95%|█████████▍| 968/1024 [00:02<00:00, 468.58it/s]
Adding requests:  99%|█████████▉| 1015/1024 [00:02<00:00, 468.99it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 451.67it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:00<00:00, 5403.79it/s, est. speed input: 5533951.89 toks/s, output: 5404.01 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 5403.79it/s, est. speed input: 830847.37 toks/s, output: 811.37 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 811.29it/s, est. speed input: 830847.37 toks/s, output: 811.37 toks/s] 
[rank0]:[W126 14:23:54.211511718 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 14:23:56
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:24:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=583349) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=583349) WARNING 01-26 14:24:25 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=583349) WARNING 01-26 14:24:32 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 447.43 requests/s, 458616.64 total tokens/s, 447.43 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 14:24:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:24:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:24:12] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:24:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:24:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:24:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:24:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:24:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:24:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:24:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:24:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:24:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:24:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:24:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:24:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:24:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:24:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:24:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:24:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:20] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=583349) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=583349) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=583349) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=583349) 
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=583349) [2026-01-26 14:24:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=583349) 2026-01-26 14:24:32,365 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=583349) 2026-01-26 14:24:32,389 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=583349) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 21.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 22.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 21.02it/s]
(EngineCore_DP0 pid=583349) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 22.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 22.58it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 39/2048 [00:00<00:05, 381.35it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 419.92it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:04, 431.67it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:04, 435.91it/s]
Adding requests:  11%|█         | 220/2048 [00:00<00:04, 444.51it/s]
Adding requests:  13%|█▎        | 268/2048 [00:00<00:03, 454.43it/s]
Adding requests:  15%|█▌        | 314/2048 [00:00<00:03, 453.36it/s]
Adding requests:  18%|█▊        | 360/2048 [00:00<00:03, 454.58it/s]
Adding requests:  20%|█▉        | 407/2048 [00:00<00:03, 459.34it/s]
Adding requests:  22%|██▏       | 454/2048 [00:01<00:03, 461.27it/s]
Adding requests:  24%|██▍       | 501/2048 [00:01<00:03, 460.81it/s]
Adding requests:  27%|██▋       | 548/2048 [00:01<00:03, 456.18it/s]
Adding requests:  29%|██▉       | 595/2048 [00:01<00:03, 459.87it/s]
Adding requests:  31%|███▏      | 643/2048 [00:01<00:03, 462.93it/s]
Adding requests:  34%|███▍      | 692/2048 [00:01<00:02, 469.62it/s]
Adding requests:  36%|███▌      | 740/2048 [00:01<00:02, 470.47it/s]
Adding requests:  38%|███▊      | 788/2048 [00:01<00:02, 467.83it/s]
Adding requests:  41%|████      | 835/2048 [00:01<00:02, 456.81it/s]
Adding requests:  43%|████▎     | 883/2048 [00:01<00:02, 463.43it/s]
Adding requests:  45%|████▌     | 931/2048 [00:02<00:02, 467.95it/s]
Adding requests:  48%|████▊     | 979/2048 [00:02<00:02, 469.18it/s]
Adding requests:  50%|█████     | 1026/2048 [00:02<00:02, 466.57it/s]
Adding requests:  52%|█████▏    | 1073/2048 [00:02<00:02, 463.85it/s]
Adding requests:  55%|█████▍    | 1120/2048 [00:02<00:02, 462.47it/s]
Adding requests:  57%|█████▋    | 1170/2048 [00:02<00:01, 470.61it/s]
Adding requests:  60%|█████▉    | 1220/2048 [00:02<00:01, 478.79it/s]
Adding requests:  62%|██████▏   | 1268/2048 [00:02<00:01, 471.57it/s]
Adding requests:  64%|██████▍   | 1316/2048 [00:02<00:01, 472.89it/s]
Adding requests:  67%|██████▋   | 1364/2048 [00:02<00:01, 474.73it/s]
Adding requests:  69%|██████▉   | 1414/2048 [00:03<00:01, 479.43it/s]
Adding requests:  71%|███████▏  | 1462/2048 [00:03<00:01, 478.29it/s]
Adding requests:  74%|███████▎  | 1510/2048 [00:03<00:01, 472.78it/s]
Adding requests:  76%|███████▌  | 1558/2048 [00:03<00:01, 473.18it/s]
Adding requests:  78%|███████▊  | 1606/2048 [00:03<00:00, 470.53it/s]
Adding requests:  81%|████████  | 1654/2048 [00:03<00:00, 471.33it/s]
Adding requests:  83%|████████▎ | 1702/2048 [00:03<00:00, 469.21it/s]
Adding requests:  85%|████████▌ | 1750/2048 [00:03<00:00, 471.27it/s]
Adding requests:  88%|████████▊ | 1798/2048 [00:03<00:00, 449.93it/s]
Adding requests:  90%|█████████ | 1846/2048 [00:03<00:00, 456.60it/s]
Adding requests:  92%|█████████▏| 1893/2048 [00:04<00:00, 457.58it/s]
Adding requests:  95%|█████████▍| 1941/2048 [00:04<00:00, 461.85it/s]
Adding requests:  97%|█████████▋| 1989/2048 [00:04<00:00, 464.49it/s]
Adding requests: 100%|█████████▉| 2038/2048 [00:04<00:00, 469.70it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 463.40it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  98%|█████████▊| 2017/2048 [00:00<00:00, 16368.21it/s, est. speed input: 16761991.17 toks/s, output: 16368.44 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 16368.21it/s, est. speed input: 13429969.80 toks/s, output: 13114.78 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 13107.02it/s, est. speed input: 13429969.80 toks/s, output: 13114.78 toks/s]
[rank0]:[W126 14:24:39.727009117 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 14:24:41
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:25:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=584650) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=584650) WARNING 01-26 14:25:19 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=584650) WARNING 01-26 14:25:26 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 465.40 requests/s, 477038.71 total tokens/s, 465.40 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 14:25:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:25:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:25:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:25:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:25:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:25:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:25:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:25:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:25:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:25:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:25:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:25:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:25:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:25:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:25:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:25:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:25:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:25:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:25:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=584650) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=584650) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.03it/s]
(EngineCore_DP0 pid=584650) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.03it/s]
(EngineCore_DP0 pid=584650) 
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=584650) [2026-01-26 14:25:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=584650) [rank0]:W0126 14:25:22.835000 584650 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=584650) [rank0]:W0126 14:25:22.904000 584650 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=584650) [rank0]:W0126 14:25:23.707000 584650 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=584650) [rank0]:W0126 14:25:23.806000 584650 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=584650) 2026-01-26 14:25:26,201 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=584650) 2026-01-26 14:25:26,225 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=584650) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 19.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 20.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 22.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 21.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 21.51it/s]
(EngineCore_DP0 pid=584650) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 23.02it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 23.36it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 23.35it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 39/4096 [00:00<00:10, 383.34it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 420.09it/s]
Adding requests:   3%|▎         | 130/4096 [00:00<00:09, 436.43it/s]
Adding requests:   4%|▍         | 175/4096 [00:00<00:08, 439.25it/s]
Adding requests:   5%|▌         | 222/4096 [00:00<00:08, 448.24it/s]
Adding requests:   7%|▋         | 270/4096 [00:00<00:08, 458.43it/s]
Adding requests:   8%|▊         | 316/4096 [00:00<00:08, 433.70it/s]
Adding requests:   9%|▉         | 363/4096 [00:00<00:08, 443.07it/s]
Adding requests:  10%|█         | 410/4096 [00:00<00:08, 450.07it/s]
Adding requests:  11%|█         | 457/4096 [00:01<00:08, 454.86it/s]
Adding requests:  12%|█▏        | 504/4096 [00:01<00:07, 457.14it/s]
Adding requests:  13%|█▎        | 550/4096 [00:01<00:07, 452.27it/s]
Adding requests:  15%|█▍        | 599/4096 [00:01<00:07, 460.47it/s]
Adding requests:  16%|█▌        | 647/4096 [00:01<00:07, 464.72it/s]
Adding requests:  17%|█▋        | 696/4096 [00:01<00:07, 471.41it/s]
Adding requests:  18%|█▊        | 744/4096 [00:01<00:07, 471.05it/s]
Adding requests:  19%|█▉        | 792/4096 [00:01<00:07, 460.01it/s]
Adding requests:  20%|██        | 839/4096 [00:01<00:07, 454.09it/s]
Adding requests:  22%|██▏       | 885/4096 [00:01<00:07, 454.16it/s]
Adding requests:  23%|██▎       | 934/4096 [00:02<00:06, 463.35it/s]
Adding requests:  24%|██▍       | 981/4096 [00:02<00:06, 464.98it/s]
Adding requests:  25%|██▌       | 1030/4096 [00:02<00:06, 470.73it/s]
Adding requests:  26%|██▋       | 1078/4096 [00:02<00:06, 466.38it/s]
Adding requests:  27%|██▋       | 1125/4096 [00:02<00:06, 465.84it/s]
Adding requests:  29%|██▊       | 1174/4096 [00:02<00:06, 471.11it/s]
Adding requests:  30%|██▉       | 1224/4096 [00:02<00:06, 478.58it/s]
Adding requests:  31%|███       | 1272/4096 [00:02<00:05, 470.67it/s]
Adding requests:  32%|███▏      | 1320/4096 [00:02<00:05, 473.18it/s]
Adding requests:  33%|███▎      | 1369/4096 [00:02<00:05, 476.50it/s]
Adding requests:  35%|███▍      | 1417/4096 [00:03<00:05, 476.65it/s]
Adding requests:  36%|███▌      | 1466/4096 [00:03<00:05, 479.80it/s]
Adding requests:  37%|███▋      | 1515/4096 [00:03<00:05, 481.29it/s]
Adding requests:  38%|███▊      | 1564/4096 [00:03<00:05, 481.79it/s]
Adding requests:  39%|███▉      | 1613/4096 [00:03<00:05, 483.84it/s]
Adding requests:  41%|████      | 1662/4096 [00:03<00:05, 481.38it/s]
Adding requests:  42%|████▏     | 1711/4096 [00:03<00:04, 480.46it/s]
Adding requests:  43%|████▎     | 1760/4096 [00:03<00:04, 479.62it/s]
Adding requests:  44%|████▍     | 1809/4096 [00:03<00:04, 480.57it/s]
Adding requests:  45%|████▌     | 1858/4096 [00:03<00:04, 480.88it/s]
Adding requests:  47%|████▋     | 1907/4096 [00:04<00:04, 478.08it/s]
Adding requests:  48%|████▊     | 1955/4096 [00:04<00:04, 468.47it/s]
Adding requests:  49%|████▉     | 2004/4096 [00:04<00:04, 473.62it/s]
Adding requests:  50%|█████     | 2053/4096 [00:04<00:04, 476.21it/s]
Adding requests:  51%|█████▏    | 2102/4096 [00:04<00:04, 474.01it/s]
Adding requests:  52%|█████▏    | 2150/4096 [00:04<00:04, 469.62it/s]
Adding requests:  54%|█████▎    | 2197/4096 [00:04<00:04, 468.63it/s]
Adding requests:  55%|█████▍    | 2246/4096 [00:04<00:03, 473.80it/s]
Adding requests:  56%|█████▌    | 2294/4096 [00:04<00:03, 472.72it/s]
Adding requests:  57%|█████▋    | 2342/4096 [00:05<00:03, 472.76it/s]
Adding requests:  58%|█████▊    | 2391/4096 [00:05<00:03, 475.27it/s]
Adding requests:  60%|█████▉    | 2439/4096 [00:05<00:03, 475.68it/s]
Adding requests:  61%|██████    | 2488/4096 [00:05<00:03, 477.79it/s]
Adding requests:  62%|██████▏   | 2536/4096 [00:05<00:03, 476.69it/s]
Adding requests:  63%|██████▎   | 2585/4096 [00:05<00:03, 479.74it/s]
Adding requests:  64%|██████▍   | 2633/4096 [00:05<00:03, 478.80it/s]
Adding requests:  65%|██████▌   | 2682/4096 [00:05<00:02, 480.07it/s]
Adding requests:  67%|██████▋   | 2731/4096 [00:05<00:02, 460.60it/s]
Adding requests:  68%|██████▊   | 2778/4096 [00:05<00:02, 462.81it/s]
Adding requests:  69%|██████▉   | 2825/4096 [00:06<00:02, 460.81it/s]
Adding requests:  70%|███████   | 2874/4096 [00:06<00:02, 467.56it/s]
Adding requests:  71%|███████▏  | 2922/4096 [00:06<00:02, 471.06it/s]
Adding requests:  73%|███████▎  | 2970/4096 [00:06<00:02, 470.81it/s]
Adding requests:  74%|███████▎  | 3018/4096 [00:06<00:02, 472.94it/s]
Adding requests:  75%|███████▍  | 3066/4096 [00:06<00:02, 471.80it/s]
Adding requests:  76%|███████▌  | 3114/4096 [00:06<00:02, 459.97it/s]
Adding requests:  77%|███████▋  | 3161/4096 [00:06<00:02, 462.50it/s]
Adding requests:  78%|███████▊  | 3208/4096 [00:06<00:01, 464.07it/s]
Adding requests:  80%|███████▉  | 3257/4096 [00:06<00:01, 470.78it/s]
Adding requests:  81%|████████  | 3305/4096 [00:07<00:01, 472.02it/s]
Adding requests:  82%|████████▏ | 3354/4096 [00:07<00:01, 474.90it/s]
Adding requests:  83%|████████▎ | 3402/4096 [00:07<00:01, 472.72it/s]
Adding requests:  84%|████████▍ | 3450/4096 [00:07<00:01, 473.85it/s]
Adding requests:  85%|████████▌ | 3498/4096 [00:07<00:01, 469.31it/s]
Adding requests:  87%|████████▋ | 3547/4096 [00:07<00:01, 472.81it/s]
Adding requests:  88%|████████▊ | 3595/4096 [00:07<00:01, 472.48it/s]
Adding requests:  89%|████████▉ | 3643/4096 [00:07<00:00, 469.07it/s]
Adding requests:  90%|█████████ | 3692/4096 [00:07<00:00, 473.04it/s]
Adding requests:  91%|█████████▏| 3740/4096 [00:07<00:00, 470.06it/s]
Adding requests:  93%|█████████▎| 3790/4096 [00:08<00:00, 478.52it/s]
Adding requests:  94%|█████████▎| 3838/4096 [00:08<00:00, 475.52it/s]
Adding requests:  95%|█████████▍| 3887/4096 [00:08<00:00, 477.77it/s]
Adding requests:  96%|█████████▌| 3935/4096 [00:08<00:00, 476.84it/s]
Adding requests:  97%|█████████▋| 3983/4096 [00:08<00:00, 477.39it/s]
Adding requests:  98%|█████████▊| 4031/4096 [00:08<00:00, 475.56it/s]
Adding requests: 100%|█████████▉| 4079/4096 [00:08<00:00, 471.67it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 469.03it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 62240.72it/s, est. speed input: 63748119.48 toks/s, output: 62250.19 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 62142.78it/s, est. speed input: 63748119.48 toks/s, output: 62250.19 toks/s]
[rank0]:[W126 14:25:37.170256074 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 14:25:39
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:26:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=586259) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=586259) WARNING 01-26 14:26:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=586259) WARNING 01-26 14:26:43 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 460.64 requests/s, 472157.66 total tokens/s, 460.64 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 14:26:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:26:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:26:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:26:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:26:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:26:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:26:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:26:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:26:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:26:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:26:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 14:26:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 14:26:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 14:26:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 14:26:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:26:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:26:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:26:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:26:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:31] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=586259) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=586259) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.50it/s]
(EngineCore_DP0 pid=586259) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.50it/s]
(EngineCore_DP0 pid=586259) 
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=586259) [2026-01-26 14:26:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=586259) [rank0]:W0126 14:26:40.131000 586259 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=586259) [rank0]:W0126 14:26:40.200000 586259 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=586259) [rank0]:W0126 14:26:41.009000 586259 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=586259) [rank0]:W0126 14:26:41.107000 586259 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=586259) 2026-01-26 14:26:43,435 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=586259) 2026-01-26 14:26:43,461 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=586259) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:00, 18.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:00, 20.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 21.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 21.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 18.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:00<00:00, 20.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 19.98it/s]
(EngineCore_DP0 pid=586259) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00, 23.62it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 23.74it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 23.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 23.79it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 33/8192 [00:00<00:24, 327.33it/s]
Adding requests:   1%|          | 76/8192 [00:00<00:21, 383.04it/s]
Adding requests:   1%|▏         | 120/8192 [00:00<00:19, 408.67it/s]
Adding requests:   2%|▏         | 164/8192 [00:00<00:19, 418.79it/s]
Adding requests:   3%|▎         | 208/8192 [00:00<00:18, 425.22it/s]
Adding requests:   3%|▎         | 256/8192 [00:00<00:17, 440.99it/s]
Adding requests:   4%|▎         | 301/8192 [00:00<00:17, 439.27it/s]
Adding requests:   4%|▍         | 348/8192 [00:00<00:17, 446.16it/s]
Adding requests:   5%|▍         | 396/8192 [00:00<00:17, 453.47it/s]
Adding requests:   5%|▌         | 442/8192 [00:01<00:17, 454.22it/s]
Adding requests:   6%|▌         | 488/8192 [00:01<00:16, 455.77it/s]
Adding requests:   7%|▋         | 534/8192 [00:01<00:17, 446.57it/s]
Adding requests:   7%|▋         | 582/8192 [00:01<00:16, 454.58it/s]
Adding requests:   8%|▊         | 630/8192 [00:01<00:16, 458.50it/s]
Adding requests:   8%|▊         | 678/8192 [00:01<00:16, 463.22it/s]
Adding requests:   9%|▉         | 726/8192 [00:01<00:15, 467.18it/s]
Adding requests:   9%|▉         | 773/8192 [00:01<00:16, 463.00it/s]
Adding requests:  10%|█         | 820/8192 [00:01<00:16, 456.45it/s]
Adding requests:  11%|█         | 867/8192 [00:01<00:16, 457.78it/s]
Adding requests:  11%|█         | 915/8192 [00:02<00:15, 462.51it/s]
Adding requests:  12%|█▏        | 962/8192 [00:02<00:15, 464.51it/s]
Adding requests:  12%|█▏        | 1009/8192 [00:02<00:15, 465.70it/s]
Adding requests:  13%|█▎        | 1056/8192 [00:02<00:15, 465.80it/s]
Adding requests:  13%|█▎        | 1103/8192 [00:02<00:15, 464.62it/s]
Adding requests:  14%|█▍        | 1150/8192 [00:02<00:15, 460.90it/s]
Adding requests:  15%|█▍        | 1200/8192 [00:02<00:14, 471.48it/s]
Adding requests:  15%|█▌        | 1248/8192 [00:02<00:14, 470.04it/s]
Adding requests:  16%|█▌        | 1296/8192 [00:02<00:15, 455.89it/s]
Adding requests:  16%|█▋        | 1343/8192 [00:02<00:14, 457.60it/s]
Adding requests:  17%|█▋        | 1391/8192 [00:03<00:14, 463.99it/s]
Adding requests:  18%|█▊        | 1438/8192 [00:03<00:14, 464.22it/s]
Adding requests:  18%|█▊        | 1487/8192 [00:03<00:14, 469.12it/s]
Adding requests:  19%|█▊        | 1534/8192 [00:03<00:14, 460.81it/s]
Adding requests:  19%|█▉        | 1582/8192 [00:03<00:14, 463.47it/s]
Adding requests:  20%|█▉        | 1632/8192 [00:03<00:13, 471.45it/s]
Adding requests:  21%|██        | 1680/8192 [00:03<00:13, 466.72it/s]
Adding requests:  21%|██        | 1729/8192 [00:03<00:13, 470.75it/s]
Adding requests:  22%|██▏       | 1777/8192 [00:03<00:13, 466.43it/s]
Adding requests:  22%|██▏       | 1825/8192 [00:03<00:13, 469.15it/s]
Adding requests:  23%|██▎       | 1872/8192 [00:04<00:13, 467.98it/s]
Adding requests:  23%|██▎       | 1919/8192 [00:04<00:13, 468.48it/s]
Adding requests:  24%|██▍       | 1966/8192 [00:04<00:13, 467.59it/s]
Adding requests:  25%|██▍       | 2014/8192 [00:04<00:13, 469.74it/s]
Adding requests:  25%|██▌       | 2063/8192 [00:04<00:12, 473.16it/s]
Adding requests:  26%|██▌       | 2111/8192 [00:04<00:12, 472.53it/s]
Adding requests:  26%|██▋       | 2159/8192 [00:04<00:13, 463.80it/s]
Adding requests:  27%|██▋       | 2206/8192 [00:04<00:13, 460.31it/s]
Adding requests:  28%|██▊       | 2255/8192 [00:04<00:12, 466.72it/s]
Adding requests:  28%|██▊       | 2302/8192 [00:05<00:12, 466.40it/s]
Adding requests:  29%|██▊       | 2349/8192 [00:05<00:12, 465.84it/s]
Adding requests:  29%|██▉       | 2396/8192 [00:05<00:12, 465.20it/s]
Adding requests:  30%|██▉       | 2443/8192 [00:05<00:12, 466.16it/s]
Adding requests:  30%|███       | 2491/8192 [00:05<00:12, 468.56it/s]
Adding requests:  31%|███       | 2538/8192 [00:05<00:12, 467.17it/s]
Adding requests:  32%|███▏      | 2586/8192 [00:05<00:11, 470.11it/s]
Adding requests:  32%|███▏      | 2634/8192 [00:05<00:12, 455.33it/s]
Adding requests:  33%|███▎      | 2681/8192 [00:05<00:11, 459.44it/s]
Adding requests:  33%|███▎      | 2728/8192 [00:05<00:11, 459.23it/s]
Adding requests:  34%|███▍      | 2775/8192 [00:06<00:11, 461.28it/s]
Adding requests:  34%|███▍      | 2822/8192 [00:06<00:11, 455.84it/s]
Adding requests:  35%|███▌      | 2870/8192 [00:06<00:11, 460.57it/s]
Adding requests:  36%|███▌      | 2917/8192 [00:06<00:11, 461.24it/s]
Adding requests:  36%|███▌      | 2964/8192 [00:06<00:11, 459.61it/s]
Adding requests:  37%|███▋      | 3012/8192 [00:06<00:11, 463.30it/s]
Adding requests:  37%|███▋      | 3059/8192 [00:06<00:11, 463.41it/s]
Adding requests:  38%|███▊      | 3106/8192 [00:06<00:11, 461.29it/s]
Adding requests:  38%|███▊      | 3153/8192 [00:06<00:10, 462.39it/s]
Adding requests:  39%|███▉      | 3200/8192 [00:06<00:10, 463.13it/s]
Adding requests:  40%|███▉      | 3248/8192 [00:07<00:10, 467.76it/s]
Adding requests:  40%|████      | 3295/8192 [00:07<00:10, 466.97it/s]
Adding requests:  41%|████      | 3343/8192 [00:07<00:10, 468.68it/s]
Adding requests:  41%|████▏     | 3390/8192 [00:07<00:10, 467.26it/s]
Adding requests:  42%|████▏     | 3439/8192 [00:07<00:10, 471.55it/s]
Adding requests:  43%|████▎     | 3487/8192 [00:07<00:10, 461.80it/s]
Adding requests:  43%|████▎     | 3534/8192 [00:07<00:10, 463.97it/s]
Adding requests:  44%|████▎     | 3581/8192 [00:07<00:09, 463.69it/s]
Adding requests:  44%|████▍     | 3628/8192 [00:07<00:09, 463.40it/s]
Adding requests:  45%|████▍     | 3676/8192 [00:07<00:09, 466.09it/s]
Adding requests:  45%|████▌     | 3723/8192 [00:08<00:09, 463.16it/s]
Adding requests:  46%|████▌     | 3772/8192 [00:08<00:09, 469.05it/s]
Adding requests:  47%|████▋     | 3819/8192 [00:08<00:09, 460.78it/s]
Adding requests:  47%|████▋     | 3867/8192 [00:08<00:09, 466.04it/s]
Adding requests:  48%|████▊     | 3914/8192 [00:08<00:09, 464.44it/s]
Adding requests:  48%|████▊     | 3962/8192 [00:08<00:09, 466.29it/s]
Adding requests:  49%|████▉     | 4009/8192 [00:08<00:09, 464.77it/s]
Adding requests:  50%|████▉     | 4056/8192 [00:08<00:08, 461.02it/s]
Adding requests:  50%|█████     | 4103/8192 [00:08<00:08, 463.10it/s]
Adding requests:  51%|█████     | 4150/8192 [00:09<00:08, 461.51it/s]
Adding requests:  51%|█████     | 4198/8192 [00:09<00:08, 466.62it/s]
Adding requests:  52%|█████▏    | 4245/8192 [00:09<00:08, 467.47it/s]
Adding requests:  52%|█████▏    | 4292/8192 [00:09<00:08, 464.90it/s]
Adding requests:  53%|█████▎    | 4341/8192 [00:09<00:08, 471.39it/s]
Adding requests:  54%|█████▎    | 4390/8192 [00:09<00:08, 474.85it/s]
Adding requests:  54%|█████▍    | 4438/8192 [00:09<00:07, 474.08it/s]
Adding requests:  55%|█████▍    | 4486/8192 [00:09<00:07, 469.04it/s]
Adding requests:  55%|█████▌    | 4533/8192 [00:09<00:07, 467.01it/s]
Adding requests:  56%|█████▌    | 4581/8192 [00:09<00:07, 468.43it/s]
Adding requests:  57%|█████▋    | 4629/8192 [00:10<00:07, 470.82it/s]
Adding requests:  57%|█████▋    | 4677/8192 [00:10<00:07, 468.61it/s]
Adding requests:  58%|█████▊    | 4725/8192 [00:10<00:07, 470.61it/s]
Adding requests:  58%|█████▊    | 4773/8192 [00:10<00:07, 469.61it/s]
Adding requests:  59%|█████▉    | 4820/8192 [00:10<00:07, 468.85it/s]
Adding requests:  59%|█████▉    | 4867/8192 [00:10<00:07, 467.89it/s]
Adding requests:  60%|█████▉    | 4914/8192 [00:10<00:07, 465.21it/s]
Adding requests:  61%|██████    | 4961/8192 [00:10<00:07, 455.32it/s]
Adding requests:  61%|██████    | 5009/8192 [00:10<00:06, 459.64it/s]
Adding requests:  62%|██████▏   | 5057/8192 [00:10<00:06, 464.58it/s]
Adding requests:  62%|██████▏   | 5106/8192 [00:11<00:06, 470.97it/s]
Adding requests:  63%|██████▎   | 5154/8192 [00:11<00:06, 465.90it/s]
Adding requests:  63%|██████▎   | 5201/8192 [00:11<00:06, 464.18it/s]
Adding requests:  64%|██████▍   | 5248/8192 [00:11<00:06, 461.34it/s]
Adding requests:  65%|██████▍   | 5295/8192 [00:11<00:06, 462.08it/s]
Adding requests:  65%|██████▌   | 5343/8192 [00:11<00:06, 465.20it/s]
Adding requests:  66%|██████▌   | 5390/8192 [00:11<00:06, 465.02it/s]
Adding requests:  66%|██████▋   | 5437/8192 [00:11<00:05, 465.55it/s]
Adding requests:  67%|██████▋   | 5484/8192 [00:11<00:05, 460.85it/s]
Adding requests:  68%|██████▊   | 5531/8192 [00:11<00:05, 460.70it/s]
Adding requests:  68%|██████▊   | 5578/8192 [00:12<00:05, 459.70it/s]
Adding requests:  69%|██████▊   | 5624/8192 [00:12<00:05, 457.67it/s]
Adding requests:  69%|██████▉   | 5670/8192 [00:12<00:05, 453.51it/s]
Adding requests:  70%|██████▉   | 5718/8192 [00:12<00:05, 459.60it/s]
Adding requests:  70%|███████   | 5765/8192 [00:12<00:05, 461.08it/s]
Adding requests:  71%|███████   | 5812/8192 [00:12<00:05, 456.79it/s]
Adding requests:  72%|███████▏  | 5859/8192 [00:12<00:05, 457.15it/s]
Adding requests:  72%|███████▏  | 5907/8192 [00:12<00:04, 462.87it/s]
Adding requests:  73%|███████▎  | 5954/8192 [00:12<00:04, 460.97it/s]
Adding requests:  73%|███████▎  | 6002/8192 [00:12<00:04, 465.35it/s]
Adding requests:  74%|███████▍  | 6051/8192 [00:13<00:04, 469.89it/s]
Adding requests:  74%|███████▍  | 6098/8192 [00:13<00:04, 466.37it/s]
Adding requests:  75%|███████▌  | 6145/8192 [00:13<00:04, 467.22it/s]
Adding requests:  76%|███████▌  | 6192/8192 [00:13<00:04, 455.91it/s]
Adding requests:  76%|███████▌  | 6241/8192 [00:13<00:04, 463.19it/s]
Adding requests:  77%|███████▋  | 6290/8192 [00:13<00:04, 468.72it/s]
Adding requests:  77%|███████▋  | 6338/8192 [00:13<00:03, 471.29it/s]
Adding requests:  78%|███████▊  | 6386/8192 [00:13<00:03, 471.42it/s]
Adding requests:  79%|███████▊  | 6436/8192 [00:13<00:03, 477.22it/s]
Adding requests:  79%|███████▉  | 6484/8192 [00:14<00:03, 477.24it/s]
Adding requests:  80%|███████▉  | 6533/8192 [00:14<00:03, 480.81it/s]
Adding requests:  80%|████████  | 6582/8192 [00:14<00:03, 474.81it/s]
Adding requests:  81%|████████  | 6630/8192 [00:14<00:03, 474.16it/s]
Adding requests:  82%|████████▏ | 6678/8192 [00:14<00:03, 471.61it/s]
Adding requests:  82%|████████▏ | 6726/8192 [00:14<00:03, 462.77it/s]
Adding requests:  83%|████████▎ | 6773/8192 [00:14<00:03, 462.73it/s]
Adding requests:  83%|████████▎ | 6822/8192 [00:14<00:02, 467.95it/s]
Adding requests:  84%|████████▍ | 6871/8192 [00:14<00:02, 473.39it/s]
Adding requests:  84%|████████▍ | 6919/8192 [00:14<00:02, 474.17it/s]
Adding requests:  85%|████████▌ | 6968/8192 [00:15<00:02, 477.52it/s]
Adding requests:  86%|████████▌ | 7016/8192 [00:15<00:02, 470.97it/s]
Adding requests:  86%|████████▌ | 7064/8192 [00:15<00:02, 471.56it/s]
Adding requests:  87%|████████▋ | 7112/8192 [00:15<00:02, 473.32it/s]
Adding requests:  87%|████████▋ | 7160/8192 [00:15<00:02, 470.17it/s]
Adding requests:  88%|████████▊ | 7208/8192 [00:15<00:02, 470.08it/s]
Adding requests:  89%|████████▊ | 7256/8192 [00:15<00:01, 472.69it/s]
Adding requests:  89%|████████▉ | 7304/8192 [00:15<00:01, 474.05it/s]
Adding requests:  90%|████████▉ | 7352/8192 [00:15<00:01, 457.79it/s]
Adding requests:  90%|█████████ | 7401/8192 [00:15<00:01, 464.78it/s]
Adding requests:  91%|█████████ | 7451/8192 [00:16<00:01, 473.85it/s]
Adding requests:  92%|█████████▏| 7499/8192 [00:16<00:01, 471.82it/s]
Adding requests:  92%|█████████▏| 7547/8192 [00:16<00:01, 472.92it/s]
Adding requests:  93%|█████████▎| 7595/8192 [00:16<00:01, 471.42it/s]
Adding requests:  93%|█████████▎| 7643/8192 [00:16<00:01, 470.22it/s]
Adding requests:  94%|█████████▍| 7692/8192 [00:16<00:01, 475.20it/s]
Adding requests:  94%|█████████▍| 7740/8192 [00:16<00:00, 472.19it/s]
Adding requests:  95%|█████████▌| 7788/8192 [00:16<00:00, 466.88it/s]
Adding requests:  96%|█████████▌| 7836/8192 [00:16<00:00, 469.96it/s]
Adding requests:  96%|█████████▌| 7884/8192 [00:16<00:00, 469.06it/s]
Adding requests:  97%|█████████▋| 7931/8192 [00:17<00:00, 466.02it/s]
Adding requests:  97%|█████████▋| 7978/8192 [00:17<00:00, 465.23it/s]
Adding requests:  98%|█████████▊| 8025/8192 [00:17<00:00, 461.64it/s]
Adding requests:  99%|█████████▊| 8074/8192 [00:17<00:00, 468.04it/s]
Adding requests:  99%|█████████▉| 8122/8192 [00:17<00:00, 471.40it/s]
Adding requests: 100%|█████████▉| 8170/8192 [00:17<00:00, 470.36it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 464.34it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  73%|███████▎  | 5976/8192 [00:00<00:00, 59585.32it/s, est. speed input: 61020154.58 toks/s, output: 59586.59 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 59585.32it/s, est. speed input: 60588333.10 toks/s, output: 59166.46 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 59119.22it/s, est. speed input: 60588333.10 toks/s, output: 59166.46 toks/s]
[rank0]:[W126 14:27:04.075224663 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 15:33:21
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:33:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=683826) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=683826) WARNING 01-26 15:33:44 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=683826) WARNING 01-26 15:33:54 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 33.63 requests/s, 17250.67 total tokens/s, 33.63 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 15:33:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:33:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:33:28] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:33:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:33:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:33:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:33:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:33:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:33:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:33:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:33:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:33:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:33:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:33:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:33:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:33:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:33:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:33:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:33:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:33:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:33:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:33:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:33:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:33:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:33:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:33:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:33:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:33:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=683826) [2026-01-26 15:33:36] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=683826) [2026-01-26 15:33:36] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=683826) [2026-01-26 15:33:36] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=683826) [2026-01-26 15:33:36] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=683826) [2026-01-26 15:33:36] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=683826) [2026-01-26 15:33:36] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=683826) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=683826) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.08it/s]
(EngineCore_DP0 pid=683826) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.08it/s]
(EngineCore_DP0 pid=683826) 
(EngineCore_DP0 pid=683826) [2026-01-26 15:33:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=683826) [2026-01-26 15:33:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=683826) [2026-01-26 15:33:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=683826) [2026-01-26 15:33:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=683826) [2026-01-26 15:33:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=683826) [2026-01-26 15:33:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=683826) [2026-01-26 15:33:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=683826) [2026-01-26 15:33:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=683826) 2026-01-26 15:33:54,107 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=683826) 2026-01-26 15:33:54,131 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=683826) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.39it/s]
(EngineCore_DP0 pid=683826) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.02it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  53%|█████▎    | 68/128 [00:00<00:00, 671.62it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 317.39it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:01, 83.64it/s, est. speed input: 42824.63 toks/s, output: 83.64 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 47.06it/s, est. speed input: 25785.27 toks/s, output: 50.36 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 42.45it/s, est. speed input: 23474.46 toks/s, output: 45.85 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 40.35it/s, est. speed input: 22436.96 toks/s, output: 43.82 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 39.03it/s, est. speed input: 21769.36 toks/s, output: 42.52 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:02, 38.10it/s, est. speed input: 21284.00 toks/s, output: 41.57 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 37.52it/s, est. speed input: 20977.87 toks/s, output: 40.97 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 37.12it/s, est. speed input: 20737.23 toks/s, output: 40.50 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 36.77it/s, est. speed input: 20529.62 toks/s, output: 40.10 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 36.52it/s, est. speed input: 20356.54 toks/s, output: 39.76 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 36.43it/s, est. speed input: 20222.50 toks/s, output: 39.50 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 36.31it/s, est. speed input: 20098.93 toks/s, output: 39.26 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 36.25it/s, est. speed input: 19994.63 toks/s, output: 39.05 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 36.28it/s, est. speed input: 19911.06 toks/s, output: 38.89 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 36.25it/s, est. speed input: 19832.37 toks/s, output: 38.73 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 36.20it/s, est. speed input: 19758.92 toks/s, output: 38.59 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 36.16it/s, est. speed input: 19691.97 toks/s, output: 38.46 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 36.17it/s, est. speed input: 19635.42 toks/s, output: 38.35 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:01, 36.19it/s, est. speed input: 19585.72 toks/s, output: 38.25 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 36.26it/s, est. speed input: 19544.05 toks/s, output: 38.17 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 36.23it/s, est. speed input: 19500.48 toks/s, output: 38.09 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 36.23it/s, est. speed input: 19461.96 toks/s, output: 38.01 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 36.20it/s, est. speed input: 19424.08 toks/s, output: 37.94 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 36.13it/s, est. speed input: 19385.79 toks/s, output: 37.86 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:03<00:00, 36.13it/s, est. speed input: 19353.50 toks/s, output: 37.80 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:03<00:00, 36.10it/s, est. speed input: 19321.72 toks/s, output: 37.74 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:03<00:00, 36.09it/s, est. speed input: 19292.53 toks/s, output: 37.68 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 36.13it/s, est. speed input: 19268.06 toks/s, output: 37.63 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.13it/s, est. speed input: 19263.86 toks/s, output: 37.62 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.62it/s, est. speed input: 19263.86 toks/s, output: 37.62 toks/s]
[rank0]:[W126 15:34:00.711160562 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 15:34:02
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:34:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=685024) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=685024) WARNING 01-26 15:34:25 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=685024) WARNING 01-26 15:34:34 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 36.01 requests/s, 36911.11 total tokens/s, 36.01 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 15:34:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:34:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:34:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:34:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:34:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:34:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:34:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:34:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:34:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:34:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:34:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:34:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:34:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:34:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:34:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:34:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:34:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:34:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=685024) [2026-01-26 15:34:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=685024) [2026-01-26 15:34:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=685024) [2026-01-26 15:34:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=685024) [2026-01-26 15:34:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=685024) [2026-01-26 15:34:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=685024) [2026-01-26 15:34:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=685024) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=685024) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.13it/s]
(EngineCore_DP0 pid=685024) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.13it/s]
(EngineCore_DP0 pid=685024) 
(EngineCore_DP0 pid=685024) [2026-01-26 15:34:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=685024) [2026-01-26 15:34:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=685024) [2026-01-26 15:34:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=685024) [2026-01-26 15:34:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=685024) [2026-01-26 15:34:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=685024) [2026-01-26 15:34:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=685024) [2026-01-26 15:34:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=685024) [2026-01-26 15:34:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=685024) 2026-01-26 15:34:34,560 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=685024) 2026-01-26 15:34:34,583 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=685024) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.34it/s]
(EngineCore_DP0 pid=685024) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.42it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|███       | 39/128 [00:00<00:00, 380.93it/s]
Adding requests:  66%|██████▋   | 85/128 [00:00<00:00, 421.49it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 424.99it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:01, 86.17it/s, est. speed input: 88247.92 toks/s, output: 86.18 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 49.11it/s, est. speed input: 53761.72 toks/s, output: 52.50 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 44.26it/s, est. speed input: 48921.38 toks/s, output: 47.77 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 42.08it/s, est. speed input: 46779.53 toks/s, output: 45.68 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 40.68it/s, est. speed input: 45377.26 toks/s, output: 44.31 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:02, 39.77it/s, est. speed input: 44392.82 toks/s, output: 43.35 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 39.15it/s, est. speed input: 43662.67 toks/s, output: 42.64 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 38.77it/s, est. speed input: 43190.12 toks/s, output: 42.18 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 38.51it/s, est. speed input: 42810.75 toks/s, output: 41.81 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 38.27it/s, est. speed input: 42475.14 toks/s, output: 41.48 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 38.14it/s, est. speed input: 42201.57 toks/s, output: 41.21 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 38.00it/s, est. speed input: 41955.63 toks/s, output: 40.97 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 37.96it/s, est. speed input: 41753.45 toks/s, output: 40.77 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 37.87it/s, est. speed input: 41562.49 toks/s, output: 40.59 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 37.83it/s, est. speed input: 41397.91 toks/s, output: 40.43 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 37.81it/s, est. speed input: 41251.72 toks/s, output: 40.28 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 37.83it/s, est. speed input: 41126.55 toks/s, output: 40.16 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 37.80it/s, est. speed input: 41006.65 toks/s, output: 40.05 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 37.81it/s, est. speed input: 40902.75 toks/s, output: 39.94 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 37.82it/s, est. speed input: 40808.09 toks/s, output: 39.85 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 37.81it/s, est. speed input: 40719.46 toks/s, output: 39.77 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 37.83it/s, est. speed input: 40640.99 toks/s, output: 39.69 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 37.85it/s, est. speed input: 40570.61 toks/s, output: 39.62 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 37.90it/s, est. speed input: 40509.29 toks/s, output: 39.56 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 37.88it/s, est. speed input: 40446.07 toks/s, output: 39.50 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 37.91it/s, est. speed input: 40391.57 toks/s, output: 39.44 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 37.96it/s, est. speed input: 40344.75 toks/s, output: 39.40 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.02it/s, est. speed input: 40304.49 toks/s, output: 39.36 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.02it/s, est. speed input: 40304.49 toks/s, output: 39.36 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.36it/s, est. speed input: 40304.49 toks/s, output: 39.36 toks/s]
[rank0]:[W126 15:34:39.085567482 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 15:34:41
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:34:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=686143) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=686143) WARNING 01-26 15:35:05 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=686143) WARNING 01-26 15:35:14 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 67.90 requests/s, 69600.49 total tokens/s, 67.90 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 15:34:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:34:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:34:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:34:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:34:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:34:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:34:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:34:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:34:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:34:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:34:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:34:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:34:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:34:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:34:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:34:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:34:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:34:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:34:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=686143) [2026-01-26 15:34:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=686143) [2026-01-26 15:34:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=686143) [2026-01-26 15:34:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=686143) [2026-01-26 15:34:57] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=686143) [2026-01-26 15:34:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=686143) [2026-01-26 15:34:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=686143) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=686143) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.01it/s]
(EngineCore_DP0 pid=686143) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.01it/s]
(EngineCore_DP0 pid=686143) 
(EngineCore_DP0 pid=686143) [2026-01-26 15:34:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=686143) [2026-01-26 15:34:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=686143) [2026-01-26 15:34:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=686143) [2026-01-26 15:34:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=686143) [2026-01-26 15:34:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=686143) [2026-01-26 15:34:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=686143) [2026-01-26 15:34:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=686143) [2026-01-26 15:34:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=686143) 2026-01-26 15:35:14,785 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=686143) 2026-01-26 15:35:14,808 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=686143) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.53it/s]
(EngineCore_DP0 pid=686143) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.74it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.72it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:49,  5.18it/s]
Adding requests:  15%|█▌        | 39/256 [00:00<00:01, 164.08it/s]
Adding requests:  32%|███▏      | 83/256 [00:00<00:00, 267.44it/s]
Adding requests:  50%|█████     | 128/256 [00:00<00:00, 328.57it/s]
Adding requests:  67%|██████▋   | 171/256 [00:00<00:00, 360.24it/s]
Adding requests:  84%|████████▍ | 215/256 [00:00<00:00, 385.43it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 325.51it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:00<00:00, 358.76it/s, est. speed input: 367393.10 toks/s, output: 358.77 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:00<00:01, 117.01it/s, est. speed input: 134809.26 toks/s, output: 131.65 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:00<00:01, 99.70it/s, est. speed input: 116677.36 toks/s, output: 113.94 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:01<00:01, 92.40it/s, est. speed input: 109489.61 toks/s, output: 106.92 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:01<00:01, 87.68it/s, est. speed input: 105083.55 toks/s, output: 102.62 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:01<00:01, 84.44it/s, est. speed input: 102179.46 toks/s, output: 99.78 toks/s] 
Processed prompts:  57%|█████▋    | 146/256 [00:01<00:01, 81.78it/s, est. speed input: 99788.77 toks/s, output: 97.45 toks/s] 
Processed prompts:  61%|██████    | 155/256 [00:01<00:01, 82.17it/s, est. speed input: 98830.76 toks/s, output: 96.51 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:01<00:01, 77.75it/s, est. speed input: 96465.81 toks/s, output: 94.20 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:01, 76.87it/s, est. speed input: 95268.93 toks/s, output: 93.04 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:01<00:00, 76.18it/s, est. speed input: 94207.47 toks/s, output: 92.00 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:02<00:00, 75.58it/s, est. speed input: 93239.68 toks/s, output: 91.05 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:02<00:00, 75.14it/s, est. speed input: 92370.46 toks/s, output: 90.21 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:02<00:00, 74.86it/s, est. speed input: 91593.65 toks/s, output: 89.45 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:02<00:00, 74.56it/s, est. speed input: 90869.31 toks/s, output: 88.74 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:02<00:00, 74.47it/s, est. speed input: 90228.35 toks/s, output: 88.11 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 74.29it/s, est. speed input: 89621.50 toks/s, output: 87.52 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 74.23it/s, est. speed input: 89074.50 toks/s, output: 86.99 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:02<00:00, 74.30it/s, est. speed input: 88584.99 toks/s, output: 86.51 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:02<00:00, 74.25it/s, est. speed input: 88118.24 toks/s, output: 86.05 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 74.25it/s, est. speed input: 87893.57 toks/s, output: 85.83 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 85.83it/s, est. speed input: 87893.57 toks/s, output: 85.83 toks/s]
[rank0]:[W126 15:35:20.690405405 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 15:35:22
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:35:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=687281) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=687281) WARNING 01-26 15:35:47 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=687281) WARNING 01-26 15:35:56 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 138.56 requests/s, 142020.27 total tokens/s, 138.56 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 15:35:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:35:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:35:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:35:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:35:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:35:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:35:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:35:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:35:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:35:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:35:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:35:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:35:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:35:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:35:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:35:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:35:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:35:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:35:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:35:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:35:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:35:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:35:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:35:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:35:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:35:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:35:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:35:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=687281) [2026-01-26 15:35:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=687281) [2026-01-26 15:35:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=687281) [2026-01-26 15:35:39] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=687281) [2026-01-26 15:35:39] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=687281) [2026-01-26 15:35:39] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=687281) [2026-01-26 15:35:39] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=687281) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=687281) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.11it/s]
(EngineCore_DP0 pid=687281) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.11it/s]
(EngineCore_DP0 pid=687281) 
(EngineCore_DP0 pid=687281) [2026-01-26 15:35:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=687281) [2026-01-26 15:35:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=687281) [2026-01-26 15:35:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=687281) [2026-01-26 15:35:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=687281) [2026-01-26 15:35:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=687281) [2026-01-26 15:35:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=687281) [2026-01-26 15:35:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=687281) [2026-01-26 15:35:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=687281) 2026-01-26 15:35:56,501 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=687281) 2026-01-26 15:35:56,524 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=687281) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.68it/s]
(EngineCore_DP0 pid=687281) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 18.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 18.22it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 39/512 [00:00<00:01, 381.53it/s]
Adding requests:  16%|█▋        | 84/512 [00:00<00:01, 419.53it/s]
Adding requests:  25%|██▌       | 130/512 [00:00<00:00, 434.45it/s]
Adding requests:  34%|███▍      | 174/512 [00:00<00:00, 427.07it/s]
Adding requests:  43%|████▎     | 220/512 [00:00<00:00, 437.57it/s]
Adding requests:  52%|█████▏    | 268/512 [00:00<00:00, 450.46it/s]
Adding requests:  61%|██████▏   | 314/512 [00:00<00:00, 451.54it/s]
Adding requests:  70%|███████   | 360/512 [00:00<00:00, 453.74it/s]
Adding requests:  80%|███████▉  | 408/512 [00:00<00:00, 459.92it/s]
Adding requests:  89%|████████▉ | 455/512 [00:01<00:00, 461.29it/s]
Adding requests:  98%|█████████▊| 502/512 [00:01<00:00, 460.93it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 449.15it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  30%|███       | 154/512 [00:00<00:00, 1324.58it/s, est. speed input: 1356463.08 toks/s, output: 1324.61 toks/s]
Processed prompts:  56%|█████▌    | 287/512 [00:01<00:00, 247.24it/s, est. speed input: 291316.49 toks/s, output: 284.49 toks/s]   
Processed prompts:  69%|██████▊   | 351/512 [00:01<00:00, 206.51it/s, est. speed input: 247332.72 toks/s, output: 241.54 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:01<00:00, 192.49it/s, est. speed input: 232888.86 toks/s, output: 227.43 toks/s]
Processed prompts:  83%|████████▎ | 423/512 [00:01<00:00, 181.43it/s, est. speed input: 223311.30 toks/s, output: 218.08 toks/s]
Processed prompts:  88%|████████▊ | 448/512 [00:02<00:00, 175.96it/s, est. speed input: 218184.92 toks/s, output: 213.07 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:02<00:00, 165.16it/s, est. speed input: 211653.65 toks/s, output: 206.69 toks/s]
Processed prompts:  96%|█████████▌| 489/512 [00:02<00:00, 166.81it/s, est. speed input: 210165.15 toks/s, output: 205.24 toks/s]
Processed prompts:  99%|█████████▉| 508/512 [00:02<00:00, 160.51it/s, est. speed input: 206552.21 toks/s, output: 201.71 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 160.51it/s, est. speed input: 205285.66 toks/s, output: 200.47 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 200.47it/s, est. speed input: 205285.66 toks/s, output: 200.47 toks/s]
[rank0]:[W126 15:36:02.821560837 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 15:36:04
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:36:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=688442) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=688442) WARNING 01-26 15:36:31 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=688442) WARNING 01-26 15:36:40 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 194.08 requests/s, 198929.50 total tokens/s, 194.08 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 15:36:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:36:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:36:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:36:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:36:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:36:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:36:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:36:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:36:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:36:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:36:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:36:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:36:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:36:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:36:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:36:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:36:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:36:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:36:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:36:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:36:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:36:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:36:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:36:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:36:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:36:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:36:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:36:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=688442) [2026-01-26 15:36:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=688442) [2026-01-26 15:36:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=688442) [2026-01-26 15:36:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=688442) [2026-01-26 15:36:23] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=688442) [2026-01-26 15:36:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=688442) [2026-01-26 15:36:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=688442) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=688442) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.22it/s]
(EngineCore_DP0 pid=688442) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.22it/s]
(EngineCore_DP0 pid=688442) 
(EngineCore_DP0 pid=688442) [2026-01-26 15:36:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=688442) [2026-01-26 15:36:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=688442) [2026-01-26 15:36:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=688442) [2026-01-26 15:36:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=688442) [2026-01-26 15:36:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=688442) [2026-01-26 15:36:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=688442) [2026-01-26 15:36:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=688442) [2026-01-26 15:36:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=688442) 2026-01-26 15:36:40,800 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=688442) 2026-01-26 15:36:40,824 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=688442) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 16.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 14.56it/s]
(EngineCore_DP0 pid=688442) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 17.67it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.25it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.15it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 38/1024 [00:00<00:02, 378.79it/s]
Adding requests:   8%|▊         | 83/1024 [00:00<00:02, 417.79it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 432.16it/s]
Adding requests:  17%|█▋        | 173/1024 [00:00<00:01, 436.40it/s]
Adding requests:  21%|██▏       | 219/1024 [00:00<00:01, 442.44it/s]
Adding requests:  26%|██▌       | 267/1024 [00:00<00:01, 453.60it/s]
Adding requests:  31%|███       | 313/1024 [00:00<00:01, 451.57it/s]
Adding requests:  35%|███▌      | 359/1024 [00:00<00:01, 454.18it/s]
Adding requests:  40%|███▉      | 406/1024 [00:00<00:01, 456.76it/s]
Adding requests:  44%|████▍     | 453/1024 [00:01<00:01, 459.11it/s]
Adding requests:  49%|████▉     | 500/1024 [00:01<00:01, 457.87it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 453.81it/s]
Adding requests:  58%|█████▊    | 595/1024 [00:01<00:00, 462.68it/s]
Adding requests:  63%|██████▎   | 642/1024 [00:01<00:00, 464.82it/s]
Adding requests:  67%|██████▋   | 691/1024 [00:01<00:00, 470.17it/s]
Adding requests:  72%|███████▏  | 740/1024 [00:01<00:00, 471.64it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:01<00:00, 468.95it/s]
Adding requests:  82%|████████▏ | 835/1024 [00:01<00:00, 458.66it/s]
Adding requests:  86%|████████▌ | 883/1024 [00:01<00:00, 463.87it/s]
Adding requests:  91%|█████████ | 932/1024 [00:02<00:00, 469.22it/s]
Adding requests:  96%|█████████▌| 979/1024 [00:02<00:00, 468.41it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 458.73it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:00<00:00, 3638.58it/s, est. speed input: 3726133.41 toks/s, output: 3638.65 toks/s]
Processed prompts:  79%|███████▊  | 806/1024 [00:01<00:00, 351.08it/s, est. speed input: 422272.47 toks/s, output: 412.37 toks/s]   
Processed prompts:  95%|█████████▍| 968/1024 [00:02<00:00, 294.65it/s, est. speed input: 358952.17 toks/s, output: 350.54 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:03<00:00, 294.65it/s, est. speed input: 344613.89 toks/s, output: 336.54 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:03<00:00, 336.52it/s, est. speed input: 344613.89 toks/s, output: 336.54 toks/s]
[rank0]:[W126 15:36:48.835851399 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 15:36:50
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:37:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=689688) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=689688) WARNING 01-26 15:37:21 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=689688) WARNING 01-26 15:37:31 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 211.59 requests/s, 216875.23 total tokens/s, 211.59 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 15:37:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:37:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:37:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:37:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:37:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:37:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:37:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:37:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:37:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:37:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:37:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:37:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:37:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:37:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:37:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:37:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:37:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:37:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:37:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:37:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:37:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:37:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:37:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:37:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:37:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:37:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:37:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:37:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=689688) [2026-01-26 15:37:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=689688) [2026-01-26 15:37:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=689688) [2026-01-26 15:37:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=689688) [2026-01-26 15:37:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=689688) [2026-01-26 15:37:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=689688) [2026-01-26 15:37:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=689688) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=689688) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]
(EngineCore_DP0 pid=689688) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]
(EngineCore_DP0 pid=689688) 
(EngineCore_DP0 pid=689688) [2026-01-26 15:37:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=689688) [2026-01-26 15:37:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=689688) [2026-01-26 15:37:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=689688) [2026-01-26 15:37:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=689688) [2026-01-26 15:37:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=689688) [2026-01-26 15:37:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=689688) [2026-01-26 15:37:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=689688) [2026-01-26 15:37:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=689688) 2026-01-26 15:37:31,421 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=689688) 2026-01-26 15:37:31,446 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=689688) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 12.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 15.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 13.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 13.38it/s]
(EngineCore_DP0 pid=689688) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 17.24it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 17.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 17.90it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 38/2048 [00:00<00:05, 379.63it/s]
Adding requests:   4%|▍         | 83/2048 [00:00<00:04, 418.03it/s]
Adding requests:   6%|▋         | 128/2048 [00:00<00:04, 430.52it/s]
Adding requests:   8%|▊         | 172/2048 [00:00<00:04, 433.81it/s]
Adding requests:  11%|█         | 218/2048 [00:00<00:04, 440.40it/s]
Adding requests:  13%|█▎        | 266/2048 [00:00<00:03, 451.23it/s]
Adding requests:  15%|█▌        | 312/2048 [00:00<00:03, 450.46it/s]
Adding requests:  17%|█▋        | 358/2048 [00:00<00:03, 453.29it/s]
Adding requests:  20%|█▉        | 406/2048 [00:00<00:03, 458.79it/s]
Adding requests:  22%|██▏       | 453/2048 [00:01<00:03, 459.97it/s]
Adding requests:  24%|██▍       | 500/2048 [00:01<00:03, 459.69it/s]
Adding requests:  27%|██▋       | 546/2048 [00:01<00:03, 454.64it/s]
Adding requests:  29%|██▉       | 595/2048 [00:01<00:03, 464.29it/s]
Adding requests:  31%|███▏      | 642/2048 [00:01<00:03, 460.95it/s]
Adding requests:  34%|███▎      | 691/2048 [00:01<00:02, 467.67it/s]
Adding requests:  36%|███▌      | 739/2048 [00:01<00:02, 470.23it/s]
Adding requests:  38%|███▊      | 787/2048 [00:01<00:02, 467.60it/s]
Adding requests:  41%|████      | 834/2048 [00:01<00:02, 455.90it/s]
Adding requests:  43%|████▎     | 882/2048 [00:01<00:02, 462.78it/s]
Adding requests:  45%|████▌     | 930/2048 [00:02<00:02, 466.43it/s]
Adding requests:  48%|████▊     | 978/2048 [00:02<00:02, 467.84it/s]
Adding requests:  50%|█████     | 1026/2048 [00:02<00:02, 470.47it/s]
Adding requests:  52%|█████▏    | 1074/2048 [00:02<00:02, 465.71it/s]
Adding requests:  55%|█████▍    | 1121/2048 [00:02<00:02, 463.38it/s]
Adding requests:  57%|█████▋    | 1170/2048 [00:02<00:01, 470.77it/s]
Adding requests:  60%|█████▉    | 1220/2048 [00:02<00:01, 477.95it/s]
Adding requests:  62%|██████▏   | 1268/2048 [00:02<00:01, 469.84it/s]
Adding requests:  64%|██████▍   | 1316/2048 [00:02<00:01, 470.91it/s]
Adding requests:  67%|██████▋   | 1364/2048 [00:02<00:01, 472.81it/s]
Adding requests:  69%|██████▉   | 1414/2048 [00:03<00:01, 477.68it/s]
Adding requests:  71%|███████▏  | 1462/2048 [00:03<00:01, 476.50it/s]
Adding requests:  74%|███████▍  | 1511/2048 [00:03<00:01, 479.46it/s]
Adding requests:  76%|███████▌  | 1559/2048 [00:03<00:01, 477.46it/s]
Adding requests:  79%|███████▊  | 1609/2048 [00:03<00:00, 484.04it/s]
Adding requests:  81%|████████  | 1658/2048 [00:03<00:00, 479.26it/s]
Adding requests:  83%|████████▎ | 1706/2048 [00:03<00:00, 477.47it/s]
Adding requests:  86%|████████▌ | 1754/2048 [00:03<00:00, 465.94it/s]
Adding requests:  88%|████████▊ | 1801/2048 [00:03<00:00, 466.15it/s]
Adding requests:  90%|█████████ | 1849/2048 [00:03<00:00, 467.09it/s]
Adding requests:  93%|█████████▎| 1897/2048 [00:04<00:00, 468.20it/s]
Adding requests:  95%|█████████▍| 1944/2048 [00:04<00:00, 468.68it/s]
Adding requests:  97%|█████████▋| 1992/2048 [00:04<00:00, 469.48it/s]
Adding requests: 100%|█████████▉| 2041/2048 [00:04<00:00, 472.23it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 465.07it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:00<00:00, 7394.57it/s, est. speed input: 7572500.01 toks/s, output: 7394.68 toks/s]
Processed prompts:  82%|████████▏ | 1670/2048 [00:03<00:00, 395.78it/s, est. speed input: 481400.85 toks/s, output: 470.12 toks/s]  
Processed prompts:  97%|█████████▋| 1988/2048 [00:05<00:00, 326.58it/s, est. speed input: 403324.69 toks/s, output: 393.87 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:05<00:00, 326.58it/s, est. speed input: 397620.68 toks/s, output: 388.30 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:05<00:00, 388.29it/s, est. speed input: 397620.68 toks/s, output: 388.30 toks/s]
[rank0]:[W126 15:37:43.969272862 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 15:37:45
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:38:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=691125) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=691125) WARNING 01-26 15:38:26 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=691125) WARNING 01-26 15:38:35 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 218.62 requests/s, 224085.27 total tokens/s, 218.62 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 15:38:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:38:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:38:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:38:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:38:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:38:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:38:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:38:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:38:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:38:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:38:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:38:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:38:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:38:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:38:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:38:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:38:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:38:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:38:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:38:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:38:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:38:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:38:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:38:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:38:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:38:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:38:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:38:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=691125) [2026-01-26 15:38:18] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=691125) [2026-01-26 15:38:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=691125) [2026-01-26 15:38:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=691125) [2026-01-26 15:38:18] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=691125) [2026-01-26 15:38:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=691125) [2026-01-26 15:38:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=691125) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=691125) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]
(EngineCore_DP0 pid=691125) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]
(EngineCore_DP0 pid=691125) 
(EngineCore_DP0 pid=691125) [2026-01-26 15:38:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=691125) [2026-01-26 15:38:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=691125) [2026-01-26 15:38:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=691125) [2026-01-26 15:38:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=691125) [2026-01-26 15:38:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=691125) [2026-01-26 15:38:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=691125) [2026-01-26 15:38:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=691125) [2026-01-26 15:38:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=691125) [rank0]:W0126 15:38:31.343000 691125 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=691125) [rank0]:W0126 15:38:31.428000 691125 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=691125) [rank0]:W0126 15:38:32.345000 691125 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=691125) [rank0]:W0126 15:38:32.467000 691125 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=691125) 2026-01-26 15:38:35,855 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=691125) 2026-01-26 15:38:35,880 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=691125) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 15.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 16.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 16.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 17.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 16.02it/s]
(EngineCore_DP0 pid=691125) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.66it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 15.13it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 14.63it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 15.27it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 39/4096 [00:00<00:10, 383.54it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 420.33it/s]
Adding requests:   3%|▎         | 130/4096 [00:00<00:09, 436.25it/s]
Adding requests:   4%|▍         | 175/4096 [00:00<00:08, 438.96it/s]
Adding requests:   5%|▌         | 222/4096 [00:00<00:08, 448.10it/s]
Adding requests:   7%|▋         | 271/4096 [00:00<00:08, 458.97it/s]
Adding requests:   8%|▊         | 317/4096 [00:00<00:08, 454.11it/s]
Adding requests:   9%|▉         | 365/4096 [00:00<00:08, 461.74it/s]
Adding requests:  10%|█         | 412/4096 [00:00<00:07, 463.02it/s]
Adding requests:  11%|█         | 460/4096 [00:01<00:07, 466.42it/s]
Adding requests:  12%|█▏        | 507/4096 [00:01<00:07, 466.03it/s]
Adding requests:  14%|█▎        | 554/4096 [00:01<00:07, 456.92it/s]
Adding requests:  15%|█▍        | 602/4096 [00:01<00:07, 461.22it/s]
Adding requests:  16%|█▌        | 652/4096 [00:01<00:07, 470.49it/s]
Adding requests:  17%|█▋        | 702/4096 [00:01<00:07, 478.50it/s]
Adding requests:  18%|█▊        | 750/4096 [00:01<00:07, 476.45it/s]
Adding requests:  19%|█▉        | 798/4096 [00:01<00:06, 473.75it/s]
Adding requests:  21%|██        | 846/4096 [00:01<00:06, 465.42it/s]
Adding requests:  22%|██▏       | 896/4096 [00:01<00:06, 473.03it/s]
Adding requests:  23%|██▎       | 944/4096 [00:02<00:06, 474.27it/s]
Adding requests:  24%|██▍       | 992/4096 [00:02<00:06, 469.82it/s]
Adding requests:  25%|██▌       | 1040/4096 [00:02<00:06, 472.27it/s]
Adding requests:  27%|██▋       | 1088/4096 [00:02<00:06, 470.07it/s]
Adding requests:  28%|██▊       | 1136/4096 [00:02<00:06, 466.45it/s]
Adding requests:  29%|██▉       | 1186/4096 [00:02<00:06, 476.17it/s]
Adding requests:  30%|███       | 1235/4096 [00:02<00:05, 478.69it/s]
Adding requests:  31%|███▏      | 1283/4096 [00:02<00:05, 469.27it/s]
Adding requests:  33%|███▎      | 1332/4096 [00:02<00:05, 473.15it/s]
Adding requests:  34%|███▎      | 1381/4096 [00:02<00:05, 475.42it/s]
Adding requests:  35%|███▍      | 1430/4096 [00:03<00:05, 477.29it/s]
Adding requests:  36%|███▌      | 1479/4096 [00:03<00:05, 479.31it/s]
Adding requests:  37%|███▋      | 1528/4096 [00:03<00:05, 481.05it/s]
Adding requests:  39%|███▊      | 1577/4096 [00:03<00:05, 481.47it/s]
Adding requests:  40%|███▉      | 1627/4096 [00:03<00:05, 484.89it/s]
Adding requests:  41%|████      | 1676/4096 [00:03<00:05, 480.25it/s]
Adding requests:  42%|████▏     | 1725/4096 [00:03<00:04, 481.39it/s]
Adding requests:  43%|████▎     | 1774/4096 [00:03<00:04, 478.10it/s]
Adding requests:  44%|████▍     | 1822/4096 [00:03<00:04, 478.60it/s]
Adding requests:  46%|████▌     | 1870/4096 [00:03<00:04, 476.10it/s]
Adding requests:  47%|████▋     | 1919/4096 [00:04<00:04, 478.18it/s]
Adding requests:  48%|████▊     | 1967/4096 [00:04<00:04, 476.68it/s]
Adding requests:  49%|████▉     | 2016/4096 [00:04<00:04, 479.96it/s]
Adding requests:  50%|█████     | 2066/4096 [00:04<00:04, 483.53it/s]
Adding requests:  52%|█████▏    | 2115/4096 [00:04<00:04, 472.62it/s]
Adding requests:  53%|█████▎    | 2163/4096 [00:04<00:04, 466.74it/s]
Adding requests:  54%|█████▍    | 2210/4096 [00:04<00:04, 465.38it/s]
Adding requests:  55%|█████▌    | 2260/4096 [00:04<00:03, 470.70it/s]
Adding requests:  56%|█████▋    | 2309/4096 [00:04<00:03, 474.35it/s]
Adding requests:  58%|█████▊    | 2357/4096 [00:05<00:03, 473.21it/s]
Adding requests:  59%|█████▊    | 2405/4096 [00:05<00:03, 473.51it/s]
Adding requests:  60%|█████▉    | 2454/4096 [00:05<00:03, 476.67it/s]
Adding requests:  61%|██████    | 2502/4096 [00:05<00:03, 475.56it/s]
Adding requests:  62%|██████▏   | 2552/4096 [00:05<00:03, 480.56it/s]
Adding requests:  64%|██████▎   | 2601/4096 [00:05<00:03, 460.10it/s]
Adding requests:  65%|██████▍   | 2650/4096 [00:05<00:03, 466.07it/s]
Adding requests:  66%|██████▌   | 2698/4096 [00:05<00:02, 467.18it/s]
Adding requests:  67%|██████▋   | 2746/4096 [00:05<00:02, 468.38it/s]
Adding requests:  68%|██████▊   | 2793/4096 [00:05<00:02, 468.64it/s]
Adding requests:  69%|██████▉   | 2841/4096 [00:06<00:02, 469.05it/s]
Adding requests:  71%|███████   | 2890/4096 [00:06<00:02, 475.22it/s]
Adding requests:  72%|███████▏  | 2938/4096 [00:06<00:02, 469.69it/s]
Adding requests:  73%|███████▎  | 2987/4096 [00:06<00:02, 474.89it/s]
Adding requests:  74%|███████▍  | 3035/4096 [00:06<00:02, 472.54it/s]
Adding requests:  75%|███████▌  | 3083/4096 [00:06<00:02, 472.26it/s]
Adding requests:  76%|███████▋  | 3132/4096 [00:06<00:02, 474.83it/s]
Adding requests:  78%|███████▊  | 3181/4096 [00:06<00:01, 477.19it/s]
Adding requests:  79%|███████▉  | 3229/4096 [00:06<00:01, 475.22it/s]
Adding requests:  80%|████████  | 3278/4096 [00:06<00:01, 478.03it/s]
Adding requests:  81%|████████  | 3326/4096 [00:07<00:01, 477.14it/s]
Adding requests:  82%|████████▏ | 3374/4096 [00:07<00:01, 464.26it/s]
Adding requests:  84%|████████▎ | 3423/4096 [00:07<00:01, 471.46it/s]
Adding requests:  85%|████████▍ | 3471/4096 [00:07<00:01, 462.51it/s]
Adding requests:  86%|████████▌ | 3520/4096 [00:07<00:01, 467.55it/s]
Adding requests:  87%|████████▋ | 3567/4096 [00:07<00:01, 466.24it/s]
Adding requests:  88%|████████▊ | 3615/4096 [00:07<00:01, 467.81it/s]
Adding requests:  89%|████████▉ | 3663/4096 [00:07<00:00, 467.82it/s]
Adding requests:  91%|█████████ | 3712/4096 [00:07<00:00, 473.90it/s]
Adding requests:  92%|█████████▏| 3760/4096 [00:07<00:00, 473.20it/s]
Adding requests:  93%|█████████▎| 3809/4096 [00:08<00:00, 476.86it/s]
Adding requests:  94%|█████████▍| 3859/4096 [00:08<00:00, 482.68it/s]
Adding requests:  95%|█████████▌| 3908/4096 [00:08<00:00, 474.16it/s]
Adding requests:  97%|█████████▋| 3957/4096 [00:08<00:00, 476.14it/s]
Adding requests:  98%|█████████▊| 4005/4096 [00:08<00:00, 474.04it/s]
Adding requests:  99%|█████████▉| 4053/4096 [00:08<00:00, 472.18it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 470.96it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  47%|████▋     | 1908/4096 [00:00<00:00, 8367.05it/s, est. speed input: 8568145.99 toks/s, output: 8367.12 toks/s]
Processed prompts:  67%|██████▋   | 2745/4096 [00:04<00:02, 550.56it/s, est. speed input: 700163.77 toks/s, output: 683.75 toks/s]   
Processed prompts:  76%|███████▌  | 3103/4096 [00:05<00:02, 430.32it/s, est. speed input: 565555.56 toks/s, output: 552.30 toks/s]
Processed prompts:  81%|████████  | 3308/4096 [00:06<00:02, 387.49it/s, est. speed input: 522322.25 toks/s, output: 510.08 toks/s]
Processed prompts:  84%|████████▍ | 3442/4096 [00:07<00:01, 359.84it/s, est. speed input: 498626.75 toks/s, output: 486.94 toks/s]
Processed prompts:  86%|████████▋ | 3536/4096 [00:07<00:01, 338.05it/s, est. speed input: 482979.74 toks/s, output: 471.66 toks/s]
Processed prompts:  88%|████████▊ | 3606/4096 [00:07<00:01, 305.48it/s, est. speed input: 465527.75 toks/s, output: 454.62 toks/s]
Processed prompts:  89%|████████▉ | 3658/4096 [00:08<00:01, 309.41it/s, est. speed input: 463643.97 toks/s, output: 452.78 toks/s]
Processed prompts:  90%|█████████ | 3705/4096 [00:08<00:01, 280.92it/s, est. speed input: 453041.36 toks/s, output: 442.42 toks/s]
Processed prompts:  91%|█████████▏| 3742/4096 [00:08<00:01, 278.83it/s, est. speed input: 449995.62 toks/s, output: 439.45 toks/s]
Processed prompts:  92%|█████████▏| 3776/4096 [00:08<00:01, 272.31it/s, est. speed input: 446456.53 toks/s, output: 435.99 toks/s]
Processed prompts:  93%|█████████▎| 3807/4096 [00:08<00:01, 262.16it/s, est. speed input: 442682.16 toks/s, output: 432.31 toks/s]
Processed prompts:  94%|█████████▎| 3835/4096 [00:08<00:01, 249.34it/s, est. speed input: 438816.80 toks/s, output: 428.53 toks/s]
Processed prompts:  94%|█████████▍| 3861/4096 [00:09<00:01, 234.40it/s, est. speed input: 434802.06 toks/s, output: 424.61 toks/s]
Processed prompts:  95%|█████████▌| 3892/4096 [00:09<00:00, 229.00it/s, est. speed input: 431354.45 toks/s, output: 421.24 toks/s]
Processed prompts:  96%|█████████▌| 3924/4096 [00:09<00:00, 226.21it/s, est. speed input: 428102.14 toks/s, output: 418.07 toks/s]
Processed prompts:  97%|█████████▋| 3956/4096 [00:09<00:00, 223.47it/s, est. speed input: 424888.41 toks/s, output: 414.93 toks/s]
Processed prompts:  97%|█████████▋| 3988/4096 [00:09<00:00, 222.34it/s, est. speed input: 421870.25 toks/s, output: 411.98 toks/s]
Processed prompts:  98%|█████████▊| 4020/4096 [00:09<00:00, 223.32it/s, est. speed input: 419122.26 toks/s, output: 409.30 toks/s]
Processed prompts:  99%|█████████▉| 4052/4096 [00:09<00:00, 224.34it/s, est. speed input: 416478.96 toks/s, output: 406.72 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:10<00:00, 224.34it/s, est. speed input: 417898.89 toks/s, output: 408.10 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:10<00:00, 408.10it/s, est. speed input: 417898.89 toks/s, output: 408.10 toks/s]
[rank0]:[W126 15:38:57.017764275 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 15:38:59
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:39:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=692958) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=692958) WARNING 01-26 15:40:00 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=692958) WARNING 01-26 15:40:09 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 223.06 requests/s, 228639.69 total tokens/s, 223.06 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 15:39:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:39:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:39:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:39:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:39:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:39:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:39:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:39:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:39:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:39:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:39:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:39:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:39:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:39:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:39:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:39:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 15:39:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 15:39:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:39:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:39:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:39:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:39:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 15:39:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 15:39:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:39:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:39:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:39:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:39:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=692958) [2026-01-26 15:39:52] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=692958) [2026-01-26 15:39:52] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=692958) [2026-01-26 15:39:52] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=692958) [2026-01-26 15:39:52] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=692958) [2026-01-26 15:39:52] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=692958) [2026-01-26 15:39:52] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=692958) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=692958) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.00s/it]
(EngineCore_DP0 pid=692958) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.00s/it]
(EngineCore_DP0 pid=692958) 
(EngineCore_DP0 pid=692958) [2026-01-26 15:39:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=692958) [2026-01-26 15:39:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=692958) [2026-01-26 15:39:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=692958) [2026-01-26 15:39:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=692958) [2026-01-26 15:39:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=692958) [2026-01-26 15:39:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=692958) [2026-01-26 15:39:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=692958) [2026-01-26 15:39:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=692958) [rank0]:W0126 15:40:04.916000 692958 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=692958) [rank0]:W0126 15:40:05.002000 692958 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=692958) [rank0]:W0126 15:40:05.986000 692958 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=692958) [rank0]:W0126 15:40:06.111000 692958 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=692958) 2026-01-26 15:40:09,495 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=692958) 2026-01-26 15:40:09,522 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=692958) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:06,  2.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:05,  3.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:02,  6.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  9.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 11.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:00, 12.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00, 13.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00, 12.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00, 14.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:01<00:00, 15.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 11.02it/s]
(EngineCore_DP0 pid=692958) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 17.44it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 17.83it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 17.95it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 18.03it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 18.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 17.97it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 39/8192 [00:00<00:21, 376.80it/s]
Adding requests:   1%|          | 83/8192 [00:00<00:19, 412.45it/s]
Adding requests:   2%|▏         | 127/8192 [00:00<00:18, 424.58it/s]
Adding requests:   2%|▏         | 171/8192 [00:00<00:18, 429.47it/s]
Adding requests:   3%|▎         | 216/8192 [00:00<00:18, 435.97it/s]
Adding requests:   3%|▎         | 263/8192 [00:00<00:17, 446.55it/s]
Adding requests:   4%|▍         | 308/8192 [00:00<00:17, 446.56it/s]
Adding requests:   4%|▍         | 355/8192 [00:00<00:17, 452.75it/s]
Adding requests:   5%|▍         | 402/8192 [00:00<00:17, 455.91it/s]
Adding requests:   5%|▌         | 449/8192 [00:01<00:16, 457.94it/s]
Adding requests:   6%|▌         | 495/8192 [00:01<00:16, 456.53it/s]
Adding requests:   7%|▋         | 541/8192 [00:01<00:17, 449.07it/s]
Adding requests:   7%|▋         | 590/8192 [00:01<00:16, 459.92it/s]
Adding requests:   8%|▊         | 637/8192 [00:01<00:16, 461.26it/s]
Adding requests:   8%|▊         | 686/8192 [00:01<00:16, 467.81it/s]
Adding requests:   9%|▉         | 735/8192 [00:01<00:15, 472.85it/s]
Adding requests:  10%|▉         | 783/8192 [00:01<00:15, 467.49it/s]
Adding requests:  10%|█         | 830/8192 [00:01<00:16, 459.16it/s]
Adding requests:  11%|█         | 878/8192 [00:01<00:15, 463.24it/s]
Adding requests:  11%|█▏        | 927/8192 [00:02<00:15, 468.65it/s]
Adding requests:  12%|█▏        | 975/8192 [00:02<00:15, 470.22it/s]
Adding requests:  12%|█▏        | 1023/8192 [00:02<00:15, 472.49it/s]
Adding requests:  13%|█▎        | 1071/8192 [00:02<00:15, 466.84it/s]
Adding requests:  14%|█▎        | 1118/8192 [00:02<00:15, 466.28it/s]
Adding requests:  14%|█▍        | 1167/8192 [00:02<00:14, 472.15it/s]
Adding requests:  15%|█▍        | 1217/8192 [00:02<00:14, 478.39it/s]
Adding requests:  15%|█▌        | 1265/8192 [00:02<00:14, 471.87it/s]
Adding requests:  16%|█▌        | 1313/8192 [00:02<00:14, 471.19it/s]
Adding requests:  17%|█▋        | 1362/8192 [00:02<00:14, 476.08it/s]
Adding requests:  17%|█▋        | 1412/8192 [00:03<00:14, 481.59it/s]
Adding requests:  18%|█▊        | 1461/8192 [00:03<00:14, 479.89it/s]
Adding requests:  18%|█▊        | 1510/8192 [00:03<00:13, 479.33it/s]
Adding requests:  19%|█▉        | 1558/8192 [00:03<00:13, 479.27it/s]
Adding requests:  20%|█▉        | 1606/8192 [00:03<00:13, 473.91it/s]
Adding requests:  20%|██        | 1654/8192 [00:03<00:13, 473.13it/s]
Adding requests:  21%|██        | 1702/8192 [00:03<00:13, 471.09it/s]
Adding requests:  21%|██▏       | 1750/8192 [00:03<00:13, 473.19it/s]
Adding requests:  22%|██▏       | 1798/8192 [00:03<00:13, 473.07it/s]
Adding requests:  23%|██▎       | 1847/8192 [00:03<00:13, 476.41it/s]
Adding requests:  23%|██▎       | 1895/8192 [00:04<00:13, 473.30it/s]
Adding requests:  24%|██▎       | 1943/8192 [00:04<00:13, 474.90it/s]
Adding requests:  24%|██▍       | 1991/8192 [00:04<00:13, 473.21it/s]
Adding requests:  25%|██▍       | 2040/8192 [00:04<00:12, 477.35it/s]
Adding requests:  26%|██▌       | 2089/8192 [00:04<00:12, 480.09it/s]
Adding requests:  26%|██▌       | 2138/8192 [00:04<00:12, 474.81it/s]
Adding requests:  27%|██▋       | 2186/8192 [00:04<00:12, 469.13it/s]
Adding requests:  27%|██▋       | 2235/8192 [00:04<00:12, 474.58it/s]
Adding requests:  28%|██▊       | 2283/8192 [00:04<00:12, 474.04it/s]
Adding requests:  28%|██▊       | 2331/8192 [00:04<00:12, 474.20it/s]
Adding requests:  29%|██▉       | 2379/8192 [00:05<00:12, 475.30it/s]
Adding requests:  30%|██▉       | 2427/8192 [00:05<00:12, 476.17it/s]
Adding requests:  30%|███       | 2475/8192 [00:05<00:12, 475.23it/s]
Adding requests:  31%|███       | 2523/8192 [00:05<00:11, 472.87it/s]
Adding requests:  31%|███▏      | 2571/8192 [00:05<00:11, 472.08it/s]
Adding requests:  32%|███▏      | 2619/8192 [00:05<00:11, 471.93it/s]
Adding requests:  33%|███▎      | 2668/8192 [00:05<00:11, 476.52it/s]
Adding requests:  33%|███▎      | 2716/8192 [00:05<00:11, 470.81it/s]
Adding requests:  34%|███▎      | 2764/8192 [00:05<00:11, 472.68it/s]
Adding requests:  34%|███▍      | 2812/8192 [00:06<00:11, 456.74it/s]
Adding requests:  35%|███▍      | 2860/8192 [00:06<00:11, 461.36it/s]
Adding requests:  36%|███▌      | 2909/8192 [00:06<00:11, 467.83it/s]
Adding requests:  36%|███▌      | 2956/8192 [00:06<00:11, 464.37it/s]
Adding requests:  37%|███▋      | 3004/8192 [00:06<00:11, 467.66it/s]
Adding requests:  37%|███▋      | 3051/8192 [00:06<00:10, 468.25it/s]
Adding requests:  38%|███▊      | 3098/8192 [00:06<00:10, 467.50it/s]
Adding requests:  38%|███▊      | 3145/8192 [00:06<00:10, 466.24it/s]
Adding requests:  39%|███▉      | 3194/8192 [00:06<00:10, 470.25it/s]
Adding requests:  40%|███▉      | 3242/8192 [00:06<00:10, 472.56it/s]
Adding requests:  40%|████      | 3291/8192 [00:07<00:10, 475.07it/s]
Adding requests:  41%|████      | 3339/8192 [00:07<00:10, 475.41it/s]
Adding requests:  41%|████▏     | 3388/8192 [00:07<00:10, 478.68it/s]
Adding requests:  42%|████▏     | 3437/8192 [00:07<00:09, 481.81it/s]
Adding requests:  43%|████▎     | 3486/8192 [00:07<00:10, 469.86it/s]
Adding requests:  43%|████▎     | 3534/8192 [00:07<00:09, 471.75it/s]
Adding requests:  44%|████▎     | 3582/8192 [00:07<00:09, 471.56it/s]
Adding requests:  44%|████▍     | 3630/8192 [00:07<00:09, 470.49it/s]
Adding requests:  45%|████▍     | 3678/8192 [00:07<00:09, 472.40it/s]
Adding requests:  45%|████▌     | 3726/8192 [00:07<00:09, 471.23it/s]
Adding requests:  46%|████▌     | 3775/8192 [00:08<00:09, 474.87it/s]
Adding requests:  47%|████▋     | 3823/8192 [00:08<00:09, 473.39it/s]
Adding requests:  47%|████▋     | 3873/8192 [00:08<00:09, 478.30it/s]
Adding requests:  48%|████▊     | 3921/8192 [00:08<00:08, 475.34it/s]
Adding requests:  48%|████▊     | 3969/8192 [00:08<00:08, 474.25it/s]
Adding requests:  49%|████▉     | 4017/8192 [00:08<00:08, 472.12it/s]
Adding requests:  50%|████▉     | 4065/8192 [00:08<00:08, 466.19it/s]
Adding requests:  50%|█████     | 4112/8192 [00:08<00:08, 454.91it/s]
Adding requests:  51%|█████     | 4160/8192 [00:08<00:08, 459.23it/s]
Adding requests:  51%|█████▏    | 4208/8192 [00:08<00:08, 464.38it/s]
Adding requests:  52%|█████▏    | 4257/8192 [00:09<00:08, 469.03it/s]
Adding requests:  53%|█████▎    | 4304/8192 [00:09<00:08, 468.19it/s]
Adding requests:  53%|█████▎    | 4352/8192 [00:09<00:08, 469.41it/s]
Adding requests:  54%|█████▎    | 4402/8192 [00:09<00:07, 475.95it/s]
Adding requests:  54%|█████▍    | 4450/8192 [00:09<00:07, 475.37it/s]
Adding requests:  55%|█████▍    | 4498/8192 [00:09<00:08, 455.78it/s]
Adding requests:  55%|█████▌    | 4544/8192 [00:09<00:07, 456.42it/s]
Adding requests:  56%|█████▌    | 4592/8192 [00:09<00:07, 461.56it/s]
Adding requests:  57%|█████▋    | 4640/8192 [00:09<00:07, 465.29it/s]
Adding requests:  57%|█████▋    | 4687/8192 [00:10<00:07, 462.53it/s]
Adding requests:  58%|█████▊    | 4736/8192 [00:10<00:07, 468.96it/s]
Adding requests:  58%|█████▊    | 4784/8192 [00:10<00:07, 469.86it/s]
Adding requests:  59%|█████▉    | 4832/8192 [00:10<00:07, 471.08it/s]
Adding requests:  60%|█████▉    | 4880/8192 [00:10<00:07, 467.61it/s]
Adding requests:  60%|██████    | 4928/8192 [00:10<00:06, 470.60it/s]
Adding requests:  61%|██████    | 4976/8192 [00:10<00:06, 469.10it/s]
Adding requests:  61%|██████▏   | 5025/8192 [00:10<00:06, 473.17it/s]
Adding requests:  62%|██████▏   | 5073/8192 [00:10<00:06, 474.50it/s]
Adding requests:  63%|██████▎   | 5122/8192 [00:10<00:06, 478.60it/s]
Adding requests:  63%|██████▎   | 5170/8192 [00:11<00:06, 477.19it/s]
Adding requests:  64%|██████▎   | 5218/8192 [00:11<00:06, 476.46it/s]
Adding requests:  64%|██████▍   | 5266/8192 [00:11<00:06, 473.60it/s]
Adding requests:  65%|██████▍   | 5315/8192 [00:11<00:06, 475.76it/s]
Adding requests:  65%|██████▌   | 5364/8192 [00:11<00:05, 478.59it/s]
Adding requests:  66%|██████▌   | 5412/8192 [00:11<00:06, 462.49it/s]
Adding requests:  67%|██████▋   | 5459/8192 [00:11<00:05, 462.45it/s]
Adding requests:  67%|██████▋   | 5506/8192 [00:11<00:05, 460.63it/s]
Adding requests:  68%|██████▊   | 5553/8192 [00:11<00:05, 463.03it/s]
Adding requests:  68%|██████▊   | 5600/8192 [00:11<00:05, 464.18it/s]
Adding requests:  69%|██████▉   | 5647/8192 [00:12<00:05, 463.78it/s]
Adding requests:  70%|██████▉   | 5695/8192 [00:12<00:05, 466.06it/s]
Adding requests:  70%|███████   | 5743/8192 [00:12<00:05, 467.76it/s]
Adding requests:  71%|███████   | 5791/8192 [00:12<00:05, 470.41it/s]
Adding requests:  71%|███████▏  | 5839/8192 [00:12<00:05, 463.75it/s]
Adding requests:  72%|███████▏  | 5888/8192 [00:12<00:04, 470.88it/s]
Adding requests:  72%|███████▏  | 5936/8192 [00:12<00:04, 471.79it/s]
Adding requests:  73%|███████▎  | 5984/8192 [00:12<00:04, 473.58it/s]
Adding requests:  74%|███████▎  | 6032/8192 [00:12<00:04, 474.05it/s]
Adding requests:  74%|███████▍  | 6081/8192 [00:12<00:04, 476.00it/s]
Adding requests:  75%|███████▍  | 6129/8192 [00:13<00:04, 475.89it/s]
Adding requests:  75%|███████▌  | 6177/8192 [00:13<00:04, 474.98it/s]
Adding requests:  76%|███████▌  | 6227/8192 [00:13<00:04, 481.18it/s]
Adding requests:  77%|███████▋  | 6277/8192 [00:13<00:03, 483.87it/s]
Adding requests:  77%|███████▋  | 6326/8192 [00:13<00:03, 484.92it/s]
Adding requests:  78%|███████▊  | 6375/8192 [00:13<00:03, 481.76it/s]
Adding requests:  78%|███████▊  | 6424/8192 [00:13<00:03, 470.44it/s]
Adding requests:  79%|███████▉  | 6473/8192 [00:13<00:03, 475.20it/s]
Adding requests:  80%|███████▉  | 6524/8192 [00:13<00:03, 482.78it/s]
Adding requests:  80%|████████  | 6573/8192 [00:14<00:03, 481.70it/s]
Adding requests:  81%|████████  | 6622/8192 [00:14<00:03, 479.12it/s]
Adding requests:  81%|████████▏ | 6670/8192 [00:14<00:03, 464.81it/s]
Adding requests:  82%|████████▏ | 6719/8192 [00:14<00:03, 469.80it/s]
Adding requests:  83%|████████▎ | 6767/8192 [00:14<00:03, 469.78it/s]
Adding requests:  83%|████████▎ | 6816/8192 [00:14<00:02, 473.94it/s]
Adding requests:  84%|████████▍ | 6866/8192 [00:14<00:02, 479.60it/s]
Adding requests:  84%|████████▍ | 6914/8192 [00:14<00:02, 479.21it/s]
Adding requests:  85%|████████▌ | 6964/8192 [00:14<00:02, 482.62it/s]
Adding requests:  86%|████████▌ | 7013/8192 [00:14<00:02, 476.11it/s]
Adding requests:  86%|████████▌ | 7061/8192 [00:15<00:02, 473.92it/s]
Adding requests:  87%|████████▋ | 7109/8192 [00:15<00:02, 474.96it/s]
Adding requests:  87%|████████▋ | 7157/8192 [00:15<00:02, 473.55it/s]
Adding requests:  88%|████████▊ | 7205/8192 [00:15<00:02, 472.56it/s]
Adding requests:  89%|████████▊ | 7254/8192 [00:15<00:01, 474.85it/s]
Adding requests:  89%|████████▉ | 7303/8192 [00:15<00:01, 477.83it/s]
Adding requests:  90%|████████▉ | 7351/8192 [00:15<00:01, 472.77it/s]
Adding requests:  90%|█████████ | 7400/8192 [00:15<00:01, 477.82it/s]
Adding requests:  91%|█████████ | 7451/8192 [00:15<00:01, 485.39it/s]
Adding requests:  92%|█████████▏| 7500/8192 [00:15<00:01, 481.54it/s]
Adding requests:  92%|█████████▏| 7549/8192 [00:16<00:01, 481.64it/s]
Adding requests:  93%|█████████▎| 7598/8192 [00:16<00:01, 478.03it/s]
Adding requests:  93%|█████████▎| 7646/8192 [00:16<00:01, 477.80it/s]
Adding requests:  94%|█████████▍| 7696/8192 [00:16<00:01, 482.68it/s]
Adding requests:  95%|█████████▍| 7745/8192 [00:16<00:00, 479.68it/s]
Adding requests:  95%|█████████▌| 7793/8192 [00:16<00:00, 475.92it/s]
Adding requests:  96%|█████████▌| 7842/8192 [00:16<00:00, 477.69it/s]
Adding requests:  96%|█████████▋| 7890/8192 [00:16<00:00, 476.78it/s]
Adding requests:  97%|█████████▋| 7938/8192 [00:16<00:00, 472.07it/s]
Adding requests:  97%|█████████▋| 7986/8192 [00:16<00:00, 459.61it/s]
Adding requests:  98%|█████████▊| 8033/8192 [00:17<00:00, 458.67it/s]
Adding requests:  99%|█████████▊| 8082/8192 [00:17<00:00, 467.42it/s]
Adding requests:  99%|█████████▉| 8131/8192 [00:17<00:00, 471.91it/s]
Adding requests: 100%|█████████▉| 8179/8192 [00:17<00:00, 474.12it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 470.46it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  47%|████▋     | 3842/8192 [00:00<00:00, 22506.28it/s, est. speed input: 23047585.22 toks/s, output: 22506.59 toks/s]
Processed prompts:  74%|███████▍  | 6093/8192 [00:10<00:04, 488.93it/s, est. speed input: 614354.29 toks/s, output: 599.96 toks/s]      
Processed prompts:  86%|████████▌ | 7033/8192 [00:14<00:02, 396.61it/s, est. speed input: 508158.90 toks/s, output: 496.25 toks/s]
Processed prompts:  92%|█████████▏| 7555/8192 [00:16<00:01, 348.58it/s, est. speed input: 462165.70 toks/s, output: 451.33 toks/s]
Processed prompts:  96%|█████████▌| 7879/8192 [00:18<00:00, 328.27it/s, est. speed input: 443943.99 toks/s, output: 433.54 toks/s]
Processed prompts:  99%|█████████▉| 8095/8192 [00:19<00:00, 318.93it/s, est. speed input: 435623.50 toks/s, output: 425.41 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:19<00:00, 318.93it/s, est. speed input: 434438.91 toks/s, output: 424.26 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:19<00:00, 424.25it/s, est. speed input: 434438.91 toks/s, output: 424.26 toks/s]
[rank0]:[W126 15:40:50.197665862 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 16:51:00
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:51:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=793771) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=793771) WARNING 01-26 16:51:24 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=793771) WARNING 01-26 16:51:34 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 37.72 requests/s, 19349.99 total tokens/s, 37.72 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 16:51:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:51:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:51:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:51:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:51:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:51:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:51:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:51:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:51:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:51:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:51:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:51:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:15] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:15] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:15] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:15] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:15] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=793771) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=793771) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.01s/it]
(EngineCore_DP0 pid=793771) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.19s/it]
(EngineCore_DP0 pid=793771) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.16s/it]
(EngineCore_DP0 pid=793771) 
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=793771) [2026-01-26 16:51:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=793771) 2026-01-26 16:51:34,207 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=793771) 2026-01-26 16:51:34,231 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=793771) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.16it/s]
(EngineCore_DP0 pid=793771) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.06it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████▏     | 53/128 [00:00<00:00, 521.42it/s]
Adding requests:  88%|████████▊ | 113/128 [00:00<00:00, 566.71it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 565.28it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:02, 58.76it/s, est. speed input: 30088.18 toks/s, output: 58.76 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:02, 46.35it/s, est. speed input: 24571.64 toks/s, output: 47.99 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 43.53it/s, est. speed input: 23248.87 toks/s, output: 45.41 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 41.98it/s, est. speed input: 22516.17 toks/s, output: 43.98 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 41.17it/s, est. speed input: 22087.99 toks/s, output: 43.14 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 40.69it/s, est. speed input: 21806.69 toks/s, output: 42.59 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:02, 40.35it/s, est. speed input: 21595.40 toks/s, output: 42.18 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 40.24it/s, est. speed input: 21458.04 toks/s, output: 41.91 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:01, 40.04it/s, est. speed input: 21329.42 toks/s, output: 41.66 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 39.81it/s, est. speed input: 21208.29 toks/s, output: 41.42 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 39.72it/s, est. speed input: 21135.04 toks/s, output: 41.28 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 39.69it/s, est. speed input: 21076.73 toks/s, output: 41.17 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 39.66it/s, est. speed input: 21025.53 toks/s, output: 41.07 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 39.61it/s, est. speed input: 20976.17 toks/s, output: 40.97 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 39.66it/s, est. speed input: 20942.68 toks/s, output: 40.90 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 39.71it/s, est. speed input: 20912.71 toks/s, output: 40.84 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 39.74it/s, est. speed input: 20886.51 toks/s, output: 40.79 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 39.81it/s, est. speed input: 20866.37 toks/s, output: 40.75 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:00, 39.86it/s, est. speed input: 20848.38 toks/s, output: 40.72 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 39.81it/s, est. speed input: 20825.22 toks/s, output: 40.67 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 39.78it/s, est. speed input: 20804.52 toks/s, output: 40.63 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 39.80it/s, est. speed input: 20787.78 toks/s, output: 40.60 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 39.78it/s, est. speed input: 20770.91 toks/s, output: 40.57 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 39.81it/s, est. speed input: 20757.63 toks/s, output: 40.54 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 39.82it/s, est. speed input: 20744.81 toks/s, output: 40.52 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 39.85it/s, est. speed input: 20734.14 toks/s, output: 40.50 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 39.85it/s, est. speed input: 20722.86 toks/s, output: 40.47 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 39.86it/s, est. speed input: 20712.92 toks/s, output: 40.45 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.86it/s, est. speed input: 20701.34 toks/s, output: 40.43 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.43it/s, est. speed input: 20701.34 toks/s, output: 40.43 toks/s]
[rank0]:[W126 16:51:39.200110755 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 16:51:42
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:51:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=794964) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=794964) WARNING 01-26 16:52:06 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=794964) WARNING 01-26 16:52:15 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 39.75 requests/s, 40740.68 total tokens/s, 39.75 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 16:51:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:51:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:51:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:51:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:51:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:51:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:51:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:51:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:51:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:51:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:51:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:51:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:51:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:51:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:57] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=794964) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=794964) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.05it/s]
(EngineCore_DP0 pid=794964) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.16s/it]
(EngineCore_DP0 pid=794964) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.13s/it]
(EngineCore_DP0 pid=794964) 
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=794964) [2026-01-26 16:51:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=794964) 2026-01-26 16:52:15,619 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=794964) 2026-01-26 16:52:15,642 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=794964) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.96it/s]
(EngineCore_DP0 pid=794964) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.37it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 274.93it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 318.75it/s]
Adding requests:  75%|███████▌  | 96/128 [00:00<00:00, 316.73it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 180.19it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 212.18it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:00, 224.57it/s, est. speed input: 229969.81 toks/s, output: 224.57 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 64.29it/s, est. speed input: 74445.62 toks/s, output: 72.70 toks/s]   
Processed prompts:  48%|████▊     | 62/128 [00:00<00:01, 54.99it/s, est. speed input: 64574.64 toks/s, output: 63.06 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 50.70it/s, est. speed input: 60322.49 toks/s, output: 58.91 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 48.24it/s, est. speed input: 58004.47 toks/s, output: 56.64 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:00, 46.48it/s, est. speed input: 56419.98 toks/s, output: 55.10 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 45.00it/s, est. speed input: 55103.28 toks/s, output: 53.81 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 43.77it/s, est. speed input: 54093.08 toks/s, output: 52.83 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:01<00:00, 42.94it/s, est. speed input: 53273.65 toks/s, output: 52.02 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 42.18it/s, est. speed input: 52526.50 toks/s, output: 51.30 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 41.78it/s, est. speed input: 51912.20 toks/s, output: 50.70 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 41.38it/s, est. speed input: 51338.95 toks/s, output: 50.14 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 41.19it/s, est. speed input: 50848.92 toks/s, output: 49.66 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 40.73it/s, est. speed input: 50339.49 toks/s, output: 49.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.73it/s, est. speed input: 50102.96 toks/s, output: 48.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 48.93it/s, est. speed input: 50102.96 toks/s, output: 48.93 toks/s]
[rank0]:[W126 16:52:20.850837220 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 16:52:22
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:52:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=796089) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=796089) WARNING 01-26 16:52:47 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=796089) WARNING 01-26 16:52:56 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 76.51 requests/s, 78427.49 total tokens/s, 76.51 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 16:52:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:52:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:52:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:52:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:52:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:52:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:52:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:52:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:52:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:52:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:52:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:52:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:52:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:52:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:52:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:52:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:52:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:38] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:38] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=796089) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=796089) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.04it/s]
(EngineCore_DP0 pid=796089) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.13s/it]
(EngineCore_DP0 pid=796089) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]
(EngineCore_DP0 pid=796089) 
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=796089) [2026-01-26 16:52:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=796089) 2026-01-26 16:52:56,791 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=796089) 2026-01-26 16:52:56,814 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=796089) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.60it/s]
(EngineCore_DP0 pid=796089) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.71it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.69it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█         | 27/256 [00:00<00:00, 265.37it/s]
Adding requests:  24%|██▍       | 62/256 [00:00<00:00, 312.77it/s]
Adding requests:  37%|███▋      | 94/256 [00:00<00:01, 136.69it/s]
Adding requests:  49%|████▉     | 126/256 [00:00<00:00, 175.93it/s]
Adding requests:  60%|█████▉    | 153/256 [00:00<00:00, 197.69it/s]
Adding requests:  73%|███████▎  | 188/256 [00:00<00:00, 235.29it/s]
Adding requests:  87%|████████▋ | 223/256 [00:00<00:00, 265.08it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 282.44it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 233.01it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:00<00:00, 858.63it/s, est. speed input: 879286.58 toks/s, output: 858.64 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:00, 127.61it/s, est. speed input: 149801.28 toks/s, output: 146.29 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:01<00:00, 108.19it/s, est. speed input: 128406.52 toks/s, output: 125.40 toks/s]
Processed prompts:  93%|█████████▎| 239/256 [00:02<00:00, 100.97it/s, est. speed input: 121009.77 toks/s, output: 118.17 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 100.97it/s, est. speed input: 116716.52 toks/s, output: 113.98 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 113.98it/s, est. speed input: 116716.52 toks/s, output: 113.98 toks/s]
[rank0]:[W126 16:53:02.254826003 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 16:53:03
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:53:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=797223) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=797223) WARNING 01-26 16:53:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=797223) WARNING 01-26 16:53:39 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 105.53 requests/s, 108168.95 total tokens/s, 105.53 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 16:53:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:53:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:53:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:53:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:53:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:53:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:53:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:53:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:53:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:53:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:53:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:53:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:53:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:53:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:53:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:53:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:53:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:21] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:21] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:21] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:21] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:21] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=797223) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=797223) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.04it/s]
(EngineCore_DP0 pid=797223) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]
(EngineCore_DP0 pid=797223) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.08s/it]
(EngineCore_DP0 pid=797223) 
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=797223) [2026-01-26 16:53:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=797223) 2026-01-26 16:53:39,841 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=797223) 2026-01-26 16:53:39,864 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=797223) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.63it/s]
(EngineCore_DP0 pid=797223) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 17.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 18.11it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▍         | 25/512 [00:00<00:02, 241.97it/s]
Adding requests:  11%|█▏        | 58/512 [00:00<00:01, 291.24it/s]
Adding requests:  18%|█▊        | 91/512 [00:00<00:01, 304.21it/s]
Adding requests:  24%|██▍       | 125/512 [00:00<00:01, 315.62it/s]
Adding requests:  31%|███       | 157/512 [00:00<00:01, 316.74it/s]
Adding requests:  38%|███▊      | 194/512 [00:00<00:00, 332.37it/s]
Adding requests:  45%|████▍     | 230/512 [00:00<00:00, 339.09it/s]
Adding requests:  52%|█████▏    | 264/512 [00:00<00:00, 336.36it/s]
Adding requests:  59%|█████▊    | 300/512 [00:00<00:00, 342.58it/s]
Adding requests:  66%|██████▌   | 337/512 [00:01<00:00, 350.21it/s]
Adding requests:  73%|███████▎  | 373/512 [00:01<00:00, 353.08it/s]
Adding requests:  80%|████████  | 410/512 [00:01<00:00, 357.61it/s]
Adding requests:  87%|████████▋ | 446/512 [00:01<00:00, 354.61it/s]
Adding requests:  95%|█████████▍| 485/512 [00:01<00:00, 363.83it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 342.29it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:00<00:00, 1515.97it/s, est. speed input: 1552456.70 toks/s, output: 1515.99 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:01<00:01, 180.39it/s, est. speed input: 213890.55 toks/s, output: 208.88 toks/s]   
Processed prompts:  75%|███████▍  | 383/512 [00:02<00:00, 153.89it/s, est. speed input: 184214.66 toks/s, output: 179.90 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:02<00:00, 141.56it/s, est. speed input: 172167.84 toks/s, output: 168.13 toks/s]
Processed prompts:  89%|████████▉ | 456/512 [00:02<00:00, 137.11it/s, est. speed input: 167482.59 toks/s, output: 163.56 toks/s]
Processed prompts:  94%|█████████▍| 480/512 [00:03<00:00, 131.71it/s, est. speed input: 163350.52 toks/s, output: 159.52 toks/s]
Processed prompts:  98%|█████████▊| 500/512 [00:03<00:00, 127.49it/s, est. speed input: 160357.59 toks/s, output: 156.60 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 127.49it/s, est. speed input: 156285.67 toks/s, output: 152.62 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 152.62it/s, est. speed input: 156285.67 toks/s, output: 152.62 toks/s]
[rank0]:[W126 16:53:46.158383990 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 16:53:48
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:54:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=798476) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=798476) WARNING 01-26 16:54:18 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=798476) WARNING 01-26 16:54:28 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 114.82 requests/s, 117687.27 total tokens/s, 114.82 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 16:54:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:54:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:54:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:54:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:54:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:54:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:54:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:54:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:54:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:54:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:54:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:54:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:54:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:54:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:54:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:54:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:09] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:09] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:09] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:09] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:09] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=798476) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=798476) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.05it/s]
(EngineCore_DP0 pid=798476) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.09s/it]
(EngineCore_DP0 pid=798476) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.07s/it]
(EngineCore_DP0 pid=798476) 
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=798476) [2026-01-26 16:54:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=798476) 2026-01-26 16:54:28,240 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=798476) 2026-01-26 16:54:28,264 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=798476) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 16.01it/s]
(EngineCore_DP0 pid=798476) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 18.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.50it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.44it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 278.84it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:02, 320.52it/s]
Adding requests:   9%|▉         | 96/1024 [00:00<00:02, 317.37it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 322.44it/s]
Adding requests:  16%|█▌        | 166/1024 [00:00<00:02, 332.02it/s]
Adding requests:  20%|█▉        | 203/1024 [00:00<00:02, 343.05it/s]
Adding requests:  23%|██▎       | 239/1024 [00:00<00:02, 347.80it/s]
Adding requests:  27%|██▋       | 274/1024 [00:00<00:02, 347.29it/s]
Adding requests:  30%|███       | 312/1024 [00:00<00:02, 354.93it/s]
Adding requests:  34%|███▍      | 349/1024 [00:01<00:01, 358.63it/s]
Adding requests:  38%|███▊      | 386/1024 [00:01<00:01, 361.91it/s]
Adding requests:  41%|████▏     | 424/1024 [00:01<00:01, 366.55it/s]
Adding requests:  45%|████▌     | 461/1024 [00:01<00:01, 362.92it/s]
Adding requests:  49%|████▉     | 501/1024 [00:01<00:01, 372.08it/s]
Adding requests:  53%|█████▎    | 541/1024 [00:01<00:01, 376.84it/s]
Adding requests:  57%|█████▋    | 579/1024 [00:01<00:01, 372.96it/s]
Adding requests:  60%|██████    | 617/1024 [00:01<00:01, 363.58it/s]
Adding requests:  64%|██████▍   | 654/1024 [00:01<00:01, 357.04it/s]
Adding requests:  68%|██████▊   | 692/1024 [00:01<00:00, 363.37it/s]
Adding requests:  71%|███████   | 729/1024 [00:02<00:00, 354.29it/s]
Adding requests:  75%|███████▍  | 765/1024 [00:02<00:00, 354.79it/s]
Adding requests:  78%|███████▊  | 801/1024 [00:02<00:00, 355.83it/s]
Adding requests:  82%|████████▏ | 839/1024 [00:02<00:00, 361.85it/s]
Adding requests:  86%|████████▌ | 876/1024 [00:02<00:00, 361.88it/s]
Adding requests:  89%|████████▉ | 913/1024 [00:02<00:00, 362.21it/s]
Adding requests:  93%|█████████▎| 950/1024 [00:02<00:00, 357.20it/s]
Adding requests:  96%|█████████▋| 987/1024 [00:02<00:00, 360.08it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 353.76it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 354.86it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:00<00:00, 2257.11it/s, est. speed input: 2311386.97 toks/s, output: 2257.13 toks/s]
Processed prompts:  55%|█████▌    | 564/1024 [00:02<00:02, 226.76it/s, est. speed input: 276996.12 toks/s, output: 270.50 toks/s]   
Processed prompts:  65%|██████▍   | 664/1024 [00:02<00:01, 187.73it/s, est. speed input: 233293.46 toks/s, output: 227.83 toks/s]
Processed prompts:  71%|███████   | 724/1024 [00:03<00:01, 167.10it/s, est. speed input: 213732.15 toks/s, output: 208.72 toks/s]
Processed prompts:  75%|███████▍  | 765/1024 [00:03<00:01, 158.03it/s, est. speed input: 205396.30 toks/s, output: 200.58 toks/s]
Processed prompts:  78%|███████▊  | 796/1024 [00:04<00:01, 150.17it/s, est. speed input: 199404.58 toks/s, output: 194.73 toks/s]
Processed prompts:  80%|████████  | 820/1024 [00:04<00:01, 144.81it/s, est. speed input: 195550.59 toks/s, output: 190.97 toks/s]
Processed prompts:  82%|████████▏ | 840/1024 [00:04<00:01, 144.77it/s, est. speed input: 194064.41 toks/s, output: 189.52 toks/s]
Processed prompts:  84%|████████▍ | 859/1024 [00:04<00:01, 132.97it/s, est. speed input: 189560.92 toks/s, output: 185.12 toks/s]
Processed prompts:  85%|████████▌ | 875/1024 [00:04<00:01, 130.01it/s, est. speed input: 187511.59 toks/s, output: 183.12 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:04<00:01, 125.92it/s, est. speed input: 185401.42 toks/s, output: 181.06 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:05<00:00, 123.76it/s, est. speed input: 183589.46 toks/s, output: 179.29 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:05<00:00, 121.03it/s, est. speed input: 181723.34 toks/s, output: 177.46 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:05<00:00, 119.82it/s, est. speed input: 180105.80 toks/s, output: 175.88 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:05<00:00, 118.31it/s, est. speed input: 178489.15 toks/s, output: 174.31 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:05<00:00, 117.60it/s, est. speed input: 177011.84 toks/s, output: 172.86 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:05<00:00, 117.32it/s, est. speed input: 175635.87 toks/s, output: 171.52 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:05<00:00, 117.01it/s, est. speed input: 174312.96 toks/s, output: 170.23 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:06<00:00, 114.91it/s, est. speed input: 172828.05 toks/s, output: 168.78 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:06<00:00, 114.91it/s, est. speed input: 173843.46 toks/s, output: 169.77 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:06<00:00, 169.77it/s, est. speed input: 173843.46 toks/s, output: 169.77 toks/s]
[rank0]:[W126 16:54:39.522773937 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 16:54:41
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:54:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=799843) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=799843) WARNING 01-26 16:55:16 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=799843) WARNING 01-26 16:55:26 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 118.44 requests/s, 121403.37 total tokens/s, 118.44 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 16:54:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:54:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:54:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:54:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:54:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:54:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:54:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:54:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:54:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:55:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:55:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:55:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:55:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:55:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:55:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:55:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:55:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:55:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:55:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:55:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:55:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:55:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:55:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:07] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:07] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:07] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:07] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:07] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=799843) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=799843) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.04it/s]
(EngineCore_DP0 pid=799843) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.09s/it]
(EngineCore_DP0 pid=799843) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.07s/it]
(EngineCore_DP0 pid=799843) 
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=799843) [2026-01-26 16:55:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=799843) [rank0]:W0126 16:55:21.381000 799843 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=799843) [rank0]:W0126 16:55:21.463000 799843 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=799843) [rank0]:W0126 16:55:22.428000 799843 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=799843) [rank0]:W0126 16:55:22.559000 799843 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=799843) 2026-01-26 16:55:26,023 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=799843) 2026-01-26 16:55:26,049 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=799843) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 12.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 15.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.01it/s]
(EngineCore_DP0 pid=799843) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 17.83it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 18.00it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 17.99it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 279.87it/s]
Adding requests:   3%|▎         | 58/2048 [00:00<00:06, 288.95it/s]
Adding requests:   4%|▍         | 91/2048 [00:00<00:06, 303.97it/s]
Adding requests:   6%|▌         | 125/2048 [00:00<00:06, 316.83it/s]
Adding requests:   8%|▊         | 160/2048 [00:00<00:05, 326.18it/s]
Adding requests:  10%|▉         | 197/2048 [00:00<00:05, 339.15it/s]
Adding requests:  11%|█▏        | 233/2048 [00:00<00:05, 344.52it/s]
Adding requests:  13%|█▎        | 268/2048 [00:00<00:05, 339.45it/s]
Adding requests:  15%|█▍        | 304/2048 [00:00<00:05, 345.53it/s]
Adding requests:  17%|█▋        | 342/2048 [00:01<00:04, 354.40it/s]
Adding requests:  19%|█▊        | 379/2048 [00:01<00:04, 357.43it/s]
Adding requests:  20%|██        | 418/2048 [00:01<00:04, 366.71it/s]
Adding requests:  22%|██▏       | 455/2048 [00:01<00:04, 362.66it/s]
Adding requests:  24%|██▍       | 495/2048 [00:01<00:04, 373.08it/s]
Adding requests:  26%|██▌       | 534/2048 [00:01<00:04, 377.22it/s]
Adding requests:  28%|██▊       | 572/2048 [00:01<00:03, 375.21it/s]
Adding requests:  30%|██▉       | 610/2048 [00:01<00:03, 361.34it/s]
Adding requests:  32%|███▏      | 647/2048 [00:01<00:03, 353.39it/s]
Adding requests:  33%|███▎      | 683/2048 [00:01<00:03, 351.39it/s]
Adding requests:  35%|███▌      | 719/2048 [00:02<00:03, 352.71it/s]
Adding requests:  37%|███▋      | 755/2048 [00:02<00:03, 350.70it/s]
Adding requests:  39%|███▊      | 792/2048 [00:02<00:03, 352.29it/s]
Adding requests:  41%|████      | 830/2048 [00:02<00:03, 359.46it/s]
Adding requests:  42%|████▏     | 867/2048 [00:02<00:03, 360.42it/s]
Adding requests:  44%|████▍     | 905/2048 [00:02<00:03, 366.06it/s]
Adding requests:  46%|████▌     | 942/2048 [00:02<00:03, 357.92it/s]
Adding requests:  48%|████▊     | 979/2048 [00:02<00:02, 358.49it/s]
Adding requests:  50%|████▉     | 1015/2048 [00:02<00:02, 352.92it/s]
Adding requests:  51%|█████▏    | 1051/2048 [00:02<00:02, 354.62it/s]
Adding requests:  53%|█████▎    | 1087/2048 [00:03<00:02, 354.83it/s]
Adding requests:  55%|█████▍    | 1123/2048 [00:03<00:02, 356.06it/s]
Adding requests:  57%|█████▋    | 1159/2048 [00:03<00:02, 354.77it/s]
Adding requests:  58%|█████▊    | 1195/2048 [00:03<00:02, 356.00it/s]
Adding requests:  60%|██████    | 1234/2048 [00:03<00:02, 363.56it/s]
Adding requests:  62%|██████▏   | 1271/2048 [00:03<00:02, 357.60it/s]
Adding requests:  64%|██████▍   | 1307/2048 [00:03<00:02, 356.63it/s]
Adding requests:  66%|██████▌   | 1343/2048 [00:03<00:01, 357.58it/s]
Adding requests:  67%|██████▋   | 1380/2048 [00:03<00:01, 359.58it/s]
Adding requests:  69%|██████▉   | 1416/2048 [00:04<00:01, 357.36it/s]
Adding requests:  71%|███████   | 1453/2048 [00:04<00:01, 360.13it/s]
Adding requests:  73%|███████▎  | 1490/2048 [00:04<00:01, 362.59it/s]
Adding requests:  75%|███████▍  | 1527/2048 [00:04<00:01, 362.62it/s]
Adding requests:  76%|███████▋  | 1564/2048 [00:04<00:01, 358.75it/s]
Adding requests:  78%|███████▊  | 1600/2048 [00:04<00:01, 353.28it/s]
Adding requests:  80%|███████▉  | 1636/2048 [00:04<00:01, 349.15it/s]
Adding requests:  82%|████████▏ | 1671/2048 [00:04<00:01, 334.56it/s]
Adding requests:  83%|████████▎ | 1708/2048 [00:04<00:00, 344.10it/s]
Adding requests:  85%|████████▌ | 1744/2048 [00:04<00:00, 347.31it/s]
Adding requests:  87%|████████▋ | 1779/2048 [00:05<00:00, 344.73it/s]
Adding requests:  89%|████████▊ | 1814/2048 [00:05<00:00, 345.68it/s]
Adding requests:  90%|█████████ | 1850/2048 [00:05<00:00, 348.98it/s]
Adding requests:  92%|█████████▏| 1887/2048 [00:05<00:00, 354.92it/s]
Adding requests:  94%|█████████▍| 1924/2048 [00:05<00:00, 358.06it/s]
Adding requests:  96%|█████████▌| 1962/2048 [00:05<00:00, 361.13it/s]
Adding requests:  98%|█████████▊| 1999/2048 [00:05<00:00, 356.09it/s]
Adding requests:  99%|█████████▉| 2035/2048 [00:05<00:00, 350.56it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 352.85it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  33%|███▎      | 685/2048 [00:00<00:00, 4652.50it/s, est. speed input: 4764411.35 toks/s, output: 4652.57 toks/s]
Processed prompts:  56%|█████▌    | 1151/2048 [00:04<00:03, 235.55it/s, est. speed input: 290436.82 toks/s, output: 283.63 toks/s]  
Processed prompts:  66%|██████▌   | 1350/2048 [00:05<00:03, 194.83it/s, est. speed input: 243941.77 toks/s, output: 238.22 toks/s]
Processed prompts:  71%|███████▏  | 1464/2048 [00:06<00:03, 177.84it/s, est. speed input: 227027.93 toks/s, output: 221.71 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:07<00:03, 164.95it/s, est. speed input: 216560.87 toks/s, output: 211.49 toks/s]
Processed prompts:  78%|███████▊  | 1590/2048 [00:07<00:02, 159.95it/s, est. speed input: 212170.66 toks/s, output: 207.20 toks/s]
Processed prompts:  80%|███████▉  | 1629/2048 [00:08<00:02, 149.44it/s, est. speed input: 206579.82 toks/s, output: 201.74 toks/s]
Processed prompts:  81%|████████  | 1658/2048 [00:08<00:02, 154.28it/s, est. speed input: 206787.42 toks/s, output: 201.94 toks/s]
Processed prompts:  82%|████████▏ | 1685/2048 [00:08<00:02, 144.63it/s, est. speed input: 203436.46 toks/s, output: 198.67 toks/s]
Processed prompts:  83%|████████▎ | 1707/2048 [00:08<00:02, 146.56it/s, est. speed input: 202840.66 toks/s, output: 198.09 toks/s]
Processed prompts:  84%|████████▍ | 1727/2048 [00:08<00:02, 129.02it/s, est. speed input: 198973.90 toks/s, output: 194.31 toks/s]
Processed prompts:  85%|████████▌ | 1743/2048 [00:09<00:02, 128.28it/s, est. speed input: 197938.49 toks/s, output: 193.30 toks/s]
Processed prompts:  86%|████████▌ | 1758/2048 [00:09<00:02, 126.38it/s, est. speed input: 196841.62 toks/s, output: 192.23 toks/s]
Processed prompts:  87%|████████▋ | 1773/2048 [00:09<00:02, 123.31it/s, est. speed input: 195632.66 toks/s, output: 191.05 toks/s]
Processed prompts:  87%|████████▋ | 1789/2048 [00:09<00:02, 123.19it/s, est. speed input: 194664.83 toks/s, output: 190.10 toks/s]
Processed prompts:  88%|████████▊ | 1805/2048 [00:09<00:01, 122.41it/s, est. speed input: 193660.13 toks/s, output: 189.12 toks/s]
Processed prompts:  89%|████████▉ | 1821/2048 [00:09<00:01, 121.45it/s, est. speed input: 192655.51 toks/s, output: 188.14 toks/s]
Processed prompts:  90%|████████▉ | 1837/2048 [00:09<00:01, 120.68it/s, est. speed input: 191676.94 toks/s, output: 187.18 toks/s]
Processed prompts:  90%|█████████ | 1853/2048 [00:09<00:01, 119.83it/s, est. speed input: 190703.45 toks/s, output: 186.23 toks/s]
Processed prompts:  91%|█████████▏| 1869/2048 [00:10<00:01, 119.35it/s, est. speed input: 189766.82 toks/s, output: 185.32 toks/s]
Processed prompts:  92%|█████████▏| 1885/2048 [00:10<00:01, 120.58it/s, est. speed input: 188968.90 toks/s, output: 184.54 toks/s]
Processed prompts:  93%|█████████▎| 1901/2048 [00:10<00:01, 119.79it/s, est. speed input: 188074.31 toks/s, output: 183.67 toks/s]
Processed prompts:  94%|█████████▎| 1917/2048 [00:10<00:01, 117.84it/s, est. speed input: 187106.50 toks/s, output: 182.72 toks/s]
Processed prompts:  94%|█████████▍| 1933/2048 [00:10<00:00, 119.34it/s, est. speed input: 186358.89 toks/s, output: 181.99 toks/s]
Processed prompts:  95%|█████████▌| 1949/2048 [00:10<00:00, 119.72it/s, est. speed input: 185584.07 toks/s, output: 181.23 toks/s]
Processed prompts:  96%|█████████▌| 1965/2048 [00:10<00:00, 118.18it/s, est. speed input: 184711.80 toks/s, output: 180.38 toks/s]
Processed prompts:  97%|█████████▋| 1981/2048 [00:11<00:00, 119.81it/s, est. speed input: 184032.53 toks/s, output: 179.72 toks/s]
Processed prompts:  98%|█████████▊| 1997/2048 [00:11<00:00, 118.99it/s, est. speed input: 183247.88 toks/s, output: 178.95 toks/s]
Processed prompts:  98%|█████████▊| 2013/2048 [00:11<00:00, 118.55it/s, est. speed input: 182489.69 toks/s, output: 178.21 toks/s]
Processed prompts:  99%|█████████▉| 2029/2048 [00:11<00:00, 118.85it/s, est. speed input: 181786.69 toks/s, output: 177.53 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:11<00:00, 118.85it/s, est. speed input: 182594.14 toks/s, output: 178.31 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:11<00:00, 178.31it/s, est. speed input: 182594.14 toks/s, output: 178.31 toks/s]
[rank0]:[W126 16:55:45.150344684 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 16:55:47
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:56:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=801471) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=801471) WARNING 01-26 16:56:35 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=801471) WARNING 01-26 16:56:45 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 121.44 requests/s, 124476.10 total tokens/s, 121.44 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 16:56:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:56:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:56:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:56:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:56:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:56:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:56:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:56:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:56:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:56:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:56:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:56:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:56:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:56:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:56:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:56:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:56:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=801471) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=801471) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.03it/s]
(EngineCore_DP0 pid=801471) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]
(EngineCore_DP0 pid=801471) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.08s/it]
(EngineCore_DP0 pid=801471) 
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=801471) [2026-01-26 16:56:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=801471) [rank0]:W0126 16:56:40.414000 801471 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=801471) [rank0]:W0126 16:56:40.494000 801471 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=801471) [rank0]:W0126 16:56:41.452000 801471 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=801471) [rank0]:W0126 16:56:41.579000 801471 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=801471) 2026-01-26 16:56:45,221 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=801471) 2026-01-26 16:56:45,247 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=801471) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 15.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 16.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 16.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 17.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 16.13it/s]
(EngineCore_DP0 pid=801471) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.79it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.59it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 17.23it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 17.28it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 275.87it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 318.04it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 315.14it/s]
Adding requests:   3%|▎         | 128/4096 [00:00<00:12, 320.77it/s]
Adding requests:   4%|▍         | 163/4096 [00:00<00:11, 330.79it/s]
Adding requests:   5%|▍         | 197/4096 [00:00<00:11, 327.94it/s]
Adding requests:   6%|▌         | 232/4096 [00:00<00:11, 333.94it/s]
Adding requests:   7%|▋         | 267/4096 [00:00<00:11, 336.18it/s]
Adding requests:   7%|▋         | 303/4096 [00:00<00:11, 341.15it/s]
Adding requests:   8%|▊         | 340/4096 [00:01<00:10, 348.41it/s]
Adding requests:   9%|▉         | 377/4096 [00:01<00:10, 352.63it/s]
Adding requests:  10%|█         | 415/4096 [00:01<00:10, 358.90it/s]
Adding requests:  11%|█         | 451/4096 [00:01<00:10, 356.03it/s]
Adding requests:  12%|█▏        | 491/4096 [00:01<00:09, 368.65it/s]
Adding requests:  13%|█▎        | 530/4096 [00:01<00:09, 374.93it/s]
Adding requests:  14%|█▍        | 568/4096 [00:01<00:09, 372.31it/s]
Adding requests:  15%|█▍        | 606/4096 [00:01<00:09, 358.91it/s]
Adding requests:  16%|█▌        | 643/4096 [00:01<00:09, 355.80it/s]
Adding requests:  17%|█▋        | 679/4096 [00:01<00:09, 354.67it/s]
Adding requests:  17%|█▋        | 716/4096 [00:02<00:09, 356.23it/s]
Adding requests:  18%|█▊        | 752/4096 [00:02<00:09, 346.10it/s]
Adding requests:  19%|█▉        | 788/4096 [00:02<00:09, 348.34it/s]
Adding requests:  20%|██        | 824/4096 [00:02<00:09, 350.21it/s]
Adding requests:  21%|██        | 862/4096 [00:02<00:09, 356.85it/s]
Adding requests:  22%|██▏       | 898/4096 [00:02<00:09, 354.12it/s]
Adding requests:  23%|██▎       | 934/4096 [00:02<00:09, 349.00it/s]
Adding requests:  24%|██▎       | 970/4096 [00:02<00:08, 352.08it/s]
Adding requests:  25%|██▍       | 1006/4096 [00:02<00:08, 348.74it/s]
Adding requests:  25%|██▌       | 1042/4096 [00:02<00:08, 351.16it/s]
Adding requests:  26%|██▋       | 1078/4096 [00:03<00:08, 348.25it/s]
Adding requests:  27%|██▋       | 1114/4096 [00:03<00:08, 349.15it/s]
Adding requests:  28%|██▊       | 1150/4096 [00:03<00:08, 351.39it/s]
Adding requests:  29%|██▉       | 1186/4096 [00:03<00:08, 351.24it/s]
Adding requests:  30%|██▉       | 1224/4096 [00:03<00:08, 358.90it/s]
Adding requests:  31%|███       | 1260/4096 [00:03<00:08, 353.94it/s]
Adding requests:  32%|███▏      | 1296/4096 [00:03<00:08, 349.52it/s]
Adding requests:  33%|███▎      | 1332/4096 [00:03<00:07, 352.21it/s]
Adding requests:  33%|███▎      | 1369/4096 [00:03<00:07, 356.15it/s]
Adding requests:  34%|███▍      | 1405/4096 [00:04<00:07, 354.74it/s]
Adding requests:  35%|███▌      | 1441/4096 [00:04<00:07, 354.51it/s]
Adding requests:  36%|███▌      | 1477/4096 [00:04<00:07, 354.61it/s]
Adding requests:  37%|███▋      | 1515/4096 [00:04<00:07, 359.55it/s]
Adding requests:  38%|███▊      | 1551/4096 [00:04<00:07, 357.03it/s]
Adding requests:  39%|███▊      | 1587/4096 [00:04<00:07, 349.55it/s]
Adding requests:  40%|███▉      | 1622/4096 [00:04<00:07, 345.31it/s]
Adding requests:  40%|████      | 1657/4096 [00:04<00:07, 340.30it/s]
Adding requests:  41%|████▏     | 1693/4096 [00:04<00:07, 342.13it/s]
Adding requests:  42%|████▏     | 1730/4096 [00:04<00:06, 347.47it/s]
Adding requests:  43%|████▎     | 1768/4096 [00:05<00:06, 355.24it/s]
Adding requests:  44%|████▍     | 1804/4096 [00:05<00:06, 352.59it/s]
Adding requests:  45%|████▍     | 1840/4096 [00:05<00:06, 352.97it/s]
Adding requests:  46%|████▌     | 1876/4096 [00:05<00:06, 352.69it/s]
Adding requests:  47%|████▋     | 1913/4096 [00:05<00:06, 357.38it/s]
Adding requests:  48%|████▊     | 1951/4096 [00:05<00:05, 362.93it/s]
Adding requests:  49%|████▊     | 1988/4096 [00:05<00:05, 358.87it/s]
Adding requests:  49%|████▉     | 2024/4096 [00:05<00:06, 341.02it/s]
Adding requests:  50%|█████     | 2059/4096 [00:05<00:05, 341.95it/s]
Adding requests:  51%|█████     | 2094/4096 [00:05<00:05, 341.03it/s]
Adding requests:  52%|█████▏    | 2129/4096 [00:06<00:05, 343.54it/s]
Adding requests:  53%|█████▎    | 2164/4096 [00:06<00:05, 342.12it/s]
Adding requests:  54%|█████▎    | 2199/4096 [00:06<00:05, 338.03it/s]
Adding requests:  55%|█████▍    | 2235/4096 [00:06<00:05, 343.47it/s]
Adding requests:  55%|█████▌    | 2272/4096 [00:06<00:05, 350.43it/s]
Adding requests:  56%|█████▋    | 2308/4096 [00:06<00:05, 352.00it/s]
Adding requests:  57%|█████▋    | 2345/4096 [00:06<00:04, 354.58it/s]
Adding requests:  58%|█████▊    | 2381/4096 [00:06<00:04, 355.23it/s]
Adding requests:  59%|█████▉    | 2419/4096 [00:06<00:04, 361.15it/s]
Adding requests:  60%|█████▉    | 2456/4096 [00:07<00:04, 358.33it/s]
Adding requests:  61%|██████    | 2493/4096 [00:07<00:04, 360.17it/s]
Adding requests:  62%|██████▏   | 2531/4096 [00:07<00:04, 364.25it/s]
Adding requests:  63%|██████▎   | 2571/4096 [00:07<00:04, 372.15it/s]
Adding requests:  64%|██████▎   | 2609/4096 [00:07<00:04, 370.75it/s]
Adding requests:  65%|██████▍   | 2647/4096 [00:07<00:04, 361.24it/s]
Adding requests:  66%|██████▌   | 2684/4096 [00:07<00:03, 355.09it/s]
Adding requests:  66%|██████▋   | 2720/4096 [00:07<00:03, 354.00it/s]
Adding requests:  67%|██████▋   | 2758/4096 [00:07<00:03, 359.63it/s]
Adding requests:  68%|██████▊   | 2796/4096 [00:07<00:03, 364.75it/s]
Adding requests:  69%|██████▉   | 2833/4096 [00:08<00:03, 362.57it/s]
Adding requests:  70%|███████   | 2870/4096 [00:08<00:03, 360.22it/s]
Adding requests:  71%|███████   | 2907/4096 [00:08<00:03, 360.79it/s]
Adding requests:  72%|███████▏  | 2945/4096 [00:08<00:03, 365.19it/s]
Adding requests:  73%|███████▎  | 2982/4096 [00:08<00:03, 361.49it/s]
Adding requests:  74%|███████▎  | 3020/4096 [00:08<00:02, 365.14it/s]
Adding requests:  75%|███████▍  | 3058/4096 [00:08<00:02, 367.89it/s]
Adding requests:  76%|███████▌  | 3095/4096 [00:08<00:02, 357.32it/s]
Adding requests:  76%|███████▋  | 3133/4096 [00:08<00:02, 363.39it/s]
Adding requests:  77%|███████▋  | 3170/4096 [00:08<00:02, 357.66it/s]
Adding requests:  78%|███████▊  | 3206/4096 [00:09<00:02, 354.47it/s]
Adding requests:  79%|███████▉  | 3243/4096 [00:09<00:02, 358.65it/s]
Adding requests:  80%|████████  | 3279/4096 [00:09<00:02, 354.21it/s]
Adding requests:  81%|████████  | 3315/4096 [00:09<00:02, 337.17it/s]
Adding requests:  82%|████████▏ | 3351/4096 [00:09<00:02, 342.28it/s]
Adding requests:  83%|████████▎ | 3388/4096 [00:09<00:02, 349.85it/s]
Adding requests:  84%|████████▎ | 3424/4096 [00:09<00:01, 351.08it/s]
Adding requests:  84%|████████▍ | 3461/4096 [00:09<00:01, 354.41it/s]
Adding requests:  85%|████████▌ | 3497/4096 [00:09<00:01, 352.42it/s]
Adding requests:  86%|████████▋ | 3536/4096 [00:10<00:01, 363.07it/s]
Adding requests:  87%|████████▋ | 3573/4096 [00:10<00:01, 362.39it/s]
Adding requests:  88%|████████▊ | 3610/4096 [00:10<00:01, 362.77it/s]
Adding requests:  89%|████████▉ | 3648/4096 [00:10<00:01, 365.28it/s]
Adding requests:  90%|████████▉ | 3685/4096 [00:10<00:01, 354.31it/s]
Adding requests:  91%|█████████ | 3722/4096 [00:10<00:01, 358.16it/s]
Adding requests:  92%|█████████▏| 3758/4096 [00:10<00:00, 350.41it/s]
Adding requests:  93%|█████████▎| 3794/4096 [00:10<00:00, 339.66it/s]
Adding requests:  93%|█████████▎| 3829/4096 [00:10<00:00, 338.18it/s]
Adding requests:  94%|█████████▍| 3864/4096 [00:10<00:00, 341.45it/s]
Adding requests:  95%|█████████▌| 3899/4096 [00:11<00:00, 341.79it/s]
Adding requests:  96%|█████████▌| 3934/4096 [00:11<00:00, 338.52it/s]
Adding requests:  97%|█████████▋| 3969/4096 [00:11<00:00, 341.37it/s]
Adding requests:  98%|█████████▊| 4004/4096 [00:11<00:00, 342.65it/s]
Adding requests:  99%|█████████▊| 4040/4096 [00:11<00:00, 344.38it/s]
Adding requests:  99%|█████████▉| 4075/4096 [00:11<00:00, 343.28it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 351.76it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  34%|███▍      | 1389/4096 [00:00<00:00, 12328.49it/s, est. speed input: 12625305.96 toks/s, output: 12328.72 toks/s]
Processed prompts:  64%|██████▍   | 2622/4096 [00:10<00:06, 219.86it/s, est. speed input: 266774.30 toks/s, output: 260.52 toks/s]      
Processed prompts:  77%|███████▋  | 3137/4096 [00:14<00:05, 183.32it/s, est. speed input: 225055.05 toks/s, output: 219.78 toks/s]
Processed prompts:  84%|████████▎ | 3423/4096 [00:16<00:03, 168.57it/s, est. speed input: 210419.25 toks/s, output: 205.49 toks/s]
Processed prompts:  88%|████████▊ | 3601/4096 [00:18<00:03, 158.54it/s, est. speed input: 202225.61 toks/s, output: 197.49 toks/s]
Processed prompts:  91%|█████████ | 3719/4096 [00:19<00:02, 157.63it/s, est. speed input: 200241.18 toks/s, output: 195.55 toks/s]
Processed prompts:  93%|█████████▎| 3802/4096 [00:19<00:01, 150.33it/s, est. speed input: 196578.29 toks/s, output: 191.97 toks/s]
Processed prompts:  94%|█████████▍| 3862/4096 [00:20<00:01, 145.98it/s, est. speed input: 194492.57 toks/s, output: 189.93 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [00:20<00:01, 147.64it/s, est. speed input: 194196.22 toks/s, output: 189.64 toks/s]
Processed prompts:  96%|█████████▌| 3942/4096 [00:20<00:01, 147.48it/s, est. speed input: 193667.67 toks/s, output: 189.13 toks/s]
Processed prompts:  97%|█████████▋| 3971/4096 [00:21<00:00, 142.96it/s, est. speed input: 192681.28 toks/s, output: 188.17 toks/s]
Processed prompts:  98%|█████████▊| 3995/4096 [00:21<00:00, 135.48it/s, est. speed input: 191503.97 toks/s, output: 187.02 toks/s]
Processed prompts:  98%|█████████▊| 4015/4096 [00:21<00:00, 124.75it/s, est. speed input: 190112.64 toks/s, output: 185.66 toks/s]
Processed prompts:  99%|█████████▉| 4045/4096 [00:21<00:00, 119.93it/s, est. speed input: 188985.81 toks/s, output: 184.56 toks/s]
Processed prompts: 100%|█████████▉| 4077/4096 [00:22<00:00, 131.22it/s, est. speed input: 189063.58 toks/s, output: 184.63 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:22<00:00, 131.22it/s, est. speed input: 189942.26 toks/s, output: 185.49 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:22<00:00, 185.49it/s, est. speed input: 189942.26 toks/s, output: 185.49 toks/s]
[rank0]:[W126 16:57:22.301887886 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 16:57:24
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:58:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=803677) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=803677) WARNING 01-26 16:58:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     def forward(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     raise e
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/tmp/torchinductor_root/g6/cg6eknmfwhlfecxv6oq5xol74qjrpwt77qbf3blpnjezrum6e5qc.py", line 1093, in call
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 10)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 221, in quant_slide_fp8_triton
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) ERROR 01-26 16:58:42 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered

STDERR:
[2026-01-26 16:58:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:58:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:58:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:58:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:58:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:58:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:58:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:58:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:58:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:58:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:58:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 16:58:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 16:58:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:58:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:58:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:58:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:58:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:27] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:27] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:27] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:27] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:27] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=803677) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=803677) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.04it/s]
(EngineCore_DP0 pid=803677) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.09s/it]
(EngineCore_DP0 pid=803677) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.07s/it]
(EngineCore_DP0 pid=803677) 
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=803677) [2026-01-26 16:58:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=803677) [rank0]:W0126 16:58:41.065000 803677 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=803677) [rank0]:W0126 16:58:41.145000 803677 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=803677) [rank0]:W0126 16:58:42.203000 803677 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=803677) [rank0]:W0126 16:58:42.325000 803677 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=803677) Process EngineCore_DP0:
(EngineCore_DP0 pid=803677) Traceback (most recent call last):
(EngineCore_DP0 pid=803677)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=803677)     self.run()
(EngineCore_DP0 pid=803677)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=803677)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=803677)     raise e
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=803677)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=803677)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=803677)     super().__init__(
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=803677)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=803677)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=803677)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=803677)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=803677)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=803677)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=803677)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=803677)     return func(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=803677)     return func(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=803677)     self.model_runner.profile_run()
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=803677)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=803677)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=803677)     return func(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=803677)     outputs = self.model(
(EngineCore_DP0 pid=803677)               ^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=803677)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=803677)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=803677)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=803677)     hidden_states = self.model(
(EngineCore_DP0 pid=803677)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=803677)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=803677)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=803677)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=803677)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=803677)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=803677)     def forward(
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=803677)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=803677)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=803677)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=803677)     raise e
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=803677)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=803677)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=803677)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=803677)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=803677)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=803677)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=803677)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=803677)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=803677)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=803677)     return compiled_fn(full_args)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=803677)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=803677)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=803677)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=803677)                             ^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=803677)     outs = compiled_fn(args)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=803677)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=803677)     return self.current_callable(inputs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=803677)     out = model(new_inputs)
(EngineCore_DP0 pid=803677)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/tmp/torchinductor_root/g6/cg6eknmfwhlfecxv6oq5xol74qjrpwt77qbf3blpnjezrum6e5qc.py", line 1093, in call
(EngineCore_DP0 pid=803677)     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 10)
(EngineCore_DP0 pid=803677)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=803677)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=803677)     return fn(input, L)
(EngineCore_DP0 pid=803677)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 221, in quant_slide_fp8_triton
(EngineCore_DP0 pid=803677)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=803677)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=803677)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=803677)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=803677)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=803677)     self._init_handles()
(EngineCore_DP0 pid=803677)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=803677)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=803677)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=803677) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 16:58:43.389307136 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-26 18:49:50
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:49:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=936842) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=936842) WARNING 01-26 18:50:21 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=936842) WARNING 01-26 18:50:35 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 23.03 requests/s, 11814.64 total tokens/s, 23.03 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 18:49:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:49:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:49:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:49:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:49:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:49:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:49:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:49:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:49:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:49:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:49:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:49:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:49:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:49:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:50:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:50:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:50:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:50:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:50:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:50:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:50:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:50:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:50:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:50:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:50:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:50:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:50:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:50:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=936842) [2026-01-26 18:50:05] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=936842) [2026-01-26 18:50:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=936842) [2026-01-26 18:50:05] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=936842) [2026-01-26 18:50:05] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=936842) [2026-01-26 18:50:05] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=936842) [2026-01-26 18:50:05] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=936842) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=936842) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.36s/it]
(EngineCore_DP0 pid=936842) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]
(EngineCore_DP0 pid=936842) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
(EngineCore_DP0 pid=936842) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.21s/it]
(EngineCore_DP0 pid=936842) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
(EngineCore_DP0 pid=936842) 
(EngineCore_DP0 pid=936842) [2026-01-26 18:50:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=936842) [2026-01-26 18:50:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=936842) [2026-01-26 18:50:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=936842) [2026-01-26 18:50:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=936842) [2026-01-26 18:50:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=936842) [2026-01-26 18:50:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=936842) [2026-01-26 18:50:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=936842) [2026-01-26 18:50:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=936842) 2026-01-26 18:50:35,694 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=936842) 2026-01-26 18:50:35,733 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=936842) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.90it/s]
(EngineCore_DP0 pid=936842) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.97it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  37%|███▋      | 47/128 [00:00<00:00, 460.85it/s]
Adding requests:  80%|████████  | 103/128 [00:00<00:00, 513.85it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 514.64it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:05, 22.16it/s, est. speed input: 11346.32 toks/s, output: 22.16 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:05, 23.37it/s, est. speed input: 11867.77 toks/s, output: 23.18 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 23.81it/s, est. speed input: 12063.40 toks/s, output: 23.56 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:04, 23.99it/s, est. speed input: 12151.00 toks/s, output: 23.73 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:04, 24.12it/s, est. speed input: 12212.94 toks/s, output: 23.85 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:04, 24.18it/s, est. speed input: 12250.87 toks/s, output: 23.93 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:04, 24.19it/s, est. speed input: 12271.91 toks/s, output: 23.97 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:04, 24.22it/s, est. speed input: 12290.82 toks/s, output: 24.01 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:04, 24.31it/s, est. speed input: 12319.85 toks/s, output: 24.06 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:04, 24.33it/s, est. speed input: 12335.80 toks/s, output: 24.09 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:03, 24.31it/s, est. speed input: 12343.77 toks/s, output: 24.11 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:03, 24.31it/s, est. speed input: 12351.89 toks/s, output: 24.12 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:03, 24.26it/s, est. speed input: 12352.92 toks/s, output: 24.13 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:03, 24.24it/s, est. speed input: 12354.97 toks/s, output: 24.13 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:03, 24.26it/s, est. speed input: 12361.21 toks/s, output: 24.14 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:03, 24.27it/s, est. speed input: 12365.74 toks/s, output: 24.15 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:03, 24.26it/s, est. speed input: 12368.87 toks/s, output: 24.16 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:02<00:03, 24.29it/s, est. speed input: 12374.14 toks/s, output: 24.17 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:02<00:02, 24.31it/s, est. speed input: 12378.89 toks/s, output: 24.18 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:02<00:02, 24.31it/s, est. speed input: 12382.90 toks/s, output: 24.19 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:02<00:02, 24.09it/s, est. speed input: 12368.03 toks/s, output: 24.16 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:02<00:02, 24.15it/s, est. speed input: 12370.65 toks/s, output: 24.16 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:02, 24.21it/s, est. speed input: 12375.24 toks/s, output: 24.17 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:02<00:02, 24.27it/s, est. speed input: 12379.88 toks/s, output: 24.18 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:03<00:02, 23.99it/s, est. speed input: 12363.11 toks/s, output: 24.15 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:03<00:02, 24.13it/s, est. speed input: 12369.31 toks/s, output: 24.16 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:03<00:01, 24.18it/s, est. speed input: 12371.69 toks/s, output: 24.16 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:03<00:01, 24.15it/s, est. speed input: 12370.15 toks/s, output: 24.16 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:03<00:01, 23.79it/s, est. speed input: 12348.53 toks/s, output: 24.12 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:03<00:01, 23.94it/s, est. speed input: 12351.47 toks/s, output: 24.12 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:03<00:01, 24.04it/s, est. speed input: 12353.78 toks/s, output: 24.13 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:03<00:01, 24.08it/s, est. speed input: 12354.91 toks/s, output: 24.13 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:04<00:01, 24.10it/s, est. speed input: 12355.22 toks/s, output: 24.13 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:04<00:01, 23.87it/s, est. speed input: 12343.14 toks/s, output: 24.11 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:04<00:00, 23.92it/s, est. speed input: 12341.77 toks/s, output: 24.10 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:04<00:00, 23.97it/s, est. speed input: 12341.60 toks/s, output: 24.10 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:04<00:00, 24.02it/s, est. speed input: 12341.94 toks/s, output: 24.11 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:04<00:00, 23.93it/s, est. speed input: 12336.69 toks/s, output: 24.10 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:04<00:00, 24.02it/s, est. speed input: 12338.67 toks/s, output: 24.10 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:04<00:00, 24.12it/s, est. speed input: 12341.85 toks/s, output: 24.11 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:05<00:00, 24.18it/s, est. speed input: 12344.50 toks/s, output: 24.11 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:05<00:00, 24.20it/s, est. speed input: 12346.04 toks/s, output: 24.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 24.20it/s, est. speed input: 12346.59 toks/s, output: 24.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 24.11it/s, est. speed input: 12346.59 toks/s, output: 24.11 toks/s]
[rank0]:[W126 18:50:43.195367914 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 18:50:45
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:50:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=938217) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=938217) WARNING 01-26 18:51:16 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=938217) WARNING 01-26 18:51:30 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 23.39 requests/s, 23974.74 total tokens/s, 23.39 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 18:50:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:50:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:50:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:50:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:50:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:50:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:50:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:50:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:50:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:50:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:50:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:50:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:50:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:50:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:51:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:51:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:51:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:51:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:51:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:51:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:51:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:51:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=938217) [2026-01-26 18:51:01] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=938217) [2026-01-26 18:51:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=938217) [2026-01-26 18:51:01] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=938217) [2026-01-26 18:51:01] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=938217) [2026-01-26 18:51:01] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=938217) [2026-01-26 18:51:01] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=938217) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=938217) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.53s/it]
(EngineCore_DP0 pid=938217) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.20it/s]
(EngineCore_DP0 pid=938217) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.04it/s]
(EngineCore_DP0 pid=938217) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.18s/it]
(EngineCore_DP0 pid=938217) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
(EngineCore_DP0 pid=938217) 
(EngineCore_DP0 pid=938217) [2026-01-26 18:51:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=938217) [2026-01-26 18:51:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=938217) [2026-01-26 18:51:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=938217) [2026-01-26 18:51:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=938217) [2026-01-26 18:51:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=938217) [2026-01-26 18:51:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=938217) [2026-01-26 18:51:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=938217) [2026-01-26 18:51:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=938217) 2026-01-26 18:51:30,583 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=938217) 2026-01-26 18:51:30,621 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=938217) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.58it/s]
(EngineCore_DP0 pid=938217) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 10.18it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  21%|██        | 27/128 [00:00<00:00, 263.18it/s]
Adding requests:  46%|████▌     | 59/128 [00:00<00:00, 295.42it/s]
Adding requests:  70%|███████   | 90/128 [00:00<00:00, 299.74it/s]
Adding requests:  94%|█████████▍| 120/128 [00:00<00:00, 159.25it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 194.78it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:00, 129.17it/s, est. speed input: 132278.06 toks/s, output: 129.17 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 39.02it/s, est. speed input: 45335.29 toks/s, output: 44.27 toks/s]   
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 32.97it/s, est. speed input: 38981.61 toks/s, output: 38.07 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 30.32it/s, est. speed input: 36378.96 toks/s, output: 35.53 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 28.71it/s, est. speed input: 34891.20 toks/s, output: 34.07 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 27.43it/s, est. speed input: 33726.15 toks/s, output: 32.94 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 26.64it/s, est. speed input: 33010.35 toks/s, output: 32.24 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:02, 25.66it/s, est. speed input: 32281.19 toks/s, output: 31.52 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:02, 25.18it/s, est. speed input: 31755.12 toks/s, output: 31.01 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:02<00:02, 24.84it/s, est. speed input: 31301.62 toks/s, output: 30.57 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:02, 24.53it/s, est. speed input: 30889.62 toks/s, output: 30.17 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:02<00:02, 24.06it/s, est. speed input: 30463.03 toks/s, output: 29.75 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:02, 23.94it/s, est. speed input: 30135.56 toks/s, output: 29.43 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:02, 23.91it/s, est. speed input: 29851.03 toks/s, output: 29.15 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:02, 23.85it/s, est. speed input: 29586.67 toks/s, output: 28.89 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:02, 23.85it/s, est. speed input: 29352.66 toks/s, output: 28.66 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 23.81it/s, est. speed input: 29132.78 toks/s, output: 28.45 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:03<00:01, 23.76it/s, est. speed input: 28927.51 toks/s, output: 28.25 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:03<00:01, 23.72it/s, est. speed input: 28738.71 toks/s, output: 28.07 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:03<00:01, 23.74it/s, est. speed input: 28570.98 toks/s, output: 27.90 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:03<00:01, 23.72it/s, est. speed input: 28410.73 toks/s, output: 27.74 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:03<00:01, 23.70it/s, est. speed input: 28261.46 toks/s, output: 27.60 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:01, 23.67it/s, est. speed input: 28118.97 toks/s, output: 27.46 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:03<00:01, 23.64it/s, est. speed input: 27985.67 toks/s, output: 27.33 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:03<00:00, 23.64it/s, est. speed input: 27864.48 toks/s, output: 27.21 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:04<00:00, 23.71it/s, est. speed input: 27758.07 toks/s, output: 27.11 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:04<00:00, 23.69it/s, est. speed input: 27651.17 toks/s, output: 27.00 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:04<00:00, 23.74it/s, est. speed input: 27556.65 toks/s, output: 26.91 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:04<00:00, 23.79it/s, est. speed input: 27469.74 toks/s, output: 26.83 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:04<00:00, 23.80it/s, est. speed input: 27384.62 toks/s, output: 26.74 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:04<00:00, 23.75it/s, est. speed input: 27298.87 toks/s, output: 26.66 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 23.81it/s, est. speed input: 27227.05 toks/s, output: 26.59 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 23.81it/s, est. speed input: 27227.05 toks/s, output: 26.59 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 26.59it/s, est. speed input: 27227.05 toks/s, output: 26.59 toks/s]
[rank0]:[W126 18:51:38.281508081 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 18:51:39
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:51:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=939520) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=939520) WARNING 01-26 18:52:10 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=939520) WARNING 01-26 18:52:24 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 47.26 requests/s, 48436.74 total tokens/s, 47.26 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 18:51:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:51:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:51:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:51:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:51:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:51:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:51:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:51:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:51:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:51:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:51:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:51:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:51:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:51:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:51:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:51:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:51:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=939520) [2026-01-26 18:51:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=939520) [2026-01-26 18:51:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=939520) [2026-01-26 18:51:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=939520) [2026-01-26 18:51:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=939520) [2026-01-26 18:51:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=939520) [2026-01-26 18:51:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=939520) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=939520) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.40s/it]
(EngineCore_DP0 pid=939520) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]
(EngineCore_DP0 pid=939520) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.07it/s]
(EngineCore_DP0 pid=939520) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
(EngineCore_DP0 pid=939520) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
(EngineCore_DP0 pid=939520) 
(EngineCore_DP0 pid=939520) [2026-01-26 18:52:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=939520) [2026-01-26 18:52:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=939520) [2026-01-26 18:52:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=939520) [2026-01-26 18:52:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=939520) [2026-01-26 18:52:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=939520) [2026-01-26 18:52:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=939520) [2026-01-26 18:52:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=939520) [2026-01-26 18:52:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=939520) 2026-01-26 18:52:24,685 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=939520) 2026-01-26 18:52:24,723 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=939520) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  9.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.43it/s]
(EngineCore_DP0 pid=939520) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.06it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█         | 28/256 [00:00<00:00, 273.69it/s]
Adding requests:  25%|██▍       | 63/256 [00:00<00:00, 316.95it/s]
Adding requests:  37%|███▋      | 95/256 [00:00<00:00, 314.41it/s]
Adding requests:  50%|████▉     | 127/256 [00:00<00:00, 177.65it/s]
Adding requests:  63%|██████▎   | 161/256 [00:00<00:00, 215.68it/s]
Adding requests:  77%|███████▋  | 197/256 [00:00<00:00, 251.35it/s]
Adding requests:  91%|█████████ | 233/256 [00:00<00:00, 277.81it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 260.81it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:00<00:00, 353.35it/s, est. speed input: 361851.04 toks/s, output: 353.36 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:00<00:02, 82.64it/s, est. speed input: 97146.92 toks/s, output: 94.87 toks/s]   
Processed prompts:  39%|███▉      | 100/256 [00:01<00:02, 69.65it/s, est. speed input: 83221.85 toks/s, output: 81.27 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:01<00:02, 64.11it/s, est. speed input: 77766.80 toks/s, output: 75.94 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:01<00:02, 60.30it/s, est. speed input: 74336.76 toks/s, output: 72.59 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:01<00:02, 57.88it/s, est. speed input: 72215.57 toks/s, output: 70.52 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:01<00:02, 57.83it/s, est. speed input: 71396.64 toks/s, output: 69.72 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:02<00:02, 53.95it/s, est. speed input: 69318.23 toks/s, output: 67.69 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:02<00:02, 52.84it/s, est. speed input: 68274.18 toks/s, output: 66.67 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:02<00:01, 51.71it/s, est. speed input: 67281.43 toks/s, output: 65.70 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:02<00:01, 50.44it/s, est. speed input: 66295.22 toks/s, output: 64.74 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:02<00:01, 50.12it/s, est. speed input: 65556.92 toks/s, output: 64.02 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:02<00:01, 49.96it/s, est. speed input: 64903.29 toks/s, output: 63.38 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:02<00:01, 49.78it/s, est. speed input: 64290.98 toks/s, output: 62.78 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:02<00:01, 49.62it/s, est. speed input: 63724.60 toks/s, output: 62.23 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:03<00:01, 49.49it/s, est. speed input: 63201.23 toks/s, output: 61.72 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:03<00:01, 49.41it/s, est. speed input: 62717.44 toks/s, output: 61.25 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:03<00:01, 48.18it/s, est. speed input: 62084.08 toks/s, output: 60.63 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:03<00:00, 47.61it/s, est. speed input: 61540.17 toks/s, output: 60.10 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:03<00:00, 47.98it/s, est. speed input: 61150.85 toks/s, output: 59.72 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:03<00:00, 48.30it/s, est. speed input: 60793.44 toks/s, output: 59.37 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:03<00:00, 48.52it/s, est. speed input: 60458.29 toks/s, output: 59.04 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:03<00:00, 48.65it/s, est. speed input: 60141.04 toks/s, output: 58.73 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:04<00:00, 48.75it/s, est. speed input: 59842.86 toks/s, output: 58.44 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:04<00:00, 48.81it/s, est. speed input: 59561.83 toks/s, output: 58.17 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:04<00:00, 48.81it/s, est. speed input: 59291.37 toks/s, output: 57.90 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 48.81it/s, est. speed input: 59111.79 toks/s, output: 57.73 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 57.73it/s, est. speed input: 59111.79 toks/s, output: 57.73 toks/s]
[rank0]:[W126 18:52:32.511287206 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 18:52:34
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:52:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=940832) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=940832) WARNING 01-26 18:53:05 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=940832) WARNING 01-26 18:53:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 56.90 requests/s, 58322.41 total tokens/s, 56.90 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 18:52:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:52:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:52:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:52:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:52:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:52:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:52:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:52:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:52:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:52:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:52:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:52:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:52:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:52:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:52:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:52:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:52:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:52:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:52:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:52:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:52:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:52:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:52:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:52:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:52:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:52:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:52:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:52:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=940832) [2026-01-26 18:52:51] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=940832) [2026-01-26 18:52:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=940832) [2026-01-26 18:52:51] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=940832) [2026-01-26 18:52:51] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=940832) [2026-01-26 18:52:51] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=940832) [2026-01-26 18:52:51] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=940832) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=940832) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.20s/it]
(EngineCore_DP0 pid=940832) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.43it/s]
(EngineCore_DP0 pid=940832) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.11it/s]
(EngineCore_DP0 pid=940832) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
(EngineCore_DP0 pid=940832) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.05s/it]
(EngineCore_DP0 pid=940832) 
(EngineCore_DP0 pid=940832) [2026-01-26 18:52:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=940832) [2026-01-26 18:52:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=940832) [2026-01-26 18:52:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=940832) [2026-01-26 18:52:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=940832) [2026-01-26 18:52:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=940832) [2026-01-26 18:52:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=940832) [2026-01-26 18:52:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=940832) [2026-01-26 18:52:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=940832) 2026-01-26 18:53:20,998 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=940832) 2026-01-26 18:53:21,036 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=940832) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.79it/s]
(EngineCore_DP0 pid=940832) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 11.10it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 11.17it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 23/512 [00:00<00:02, 227.68it/s]
Adding requests:  11%|█         | 55/512 [00:00<00:01, 280.41it/s]
Adding requests:  17%|█▋        | 88/512 [00:00<00:01, 301.02it/s]
Adding requests:  24%|██▎       | 121/512 [00:00<00:01, 311.74it/s]
Adding requests:  30%|███       | 154/512 [00:00<00:01, 316.90it/s]
Adding requests:  37%|███▋      | 191/512 [00:00<00:00, 333.01it/s]
Adding requests:  44%|████▍     | 227/512 [00:00<00:00, 338.84it/s]
Adding requests:  51%|█████     | 262/512 [00:00<00:00, 338.92it/s]
Adding requests:  58%|█████▊    | 298/512 [00:00<00:00, 344.01it/s]
Adding requests:  65%|██████▌   | 335/512 [00:01<00:00, 351.46it/s]
Adding requests:  72%|███████▏  | 371/512 [00:01<00:00, 353.52it/s]
Adding requests:  80%|███████▉  | 408/512 [00:01<00:00, 358.19it/s]
Adding requests:  87%|████████▋ | 444/512 [00:01<00:00, 356.19it/s]
Adding requests:  94%|█████████▍| 482/512 [00:01<00:00, 362.56it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 341.94it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:00<00:00, 741.93it/s, est. speed input: 759781.85 toks/s, output: 741.94 toks/s]
Processed prompts:  31%|███▏      | 161/512 [00:01<00:03, 101.13it/s, est. speed input: 120194.47 toks/s, output: 117.38 toks/s]
Processed prompts:  38%|███▊      | 196/512 [00:01<00:03, 82.99it/s, est. speed input: 100455.47 toks/s, output: 98.10 toks/s]  
Processed prompts:  43%|████▎     | 218/512 [00:02<00:03, 74.66it/s, est. speed input: 92454.05 toks/s, output: 90.29 toks/s] 
Processed prompts:  46%|████▌     | 233/512 [00:02<00:03, 74.07it/s, est. speed input: 90873.83 toks/s, output: 88.74 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:02<00:03, 67.47it/s, est. speed input: 86714.97 toks/s, output: 84.68 toks/s]
Processed prompts:  50%|█████     | 256/512 [00:03<00:03, 67.98it/s, est. speed input: 86063.64 toks/s, output: 84.05 toks/s]
Processed prompts:  52%|█████▏    | 265/512 [00:03<00:03, 67.45it/s, est. speed input: 85208.62 toks/s, output: 83.21 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:03<00:03, 60.87it/s, est. speed input: 82675.39 toks/s, output: 80.74 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:03<00:03, 60.06it/s, est. speed input: 81706.28 toks/s, output: 79.79 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:03<00:03, 59.44it/s, est. speed input: 80831.94 toks/s, output: 78.94 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:03<00:03, 58.77it/s, est. speed input: 79990.59 toks/s, output: 78.12 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:03<00:03, 59.14it/s, est. speed input: 79375.70 toks/s, output: 77.52 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:04<00:03, 58.87it/s, est. speed input: 78706.64 toks/s, output: 76.86 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:04<00:03, 58.51it/s, est. speed input: 78057.01 toks/s, output: 76.23 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:04<00:03, 58.13it/s, est. speed input: 77431.58 toks/s, output: 75.62 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:04<00:03, 57.79it/s, est. speed input: 76836.63 toks/s, output: 75.04 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:04<00:02, 57.87it/s, est. speed input: 76320.19 toks/s, output: 74.53 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:04<00:02, 57.49it/s, est. speed input: 75778.32 toks/s, output: 74.00 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:04<00:02, 57.28it/s, est. speed input: 75273.10 toks/s, output: 73.51 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:05<00:02, 57.28it/s, est. speed input: 74815.33 toks/s, output: 73.06 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:05<00:02, 57.28it/s, est. speed input: 74381.81 toks/s, output: 72.64 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:05<00:02, 57.43it/s, est. speed input: 73988.00 toks/s, output: 72.25 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:05<00:02, 57.38it/s, est. speed input: 73596.63 toks/s, output: 71.87 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:05<00:01, 57.35it/s, est. speed input: 73225.30 toks/s, output: 71.51 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:05<00:01, 57.17it/s, est. speed input: 72856.03 toks/s, output: 71.15 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:05<00:01, 57.16it/s, est. speed input: 72515.72 toks/s, output: 70.82 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:06<00:01, 57.23it/s, est. speed input: 72198.95 toks/s, output: 70.51 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:06<00:01, 58.27it/s, est. speed input: 71987.99 toks/s, output: 70.30 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:06<00:01, 58.04it/s, est. speed input: 71699.10 toks/s, output: 70.02 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:06<00:01, 57.68it/s, est. speed input: 71405.81 toks/s, output: 69.73 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:06<00:00, 57.54it/s, est. speed input: 71133.60 toks/s, output: 69.47 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:06<00:00, 57.33it/s, est. speed input: 70863.94 toks/s, output: 69.20 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:06<00:00, 57.36it/s, est. speed input: 70619.52 toks/s, output: 68.96 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:07<00:00, 57.32it/s, est. speed input: 70379.83 toks/s, output: 68.73 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:07<00:00, 57.32it/s, est. speed input: 70151.61 toks/s, output: 68.51 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:07<00:00, 57.32it/s, est. speed input: 69932.62 toks/s, output: 68.29 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:07<00:00, 57.17it/s, est. speed input: 69710.10 toks/s, output: 68.08 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 57.17it/s, est. speed input: 69908.78 toks/s, output: 68.27 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 68.27it/s, est. speed input: 69908.78 toks/s, output: 68.27 toks/s]
[rank0]:[W126 18:53:32.018116859 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 18:53:34
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:53:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=942245) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=942245) WARNING 01-26 18:54:09 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=942245) WARNING 01-26 18:54:25 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 61.92 requests/s, 63467.87 total tokens/s, 61.92 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 18:53:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:53:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:53:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:53:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:53:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:53:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:53:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:53:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:53:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:53:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:53:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:53:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:53:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:53:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:53:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:53:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:53:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:53:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:53:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:53:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:53:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:53:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:53:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:53:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:53:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:53:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:53:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:53:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=942245) [2026-01-26 18:53:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=942245) [2026-01-26 18:53:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=942245) [2026-01-26 18:53:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=942245) [2026-01-26 18:53:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=942245) [2026-01-26 18:53:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=942245) [2026-01-26 18:53:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=942245) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=942245) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.22s/it]
(EngineCore_DP0 pid=942245) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.41it/s]
(EngineCore_DP0 pid=942245) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.11it/s]
(EngineCore_DP0 pid=942245) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
(EngineCore_DP0 pid=942245) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.05s/it]
(EngineCore_DP0 pid=942245) 
(EngineCore_DP0 pid=942245) [2026-01-26 18:53:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=942245) [2026-01-26 18:53:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=942245) [2026-01-26 18:53:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=942245) [2026-01-26 18:53:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=942245) [2026-01-26 18:53:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=942245) [2026-01-26 18:53:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=942245) [2026-01-26 18:53:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=942245) [2026-01-26 18:53:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=942245) 2026-01-26 18:54:25,419 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=942245) 2026-01-26 18:54:25,459 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=942245) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  7.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  9.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.22it/s]
(EngineCore_DP0 pid=942245) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 11.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 10.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 10.48it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 275.01it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 316.57it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 313.76it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 319.14it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 328.42it/s]
Adding requests:  19%|█▉        | 198/1024 [00:00<00:02, 332.74it/s]
Adding requests:  23%|██▎       | 233/1024 [00:00<00:02, 338.09it/s]
Adding requests:  26%|██▌       | 268/1024 [00:00<00:02, 339.41it/s]
Adding requests:  30%|██▉       | 304/1024 [00:00<00:02, 343.76it/s]
Adding requests:  33%|███▎      | 341/1024 [00:01<00:01, 349.84it/s]
Adding requests:  37%|███▋      | 378/1024 [00:01<00:01, 352.53it/s]
Adding requests:  41%|████      | 417/1024 [00:01<00:01, 361.43it/s]
Adding requests:  44%|████▍     | 454/1024 [00:01<00:01, 357.96it/s]
Adding requests:  48%|████▊     | 494/1024 [00:01<00:01, 369.64it/s]
Adding requests:  52%|█████▏    | 531/1024 [00:01<00:01, 339.09it/s]
Adding requests:  55%|█████▌    | 566/1024 [00:01<00:01, 337.50it/s]
Adding requests:  59%|█████▊    | 601/1024 [00:01<00:01, 327.49it/s]
Adding requests:  62%|██████▏   | 635/1024 [00:01<00:01, 329.91it/s]
Adding requests:  65%|██████▌   | 669/1024 [00:01<00:01, 327.34it/s]
Adding requests:  69%|██████▉   | 706/1024 [00:02<00:00, 336.94it/s]
Adding requests:  72%|███████▏  | 740/1024 [00:02<00:00, 337.75it/s]
Adding requests:  76%|███████▌  | 775/1024 [00:02<00:00, 340.57it/s]
Adding requests:  79%|███████▉  | 810/1024 [00:02<00:00, 339.25it/s]
Adding requests:  83%|████████▎ | 848/1024 [00:02<00:00, 351.03it/s]
Adding requests:  86%|████████▋ | 884/1024 [00:02<00:00, 352.28it/s]
Adding requests:  90%|████████▉ | 920/1024 [00:02<00:00, 348.04it/s]
Adding requests:  93%|█████████▎| 956/1024 [00:02<00:00, 350.84it/s]
Adding requests:  97%|█████████▋| 992/1024 [00:02<00:00, 347.77it/s]
Adding requests: 100%|██████████| 1024/1024 [00:03<00:00, 341.16it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:00<00:00, 996.01it/s, est. speed input: 1019956.92 toks/s, output: 996.02 toks/s]
Processed prompts:  28%|██▊       | 286/1024 [00:01<00:05, 138.00it/s, est. speed input: 169864.59 toks/s, output: 165.88 toks/s] 
Processed prompts:  32%|███▏      | 331/1024 [00:02<00:06, 106.91it/s, est. speed input: 136419.17 toks/s, output: 133.22 toks/s]
Processed prompts:  35%|███▌      | 359/1024 [00:02<00:06, 99.21it/s, est. speed input: 128015.33 toks/s, output: 125.01 toks/s] 
Processed prompts:  37%|███▋      | 379/1024 [00:03<00:07, 87.84it/s, est. speed input: 119168.37 toks/s, output: 116.38 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:03<00:07, 82.34it/s, est. speed input: 114825.98 toks/s, output: 112.13 toks/s]
Processed prompts:  40%|███▉      | 406/1024 [00:03<00:07, 83.58it/s, est. speed input: 114121.91 toks/s, output: 111.45 toks/s]
Processed prompts:  41%|████      | 417/1024 [00:03<00:07, 83.78it/s, est. speed input: 113190.55 toks/s, output: 110.54 toks/s]
Processed prompts:  42%|████▏     | 427/1024 [00:04<00:08, 70.67it/s, est. speed input: 108495.73 toks/s, output: 105.95 toks/s]
Processed prompts:  43%|████▎     | 436/1024 [00:04<00:08, 71.26it/s, est. speed input: 107546.46 toks/s, output: 105.03 toks/s]
Processed prompts:  43%|████▎     | 444/1024 [00:04<00:08, 69.23it/s, est. speed input: 106176.81 toks/s, output: 103.69 toks/s]
Processed prompts:  44%|████▍     | 452/1024 [00:04<00:08, 67.73it/s, est. speed input: 104947.59 toks/s, output: 102.49 toks/s]
Processed prompts:  45%|████▍     | 460/1024 [00:04<00:08, 66.44it/s, est. speed input: 103786.49 toks/s, output: 101.35 toks/s]
Processed prompts:  46%|████▌     | 467/1024 [00:04<00:08, 63.28it/s, est. speed input: 102446.23 toks/s, output: 100.05 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:04<00:09, 61.00it/s, est. speed input: 101207.82 toks/s, output: 98.84 toks/s] 
Processed prompts:  47%|████▋     | 482/1024 [00:04<00:08, 61.36it/s, est. speed input: 100232.05 toks/s, output: 97.88 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:05<00:08, 61.68it/s, est. speed input: 99313.98 toks/s, output: 96.99 toks/s] 
Processed prompts:  49%|████▊     | 498/1024 [00:05<00:08, 61.72it/s, est. speed input: 98415.02 toks/s, output: 96.11 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:05<00:08, 61.94it/s, est. speed input: 97584.45 toks/s, output: 95.30 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:05<00:08, 62.34it/s, est. speed input: 96822.80 toks/s, output: 94.55 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:05<00:08, 62.25it/s, est. speed input: 96051.54 toks/s, output: 93.80 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:05<00:07, 62.35it/s, est. speed input: 95333.99 toks/s, output: 93.10 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:05<00:07, 62.14it/s, est. speed input: 94615.97 toks/s, output: 92.40 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:05<00:07, 62.18it/s, est. speed input: 93950.67 toks/s, output: 91.75 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:06<00:07, 62.27it/s, est. speed input: 93319.25 toks/s, output: 91.13 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:06<00:07, 62.26it/s, est. speed input: 92706.23 toks/s, output: 90.53 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:06<00:07, 62.23it/s, est. speed input: 92115.95 toks/s, output: 89.96 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:06<00:07, 61.70it/s, est. speed input: 91499.87 toks/s, output: 89.36 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:06<00:07, 61.84it/s, est. speed input: 90956.35 toks/s, output: 88.82 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:06<00:06, 62.09it/s, est. speed input: 90448.81 toks/s, output: 88.33 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:06<00:06, 62.20it/s, est. speed input: 89953.44 toks/s, output: 87.85 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:06<00:06, 62.10it/s, est. speed input: 89460.46 toks/s, output: 87.36 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:07<00:06, 62.03it/s, est. speed input: 88985.80 toks/s, output: 86.90 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:07<00:06, 62.21it/s, est. speed input: 88547.16 toks/s, output: 86.47 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:07<00:06, 62.10it/s, est. speed input: 88104.58 toks/s, output: 86.04 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:07<00:06, 62.19it/s, est. speed input: 87690.67 toks/s, output: 85.64 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:07<00:06, 62.14it/s, est. speed input: 87282.13 toks/s, output: 85.24 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:07<00:05, 62.24it/s, est. speed input: 86896.56 toks/s, output: 84.86 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:07<00:05, 62.09it/s, est. speed input: 86507.73 toks/s, output: 84.48 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:08<00:05, 62.19it/s, est. speed input: 86146.19 toks/s, output: 84.13 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:08<00:05, 62.10it/s, est. speed input: 85785.02 toks/s, output: 83.77 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:08<00:05, 62.15it/s, est. speed input: 85442.58 toks/s, output: 83.44 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:08<00:05, 62.40it/s, est. speed input: 85126.34 toks/s, output: 83.13 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:08<00:05, 62.17it/s, est. speed input: 84790.86 toks/s, output: 82.80 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:08<00:04, 62.07it/s, est. speed input: 84470.37 toks/s, output: 82.49 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:08<00:04, 61.80it/s, est. speed input: 84145.72 toks/s, output: 82.17 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:08<00:04, 61.81it/s, est. speed input: 83843.54 toks/s, output: 81.88 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:09<00:04, 61.94it/s, est. speed input: 83557.91 toks/s, output: 81.60 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:09<00:04, 62.02it/s, est. speed input: 83279.44 toks/s, output: 81.33 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:09<00:04, 61.89it/s, est. speed input: 82997.00 toks/s, output: 81.05 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:09<00:04, 62.11it/s, est. speed input: 82741.39 toks/s, output: 80.80 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:09<00:04, 62.29it/s, est. speed input: 82494.46 toks/s, output: 80.56 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:09<00:03, 62.19it/s, est. speed input: 82240.31 toks/s, output: 80.31 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:09<00:03, 63.75it/s, est. speed input: 82085.15 toks/s, output: 80.16 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:09<00:03, 63.19it/s, est. speed input: 81841.92 toks/s, output: 79.92 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:10<00:03, 62.97it/s, est. speed input: 81614.48 toks/s, output: 79.70 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:10<00:03, 62.63it/s, est. speed input: 81382.30 toks/s, output: 79.47 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:10<00:03, 62.60it/s, est. speed input: 81167.08 toks/s, output: 79.26 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:10<00:03, 62.58it/s, est. speed input: 80957.41 toks/s, output: 79.06 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:10<00:03, 62.33it/s, est. speed input: 80740.33 toks/s, output: 78.85 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:10<00:02, 62.24it/s, est. speed input: 80532.97 toks/s, output: 78.65 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:10<00:02, 62.32it/s, est. speed input: 80337.73 toks/s, output: 78.45 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:10<00:02, 62.32it/s, est. speed input: 80144.30 toks/s, output: 78.27 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:11<00:02, 62.19it/s, est. speed input: 79948.88 toks/s, output: 78.07 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:11<00:02, 62.25it/s, est. speed input: 79765.18 toks/s, output: 77.90 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:11<00:02, 62.20it/s, est. speed input: 79581.27 toks/s, output: 77.72 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:11<00:02, 62.13it/s, est. speed input: 79399.89 toks/s, output: 77.54 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:11<00:02, 61.95it/s, est. speed input: 79216.29 toks/s, output: 77.36 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:11<00:01, 62.17it/s, est. speed input: 79053.13 toks/s, output: 77.20 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:11<00:01, 62.17it/s, est. speed input: 78886.08 toks/s, output: 77.04 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:11<00:01, 62.15it/s, est. speed input: 78721.93 toks/s, output: 76.88 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:12<00:01, 62.00it/s, est. speed input: 78555.11 toks/s, output: 76.71 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:12<00:01, 62.15it/s, est. speed input: 78402.78 toks/s, output: 76.57 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:12<00:01, 62.00it/s, est. speed input: 78242.71 toks/s, output: 76.41 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:12<00:01, 62.11it/s, est. speed input: 78095.25 toks/s, output: 76.26 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:12<00:00, 62.02it/s, est. speed input: 77943.67 toks/s, output: 76.12 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:12<00:00, 61.95it/s, est. speed input: 77794.94 toks/s, output: 75.97 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:12<00:00, 61.96it/s, est. speed input: 77651.78 toks/s, output: 75.83 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:13<00:00, 62.03it/s, est. speed input: 77513.57 toks/s, output: 75.70 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:13<00:00, 61.97it/s, est. speed input: 77373.94 toks/s, output: 75.56 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:13<00:00, 61.94it/s, est. speed input: 77237.75 toks/s, output: 75.43 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:13<00:00, 61.99it/s, est. speed input: 77106.43 toks/s, output: 75.30 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:13<00:00, 63.12it/s, est. speed input: 77020.32 toks/s, output: 75.22 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:13<00:00, 63.12it/s, est. speed input: 77473.56 toks/s, output: 75.66 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:13<00:00, 75.66it/s, est. speed input: 77473.56 toks/s, output: 75.66 toks/s]
[rank0]:[W126 18:54:45.274498677 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 18:54:47
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:55:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=943887) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=943887) WARNING 01-26 18:55:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=943887) WARNING 01-26 18:55:46 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 63.04 requests/s, 64611.05 total tokens/s, 63.04 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 18:55:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:55:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:55:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:55:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:55:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:55:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:55:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:55:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:55:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:55:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:55:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:55:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:55:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:55:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:55:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:55:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:55:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:55:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:55:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:55:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:55:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:55:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:55:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:55:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:55:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:55:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:55:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:55:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=943887) [2026-01-26 18:55:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=943887) [2026-01-26 18:55:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=943887) [2026-01-26 18:55:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=943887) [2026-01-26 18:55:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=943887) [2026-01-26 18:55:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=943887) [2026-01-26 18:55:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=943887) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=943887) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.22s/it]
(EngineCore_DP0 pid=943887) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.41it/s]
(EngineCore_DP0 pid=943887) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.12it/s]
(EngineCore_DP0 pid=943887) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.13s/it]
(EngineCore_DP0 pid=943887) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.04s/it]
(EngineCore_DP0 pid=943887) 
(EngineCore_DP0 pid=943887) [2026-01-26 18:55:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=943887) [2026-01-26 18:55:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=943887) [2026-01-26 18:55:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=943887) [2026-01-26 18:55:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=943887) [2026-01-26 18:55:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=943887) [2026-01-26 18:55:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=943887) [2026-01-26 18:55:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=943887) [2026-01-26 18:55:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=943887) [rank0]:W0126 18:55:36.662000 943887 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=943887) [rank0]:W0126 18:55:36.731000 943887 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=943887) [rank0]:W0126 18:55:37.538000 943887 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=943887) [rank0]:W0126 18:55:37.647000 943887 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=943887) 2026-01-26 18:55:46,485 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=943887) 2026-01-26 18:55:46,542 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=943887) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  9.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  9.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.35it/s]
(EngineCore_DP0 pid=943887) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 10.96it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 11.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 11.16it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 275.71it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 315.69it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 304.98it/s]
Adding requests:   6%|▋         | 128/2048 [00:00<00:06, 314.44it/s]
Adding requests:   8%|▊         | 163/2048 [00:00<00:05, 324.21it/s]
Adding requests:  10%|▉         | 199/2048 [00:00<00:05, 335.13it/s]
Adding requests:  11%|█▏        | 235/2048 [00:00<00:05, 342.01it/s]
Adding requests:  13%|█▎        | 270/2048 [00:00<00:05, 340.06it/s]
Adding requests:  15%|█▍        | 306/2048 [00:00<00:05, 345.10it/s]
Adding requests:  17%|█▋        | 343/2048 [00:01<00:04, 352.28it/s]
Adding requests:  19%|█▊        | 380/2048 [00:01<00:04, 354.14it/s]
Adding requests:  20%|██        | 419/2048 [00:01<00:04, 362.69it/s]
Adding requests:  22%|██▏       | 456/2048 [00:01<00:04, 357.86it/s]
Adding requests:  24%|██▍       | 496/2048 [00:01<00:04, 367.68it/s]
Adding requests:  26%|██▌       | 535/2048 [00:01<00:04, 372.36it/s]
Adding requests:  28%|██▊       | 573/2048 [00:01<00:03, 369.97it/s]
Adding requests:  30%|██▉       | 611/2048 [00:01<00:04, 355.64it/s]
Adding requests:  32%|███▏      | 647/2048 [00:01<00:04, 347.75it/s]
Adding requests:  33%|███▎      | 683/2048 [00:01<00:03, 350.48it/s]
Adding requests:  35%|███▌      | 719/2048 [00:02<00:03, 352.16it/s]
Adding requests:  37%|███▋      | 755/2048 [00:02<00:03, 347.74it/s]
Adding requests:  39%|███▊      | 791/2048 [00:02<00:03, 349.85it/s]
Adding requests:  40%|████      | 828/2048 [00:02<00:03, 352.37it/s]
Adding requests:  42%|████▏     | 864/2048 [00:02<00:03, 354.30it/s]
Adding requests:  44%|████▍     | 901/2048 [00:02<00:03, 357.83it/s]
Adding requests:  46%|████▌     | 937/2048 [00:02<00:03, 352.01it/s]
Adding requests:  48%|████▊     | 974/2048 [00:02<00:03, 353.99it/s]
Adding requests:  49%|████▉     | 1010/2048 [00:02<00:02, 349.37it/s]
Adding requests:  51%|█████     | 1045/2048 [00:02<00:02, 348.33it/s]
Adding requests:  53%|█████▎    | 1080/2048 [00:03<00:02, 337.96it/s]
Adding requests:  54%|█████▍    | 1114/2048 [00:03<00:02, 337.67it/s]
Adding requests:  56%|█████▌    | 1150/2048 [00:03<00:02, 342.51it/s]
Adding requests:  58%|█████▊    | 1186/2048 [00:03<00:02, 344.31it/s]
Adding requests:  60%|█████▉    | 1224/2048 [00:03<00:02, 352.17it/s]
Adding requests:  62%|██████▏   | 1260/2048 [00:03<00:02, 349.82it/s]
Adding requests:  63%|██████▎   | 1296/2048 [00:03<00:02, 346.30it/s]
Adding requests:  65%|██████▌   | 1332/2048 [00:03<00:02, 349.06it/s]
Adding requests:  67%|██████▋   | 1369/2048 [00:03<00:01, 354.52it/s]
Adding requests:  69%|██████▊   | 1405/2048 [00:04<00:01, 353.54it/s]
Adding requests:  70%|███████   | 1441/2048 [00:04<00:01, 353.36it/s]
Adding requests:  72%|███████▏  | 1477/2048 [00:04<00:01, 353.12it/s]
Adding requests:  74%|███████▍  | 1514/2048 [00:04<00:01, 357.58it/s]
Adding requests:  76%|███████▌  | 1550/2048 [00:04<00:01, 354.66it/s]
Adding requests:  77%|███████▋  | 1586/2048 [00:04<00:01, 347.01it/s]
Adding requests:  79%|███████▉  | 1621/2048 [00:04<00:01, 337.68it/s]
Adding requests:  81%|████████  | 1655/2048 [00:04<00:01, 335.19it/s]
Adding requests:  83%|████████▎ | 1690/2048 [00:04<00:01, 338.44it/s]
Adding requests:  84%|████████▍ | 1727/2048 [00:04<00:00, 344.80it/s]
Adding requests:  86%|████████▌ | 1763/2048 [00:05<00:00, 348.26it/s]
Adding requests:  88%|████████▊ | 1798/2048 [00:05<00:00, 337.65it/s]
Adding requests:  90%|████████▉ | 1834/2048 [00:05<00:00, 342.67it/s]
Adding requests:  91%|█████████▏| 1869/2048 [00:05<00:00, 344.20it/s]
Adding requests:  93%|█████████▎| 1905/2048 [00:05<00:00, 347.68it/s]
Adding requests:  95%|█████████▍| 1944/2048 [00:05<00:00, 357.63it/s]
Adding requests:  97%|█████████▋| 1981/2048 [00:05<00:00, 357.33it/s]
Adding requests:  98%|█████████▊| 2017/2048 [00:05<00:00, 346.86it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 347.64it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:00<00:01, 1208.73it/s, est. speed input: 1237778.41 toks/s, output: 1208.74 toks/s]
Processed prompts:  24%|██▍       | 491/2048 [00:02<00:08, 192.17it/s, est. speed input: 242974.39 toks/s, output: 237.28 toks/s]   
Processed prompts:  27%|██▋       | 545/2048 [00:02<00:10, 149.18it/s, est. speed input: 197194.89 toks/s, output: 192.57 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:03<00:12, 113.47it/s, est. speed input: 164708.07 toks/s, output: 160.85 toks/s]
Processed prompts:  29%|██▉       | 600/2048 [00:03<00:13, 109.71it/s, est. speed input: 159655.22 toks/s, output: 155.91 toks/s]
Processed prompts:  30%|███       | 617/2048 [00:04<00:13, 102.32it/s, est. speed input: 153818.84 toks/s, output: 150.21 toks/s]
Processed prompts:  31%|███       | 631/2048 [00:04<00:15, 93.35it/s, est. speed input: 148147.11 toks/s, output: 144.67 toks/s] 
Processed prompts:  31%|███▏      | 642/2048 [00:04<00:17, 82.63it/s, est. speed input: 142415.03 toks/s, output: 139.08 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:04<00:17, 78.21it/s, est. speed input: 138418.46 toks/s, output: 135.17 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:05<00:18, 74.28it/s, est. speed input: 134712.44 toks/s, output: 131.55 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:05<00:19, 71.28it/s, est. speed input: 131387.44 toks/s, output: 128.31 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:05<00:19, 69.23it/s, est. speed input: 128435.91 toks/s, output: 125.43 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:05<00:19, 67.34it/s, est. speed input: 125636.96 toks/s, output: 122.69 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:06<00:19, 65.95it/s, est. speed input: 123068.45 toks/s, output: 120.18 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:06<00:19, 65.13it/s, est. speed input: 120752.77 toks/s, output: 117.92 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:06<00:19, 64.46it/s, est. speed input: 118592.35 toks/s, output: 115.81 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:06<00:19, 64.75it/s, est. speed input: 116764.34 toks/s, output: 114.03 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:07<00:19, 64.38it/s, est. speed input: 114940.71 toks/s, output: 112.25 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:07<00:19, 63.88it/s, est. speed input: 113192.83 toks/s, output: 110.54 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:07<00:19, 63.73it/s, est. speed input: 111599.27 toks/s, output: 108.98 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:07<00:18, 63.51it/s, est. speed input: 110085.99 toks/s, output: 107.51 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:08<00:18, 63.38it/s, est. speed input: 108672.75 toks/s, output: 106.13 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:08<00:18, 63.27it/s, est. speed input: 107340.05 toks/s, output: 104.82 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:08<00:18, 63.30it/s, est. speed input: 106103.98 toks/s, output: 103.62 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:08<00:17, 63.15it/s, est. speed input: 104910.62 toks/s, output: 102.45 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:09<00:17, 62.97it/s, est. speed input: 103771.09 toks/s, output: 101.34 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:09<00:17, 62.97it/s, est. speed input: 102713.17 toks/s, output: 100.31 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:09<00:17, 62.90it/s, est. speed input: 101700.27 toks/s, output: 99.32 toks/s] 
Processed prompts:  48%|████▊     | 978/2048 [00:09<00:16, 62.96it/s, est. speed input: 100754.29 toks/s, output: 98.39 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:10<00:16, 62.93it/s, est. speed input: 99846.11 toks/s, output: 97.51 toks/s] 
Processed prompts:  49%|████▉     | 1010/2048 [00:10<00:16, 62.94it/s, est. speed input: 98985.10 toks/s, output: 96.67 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:10<00:16, 62.96it/s, est. speed input: 98167.40 toks/s, output: 95.87 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:10<00:16, 62.79it/s, est. speed input: 97365.54 toks/s, output: 95.08 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:11<00:15, 62.69it/s, est. speed input: 96602.10 toks/s, output: 94.34 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:11<00:15, 62.80it/s, est. speed input: 95893.14 toks/s, output: 93.65 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:11<00:15, 62.96it/s, est. speed input: 95224.75 toks/s, output: 92.99 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:11<00:14, 62.88it/s, est. speed input: 94563.12 toks/s, output: 92.35 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:12<00:14, 62.87it/s, est. speed input: 93935.05 toks/s, output: 91.73 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:12<00:14, 62.98it/s, est. speed input: 93343.22 toks/s, output: 91.16 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:12<00:14, 62.75it/s, est. speed input: 92745.29 toks/s, output: 90.57 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:12<00:13, 62.99it/s, est. speed input: 92209.88 toks/s, output: 90.05 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:13<00:13, 63.04it/s, est. speed input: 91683.17 toks/s, output: 89.53 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:13<00:13, 63.92it/s, est. speed input: 91251.20 toks/s, output: 89.11 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:13<00:13, 63.59it/s, est. speed input: 90752.65 toks/s, output: 88.63 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:13<00:12, 63.33it/s, est. speed input: 90269.66 toks/s, output: 88.15 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:14<00:12, 63.24it/s, est. speed input: 89811.85 toks/s, output: 87.71 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:14<00:12, 63.15it/s, est. speed input: 89367.07 toks/s, output: 87.27 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:14<00:12, 63.18it/s, est. speed input: 88945.19 toks/s, output: 86.86 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:15<00:11, 63.17it/s, est. speed input: 88535.55 toks/s, output: 86.46 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:15<00:11, 63.02it/s, est. speed input: 88128.11 toks/s, output: 86.06 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:15<00:11, 63.00it/s, est. speed input: 87741.01 toks/s, output: 85.68 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:15<00:11, 63.04it/s, est. speed input: 87370.09 toks/s, output: 85.32 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:16<00:10, 62.84it/s, est. speed input: 86994.25 toks/s, output: 84.96 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:16<00:10, 62.86it/s, est. speed input: 86641.59 toks/s, output: 84.61 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:16<00:10, 62.75it/s, est. speed input: 86290.78 toks/s, output: 84.27 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:16<00:10, 62.85it/s, est. speed input: 85963.63 toks/s, output: 83.95 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:17<00:09, 63.09it/s, est. speed input: 85656.81 toks/s, output: 83.65 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:17<00:09, 62.97it/s, est. speed input: 85340.72 toks/s, output: 83.34 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:17<00:09, 63.94it/s, est. speed input: 85100.65 toks/s, output: 83.11 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:17<00:08, 63.81it/s, est. speed input: 84816.62 toks/s, output: 82.83 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:18<00:08, 63.79it/s, est. speed input: 84544.85 toks/s, output: 82.56 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:18<00:08, 63.46it/s, est. speed input: 84261.31 toks/s, output: 82.29 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:18<00:08, 64.21it/s, est. speed input: 84043.84 toks/s, output: 82.07 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:18<00:07, 63.85it/s, est. speed input: 83780.19 toks/s, output: 81.82 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:19<00:07, 64.47it/s, est. speed input: 83573.65 toks/s, output: 81.61 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:19<00:07, 63.89it/s, est. speed input: 83315.31 toks/s, output: 81.36 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:19<00:07, 63.89it/s, est. speed input: 83086.19 toks/s, output: 81.14 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:19<00:07, 63.63it/s, est. speed input: 82848.28 toks/s, output: 80.91 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:20<00:06, 64.35it/s, est. speed input: 82665.42 toks/s, output: 80.73 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:20<00:06, 63.90it/s, est. speed input: 82435.82 toks/s, output: 80.50 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:20<00:06, 63.60it/s, est. speed input: 82213.02 toks/s, output: 80.29 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:20<00:06, 63.47it/s, est. speed input: 81999.79 toks/s, output: 80.08 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:21<00:05, 63.34it/s, est. speed input: 81789.25 toks/s, output: 79.87 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:21<00:05, 63.24it/s, est. speed input: 81583.66 toks/s, output: 79.67 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:21<00:05, 63.21it/s, est. speed input: 81384.70 toks/s, output: 79.48 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:21<00:04, 63.99it/s, est. speed input: 81229.61 toks/s, output: 79.33 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:22<00:04, 64.76it/s, est. speed input: 81088.34 toks/s, output: 79.19 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:22<00:04, 64.17it/s, est. speed input: 80897.12 toks/s, output: 79.00 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:22<00:04, 63.83it/s, est. speed input: 80713.19 toks/s, output: 78.82 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:22<00:03, 63.62it/s, est. speed input: 80534.76 toks/s, output: 78.65 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:23<00:03, 63.47it/s, est. speed input: 80359.94 toks/s, output: 78.48 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:23<00:03, 63.27it/s, est. speed input: 80184.72 toks/s, output: 78.31 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:23<00:03, 63.09it/s, est. speed input: 80011.35 toks/s, output: 78.14 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:23<00:03, 63.04it/s, est. speed input: 79845.18 toks/s, output: 77.97 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:24<00:02, 62.91it/s, est. speed input: 79678.33 toks/s, output: 77.81 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:24<00:02, 63.66it/s, est. speed input: 79551.30 toks/s, output: 77.69 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:24<00:02, 63.45it/s, est. speed input: 79395.65 toks/s, output: 77.53 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:24<00:01, 63.29it/s, est. speed input: 79242.32 toks/s, output: 77.39 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:25<00:01, 63.26it/s, est. speed input: 79095.76 toks/s, output: 77.24 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:25<00:01, 63.10it/s, est. speed input: 78946.00 toks/s, output: 77.10 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:25<00:01, 62.99it/s, est. speed input: 78799.63 toks/s, output: 76.95 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:25<00:00, 63.99it/s, est. speed input: 78699.42 toks/s, output: 76.85 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:26<00:00, 63.65it/s, est. speed input: 78559.72 toks/s, output: 76.72 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:26<00:00, 63.32it/s, est. speed input: 78419.20 toks/s, output: 76.58 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:26<00:00, 63.85it/s, est. speed input: 78310.64 toks/s, output: 76.48 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:26<00:00, 63.85it/s, est. speed input: 78848.92 toks/s, output: 77.00 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:26<00:00, 77.00it/s, est. speed input: 78848.92 toks/s, output: 77.00 toks/s]
[rank0]:[W126 18:56:22.494165257 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 18:56:24
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:56:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=945925) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=945925) WARNING 01-26 18:57:17 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=945925) WARNING 01-26 18:57:36 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 64.91 requests/s, 66532.07 total tokens/s, 64.91 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 18:56:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:56:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:56:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:56:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:56:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:56:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:56:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:56:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:56:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:56:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:56:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:56:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:56:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:56:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:57:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:57:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:57:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:57:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:57:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:57:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:57:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:57:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:57:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:57:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:57:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:57:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:57:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:57:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=945925) [2026-01-26 18:57:03] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=945925) [2026-01-26 18:57:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=945925) [2026-01-26 18:57:03] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=945925) [2026-01-26 18:57:03] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=945925) [2026-01-26 18:57:03] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=945925) [2026-01-26 18:57:03] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=945925) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=945925) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.22s/it]
(EngineCore_DP0 pid=945925) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.41it/s]
(EngineCore_DP0 pid=945925) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.06it/s]
(EngineCore_DP0 pid=945925) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
(EngineCore_DP0 pid=945925) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]
(EngineCore_DP0 pid=945925) 
(EngineCore_DP0 pid=945925) [2026-01-26 18:57:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=945925) [2026-01-26 18:57:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=945925) [2026-01-26 18:57:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=945925) [2026-01-26 18:57:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=945925) [2026-01-26 18:57:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=945925) [2026-01-26 18:57:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=945925) [2026-01-26 18:57:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=945925) [2026-01-26 18:57:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=945925) [rank0]:W0126 18:57:25.668000 945925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=945925) [rank0]:W0126 18:57:25.737000 945925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=945925) [rank0]:W0126 18:57:26.691000 945925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=945925) [rank0]:W0126 18:57:26.801000 945925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=945925) 2026-01-26 18:57:36,308 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=945925) 2026-01-26 18:57:36,413 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=945925) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  9.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  9.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  9.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 10.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 10.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.69it/s]
(EngineCore_DP0 pid=945925) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 10.53it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 10.89it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 11.00it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 10.94it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 272.86it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 314.28it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 311.52it/s]
Adding requests:   3%|▎         | 128/4096 [00:00<00:12, 316.88it/s]
Adding requests:   4%|▍         | 163/4096 [00:00<00:12, 326.43it/s]
Adding requests:   5%|▍         | 199/4096 [00:00<00:11, 335.96it/s]
Adding requests:   6%|▌         | 234/4096 [00:00<00:11, 339.89it/s]
Adding requests:   7%|▋         | 269/4096 [00:00<00:11, 339.98it/s]
Adding requests:   7%|▋         | 305/4096 [00:00<00:11, 343.63it/s]
Adding requests:   8%|▊         | 342/4096 [00:01<00:10, 350.77it/s]
Adding requests:   9%|▉         | 378/4096 [00:01<00:10, 353.32it/s]
Adding requests:  10%|█         | 416/4096 [00:01<00:10, 360.15it/s]
Adding requests:  11%|█         | 453/4096 [00:01<00:10, 357.72it/s]
Adding requests:  12%|█▏        | 493/4096 [00:01<00:09, 367.86it/s]
Adding requests:  13%|█▎        | 532/4096 [00:01<00:09, 372.42it/s]
Adding requests:  14%|█▍        | 570/4096 [00:01<00:09, 369.52it/s]
Adding requests:  15%|█▍        | 607/4096 [00:01<00:09, 357.19it/s]
Adding requests:  16%|█▌        | 643/4096 [00:01<00:09, 354.71it/s]
Adding requests:  17%|█▋        | 679/4096 [00:01<00:09, 353.54it/s]
Adding requests:  17%|█▋        | 716/4096 [00:02<00:09, 355.20it/s]
Adding requests:  18%|█▊        | 752/4096 [00:02<00:09, 349.74it/s]
Adding requests:  19%|█▉        | 788/4096 [00:02<00:09, 351.76it/s]
Adding requests:  20%|██        | 824/4096 [00:02<00:09, 351.37it/s]
Adding requests:  21%|██        | 862/4096 [00:02<00:09, 357.25it/s]
Adding requests:  22%|██▏       | 899/4096 [00:02<00:08, 359.18it/s]
Adding requests:  23%|██▎       | 935/4096 [00:02<00:08, 351.89it/s]
Adding requests:  24%|██▎       | 972/4096 [00:02<00:08, 355.33it/s]
Adding requests:  25%|██▍       | 1008/4096 [00:02<00:08, 343.26it/s]
Adding requests:  25%|██▌       | 1044/4096 [00:02<00:08, 345.64it/s]
Adding requests:  26%|██▋       | 1079/4096 [00:03<00:09, 333.43it/s]
Adding requests:  27%|██▋       | 1114/4096 [00:03<00:08, 335.99it/s]
Adding requests:  28%|██▊       | 1150/4096 [00:03<00:08, 341.08it/s]
Adding requests:  29%|██▉       | 1185/4096 [00:03<00:08, 342.84it/s]
Adding requests:  30%|██▉       | 1222/4096 [00:03<00:08, 350.76it/s]
Adding requests:  31%|███       | 1258/4096 [00:03<00:08, 349.24it/s]
Adding requests:  32%|███▏      | 1293/4096 [00:03<00:08, 343.14it/s]
Adding requests:  32%|███▏      | 1330/4096 [00:03<00:07, 349.04it/s]
Adding requests:  33%|███▎      | 1367/4096 [00:03<00:07, 353.95it/s]
Adding requests:  34%|███▍      | 1403/4096 [00:04<00:07, 350.93it/s]
Adding requests:  35%|███▌      | 1439/4096 [00:04<00:07, 353.33it/s]
Adding requests:  36%|███▌      | 1475/4096 [00:04<00:07, 350.69it/s]
Adding requests:  37%|███▋      | 1513/4096 [00:04<00:07, 356.79it/s]
Adding requests:  38%|███▊      | 1549/4096 [00:04<00:07, 354.60it/s]
Adding requests:  39%|███▊      | 1585/4096 [00:04<00:07, 346.40it/s]
Adding requests:  40%|███▉      | 1620/4096 [00:04<00:07, 343.67it/s]
Adding requests:  40%|████      | 1655/4096 [00:04<00:07, 338.20it/s]
Adding requests:  41%|████▏     | 1690/4096 [00:04<00:07, 339.75it/s]
Adding requests:  42%|████▏     | 1727/4096 [00:04<00:06, 346.52it/s]
Adding requests:  43%|████▎     | 1763/4096 [00:05<00:06, 348.82it/s]
Adding requests:  44%|████▍     | 1798/4096 [00:05<00:06, 348.18it/s]
Adding requests:  45%|████▍     | 1834/4096 [00:05<00:06, 350.55it/s]
Adding requests:  46%|████▌     | 1870/4096 [00:05<00:06, 350.25it/s]
Adding requests:  47%|████▋     | 1906/4096 [00:05<00:06, 352.57it/s]
Adding requests:  47%|████▋     | 1944/4096 [00:05<00:05, 360.58it/s]
Adding requests:  48%|████▊     | 1981/4096 [00:05<00:05, 358.01it/s]
Adding requests:  49%|████▉     | 2017/4096 [00:05<00:05, 349.29it/s]
Adding requests:  50%|█████     | 2052/4096 [00:05<00:05, 347.31it/s]
Adding requests:  51%|█████     | 2087/4096 [00:06<00:05, 338.53it/s]
Adding requests:  52%|█████▏    | 2125/4096 [00:06<00:05, 347.83it/s]
Adding requests:  53%|█████▎    | 2160/4096 [00:06<00:05, 343.93it/s]
Adding requests:  54%|█████▎    | 2195/4096 [00:06<00:05, 330.46it/s]
Adding requests:  54%|█████▍    | 2230/4096 [00:06<00:05, 334.35it/s]
Adding requests:  55%|█████▌    | 2267/4096 [00:06<00:05, 342.96it/s]
Adding requests:  56%|█████▌    | 2302/4096 [00:06<00:05, 343.62it/s]
Adding requests:  57%|█████▋    | 2339/4096 [00:06<00:05, 349.94it/s]
Adding requests:  58%|█████▊    | 2375/4096 [00:06<00:04, 350.50it/s]
Adding requests:  59%|█████▉    | 2412/4096 [00:06<00:04, 356.11it/s]
Adding requests:  60%|█████▉    | 2448/4096 [00:07<00:04, 356.81it/s]
Adding requests:  61%|██████    | 2484/4096 [00:07<00:04, 356.06it/s]
Adding requests:  62%|██████▏   | 2521/4096 [00:07<00:04, 358.77it/s]
Adding requests:  62%|██████▎   | 2560/4096 [00:07<00:04, 366.93it/s]
Adding requests:  63%|██████▎   | 2599/4096 [00:07<00:04, 372.60it/s]
Adding requests:  64%|██████▍   | 2637/4096 [00:07<00:04, 357.60it/s]
Adding requests:  65%|██████▌   | 2673/4096 [00:07<00:04, 354.69it/s]
Adding requests:  66%|██████▌   | 2709/4096 [00:07<00:03, 350.08it/s]
Adding requests:  67%|██████▋   | 2746/4096 [00:07<00:03, 354.66it/s]
Adding requests:  68%|██████▊   | 2783/4096 [00:07<00:03, 359.12it/s]
Adding requests:  69%|██████▉   | 2821/4096 [00:08<00:03, 362.55it/s]
Adding requests:  70%|██████▉   | 2858/4096 [00:08<00:03, 361.51it/s]
Adding requests:  71%|███████   | 2895/4096 [00:08<00:03, 360.77it/s]
Adding requests:  72%|███████▏  | 2932/4096 [00:08<00:03, 358.07it/s]
Adding requests:  73%|███████▎  | 2970/4096 [00:08<00:03, 361.14it/s]
Adding requests:  73%|███████▎  | 3007/4096 [00:08<00:03, 362.19it/s]
Adding requests:  74%|███████▍  | 3045/4096 [00:08<00:02, 365.64it/s]
Adding requests:  75%|███████▌  | 3082/4096 [00:08<00:02, 366.22it/s]
Adding requests:  76%|███████▌  | 3120/4096 [00:08<00:02, 369.28it/s]
Adding requests:  77%|███████▋  | 3157/4096 [00:08<00:02, 365.38it/s]
Adding requests:  78%|███████▊  | 3194/4096 [00:09<00:02, 358.55it/s]
Adding requests:  79%|███████▉  | 3232/4096 [00:09<00:02, 362.67it/s]
Adding requests:  80%|███████▉  | 3269/4096 [00:09<00:02, 356.26it/s]
Adding requests:  81%|████████  | 3305/4096 [00:09<00:02, 347.48it/s]
Adding requests:  82%|████████▏ | 3341/4096 [00:09<00:02, 349.76it/s]
Adding requests:  82%|████████▏ | 3378/4096 [00:09<00:02, 354.93it/s]
Adding requests:  83%|████████▎ | 3414/4096 [00:09<00:01, 355.38it/s]
Adding requests:  84%|████████▍ | 3451/4096 [00:09<00:01, 356.81it/s]
Adding requests:  85%|████████▌ | 3487/4096 [00:09<00:01, 351.96it/s]
Adding requests:  86%|████████▌ | 3523/4096 [00:10<00:01, 348.37it/s]
Adding requests:  87%|████████▋ | 3563/4096 [00:10<00:01, 359.99it/s]
Adding requests:  88%|████████▊ | 3600/4096 [00:10<00:01, 354.89it/s]
Adding requests:  89%|████████▉ | 3637/4096 [00:10<00:01, 357.71it/s]
Adding requests:  90%|████████▉ | 3673/4096 [00:10<00:01, 351.87it/s]
Adding requests:  91%|█████████ | 3709/4096 [00:10<00:01, 350.69it/s]
Adding requests:  91%|█████████▏| 3745/4096 [00:10<00:00, 351.16it/s]
Adding requests:  92%|█████████▏| 3781/4096 [00:10<00:00, 341.67it/s]
Adding requests:  93%|█████████▎| 3816/4096 [00:10<00:00, 333.60it/s]
Adding requests:  94%|█████████▍| 3851/4096 [00:10<00:00, 337.99it/s]
Adding requests:  95%|█████████▍| 3886/4096 [00:11<00:00, 341.39it/s]
Adding requests:  96%|█████████▌| 3921/4096 [00:11<00:00, 336.38it/s]
Adding requests:  97%|█████████▋| 3957/4096 [00:11<00:00, 341.10it/s]
Adding requests:  97%|█████████▋| 3992/4096 [00:11<00:00, 338.36it/s]
Adding requests:  98%|█████████▊| 4027/4096 [00:11<00:00, 340.43it/s]
Adding requests:  99%|█████████▉| 4062/4096 [00:11<00:00, 339.94it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 350.04it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 756/4096 [00:00<00:02, 1582.84it/s, est. speed input: 1620851.60 toks/s, output: 1582.84 toks/s]
Processed prompts:  22%|██▏       | 915/4096 [00:02<00:10, 297.43it/s, est. speed input: 381321.49 toks/s, output: 372.38 toks/s]   
Processed prompts:  24%|██▍       | 985/4096 [00:03<00:17, 177.66it/s, est. speed input: 255805.36 toks/s, output: 249.81 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:04<00:19, 160.01it/s, est. speed input: 236677.91 toks/s, output: 231.13 toks/s]
Processed prompts:  26%|██▌       | 1054/4096 [00:04<00:21, 138.46it/s, est. speed input: 218827.91 toks/s, output: 213.70 toks/s]
Processed prompts:  26%|██▋       | 1076/4096 [00:05<00:25, 116.73it/s, est. speed input: 202975.32 toks/s, output: 198.22 toks/s]
Processed prompts:  27%|██▋       | 1108/4096 [00:05<00:28, 103.86it/s, est. speed input: 191559.96 toks/s, output: 187.07 toks/s]
Processed prompts:  28%|██▊       | 1140/4096 [00:06<00:31, 93.79it/s, est. speed input: 181990.82 toks/s, output: 177.73 toks/s] 
Processed prompts:  29%|██▊       | 1172/4096 [00:06<00:33, 86.01it/s, est. speed input: 173778.32 toks/s, output: 169.71 toks/s]
Processed prompts:  29%|██▉       | 1204/4096 [00:07<00:35, 80.62it/s, est. speed input: 166876.27 toks/s, output: 162.96 toks/s]
Processed prompts:  30%|███       | 1236/4096 [00:07<00:37, 76.07it/s, est. speed input: 160574.79 toks/s, output: 156.81 toks/s]
Processed prompts:  31%|███       | 1268/4096 [00:08<00:38, 72.91it/s, est. speed input: 155069.58 toks/s, output: 151.43 toks/s]
Processed prompts:  32%|███▏      | 1300/4096 [00:08<00:39, 70.66it/s, est. speed input: 150182.10 toks/s, output: 146.66 toks/s]
Processed prompts:  33%|███▎      | 1332/4096 [00:09<00:40, 69.07it/s, est. speed input: 145813.91 toks/s, output: 142.40 toks/s]
Processed prompts:  33%|███▎      | 1364/4096 [00:09<00:40, 67.90it/s, est. speed input: 141865.53 toks/s, output: 138.54 toks/s]
Processed prompts:  34%|███▍      | 1396/4096 [00:10<00:40, 67.22it/s, est. speed input: 138338.90 toks/s, output: 135.10 toks/s]
Processed prompts:  35%|███▍      | 1428/4096 [00:10<00:40, 66.58it/s, est. speed input: 135083.69 toks/s, output: 131.92 toks/s]
Processed prompts:  36%|███▌      | 1460/4096 [00:11<00:39, 66.56it/s, est. speed input: 132231.49 toks/s, output: 129.13 toks/s]
Processed prompts:  36%|███▋      | 1492/4096 [00:11<00:39, 65.89it/s, est. speed input: 129437.10 toks/s, output: 126.40 toks/s]
Processed prompts:  37%|███▋      | 1524/4096 [00:12<00:38, 65.96it/s, est. speed input: 127006.54 toks/s, output: 124.03 toks/s]
Processed prompts:  38%|███▊      | 1556/4096 [00:12<00:38, 66.13it/s, est. speed input: 124788.95 toks/s, output: 121.86 toks/s]
Processed prompts:  39%|███▉      | 1588/4096 [00:13<00:38, 65.74it/s, est. speed input: 122616.09 toks/s, output: 119.74 toks/s]
Processed prompts:  40%|███▉      | 1620/4096 [00:13<00:37, 66.07it/s, est. speed input: 120730.11 toks/s, output: 117.90 toks/s]
Processed prompts:  40%|████      | 1652/4096 [00:14<00:37, 65.68it/s, est. speed input: 118843.20 toks/s, output: 116.06 toks/s]
Processed prompts:  41%|████      | 1684/4096 [00:14<00:36, 65.57it/s, est. speed input: 117113.88 toks/s, output: 114.37 toks/s]
Processed prompts:  42%|████▏     | 1716/4096 [00:15<00:36, 65.85it/s, est. speed input: 115563.93 toks/s, output: 112.86 toks/s]
Processed prompts:  43%|████▎     | 1748/4096 [00:15<00:35, 66.07it/s, est. speed input: 114112.56 toks/s, output: 111.44 toks/s]
Processed prompts:  43%|████▎     | 1780/4096 [00:16<00:35, 65.59it/s, est. speed input: 112638.63 toks/s, output: 110.00 toks/s]
Processed prompts:  44%|████▍     | 1812/4096 [00:16<00:34, 65.37it/s, est. speed input: 111271.27 toks/s, output: 108.66 toks/s]
Processed prompts:  45%|████▌     | 1844/4096 [00:17<00:34, 65.10it/s, est. speed input: 109964.03 toks/s, output: 107.39 toks/s]
Processed prompts:  46%|████▌     | 1876/4096 [00:17<00:33, 65.44it/s, est. speed input: 108810.44 toks/s, output: 106.26 toks/s]
Processed prompts:  47%|████▋     | 1908/4096 [00:18<00:33, 65.37it/s, est. speed input: 107673.69 toks/s, output: 105.15 toks/s]
Processed prompts:  47%|████▋     | 1940/4096 [00:18<00:32, 65.38it/s, est. speed input: 106604.71 toks/s, output: 104.11 toks/s]
Processed prompts:  48%|████▊     | 1972/4096 [00:19<00:32, 65.75it/s, est. speed input: 105640.21 toks/s, output: 103.16 toks/s]
Processed prompts:  49%|████▉     | 2004/4096 [00:19<00:31, 65.67it/s, est. speed input: 104679.38 toks/s, output: 102.23 toks/s]
Processed prompts:  50%|████▉     | 2036/4096 [00:20<00:31, 65.36it/s, est. speed input: 103731.97 toks/s, output: 101.30 toks/s]
Processed prompts:  50%|█████     | 2068/4096 [00:20<00:30, 65.72it/s, est. speed input: 102900.95 toks/s, output: 100.49 toks/s]
Processed prompts:  51%|█████▏    | 2100/4096 [00:21<00:30, 65.44it/s, est. speed input: 102044.85 toks/s, output: 99.65 toks/s] 
Processed prompts:  52%|█████▏    | 2132/4096 [00:21<00:30, 64.97it/s, est. speed input: 101195.01 toks/s, output: 98.82 toks/s]
Processed prompts:  53%|█████▎    | 2164/4096 [00:22<00:29, 65.13it/s, est. speed input: 100439.05 toks/s, output: 98.08 toks/s]
Processed prompts:  54%|█████▎    | 2196/4096 [00:22<00:29, 65.42it/s, est. speed input: 99735.98 toks/s, output: 97.40 toks/s] 
Processed prompts:  54%|█████▍    | 2228/4096 [00:23<00:28, 65.13it/s, est. speed input: 99009.96 toks/s, output: 96.69 toks/s]
Processed prompts:  55%|█████▌    | 2260/4096 [00:23<00:28, 65.09it/s, est. speed input: 98331.09 toks/s, output: 96.03 toks/s]
Processed prompts:  56%|█████▌    | 2292/4096 [00:24<00:27, 64.82it/s, est. speed input: 97654.47 toks/s, output: 95.37 toks/s]
Processed prompts:  57%|█████▋    | 2324/4096 [00:24<00:27, 64.93it/s, est. speed input: 97036.31 toks/s, output: 94.76 toks/s]
Processed prompts:  58%|█████▊    | 2356/4096 [00:25<00:26, 64.84it/s, est. speed input: 96425.51 toks/s, output: 94.17 toks/s]
Processed prompts:  58%|█████▊    | 2388/4096 [00:25<00:26, 64.80it/s, est. speed input: 95841.06 toks/s, output: 93.59 toks/s]
Processed prompts:  59%|█████▉    | 2420/4096 [00:26<00:25, 64.92it/s, est. speed input: 95292.39 toks/s, output: 93.06 toks/s]
Processed prompts:  60%|█████▉    | 2452/4096 [00:26<00:25, 64.88it/s, est. speed input: 94752.26 toks/s, output: 92.53 toks/s]
Processed prompts:  61%|██████    | 2484/4096 [00:26<00:24, 64.86it/s, est. speed input: 94233.55 toks/s, output: 92.02 toks/s]
Processed prompts:  61%|██████▏   | 2516/4096 [00:27<00:24, 65.29it/s, est. speed input: 93771.23 toks/s, output: 91.57 toks/s]
Processed prompts:  62%|██████▏   | 2548/4096 [00:27<00:23, 65.01it/s, est. speed input: 93275.82 toks/s, output: 91.09 toks/s]
Processed prompts:  63%|██████▎   | 2580/4096 [00:28<00:23, 65.36it/s, est. speed input: 92842.34 toks/s, output: 90.67 toks/s]
Processed prompts:  64%|██████▍   | 2612/4096 [00:28<00:22, 65.24it/s, est. speed input: 92394.71 toks/s, output: 90.23 toks/s]
Processed prompts:  65%|██████▍   | 2644/4096 [00:29<00:22, 65.08it/s, est. speed input: 91955.48 toks/s, output: 89.80 toks/s]
Processed prompts:  65%|██████▌   | 2676/4096 [00:29<00:21, 64.91it/s, est. speed input: 91526.45 toks/s, output: 89.38 toks/s]
Processed prompts:  66%|██████▌   | 2708/4096 [00:30<00:21, 64.89it/s, est. speed input: 91119.47 toks/s, output: 88.98 toks/s]
Processed prompts:  67%|██████▋   | 2740/4096 [00:30<00:20, 65.27it/s, est. speed input: 90753.83 toks/s, output: 88.63 toks/s]
Processed prompts:  68%|██████▊   | 2772/4096 [00:31<00:20, 64.97it/s, est. speed input: 90359.21 toks/s, output: 88.24 toks/s]
Processed prompts:  68%|██████▊   | 2804/4096 [00:31<00:19, 64.94it/s, est. speed input: 89989.15 toks/s, output: 87.88 toks/s]
Processed prompts:  69%|██████▉   | 2836/4096 [00:32<00:19, 64.86it/s, est. speed input: 89626.38 toks/s, output: 87.53 toks/s]
Processed prompts:  70%|███████   | 2868/4096 [00:32<00:18, 64.96it/s, est. speed input: 89284.85 toks/s, output: 87.19 toks/s]
Processed prompts:  71%|███████   | 2900/4096 [00:33<00:18, 65.69it/s, est. speed input: 88997.60 toks/s, output: 86.91 toks/s]
Processed prompts:  72%|███████▏  | 2932/4096 [00:33<00:17, 65.43it/s, est. speed input: 88667.82 toks/s, output: 86.59 toks/s]
Processed prompts:  72%|███████▏  | 2964/4096 [00:34<00:17, 65.25it/s, est. speed input: 88347.99 toks/s, output: 86.28 toks/s]
Processed prompts:  73%|███████▎  | 2996/4096 [00:34<00:16, 65.00it/s, est. speed input: 88028.86 toks/s, output: 85.97 toks/s]
Processed prompts:  74%|███████▍  | 3028/4096 [00:35<00:16, 64.85it/s, est. speed input: 87720.25 toks/s, output: 85.66 toks/s]
Processed prompts:  75%|███████▍  | 3060/4096 [00:35<00:15, 64.78it/s, est. speed input: 87422.76 toks/s, output: 85.37 toks/s]
Processed prompts:  75%|███████▌  | 3092/4096 [00:36<00:15, 64.95it/s, est. speed input: 87146.17 toks/s, output: 85.10 toks/s]
Processed prompts:  76%|███████▋  | 3124/4096 [00:36<00:15, 64.76it/s, est. speed input: 86858.99 toks/s, output: 84.82 toks/s]
Processed prompts:  77%|███████▋  | 3156/4096 [00:37<00:14, 64.74it/s, est. speed input: 86585.62 toks/s, output: 84.56 toks/s]
Processed prompts:  78%|███████▊  | 3188/4096 [00:37<00:14, 64.77it/s, est. speed input: 86322.10 toks/s, output: 84.30 toks/s]
Processed prompts:  79%|███████▊  | 3220/4096 [00:38<00:13, 64.66it/s, est. speed input: 86057.79 toks/s, output: 84.04 toks/s]
Processed prompts:  79%|███████▉  | 3252/4096 [00:38<00:13, 64.61it/s, est. speed input: 85801.80 toks/s, output: 83.79 toks/s]
Processed prompts:  80%|████████  | 3284/4096 [00:39<00:12, 64.52it/s, est. speed input: 85549.45 toks/s, output: 83.54 toks/s]
Processed prompts:  81%|████████  | 3316/4096 [00:39<00:12, 64.50it/s, est. speed input: 85305.41 toks/s, output: 83.31 toks/s]
Processed prompts:  82%|████████▏ | 3348/4096 [00:40<00:11, 64.54it/s, est. speed input: 85070.71 toks/s, output: 83.08 toks/s]
Processed prompts:  83%|████████▎ | 3380/4096 [00:40<00:11, 64.65it/s, est. speed input: 84845.91 toks/s, output: 82.86 toks/s]
Processed prompts:  83%|████████▎ | 3412/4096 [00:41<00:10, 64.64it/s, est. speed input: 84621.56 toks/s, output: 82.64 toks/s]
Processed prompts:  84%|████████▍ | 3444/4096 [00:41<00:10, 64.66it/s, est. speed input: 84404.61 toks/s, output: 82.43 toks/s]
Processed prompts:  85%|████████▍ | 3476/4096 [00:42<00:09, 64.62it/s, est. speed input: 84189.60 toks/s, output: 82.22 toks/s]
Processed prompts:  86%|████████▌ | 3508/4096 [00:42<00:09, 64.55it/s, est. speed input: 83977.37 toks/s, output: 82.01 toks/s]
Processed prompts:  86%|████████▋ | 3540/4096 [00:43<00:08, 63.01it/s, est. speed input: 83694.32 toks/s, output: 81.73 toks/s]
Processed prompts:  87%|████████▋ | 3572/4096 [00:43<00:08, 63.46it/s, est. speed input: 83494.83 toks/s, output: 81.54 toks/s]
Processed prompts:  88%|████████▊ | 3604/4096 [00:44<00:07, 63.84it/s, est. speed input: 83303.03 toks/s, output: 81.35 toks/s]
Processed prompts:  89%|████████▉ | 3636/4096 [00:44<00:07, 64.07it/s, est. speed input: 83113.42 toks/s, output: 81.17 toks/s]
Processed prompts:  90%|████████▉ | 3668/4096 [00:45<00:06, 64.80it/s, est. speed input: 82955.04 toks/s, output: 81.01 toks/s]
Processed prompts:  90%|█████████ | 3700/4096 [00:45<00:06, 64.75it/s, est. speed input: 82773.48 toks/s, output: 80.83 toks/s]
Processed prompts:  91%|█████████ | 3732/4096 [00:46<00:05, 64.84it/s, est. speed input: 82601.72 toks/s, output: 80.67 toks/s]
Processed prompts:  92%|█████████▏| 3764/4096 [00:46<00:05, 64.81it/s, est. speed input: 82429.45 toks/s, output: 80.50 toks/s]
Processed prompts:  93%|█████████▎| 3796/4096 [00:47<00:04, 64.74it/s, est. speed input: 82258.59 toks/s, output: 80.33 toks/s]
Processed prompts:  93%|█████████▎| 3828/4096 [00:47<00:04, 64.69it/s, est. speed input: 82091.11 toks/s, output: 80.17 toks/s]
Processed prompts:  94%|█████████▍| 3860/4096 [00:48<00:03, 64.75it/s, est. speed input: 81931.13 toks/s, output: 80.01 toks/s]
Processed prompts:  95%|█████████▌| 3892/4096 [00:48<00:03, 64.76it/s, est. speed input: 81773.21 toks/s, output: 79.86 toks/s]
Processed prompts:  96%|█████████▌| 3924/4096 [00:49<00:02, 65.31it/s, est. speed input: 81640.99 toks/s, output: 79.73 toks/s]
Processed prompts:  97%|█████████▋| 3956/4096 [00:49<00:02, 65.08it/s, est. speed input: 81486.07 toks/s, output: 79.58 toks/s]
Processed prompts:  97%|█████████▋| 3988/4096 [00:50<00:01, 65.28it/s, est. speed input: 81348.85 toks/s, output: 79.44 toks/s]
Processed prompts:  98%|█████████▊| 4020/4096 [00:50<00:01, 65.19it/s, est. speed input: 81204.87 toks/s, output: 79.30 toks/s]
Processed prompts:  99%|█████████▉| 4052/4096 [00:51<00:00, 65.16it/s, est. speed input: 81065.12 toks/s, output: 79.17 toks/s]
Processed prompts: 100%|█████████▉| 4084/4096 [00:51<00:00, 78.37it/s, est. speed input: 81363.32 toks/s, output: 79.46 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:51<00:00, 78.37it/s, est. speed input: 81602.06 toks/s, output: 79.69 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:51<00:00, 79.69it/s, est. speed input: 81602.06 toks/s, output: 79.69 toks/s]
[rank0]:[W126 18:58:43.876999675 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 18:58:45
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 18:59:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=948722) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=948722) WARNING 01-26 19:00:03 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=948722) WARNING 01-26 19:00:22 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 65.62 requests/s, 67261.72 total tokens/s, 65.62 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 18:59:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:59:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:59:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:59:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:59:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:59:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:59:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:59:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:59:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:59:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:59:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:59:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:59:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:59:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 18:59:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 18:59:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:59:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 18:59:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:59:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:59:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:59:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:59:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 18:59:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 18:59:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 18:59:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 18:59:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 18:59:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 18:59:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=948722) [2026-01-26 18:59:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=948722) [2026-01-26 18:59:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=948722) [2026-01-26 18:59:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=948722) [2026-01-26 18:59:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=948722) [2026-01-26 18:59:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=948722) [2026-01-26 18:59:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=948722) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=948722) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.22s/it]
(EngineCore_DP0 pid=948722) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.41it/s]
(EngineCore_DP0 pid=948722) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.09it/s]
(EngineCore_DP0 pid=948722) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]
(EngineCore_DP0 pid=948722) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.06s/it]
(EngineCore_DP0 pid=948722) 
(EngineCore_DP0 pid=948722) [2026-01-26 18:59:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=948722) [2026-01-26 18:59:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=948722) [2026-01-26 18:59:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=948722) [2026-01-26 18:59:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=948722) [2026-01-26 18:59:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=948722) [2026-01-26 18:59:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=948722) [2026-01-26 18:59:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=948722) [2026-01-26 18:59:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=948722) [rank0]:W0126 19:00:11.052000 948722 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=948722) [rank0]:W0126 19:00:11.123000 948722 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=948722) [rank0]:W0126 19:00:11.936000 948722 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=948722) [rank0]:W0126 19:00:12.044000 948722 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=948722) 2026-01-26 19:00:22,151 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=948722) 2026-01-26 19:00:22,369 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=948722) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:04,  4.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:03,  4.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:02,  6.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  7.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:01,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  9.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:01<00:00,  9.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:01<00:00, 10.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.69it/s]
(EngineCore_DP0 pid=948722) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00,  9.18it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  9.30it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00, 10.20it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 10.62it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 10.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00, 11.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00, 10.63it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 27/8192 [00:00<00:30, 265.21it/s]
Adding requests:   1%|          | 61/8192 [00:00<00:26, 306.51it/s]
Adding requests:   1%|          | 92/8192 [00:00<00:26, 305.17it/s]
Adding requests:   2%|▏         | 125/8192 [00:00<00:25, 313.87it/s]
Adding requests:   2%|▏         | 159/8192 [00:00<00:24, 322.39it/s]
Adding requests:   2%|▏         | 195/8192 [00:00<00:24, 331.33it/s]
Adding requests:   3%|▎         | 230/8192 [00:00<00:23, 337.07it/s]
Adding requests:   3%|▎         | 264/8192 [00:00<00:23, 334.04it/s]
Adding requests:   4%|▎         | 298/8192 [00:00<00:23, 335.71it/s]
Adding requests:   4%|▍         | 332/8192 [00:01<00:23, 333.96it/s]
Adding requests:   4%|▍         | 366/8192 [00:01<00:23, 335.52it/s]
Adding requests:   5%|▍         | 401/8192 [00:01<00:23, 338.52it/s]
Adding requests:   5%|▌         | 436/8192 [00:01<00:22, 340.81it/s]
Adding requests:   6%|▌         | 472/8192 [00:01<00:22, 346.26it/s]
Adding requests:   6%|▌         | 511/8192 [00:01<00:21, 357.38it/s]
Adding requests:   7%|▋         | 549/8192 [00:01<00:21, 362.38it/s]
Adding requests:   7%|▋         | 586/8192 [00:01<00:21, 359.50it/s]
Adding requests:   8%|▊         | 622/8192 [00:01<00:21, 358.56it/s]
Adding requests:   8%|▊         | 658/8192 [00:01<00:21, 349.60it/s]
Adding requests:   8%|▊         | 695/8192 [00:02<00:21, 353.96it/s]
Adding requests:   9%|▉         | 731/8192 [00:02<00:21, 349.77it/s]
Adding requests:   9%|▉         | 767/8192 [00:02<00:21, 349.79it/s]
Adding requests:  10%|▉         | 803/8192 [00:02<00:21, 348.67it/s]
Adding requests:  10%|█         | 841/8192 [00:02<00:20, 355.44it/s]
Adding requests:  11%|█         | 877/8192 [00:02<00:20, 352.79it/s]
Adding requests:  11%|█         | 913/8192 [00:02<00:20, 347.24it/s]
Adding requests:  12%|█▏        | 948/8192 [00:02<00:21, 338.51it/s]
Adding requests:  12%|█▏        | 982/8192 [00:02<00:21, 337.87it/s]
Adding requests:  12%|█▏        | 1016/8192 [00:02<00:21, 330.95it/s]
Adding requests:  13%|█▎        | 1050/8192 [00:03<00:21, 330.33it/s]
Adding requests:  13%|█▎        | 1084/8192 [00:03<00:21, 329.54it/s]
Adding requests:  14%|█▎        | 1117/8192 [00:03<00:21, 327.38it/s]
Adding requests:  14%|█▍        | 1151/8192 [00:03<00:21, 331.03it/s]
Adding requests:  14%|█▍        | 1186/8192 [00:03<00:20, 334.62it/s]
Adding requests:  15%|█▍        | 1224/8192 [00:03<00:20, 345.75it/s]
Adding requests:  15%|█▌        | 1259/8192 [00:03<00:20, 344.30it/s]
Adding requests:  16%|█▌        | 1294/8192 [00:03<00:20, 342.42it/s]
Adding requests:  16%|█▌        | 1330/8192 [00:03<00:19, 346.91it/s]
Adding requests:  17%|█▋        | 1367/8192 [00:04<00:19, 351.79it/s]
Adding requests:  17%|█▋        | 1403/8192 [00:04<00:19, 349.90it/s]
Adding requests:  18%|█▊        | 1439/8192 [00:04<00:19, 352.40it/s]
Adding requests:  18%|█▊        | 1475/8192 [00:04<00:19, 349.78it/s]
Adding requests:  18%|█▊        | 1513/8192 [00:04<00:18, 355.92it/s]
Adding requests:  19%|█▉        | 1549/8192 [00:04<00:18, 353.99it/s]
Adding requests:  19%|█▉        | 1585/8192 [00:04<00:19, 338.41it/s]
Adding requests:  20%|█▉        | 1619/8192 [00:04<00:19, 335.32it/s]
Adding requests:  20%|██        | 1653/8192 [00:04<00:19, 333.13it/s]
Adding requests:  21%|██        | 1688/8192 [00:04<00:19, 334.82it/s]
Adding requests:  21%|██        | 1724/8192 [00:05<00:18, 340.93it/s]
Adding requests:  21%|██▏       | 1760/8192 [00:05<00:18, 344.16it/s]
Adding requests:  22%|██▏       | 1796/8192 [00:05<00:18, 347.57it/s]
Adding requests:  22%|██▏       | 1832/8192 [00:05<00:18, 350.76it/s]
Adding requests:  23%|██▎       | 1868/8192 [00:05<00:18, 335.92it/s]
Adding requests:  23%|██▎       | 1904/8192 [00:05<00:18, 340.83it/s]
Adding requests:  24%|██▎       | 1941/8192 [00:05<00:17, 348.77it/s]
Adding requests:  24%|██▍       | 1976/8192 [00:05<00:18, 344.17it/s]
Adding requests:  25%|██▍       | 2011/8192 [00:05<00:18, 342.12it/s]
Adding requests:  25%|██▍       | 2046/8192 [00:05<00:18, 338.51it/s]
Adding requests:  25%|██▌       | 2080/8192 [00:06<00:18, 332.46it/s]
Adding requests:  26%|██▌       | 2117/8192 [00:06<00:17, 343.15it/s]
Adding requests:  26%|██▋       | 2152/8192 [00:06<00:17, 341.78it/s]
Adding requests:  27%|██▋       | 2187/8192 [00:06<00:17, 336.46it/s]
Adding requests:  27%|██▋       | 2222/8192 [00:06<00:17, 339.74it/s]
Adding requests:  28%|██▊       | 2258/8192 [00:06<00:17, 344.77it/s]
Adding requests:  28%|██▊       | 2295/8192 [00:06<00:16, 349.63it/s]
Adding requests:  28%|██▊       | 2331/8192 [00:06<00:16, 350.00it/s]
Adding requests:  29%|██▉       | 2368/8192 [00:06<00:16, 353.43it/s]
Adding requests:  29%|██▉       | 2405/8192 [00:07<00:16, 358.20it/s]
Adding requests:  30%|██▉       | 2442/8192 [00:07<00:15, 360.67it/s]
Adding requests:  30%|███       | 2479/8192 [00:07<00:15, 358.74it/s]
Adding requests:  31%|███       | 2516/8192 [00:07<00:15, 361.81it/s]
Adding requests:  31%|███       | 2555/8192 [00:07<00:15, 367.62it/s]
Adding requests:  32%|███▏      | 2594/8192 [00:07<00:15, 372.03it/s]
Adding requests:  32%|███▏      | 2632/8192 [00:07<00:15, 360.79it/s]
Adding requests:  33%|███▎      | 2669/8192 [00:07<00:15, 355.27it/s]
Adding requests:  33%|███▎      | 2705/8192 [00:07<00:15, 352.81it/s]
Adding requests:  33%|███▎      | 2741/8192 [00:07<00:15, 351.87it/s]
Adding requests:  34%|███▍      | 2779/8192 [00:08<00:15, 359.22it/s]
Adding requests:  34%|███▍      | 2817/8192 [00:08<00:14, 364.73it/s]
Adding requests:  35%|███▍      | 2854/8192 [00:08<00:15, 350.74it/s]
Adding requests:  35%|███▌      | 2890/8192 [00:08<00:15, 351.68it/s]
Adding requests:  36%|███▌      | 2926/8192 [00:08<00:14, 353.46it/s]
Adding requests:  36%|███▌      | 2963/8192 [00:08<00:14, 357.08it/s]
Adding requests:  37%|███▋      | 3000/8192 [00:08<00:14, 358.50it/s]
Adding requests:  37%|███▋      | 3038/8192 [00:08<00:14, 362.74it/s]
Adding requests:  38%|███▊      | 3076/8192 [00:08<00:14, 364.62it/s]
Adding requests:  38%|███▊      | 3113/8192 [00:08<00:13, 364.49it/s]
Adding requests:  38%|███▊      | 3150/8192 [00:09<00:13, 365.70it/s]
Adding requests:  39%|███▉      | 3187/8192 [00:09<00:13, 360.21it/s]
Adding requests:  39%|███▉      | 3224/8192 [00:09<00:13, 358.03it/s]
Adding requests:  40%|███▉      | 3261/8192 [00:09<00:13, 359.33it/s]
Adding requests:  40%|████      | 3297/8192 [00:09<00:14, 347.50it/s]
Adding requests:  41%|████      | 3332/8192 [00:09<00:14, 345.33it/s]
Adding requests:  41%|████      | 3370/8192 [00:09<00:13, 354.59it/s]
Adding requests:  42%|████▏     | 3406/8192 [00:09<00:13, 354.98it/s]
Adding requests:  42%|████▏     | 3443/8192 [00:09<00:13, 359.06it/s]
Adding requests:  42%|████▏     | 3480/8192 [00:10<00:13, 359.32it/s]
Adding requests:  43%|████▎     | 3516/8192 [00:10<00:13, 357.84it/s]
Adding requests:  43%|████▎     | 3555/8192 [00:10<00:12, 366.82it/s]
Adding requests:  44%|████▍     | 3592/8192 [00:10<00:12, 362.17it/s]
Adding requests:  44%|████▍     | 3629/8192 [00:10<00:12, 363.33it/s]
Adding requests:  45%|████▍     | 3666/8192 [00:10<00:12, 358.27it/s]
Adding requests:  45%|████▌     | 3702/8192 [00:10<00:12, 353.82it/s]
Adding requests:  46%|████▌     | 3738/8192 [00:10<00:12, 353.89it/s]
Adding requests:  46%|████▌     | 3774/8192 [00:10<00:12, 344.67it/s]
Adding requests:  46%|████▋     | 3809/8192 [00:10<00:13, 334.18it/s]
Adding requests:  47%|████▋     | 3845/8192 [00:11<00:12, 339.44it/s]
Adding requests:  47%|████▋     | 3881/8192 [00:11<00:12, 342.35it/s]
Adding requests:  48%|████▊     | 3916/8192 [00:11<00:12, 337.95it/s]
Adding requests:  48%|████▊     | 3951/8192 [00:11<00:12, 339.69it/s]
Adding requests:  49%|████▊     | 3986/8192 [00:11<00:12, 340.32it/s]
Adding requests:  49%|████▉     | 4021/8192 [00:11<00:12, 341.47it/s]
Adding requests:  50%|████▉     | 4056/8192 [00:11<00:12, 339.09it/s]
Adding requests:  50%|████▉     | 4090/8192 [00:11<00:12, 338.86it/s]
Adding requests:  50%|█████     | 4127/8192 [00:11<00:11, 345.94it/s]
Adding requests:  51%|█████     | 4162/8192 [00:12<00:12, 335.04it/s]
Adding requests:  51%|█████     | 4197/8192 [00:12<00:11, 339.17it/s]
Adding requests:  52%|█████▏    | 4231/8192 [00:12<00:11, 336.64it/s]
Adding requests:  52%|█████▏    | 4267/8192 [00:12<00:11, 341.58it/s]
Adding requests:  53%|█████▎    | 4302/8192 [00:12<00:11, 335.90it/s]
Adding requests:  53%|█████▎    | 4336/8192 [00:12<00:11, 336.29it/s]
Adding requests:  53%|█████▎    | 4373/8192 [00:12<00:11, 344.83it/s]
Adding requests:  54%|█████▍    | 4408/8192 [00:12<00:11, 341.48it/s]
Adding requests:  54%|█████▍    | 4444/8192 [00:12<00:10, 345.84it/s]
Adding requests:  55%|█████▍    | 4480/8192 [00:12<00:10, 349.13it/s]
Adding requests:  55%|█████▌    | 4515/8192 [00:13<00:10, 347.67it/s]
Adding requests:  56%|█████▌    | 4551/8192 [00:13<00:10, 349.98it/s]
Adding requests:  56%|█████▌    | 4587/8192 [00:13<00:10, 349.63it/s]
Adding requests:  56%|█████▋    | 4622/8192 [00:13<00:10, 346.02it/s]
Adding requests:  57%|█████▋    | 4657/8192 [00:13<00:10, 343.64it/s]
Adding requests:  57%|█████▋    | 4692/8192 [00:13<00:10, 337.86it/s]
Adding requests:  58%|█████▊    | 4728/8192 [00:13<00:10, 344.01it/s]
Adding requests:  58%|█████▊    | 4766/8192 [00:13<00:09, 351.91it/s]
Adding requests:  59%|█████▊    | 4802/8192 [00:13<00:09, 340.02it/s]
Adding requests:  59%|█████▉    | 4837/8192 [00:13<00:10, 332.61it/s]
Adding requests:  59%|█████▉    | 4873/8192 [00:14<00:09, 338.92it/s]
Adding requests:  60%|█████▉    | 4908/8192 [00:14<00:09, 338.45it/s]
Adding requests:  60%|██████    | 4946/8192 [00:14<00:09, 348.83it/s]
Adding requests:  61%|██████    | 4981/8192 [00:14<00:09, 348.75it/s]
Adding requests:  61%|██████    | 5017/8192 [00:14<00:09, 350.32it/s]
Adding requests:  62%|██████▏   | 5054/8192 [00:14<00:08, 354.04it/s]
Adding requests:  62%|██████▏   | 5090/8192 [00:14<00:08, 352.76it/s]
Adding requests:  63%|██████▎   | 5126/8192 [00:14<00:08, 353.60it/s]
Adding requests:  63%|██████▎   | 5162/8192 [00:14<00:08, 355.11it/s]
Adding requests:  63%|██████▎   | 5198/8192 [00:14<00:08, 348.32it/s]
Adding requests:  64%|██████▍   | 5234/8192 [00:15<00:08, 350.01it/s]
Adding requests:  64%|██████▍   | 5270/8192 [00:15<00:08, 347.83it/s]
Adding requests:  65%|██████▍   | 5306/8192 [00:15<00:08, 350.38it/s]
Adding requests:  65%|██████▌   | 5342/8192 [00:15<00:08, 350.34it/s]
Adding requests:  66%|██████▌   | 5378/8192 [00:15<00:08, 351.65it/s]
Adding requests:  66%|██████▌   | 5414/8192 [00:15<00:08, 340.70it/s]
Adding requests:  67%|██████▋   | 5451/8192 [00:15<00:07, 347.75it/s]
Adding requests:  67%|██████▋   | 5488/8192 [00:15<00:07, 352.52it/s]
Adding requests:  67%|██████▋   | 5524/8192 [00:15<00:07, 341.61it/s]
Adding requests:  68%|██████▊   | 5559/8192 [00:16<00:07, 342.05it/s]
Adding requests:  68%|██████▊   | 5594/8192 [00:16<00:07, 340.54it/s]
Adding requests:  69%|██████▊   | 5631/8192 [00:16<00:07, 347.43it/s]
Adding requests:  69%|██████▉   | 5667/8192 [00:16<00:07, 349.10it/s]
Adding requests:  70%|██████▉   | 5703/8192 [00:16<00:07, 349.79it/s]
Adding requests:  70%|███████   | 5739/8192 [00:16<00:07, 346.91it/s]
Adding requests:  71%|███████   | 5776/8192 [00:16<00:06, 352.07it/s]
Adding requests:  71%|███████   | 5812/8192 [00:16<00:06, 354.30it/s]
Adding requests:  71%|███████▏  | 5848/8192 [00:16<00:06, 350.78it/s]
Adding requests:  72%|███████▏  | 5884/8192 [00:16<00:06, 353.35it/s]
Adding requests:  72%|███████▏  | 5921/8192 [00:17<00:06, 355.93it/s]
Adding requests:  73%|███████▎  | 5958/8192 [00:17<00:06, 359.59it/s]
Adding requests:  73%|███████▎  | 5995/8192 [00:17<00:06, 361.46it/s]
Adding requests:  74%|███████▎  | 6032/8192 [00:17<00:06, 359.54it/s]
Adding requests:  74%|███████▍  | 6068/8192 [00:17<00:05, 355.14it/s]
Adding requests:  75%|███████▍  | 6104/8192 [00:17<00:05, 350.91it/s]
Adding requests:  75%|███████▍  | 6141/8192 [00:17<00:05, 354.98it/s]
Adding requests:  75%|███████▌  | 6177/8192 [00:17<00:05, 355.70it/s]
Adding requests:  76%|███████▌  | 6213/8192 [00:17<00:05, 351.38it/s]
Adding requests:  76%|███████▋  | 6249/8192 [00:17<00:05, 351.42it/s]
Adding requests:  77%|███████▋  | 6285/8192 [00:18<00:05, 350.07it/s]
Adding requests:  77%|███████▋  | 6323/8192 [00:18<00:05, 356.79it/s]
Adding requests:  78%|███████▊  | 6360/8192 [00:18<00:05, 360.22it/s]
Adding requests:  78%|███████▊  | 6397/8192 [00:18<00:05, 355.47it/s]
Adding requests:  79%|███████▊  | 6433/8192 [00:18<00:05, 346.33it/s]
Adding requests:  79%|███████▉  | 6468/8192 [00:18<00:04, 345.60it/s]
Adding requests:  79%|███████▉  | 6503/8192 [00:18<00:04, 343.54it/s]
Adding requests:  80%|███████▉  | 6539/8192 [00:18<00:04, 348.31it/s]
Adding requests:  80%|████████  | 6574/8192 [00:18<00:04, 348.78it/s]
Adding requests:  81%|████████  | 6609/8192 [00:19<00:04, 346.08it/s]
Adding requests:  81%|████████  | 6647/8192 [00:19<00:04, 354.98it/s]
Adding requests:  82%|████████▏ | 6683/8192 [00:19<00:04, 349.90it/s]
Adding requests:  82%|████████▏ | 6719/8192 [00:19<00:04, 352.03it/s]
Adding requests:  82%|████████▏ | 6756/8192 [00:19<00:04, 355.72it/s]
Adding requests:  83%|████████▎ | 6792/8192 [00:19<00:03, 353.83it/s]
Adding requests:  83%|████████▎ | 6828/8192 [00:19<00:03, 347.00it/s]
Adding requests:  84%|████████▍ | 6863/8192 [00:19<00:03, 339.60it/s]
Adding requests:  84%|████████▍ | 6899/8192 [00:19<00:03, 344.11it/s]
Adding requests:  85%|████████▍ | 6936/8192 [00:19<00:03, 350.20it/s]
Adding requests:  85%|████████▌ | 6972/8192 [00:20<00:03, 344.62it/s]
Adding requests:  86%|████████▌ | 7010/8192 [00:20<00:03, 352.65it/s]
Adding requests:  86%|████████▌ | 7046/8192 [00:20<00:03, 349.55it/s]
Adding requests:  86%|████████▋ | 7082/8192 [00:20<00:03, 351.00it/s]
Adding requests:  87%|████████▋ | 7118/8192 [00:20<00:03, 348.50it/s]
Adding requests:  87%|████████▋ | 7154/8192 [00:20<00:02, 348.90it/s]
Adding requests:  88%|████████▊ | 7192/8192 [00:20<00:02, 355.07it/s]
Adding requests:  88%|████████▊ | 7229/8192 [00:20<00:02, 358.24it/s]
Adding requests:  89%|████████▊ | 7266/8192 [00:20<00:02, 358.77it/s]
Adding requests:  89%|████████▉ | 7302/8192 [00:20<00:02, 357.58it/s]
Adding requests:  90%|████████▉ | 7338/8192 [00:21<00:02, 355.73it/s]
Adding requests:  90%|█████████ | 7375/8192 [00:21<00:02, 359.41it/s]
Adding requests:  90%|█████████ | 7411/8192 [00:21<00:02, 354.61it/s]
Adding requests:  91%|█████████ | 7447/8192 [00:21<00:02, 351.99it/s]
Adding requests:  91%|█████████▏| 7483/8192 [00:21<00:02, 347.13it/s]
Adding requests:  92%|█████████▏| 7520/8192 [00:21<00:01, 352.31it/s]
Adding requests:  92%|█████████▏| 7556/8192 [00:21<00:01, 352.15it/s]
Adding requests:  93%|█████████▎| 7592/8192 [00:21<00:01, 349.97it/s]
Adding requests:  93%|█████████▎| 7628/8192 [00:21<00:01, 351.20it/s]
Adding requests:  94%|█████████▎| 7666/8192 [00:22<00:01, 356.69it/s]
Adding requests:  94%|█████████▍| 7704/8192 [00:22<00:01, 362.11it/s]
Adding requests:  94%|█████████▍| 7741/8192 [00:22<00:01, 357.32it/s]
Adding requests:  95%|█████████▍| 7777/8192 [00:22<00:01, 355.05it/s]
Adding requests:  95%|█████████▌| 7813/8192 [00:22<00:01, 353.74it/s]
Adding requests:  96%|█████████▌| 7849/8192 [00:22<00:00, 346.09it/s]
Adding requests:  96%|█████████▌| 7884/8192 [00:22<00:00, 345.34it/s]
Adding requests:  97%|█████████▋| 7923/8192 [00:22<00:00, 355.84it/s]
Adding requests:  97%|█████████▋| 7961/8192 [00:22<00:00, 361.46it/s]
Adding requests:  98%|█████████▊| 7999/8192 [00:22<00:00, 364.27it/s]
Adding requests:  98%|█████████▊| 8036/8192 [00:23<00:00, 355.16it/s]
Adding requests:  99%|█████████▊| 8073/8192 [00:23<00:00, 359.13it/s]
Adding requests:  99%|█████████▉| 8109/8192 [00:23<00:00, 357.58it/s]
Adding requests:  99%|█████████▉| 8145/8192 [00:23<00:00, 353.20it/s]
Adding requests: 100%|█████████▉| 8181/8192 [00:23<00:00, 338.18it/s]
Adding requests: 100%|██████████| 8192/8192 [00:23<00:00, 348.34it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 1492/8192 [00:00<00:01, 4551.18it/s, est. speed input: 4660579.11 toks/s, output: 4551.22 toks/s]
Processed prompts:  24%|██▍       | 1948/8192 [00:07<00:29, 214.63it/s, est. speed input: 281383.87 toks/s, output: 274.79 toks/s]   
Processed prompts:  26%|██▌       | 2140/8192 [00:10<00:38, 158.94it/s, est. speed input: 218879.20 toks/s, output: 213.75 toks/s]
Processed prompts:  27%|██▋       | 2247/8192 [00:10<00:39, 151.95it/s, est. speed input: 209791.99 toks/s, output: 204.87 toks/s]
Processed prompts:  28%|██▊       | 2317/8192 [00:11<00:42, 137.61it/s, est. speed input: 198626.84 toks/s, output: 193.97 toks/s]
Processed prompts:  29%|██▉       | 2364/8192 [00:12<00:48, 119.74it/s, est. speed input: 187515.09 toks/s, output: 183.12 toks/s]
Processed prompts:  29%|██▉       | 2397/8192 [00:13<00:57, 100.29it/s, est. speed input: 176709.21 toks/s, output: 172.57 toks/s]
Processed prompts:  30%|██▉       | 2452/8192 [00:14<01:04, 89.53it/s, est. speed input: 168859.17 toks/s, output: 164.90 toks/s] 
Processed prompts:  31%|███       | 2516/8192 [00:15<01:07, 83.71it/s, est. speed input: 162759.55 toks/s, output: 158.94 toks/s]
Processed prompts:  31%|███▏      | 2580/8192 [00:16<01:11, 78.98it/s, est. speed input: 157275.58 toks/s, output: 153.59 toks/s]
Processed prompts:  32%|███▏      | 2644/8192 [00:17<01:13, 75.32it/s, est. speed input: 152358.95 toks/s, output: 148.79 toks/s]
Processed prompts:  33%|███▎      | 2708/8192 [00:18<01:15, 72.90it/s, est. speed input: 148054.79 toks/s, output: 144.58 toks/s]
Processed prompts:  34%|███▍      | 2772/8192 [00:19<01:16, 70.69it/s, est. speed input: 144019.36 toks/s, output: 140.64 toks/s]
Processed prompts:  35%|███▍      | 2836/8192 [00:20<01:17, 69.12it/s, est. speed input: 140369.85 toks/s, output: 137.08 toks/s]
Processed prompts:  35%|███▌      | 2900/8192 [00:21<01:17, 68.59it/s, est. speed input: 137228.87 toks/s, output: 134.01 toks/s]
Processed prompts:  36%|███▌      | 2964/8192 [00:22<01:17, 67.80it/s, est. speed input: 134233.41 toks/s, output: 131.09 toks/s]
Processed prompts:  37%|███▋      | 3028/8192 [00:23<01:17, 67.02it/s, est. speed input: 131426.52 toks/s, output: 128.35 toks/s]
Processed prompts:  38%|███▊      | 3092/8192 [00:24<01:16, 66.55it/s, est. speed input: 128863.13 toks/s, output: 125.84 toks/s]
Processed prompts:  39%|███▊      | 3156/8192 [00:25<01:16, 66.19it/s, est. speed input: 126489.97 toks/s, output: 123.53 toks/s]
Processed prompts:  39%|███▉      | 3220/8192 [00:26<01:15, 65.91it/s, est. speed input: 124284.18 toks/s, output: 121.37 toks/s]
Processed prompts:  40%|████      | 3284/8192 [00:27<01:14, 65.75it/s, est. speed input: 122242.70 toks/s, output: 119.38 toks/s]
Processed prompts:  41%|████      | 3348/8192 [00:28<01:13, 65.61it/s, est. speed input: 120337.54 toks/s, output: 117.52 toks/s]
Processed prompts:  42%|████▏     | 3412/8192 [00:29<01:12, 65.53it/s, est. speed input: 118561.73 toks/s, output: 115.78 toks/s]
Processed prompts:  42%|████▏     | 3476/8192 [00:30<01:12, 65.46it/s, est. speed input: 116898.15 toks/s, output: 114.16 toks/s]
Processed prompts:  43%|████▎     | 3540/8192 [00:31<01:11, 65.41it/s, est. speed input: 115337.57 toks/s, output: 112.63 toks/s]
Processed prompts:  44%|████▍     | 3604/8192 [00:32<01:10, 65.37it/s, est. speed input: 113870.24 toks/s, output: 111.20 toks/s]
Processed prompts:  45%|████▍     | 3668/8192 [00:33<01:09, 65.52it/s, est. speed input: 112518.90 toks/s, output: 109.88 toks/s]
Processed prompts:  46%|████▌     | 3732/8192 [00:34<01:07, 65.68it/s, est. speed input: 111253.64 toks/s, output: 108.65 toks/s]
Processed prompts:  46%|████▋     | 3796/8192 [00:35<01:07, 65.56it/s, est. speed input: 110020.88 toks/s, output: 107.44 toks/s]
Processed prompts:  47%|████▋     | 3860/8192 [00:36<01:06, 65.50it/s, est. speed input: 108859.35 toks/s, output: 106.31 toks/s]
Processed prompts:  48%|████▊     | 3924/8192 [00:37<01:04, 65.85it/s, est. speed input: 107814.29 toks/s, output: 105.29 toks/s]
Processed prompts:  49%|████▊     | 3988/8192 [00:38<01:03, 66.13it/s, est. speed input: 106825.54 toks/s, output: 104.32 toks/s]
Processed prompts:  49%|████▉     | 4052/8192 [00:39<01:02, 66.26it/s, est. speed input: 105877.20 toks/s, output: 103.40 toks/s]
Processed prompts:  50%|█████     | 4116/8192 [00:40<01:01, 66.09it/s, est. speed input: 104940.46 toks/s, output: 102.48 toks/s]
Processed prompts:  51%|█████     | 4180/8192 [00:41<01:00, 66.16it/s, est. speed input: 104071.81 toks/s, output: 101.63 toks/s]
Processed prompts:  52%|█████▏    | 4244/8192 [00:42<00:59, 65.84it/s, est. speed input: 103198.79 toks/s, output: 100.78 toks/s]
Processed prompts:  53%|█████▎    | 4308/8192 [00:43<00:59, 65.73it/s, est. speed input: 102378.44 toks/s, output: 99.98 toks/s] 
Processed prompts:  53%|█████▎    | 4372/8192 [00:44<00:58, 65.60it/s, est. speed input: 101588.68 toks/s, output: 99.21 toks/s]
Processed prompts:  54%|█████▍    | 4436/8192 [00:45<00:57, 65.67it/s, est. speed input: 100850.94 toks/s, output: 98.49 toks/s]
Processed prompts:  55%|█████▍    | 4500/8192 [00:46<00:56, 65.80it/s, est. speed input: 100153.81 toks/s, output: 97.81 toks/s]
Processed prompts:  56%|█████▌    | 4564/8192 [00:46<00:55, 65.79it/s, est. speed input: 99473.72 toks/s, output: 97.14 toks/s] 
Processed prompts:  56%|█████▋    | 4628/8192 [00:47<00:54, 65.89it/s, est. speed input: 98832.55 toks/s, output: 96.52 toks/s]
Processed prompts:  57%|█████▋    | 4692/8192 [00:48<00:53, 65.71it/s, est. speed input: 98192.16 toks/s, output: 95.89 toks/s]
Processed prompts:  58%|█████▊    | 4756/8192 [00:49<00:52, 65.61it/s, est. speed input: 97579.32 toks/s, output: 95.29 toks/s]
Processed prompts:  59%|█████▉    | 4820/8192 [00:50<00:51, 65.53it/s, est. speed input: 96989.24 toks/s, output: 94.72 toks/s]
Processed prompts:  60%|█████▉    | 4884/8192 [00:51<00:50, 65.87it/s, est. speed input: 96457.54 toks/s, output: 94.20 toks/s]
Processed prompts:  60%|██████    | 4948/8192 [00:52<00:49, 65.82it/s, est. speed input: 95919.93 toks/s, output: 93.67 toks/s]
Processed prompts:  61%|██████    | 5012/8192 [00:53<00:48, 65.94it/s, est. speed input: 95414.65 toks/s, output: 93.18 toks/s]
Processed prompts:  62%|██████▏   | 5076/8192 [00:54<00:47, 65.76it/s, est. speed input: 94904.73 toks/s, output: 92.68 toks/s]
Processed prompts:  63%|██████▎   | 5140/8192 [00:55<00:46, 65.60it/s, est. speed input: 94410.51 toks/s, output: 92.20 toks/s]
Processed prompts:  64%|██████▎   | 5204/8192 [00:56<00:45, 65.68it/s, est. speed input: 93948.43 toks/s, output: 91.75 toks/s]
Processed prompts:  64%|██████▍   | 5268/8192 [00:57<00:44, 65.84it/s, est. speed input: 93510.37 toks/s, output: 91.32 toks/s]
Processed prompts:  65%|██████▌   | 5332/8192 [00:58<00:43, 65.64it/s, est. speed input: 93062.00 toks/s, output: 90.88 toks/s]
Processed prompts:  66%|██████▌   | 5396/8192 [00:59<00:42, 65.55it/s, est. speed input: 92632.53 toks/s, output: 90.46 toks/s]
Processed prompts:  67%|██████▋   | 5460/8192 [01:00<00:41, 65.50it/s, est. speed input: 92218.13 toks/s, output: 90.06 toks/s]
Processed prompts:  67%|██████▋   | 5524/8192 [01:01<00:40, 65.85it/s, est. speed input: 91845.40 toks/s, output: 89.69 toks/s]
Processed prompts:  68%|██████▊   | 5588/8192 [01:02<00:39, 65.67it/s, est. speed input: 91453.15 toks/s, output: 89.31 toks/s]
Processed prompts:  69%|██████▉   | 5652/8192 [01:03<00:38, 65.55it/s, est. speed input: 91073.50 toks/s, output: 88.94 toks/s]
Processed prompts:  70%|██████▉   | 5716/8192 [01:04<00:37, 65.45it/s, est. speed input: 90703.86 toks/s, output: 88.58 toks/s]
Processed prompts:  71%|███████   | 5780/8192 [01:05<00:36, 65.40it/s, est. speed input: 90346.88 toks/s, output: 88.23 toks/s]
Processed prompts:  71%|███████▏  | 5844/8192 [01:06<00:35, 65.40it/s, est. speed input: 90002.95 toks/s, output: 87.89 toks/s]
Processed prompts:  72%|███████▏  | 5908/8192 [01:07<00:34, 65.55it/s, est. speed input: 89678.85 toks/s, output: 87.58 toks/s]
Processed prompts:  73%|███████▎  | 5972/8192 [01:08<00:33, 65.71it/s, est. speed input: 89367.07 toks/s, output: 87.27 toks/s]
Processed prompts:  74%|███████▎  | 6036/8192 [01:09<00:32, 65.55it/s, est. speed input: 89047.06 toks/s, output: 86.96 toks/s]
Processed prompts:  74%|███████▍  | 6100/8192 [01:10<00:31, 65.50it/s, est. speed input: 88739.88 toks/s, output: 86.66 toks/s]
Processed prompts:  75%|███████▌  | 6164/8192 [01:11<00:30, 65.43it/s, est. speed input: 88439.06 toks/s, output: 86.37 toks/s]
Processed prompts:  76%|███████▌  | 6228/8192 [01:12<00:29, 65.83it/s, est. speed input: 88173.00 toks/s, output: 86.11 toks/s]
Processed prompts:  77%|███████▋  | 6292/8192 [01:13<00:28, 65.65it/s, est. speed input: 87887.15 toks/s, output: 85.83 toks/s]
Processed prompts:  78%|███████▊  | 6356/8192 [01:14<00:28, 65.57it/s, est. speed input: 87611.32 toks/s, output: 85.56 toks/s]
Processed prompts:  78%|███████▊  | 6420/8192 [01:15<00:27, 65.49it/s, est. speed input: 87341.10 toks/s, output: 85.29 toks/s]
Processed prompts:  79%|███████▉  | 6484/8192 [01:16<00:26, 65.42it/s, est. speed input: 87077.50 toks/s, output: 85.04 toks/s]
Processed prompts:  80%|███████▉  | 6548/8192 [01:17<00:25, 65.39it/s, est. speed input: 86821.33 toks/s, output: 84.79 toks/s]
Processed prompts:  81%|████████  | 6612/8192 [01:18<00:24, 65.34it/s, est. speed input: 86570.16 toks/s, output: 84.54 toks/s]
Processed prompts:  81%|████████▏ | 6676/8192 [01:19<00:23, 65.34it/s, est. speed input: 86326.80 toks/s, output: 84.30 toks/s]
Processed prompts:  82%|████████▏ | 6740/8192 [01:20<00:22, 65.34it/s, est. speed input: 86089.41 toks/s, output: 84.07 toks/s]
Processed prompts:  83%|████████▎ | 6804/8192 [01:21<00:21, 65.34it/s, est. speed input: 85857.97 toks/s, output: 83.85 toks/s]
Processed prompts:  84%|████████▍ | 6868/8192 [01:22<00:20, 65.26it/s, est. speed input: 85627.73 toks/s, output: 83.62 toks/s]
Processed prompts:  85%|████████▍ | 6932/8192 [01:23<00:19, 65.32it/s, est. speed input: 85409.09 toks/s, output: 83.41 toks/s]
Processed prompts:  85%|████████▌ | 6996/8192 [01:24<00:18, 65.33it/s, est. speed input: 85193.61 toks/s, output: 83.20 toks/s]
Processed prompts:  86%|████████▌ | 7060/8192 [01:25<00:17, 65.50it/s, est. speed input: 84991.33 toks/s, output: 83.00 toks/s]
Processed prompts:  87%|████████▋ | 7124/8192 [01:26<00:16, 65.68it/s, est. speed input: 84796.92 toks/s, output: 82.81 toks/s]
Processed prompts:  88%|████████▊ | 7188/8192 [01:27<00:15, 65.52it/s, est. speed input: 84592.51 toks/s, output: 82.61 toks/s]
Processed prompts:  89%|████████▊ | 7252/8192 [01:27<00:14, 65.65it/s, est. speed input: 84404.45 toks/s, output: 82.43 toks/s]
Processed prompts:  89%|████████▉ | 7316/8192 [01:28<00:13, 65.80it/s, est. speed input: 84223.34 toks/s, output: 82.25 toks/s]
Processed prompts:  90%|█████████ | 7380/8192 [01:29<00:12, 65.63it/s, est. speed input: 84033.28 toks/s, output: 82.06 toks/s]
Processed prompts:  91%|█████████ | 7444/8192 [01:30<00:11, 65.71it/s, est. speed input: 83856.36 toks/s, output: 81.89 toks/s]
Processed prompts:  92%|█████████▏| 7508/8192 [01:31<00:10, 65.76it/s, est. speed input: 83683.15 toks/s, output: 81.72 toks/s]
Processed prompts:  92%|█████████▏| 7572/8192 [01:32<00:09, 65.65it/s, est. speed input: 83507.00 toks/s, output: 81.55 toks/s]
Processed prompts:  93%|█████████▎| 7636/8192 [01:33<00:08, 65.76it/s, est. speed input: 83342.64 toks/s, output: 81.39 toks/s]
Processed prompts:  94%|█████████▍| 7700/8192 [01:34<00:07, 65.89it/s, est. speed input: 83183.67 toks/s, output: 81.23 toks/s]
Processed prompts:  95%|█████████▍| 7764/8192 [01:35<00:06, 65.81it/s, est. speed input: 83021.21 toks/s, output: 81.08 toks/s]
Processed prompts:  96%|█████████▌| 7828/8192 [01:36<00:05, 65.94it/s, est. speed input: 82869.50 toks/s, output: 80.93 toks/s]
Processed prompts:  96%|█████████▋| 7892/8192 [01:37<00:04, 65.88it/s, est. speed input: 82714.66 toks/s, output: 80.78 toks/s]
Processed prompts:  97%|█████████▋| 7956/8192 [01:38<00:03, 65.96it/s, est. speed input: 82567.66 toks/s, output: 80.63 toks/s]
Processed prompts:  98%|█████████▊| 8020/8192 [01:39<00:02, 65.74it/s, est. speed input: 82412.24 toks/s, output: 80.48 toks/s]
Processed prompts:  99%|█████████▊| 8084/8192 [01:40<00:01, 65.59it/s, est. speed input: 82260.26 toks/s, output: 80.33 toks/s]
Processed prompts:  99%|█████████▉| 8148/8192 [01:41<00:00, 72.04it/s, est. speed input: 82351.41 toks/s, output: 80.42 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:41<00:00, 72.04it/s, est. speed input: 82795.64 toks/s, output: 80.86 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:41<00:00, 80.86it/s, est. speed input: 82795.64 toks/s, output: 80.86 toks/s]
[rank0]:[W126 19:02:33.488042204 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

