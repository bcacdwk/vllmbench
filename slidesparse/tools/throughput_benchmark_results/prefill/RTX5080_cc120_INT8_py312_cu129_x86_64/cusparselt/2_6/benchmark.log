
========== M=16 ==========
Time: 2026-01-25 18:03:14
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:03:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=264530) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=264530) WARNING 01-25 18:03:25 [backends.py:609] Failed to read file <frozen os>
Throughput: 66.26 requests/s, 1126.34 total tokens/s, 66.26 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-25 18:03:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:03:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:03:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:03:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:03:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:03:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:03:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:03:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:03:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:03:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:03:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:03:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:03:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:03:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:03:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:03:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:03:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:03:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=264530) [2026-01-25 18:03:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=264530) [2026-01-25 18:03:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=264530) [2026-01-25 18:03:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=264530) [2026-01-25 18:03:22] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=264530) [2026-01-25 18:03:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=264530) [2026-01-25 18:03:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=264530) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=264530) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]
(EngineCore_DP0 pid=264530) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]
(EngineCore_DP0 pid=264530) 
(EngineCore_DP0 pid=264530) [2026-01-25 18:03:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=264530) [2026-01-25 18:03:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=264530) [2026-01-25 18:03:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=264530) [2026-01-25 18:03:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=264530) [2026-01-25 18:03:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=264530) [2026-01-25 18:03:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=264530) [2026-01-25 18:03:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=264530) [2026-01-25 18:03:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=264530) 2026-01-25 18:03:30,239 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=264530) 2026-01-25 18:03:30,264 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=264530) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.79it/s]
(EngineCore_DP0 pid=264530) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.15it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4479.04it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:28,  4.40it/s, est. speed input: 70.41 toks/s, output: 4.40 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 32.75it/s, est. speed input: 431.33 toks/s, output: 26.96 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 48.35it/s, est. speed input: 619.15 toks/s, output: 38.70 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 57.81it/s, est. speed input: 734.87 toks/s, output: 45.93 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 64.05it/s, est. speed input: 814.67 toks/s, output: 50.92 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 68.14it/s, est. speed input: 872.24 toks/s, output: 54.51 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 70.82it/s, est. speed input: 915.53 toks/s, output: 57.22 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 72.19it/s, est. speed input: 947.31 toks/s, output: 59.21 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 73.41it/s, est. speed input: 974.02 toks/s, output: 60.88 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 74.65it/s, est. speed input: 997.51 toks/s, output: 62.34 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 75.30it/s, est. speed input: 1016.39 toks/s, output: 63.52 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 74.09it/s, est. speed input: 1026.67 toks/s, output: 64.17 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 74.89it/s, est. speed input: 1040.77 toks/s, output: 65.05 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 75.37it/s, est. speed input: 1052.78 toks/s, output: 65.80 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 75.62it/s, est. speed input: 1063.05 toks/s, output: 66.44 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 74.77it/s, est. speed input: 1069.29 toks/s, output: 66.83 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 74.77it/s, est. speed input: 1076.97 toks/s, output: 67.31 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 67.31it/s, est. speed input: 1076.97 toks/s, output: 67.31 toks/s]
[rank0]:[W125 18:03:33.461071545 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 18:03:34
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:03:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=265127) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=265127) WARNING 01-25 18:03:46 [backends.py:609] Failed to read file <frozen os>
Throughput: 63.98 requests/s, 8253.94 total tokens/s, 63.98 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-25 18:03:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:03:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:03:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:03:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:03:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:03:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:03:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:03:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:03:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:03:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:03:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:03:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:03:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:03:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:03:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:03:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:03:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:03:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=265127) [2026-01-25 18:03:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=265127) [2026-01-25 18:03:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=265127) [2026-01-25 18:03:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=265127) [2026-01-25 18:03:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=265127) [2026-01-25 18:03:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=265127) [2026-01-25 18:03:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=265127) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=265127) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.03it/s]
(EngineCore_DP0 pid=265127) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.02it/s]
(EngineCore_DP0 pid=265127) 
(EngineCore_DP0 pid=265127) [2026-01-25 18:03:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=265127) [2026-01-25 18:03:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=265127) [2026-01-25 18:03:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=265127) [2026-01-25 18:03:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=265127) [2026-01-25 18:03:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=265127) [2026-01-25 18:03:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=265127) [2026-01-25 18:03:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=265127) [2026-01-25 18:03:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=265127) 2026-01-25 18:03:50,308 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=265127) 2026-01-25 18:03:50,321 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=265127) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 32.51it/s]
(EngineCore_DP0 pid=265127) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2512.53it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:27,  4.58it/s, est. speed input: 586.50 toks/s, output: 4.58 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 33.39it/s, est. speed input: 3533.18 toks/s, output: 27.60 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 48.37it/s, est. speed input: 5002.68 toks/s, output: 39.08 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 57.11it/s, est. speed input: 5884.01 toks/s, output: 45.97 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 62.26it/s, est. speed input: 6456.34 toks/s, output: 50.44 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 65.87it/s, est. speed input: 6877.74 toks/s, output: 53.73 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 67.99it/s, est. speed input: 7182.12 toks/s, output: 56.11 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:01, 69.64it/s, est. speed input: 7426.71 toks/s, output: 58.02 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 70.74it/s, est. speed input: 7621.67 toks/s, output: 59.54 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 71.46it/s, est. speed input: 7779.54 toks/s, output: 60.78 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 72.03it/s, est. speed input: 7913.50 toks/s, output: 61.82 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 72.68it/s, est. speed input: 8033.80 toks/s, output: 62.76 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 72.93it/s, est. speed input: 8131.88 toks/s, output: 63.53 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 73.23it/s, est. speed input: 8219.97 toks/s, output: 64.22 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 73.20it/s, est. speed input: 8291.49 toks/s, output: 64.78 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 73.08it/s, est. speed input: 8352.38 toks/s, output: 65.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 73.08it/s, est. speed input: 8411.47 toks/s, output: 65.71 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 65.71it/s, est. speed input: 8411.47 toks/s, output: 65.71 toks/s]
[rank0]:[W125 18:03:53.376036935 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 18:03:54
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:03:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=265706) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=265706) WARNING 01-25 18:04:06 [backends.py:609] Failed to read file <frozen os>
Throughput: 68.26 requests/s, 17542.29 total tokens/s, 68.26 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-25 18:03:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:03:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:03:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:03:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:03:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:03:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:03:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:03:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:03:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:03:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:04:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:04:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:04:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:04:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:04:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:04:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:04:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:04:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:04:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:04:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:04:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:04:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:04:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:04:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=265706) [2026-01-25 18:04:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=265706) [2026-01-25 18:04:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=265706) [2026-01-25 18:04:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=265706) [2026-01-25 18:04:03] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=265706) [2026-01-25 18:04:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=265706) [2026-01-25 18:04:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=265706) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=265706) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.87it/s]
(EngineCore_DP0 pid=265706) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.86it/s]
(EngineCore_DP0 pid=265706) 
(EngineCore_DP0 pid=265706) [2026-01-25 18:04:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=265706) [2026-01-25 18:04:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=265706) [2026-01-25 18:04:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=265706) [2026-01-25 18:04:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=265706) [2026-01-25 18:04:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=265706) [2026-01-25 18:04:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=265706) [2026-01-25 18:04:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=265706) [2026-01-25 18:04:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=265706) 2026-01-25 18:04:10,203 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=265706) 2026-01-25 18:04:10,217 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=265706) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 31.84it/s]
(EngineCore_DP0 pid=265706) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.05it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2124.64it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 17.96it/s, est. speed input: 4598.27 toks/s, output: 17.96 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:02, 50.74it/s, est. speed input: 11708.59 toks/s, output: 45.73 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:01, 61.58it/s, est. speed input: 14146.24 toks/s, output: 55.26 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:01, 66.52it/s, est. speed input: 15347.20 toks/s, output: 59.95 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:01, 69.17it/s, est. speed input: 16058.98 toks/s, output: 62.73 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 70.77it/s, est. speed input: 16533.71 toks/s, output: 64.58 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 72.10it/s, est. speed input: 16905.54 toks/s, output: 66.04 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:00<00:00, 70.01it/s, est. speed input: 16905.29 toks/s, output: 66.04 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:00<00:00, 70.80it/s, est. speed input: 17091.14 toks/s, output: 66.76 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:00, 72.02it/s, est. speed input: 17293.87 toks/s, output: 67.55 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:00, 72.66it/s, est. speed input: 17445.42 toks/s, output: 68.15 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 73.39it/s, est. speed input: 17590.17 toks/s, output: 68.71 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:01<00:00, 73.66it/s, est. speed input: 17698.07 toks/s, output: 69.13 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 74.15it/s, est. speed input: 17808.62 toks/s, output: 69.56 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:01<00:00, 74.54it/s, est. speed input: 17906.40 toks/s, output: 69.95 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:01<00:00, 74.90it/s, est. speed input: 17996.61 toks/s, output: 70.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 74.90it/s, est. speed input: 18061.77 toks/s, output: 70.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 70.55it/s, est. speed input: 18061.77 toks/s, output: 70.55 toks/s]
[rank0]:[W125 18:04:13.120374952 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-25 18:28:30
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:28:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=302272) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=302272) WARNING 01-25 18:28:41 [backends.py:609] Failed to read file <frozen os>
Throughput: 70.95 requests/s, 36395.93 total tokens/s, 70.95 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-25 18:28:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:28:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:28:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:28:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:28:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:28:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:28:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:28:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:28:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:28:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:28:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:28:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:28:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:28:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:28:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:28:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:28:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:28:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=302272) [2026-01-25 18:28:38] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=302272) [2026-01-25 18:28:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=302272) [2026-01-25 18:28:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=302272) [2026-01-25 18:28:38] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=302272) [2026-01-25 18:28:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=302272) [2026-01-25 18:28:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=302272) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=302272) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.89it/s]
(EngineCore_DP0 pid=302272) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.89it/s]
(EngineCore_DP0 pid=302272) 
(EngineCore_DP0 pid=302272) [2026-01-25 18:28:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=302272) [2026-01-25 18:28:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=302272) [2026-01-25 18:28:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=302272) [2026-01-25 18:28:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=302272) [2026-01-25 18:28:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=302272) [2026-01-25 18:28:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=302272) [2026-01-25 18:28:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=302272) [2026-01-25 18:28:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=302272) 2026-01-25 18:28:45,634 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=302272) 2026-01-25 18:28:45,648 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=302272) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 31.91it/s]
(EngineCore_DP0 pid=302272) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.05it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  98%|█████████▊| 125/128 [00:00<00:00, 1242.32it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1246.02it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:01, 100.72it/s, est. speed input: 51572.84 toks/s, output: 100.72 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:01, 83.83it/s, est. speed input: 44031.16 toks/s, output: 86.00 toks/s]  
Processed prompts:  24%|██▍       | 31/128 [00:00<00:01, 78.11it/s, est. speed input: 41496.75 toks/s, output: 81.05 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 76.83it/s, est. speed input: 40758.60 toks/s, output: 79.60 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 75.98it/s, est. speed input: 40268.43 toks/s, output: 78.65 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:00<00:00, 75.48it/s, est. speed input: 39938.26 toks/s, output: 78.00 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:00<00:00, 75.29it/s, est. speed input: 39727.54 toks/s, output: 77.59 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:00<00:00, 75.33it/s, est. speed input: 39599.21 toks/s, output: 77.34 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:00, 74.15it/s, est. speed input: 39280.08 toks/s, output: 76.72 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 73.44it/s, est. speed input: 39038.40 toks/s, output: 76.25 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 73.42it/s, est. speed input: 38909.49 toks/s, output: 75.99 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:01<00:00, 73.71it/s, est. speed input: 38844.91 toks/s, output: 75.87 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:01<00:00, 74.00it/s, est. speed input: 38799.77 toks/s, output: 75.78 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:01<00:00, 73.06it/s, est. speed input: 38623.76 toks/s, output: 75.44 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:01<00:00, 73.21it/s, est. speed input: 38562.29 toks/s, output: 75.32 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 73.21it/s, est. speed input: 38558.64 toks/s, output: 75.31 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 75.31it/s, est. speed input: 38558.64 toks/s, output: 75.31 toks/s]
[rank0]:[W125 18:28:48.486576345 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-25 18:28:50
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:28:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=302825) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=302825) WARNING 01-25 18:29:01 [backends.py:609] Failed to read file <frozen os>
Throughput: 67.74 requests/s, 69437.50 total tokens/s, 67.74 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-25 18:28:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:28:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:28:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:28:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:28:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:28:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:28:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:28:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:28:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:28:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:28:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:28:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:28:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:28:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:28:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:28:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:28:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:28:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:28:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=302825) [2026-01-25 18:28:58] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=302825) [2026-01-25 18:28:58] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=302825) [2026-01-25 18:28:58] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=302825) [2026-01-25 18:28:58] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=302825) [2026-01-25 18:28:58] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=302825) [2026-01-25 18:28:58] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=302825) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=302825) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.98it/s]
(EngineCore_DP0 pid=302825) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.98it/s]
(EngineCore_DP0 pid=302825) 
(EngineCore_DP0 pid=302825) [2026-01-25 18:28:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=302825) [2026-01-25 18:28:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=302825) [2026-01-25 18:28:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=302825) [2026-01-25 18:28:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=302825) [2026-01-25 18:28:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=302825) [2026-01-25 18:28:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=302825) [2026-01-25 18:28:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=302825) [2026-01-25 18:28:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=302825) 2026-01-25 18:29:05,625 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=302825) 2026-01-25 18:29:05,638 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=302825) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 28.38it/s]
(EngineCore_DP0 pid=302825) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.19it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  57%|█████▋    | 73/128 [00:00<00:00, 728.44it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 731.86it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:01, 90.22it/s, est. speed input: 92389.33 toks/s, output: 90.22 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:01, 78.07it/s, est. speed input: 81596.90 toks/s, output: 79.68 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:01, 76.57it/s, est. speed input: 79986.46 toks/s, output: 78.11 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 75.69it/s, est. speed input: 79061.90 toks/s, output: 77.21 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:00<00:01, 75.12it/s, est. speed input: 78453.24 toks/s, output: 76.61 toks/s]
Processed prompts:  41%|████      | 52/128 [00:00<00:01, 74.57it/s, est. speed input: 77937.65 toks/s, output: 76.11 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:00<00:00, 74.22it/s, est. speed input: 77569.63 toks/s, output: 75.75 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:00<00:00, 74.04it/s, est. speed input: 77309.71 toks/s, output: 75.50 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:00, 73.37it/s, est. speed input: 76905.11 toks/s, output: 75.10 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:00, 73.91it/s, est. speed input: 76911.50 toks/s, output: 75.11 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 74.17it/s, est. speed input: 76882.47 toks/s, output: 75.08 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:01<00:00, 74.21it/s, est. speed input: 76818.30 toks/s, output: 75.02 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:01<00:00, 74.21it/s, est. speed input: 76755.07 toks/s, output: 74.96 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:01<00:00, 74.12it/s, est. speed input: 76679.66 toks/s, output: 74.88 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:01<00:00, 74.22it/s, est. speed input: 76651.87 toks/s, output: 74.86 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 74.22it/s, est. speed input: 76491.20 toks/s, output: 74.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 74.69it/s, est. speed input: 76491.20 toks/s, output: 74.70 toks/s]
[rank0]:[W125 18:29:08.617567675 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-25 18:29:09
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:29:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=303386) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=303386) WARNING 01-25 18:29:21 [backends.py:609] Failed to read file <frozen os>
Throughput: 94.28 requests/s, 96639.65 total tokens/s, 94.28 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-25 18:29:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:29:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:29:14] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:29:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:29:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:29:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:29:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:29:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:29:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:29:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:29:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:29:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:29:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:29:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:29:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:29:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:29:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:29:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=303386) [2026-01-25 18:29:19] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=303386) [2026-01-25 18:29:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=303386) [2026-01-25 18:29:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=303386) [2026-01-25 18:29:19] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=303386) [2026-01-25 18:29:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=303386) [2026-01-25 18:29:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=303386) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=303386) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.03it/s]
(EngineCore_DP0 pid=303386) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.03it/s]
(EngineCore_DP0 pid=303386) 
(EngineCore_DP0 pid=303386) [2026-01-25 18:29:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=303386) [2026-01-25 18:29:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=303386) [2026-01-25 18:29:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=303386) [2026-01-25 18:29:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=303386) [2026-01-25 18:29:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=303386) [2026-01-25 18:29:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=303386) [2026-01-25 18:29:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=303386) [2026-01-25 18:29:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=303386) 2026-01-25 18:29:26,073 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=303386) 2026-01-25 18:29:26,086 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=303386) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 30.92it/s]
(EngineCore_DP0 pid=303386) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 12.13it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  25%|██▌       | 64/256 [00:00<00:00, 633.83it/s]
Adding requests:  53%|█████▎    | 135/256 [00:00<00:00, 675.95it/s]
Adding requests:  81%|████████  | 207/256 [00:00<00:00, 693.54it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 699.57it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:00<00:00, 337.85it/s, est. speed input: 345997.07 toks/s, output: 337.86 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:00<00:01, 140.71it/s, est. speed input: 158349.57 toks/s, output: 154.64 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:00<00:01, 123.36it/s, est. speed input: 140440.61 toks/s, output: 137.15 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:00<00:01, 115.19it/s, est. speed input: 132394.53 toks/s, output: 129.29 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:00<00:01, 110.13it/s, est. speed input: 127539.35 toks/s, output: 124.55 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:01<00:01, 106.87it/s, est. speed input: 124411.13 toks/s, output: 121.49 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:01<00:01, 104.36it/s, est. speed input: 121920.19 toks/s, output: 119.06 toks/s]
Processed prompts:  61%|██████    | 155/256 [00:01<00:00, 105.19it/s, est. speed input: 121022.71 toks/s, output: 118.19 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:01<00:00, 100.67it/s, est. speed input: 118538.02 toks/s, output: 115.76 toks/s]
Processed prompts:  69%|██████▉   | 177/256 [00:01<00:00, 102.07it/s, est. speed input: 117865.97 toks/s, output: 115.10 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:01<00:00, 98.38it/s, est. speed input: 115973.46 toks/s, output: 113.25 toks/s] 
Processed prompts:  77%|███████▋  | 198/256 [00:01<00:00, 98.34it/s, est. speed input: 115086.01 toks/s, output: 112.39 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:01<00:00, 98.29it/s, est. speed input: 114288.54 toks/s, output: 111.61 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:01<00:00, 98.20it/s, est. speed input: 113563.08 toks/s, output: 110.90 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 98.19it/s, est. speed input: 112920.91 toks/s, output: 110.27 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:02<00:00, 97.99it/s, est. speed input: 112302.43 toks/s, output: 109.67 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:02<00:00, 97.91it/s, est. speed input: 111750.82 toks/s, output: 109.13 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 97.91it/s, est. speed input: 111673.15 toks/s, output: 109.06 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 109.05it/s, est. speed input: 111673.15 toks/s, output: 109.06 toks/s]
[rank0]:[W125 18:29:30.892026036 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-25 18:29:31
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:29:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=303970) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=303970) WARNING 01-25 18:29:44 [backends.py:609] Failed to read file <frozen os>
Throughput: 98.25 requests/s, 100702.53 total tokens/s, 98.25 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-25 18:29:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:29:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:29:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:29:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:29:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:29:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:29:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:29:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:29:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:29:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:29:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:29:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:29:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:29:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:29:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:29:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:29:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:29:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:29:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=303970) [2026-01-25 18:29:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=303970) [2026-01-25 18:29:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=303970) [2026-01-25 18:29:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=303970) [2026-01-25 18:29:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=303970) [2026-01-25 18:29:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=303970) [2026-01-25 18:29:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=303970) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=303970) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.97it/s]
(EngineCore_DP0 pid=303970) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.97it/s]
(EngineCore_DP0 pid=303970) 
(EngineCore_DP0 pid=303970) [2026-01-25 18:29:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=303970) [2026-01-25 18:29:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=303970) [2026-01-25 18:29:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=303970) [2026-01-25 18:29:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=303970) [2026-01-25 18:29:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=303970) [2026-01-25 18:29:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=303970) [2026-01-25 18:29:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=303970) [2026-01-25 18:29:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=303970) 2026-01-25 18:29:48,096 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=303970) 2026-01-25 18:29:48,115 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=303970) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 19.34it/s]
(EngineCore_DP0 pid=303970) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 16.13it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 74/512 [00:00<00:00, 731.81it/s]
Adding requests:  29%|██▉       | 150/512 [00:00<00:00, 744.15it/s]
Adding requests:  44%|████▍     | 226/512 [00:00<00:00, 748.40it/s]
Adding requests:  59%|█████▉    | 301/512 [00:00<00:00, 747.32it/s]
Adding requests:  74%|███████▍  | 378/512 [00:00<00:00, 754.04it/s]
Adding requests:  89%|████████▊ | 454/512 [00:00<00:00, 739.31it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 742.80it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:00<00:00, 612.39it/s, est. speed input: 627151.63 toks/s, output: 612.41 toks/s]
Processed prompts:  26%|██▌       | 132/512 [00:00<00:02, 162.92it/s, est. speed input: 188887.39 toks/s, output: 184.46 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:01<00:02, 137.43it/s, est. speed input: 161963.07 toks/s, output: 158.17 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:01<00:02, 123.63it/s, est. speed input: 149127.08 toks/s, output: 145.63 toks/s]
Processed prompts:  40%|███▉      | 203/512 [00:01<00:02, 119.64it/s, est. speed input: 144560.12 toks/s, output: 141.17 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:01<00:02, 113.36it/s, est. speed input: 139674.59 toks/s, output: 136.40 toks/s]
Processed prompts:  45%|████▌     | 231/512 [00:01<00:02, 112.33it/s, est. speed input: 137666.54 toks/s, output: 134.44 toks/s]
Processed prompts:  48%|████▊     | 244/512 [00:01<00:02, 111.38it/s, est. speed input: 135907.31 toks/s, output: 132.72 toks/s]
Processed prompts:  50%|█████     | 256/512 [00:01<00:02, 108.51it/s, est. speed input: 133826.61 toks/s, output: 130.69 toks/s]
Processed prompts:  52%|█████▏    | 268/512 [00:02<00:02, 106.10it/s, est. speed input: 131955.71 toks/s, output: 128.86 toks/s]
Processed prompts:  54%|█████▍    | 279/512 [00:02<00:02, 102.07it/s, est. speed input: 129841.92 toks/s, output: 126.80 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:02<00:02, 99.06it/s, est. speed input: 127952.02 toks/s, output: 124.95 toks/s] 
Processed prompts:  59%|█████▉    | 302/512 [00:02<00:02, 99.20it/s, est. speed input: 126667.55 toks/s, output: 123.70 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:02<00:01, 99.33it/s, est. speed input: 125509.86 toks/s, output: 122.57 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:02<00:01, 99.45it/s, est. speed input: 124461.27 toks/s, output: 121.54 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:02<00:01, 99.46it/s, est. speed input: 123489.21 toks/s, output: 120.59 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:02<00:01, 100.13it/s, est. speed input: 122710.31 toks/s, output: 119.83 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:03<00:01, 99.99it/s, est. speed input: 121892.64 toks/s, output: 119.04 toks/s] 
Processed prompts:  73%|███████▎  | 374/512 [00:03<00:01, 99.83it/s, est. speed input: 121127.81 toks/s, output: 118.29 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:03<00:01, 99.77it/s, est. speed input: 120425.92 toks/s, output: 117.60 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:03<00:01, 99.79it/s, est. speed input: 119783.25 toks/s, output: 116.98 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:03<00:01, 99.83it/s, est. speed input: 119187.50 toks/s, output: 116.39 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:03<00:00, 99.74it/s, est. speed input: 118616.08 toks/s, output: 115.84 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:03<00:00, 99.69it/s, est. speed input: 118083.45 toks/s, output: 115.32 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:03<00:00, 99.33it/s, est. speed input: 117542.76 toks/s, output: 114.79 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:04<00:00, 100.49it/s, est. speed input: 117201.68 toks/s, output: 114.45 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:04<00:00, 100.06it/s, est. speed input: 116738.34 toks/s, output: 114.00 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:04<00:00, 99.83it/s, est. speed input: 116310.38 toks/s, output: 113.58 toks/s] 
Processed prompts:  96%|█████████▋| 494/512 [00:04<00:00, 99.46it/s, est. speed input: 115882.63 toks/s, output: 113.17 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:04<00:00, 99.28it/s, est. speed input: 115486.00 toks/s, output: 112.78 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:04<00:00, 99.28it/s, est. speed input: 115956.96 toks/s, output: 113.24 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:04<00:00, 113.24it/s, est. speed input: 115956.96 toks/s, output: 113.24 toks/s]
[rank0]:[W125 18:29:54.573979201 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-25 18:29:55
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:30:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=304598) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=304598) WARNING 01-25 18:30:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 97.05 requests/s, 99474.72 total tokens/s, 97.05 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-25 18:30:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:30:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:30:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:30:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:30:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:30:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:30:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:30:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:30:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:30:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:30:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:30:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:30:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:30:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:30:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:30:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:30:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:30:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=304598) [2026-01-25 18:30:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=304598) [2026-01-25 18:30:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=304598) [2026-01-25 18:30:07] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=304598) [2026-01-25 18:30:07] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=304598) [2026-01-25 18:30:07] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=304598) [2026-01-25 18:30:07] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=304598) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=304598) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.94it/s]
(EngineCore_DP0 pid=304598) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.94it/s]
(EngineCore_DP0 pid=304598) 
(EngineCore_DP0 pid=304598) [2026-01-25 18:30:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=304598) [2026-01-25 18:30:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=304598) [2026-01-25 18:30:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=304598) [2026-01-25 18:30:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=304598) [2026-01-25 18:30:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=304598) [2026-01-25 18:30:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=304598) [2026-01-25 18:30:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=304598) [2026-01-25 18:30:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=304598) 2026-01-25 18:30:14,069 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=304598) 2026-01-25 18:30:14,082 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=304598) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 37.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 34.13it/s]
(EngineCore_DP0 pid=304598) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 19.16it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 70/1024 [00:00<00:01, 699.08it/s]
Adding requests:  14%|█▍        | 145/1024 [00:00<00:01, 725.62it/s]
Adding requests:  21%|██▏       | 218/1024 [00:00<00:01, 727.29it/s]
Adding requests:  29%|██▊       | 293/1024 [00:00<00:00, 731.91it/s]
Adding requests:  36%|███▌      | 368/1024 [00:00<00:00, 736.73it/s]
Adding requests:  43%|████▎     | 442/1024 [00:00<00:00, 734.21it/s]
Adding requests:  51%|█████     | 518/1024 [00:00<00:00, 737.32it/s]
Adding requests:  58%|█████▊    | 592/1024 [00:00<00:00, 735.41it/s]
Adding requests:  65%|██████▌   | 668/1024 [00:00<00:00, 741.79it/s]
Adding requests:  73%|███████▎  | 745/1024 [00:01<00:00, 750.04it/s]
Adding requests:  80%|████████  | 821/1024 [00:01<00:00, 742.41it/s]
Adding requests:  88%|████████▊ | 897/1024 [00:01<00:00, 747.27it/s]
Adding requests:  95%|█████████▌| 974/1024 [00:01<00:00, 752.66it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 742.70it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:00<00:01, 771.63it/s, est. speed input: 790188.64 toks/s, output: 771.64 toks/s]
Processed prompts:  21%|██        | 216/1024 [00:00<00:03, 202.51it/s, est. speed input: 241512.73 toks/s, output: 235.85 toks/s]
Processed prompts:  25%|██▍       | 254/1024 [00:01<00:04, 157.88it/s, est. speed input: 195984.44 toks/s, output: 191.39 toks/s]
Processed prompts:  27%|██▋       | 279/1024 [00:01<00:05, 143.40it/s, est. speed input: 181603.90 toks/s, output: 177.35 toks/s]
Processed prompts:  29%|██▉       | 299/1024 [00:01<00:05, 126.75it/s, est. speed input: 168329.66 toks/s, output: 164.38 toks/s]
Processed prompts:  31%|███       | 315/1024 [00:01<00:05, 120.77it/s, est. speed input: 162650.86 toks/s, output: 158.84 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:02<00:06, 114.11it/s, est. speed input: 157360.55 toks/s, output: 153.67 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:02<00:06, 110.73it/s, est. speed input: 153544.82 toks/s, output: 149.94 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:02<00:06, 107.37it/s, est. speed input: 150004.93 toks/s, output: 146.49 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:02<00:06, 104.72it/s, est. speed input: 146886.37 toks/s, output: 143.44 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:02<00:06, 102.70it/s, est. speed input: 144127.70 toks/s, output: 140.75 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:02<00:06, 101.16it/s, est. speed input: 141659.31 toks/s, output: 138.34 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:03<00:05, 100.08it/s, est. speed input: 139459.17 toks/s, output: 136.19 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:03<00:05, 99.27it/s, est. speed input: 137470.70 toks/s, output: 134.25 toks/s] 
Processed prompts:  45%|████▍     | 458/1024 [00:03<00:05, 99.92it/s, est. speed input: 135941.69 toks/s, output: 132.76 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:03<00:05, 99.16it/s, est. speed input: 134294.79 toks/s, output: 131.15 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:03<00:05, 98.67it/s, est. speed input: 132799.08 toks/s, output: 129.69 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:03<00:05, 98.36it/s, est. speed input: 131435.01 toks/s, output: 128.35 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:04<00:05, 98.06it/s, est. speed input: 130164.28 toks/s, output: 127.11 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:04<00:04, 97.84it/s, est. speed input: 128990.32 toks/s, output: 125.97 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:04<00:04, 97.75it/s, est. speed input: 127913.90 toks/s, output: 124.92 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:04<00:04, 97.69it/s, est. speed input: 126914.98 toks/s, output: 123.94 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:04<00:04, 97.57it/s, est. speed input: 125971.85 toks/s, output: 123.02 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:04<00:04, 97.36it/s, est. speed input: 125074.44 toks/s, output: 122.14 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:05<00:04, 97.31it/s, est. speed input: 124249.23 toks/s, output: 121.34 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:05<00:04, 97.26it/s, est. speed input: 123473.53 toks/s, output: 120.58 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:05<00:03, 97.34it/s, est. speed input: 122759.57 toks/s, output: 119.88 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:05<00:03, 97.40it/s, est. speed input: 122087.41 toks/s, output: 119.23 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:05<00:03, 97.50it/s, est. speed input: 121460.88 toks/s, output: 118.61 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:05<00:03, 97.44it/s, est. speed input: 120854.28 toks/s, output: 118.02 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:06<00:03, 97.42it/s, est. speed input: 120282.02 toks/s, output: 117.46 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:06<00:03, 97.47it/s, est. speed input: 119747.88 toks/s, output: 116.94 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:06<00:02, 97.52it/s, est. speed input: 119241.90 toks/s, output: 116.45 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:06<00:02, 97.57it/s, est. speed input: 118763.21 toks/s, output: 115.98 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:06<00:02, 97.59it/s, est. speed input: 118306.09 toks/s, output: 115.53 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:06<00:02, 97.52it/s, est. speed input: 117862.87 toks/s, output: 115.10 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:07<00:02, 97.38it/s, est. speed input: 117431.60 toks/s, output: 114.68 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:07<00:02, 97.36it/s, est. speed input: 117026.80 toks/s, output: 114.28 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:07<00:01, 97.25it/s, est. speed input: 116631.63 toks/s, output: 113.90 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:07<00:01, 97.28it/s, est. speed input: 116262.85 toks/s, output: 113.54 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:07<00:01, 97.28it/s, est. speed input: 115908.73 toks/s, output: 113.19 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:07<00:01, 97.24it/s, est. speed input: 115565.51 toks/s, output: 112.86 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:08<00:01, 97.29it/s, est. speed input: 115243.08 toks/s, output: 112.54 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:08<00:01, 97.36it/s, est. speed input: 114935.18 toks/s, output: 112.24 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:08<00:00, 99.24it/s, est. speed input: 114778.52 toks/s, output: 112.09 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:08<00:00, 98.76it/s, est. speed input: 114494.57 toks/s, output: 111.81 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:08<00:00, 98.45it/s, est. speed input: 114223.70 toks/s, output: 111.55 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:08<00:00, 100.00it/s, est. speed input: 114085.82 toks/s, output: 111.41 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:09<00:00, 99.24it/s, est. speed input: 113826.74 toks/s, output: 111.16 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:09<00:00, 100.01it/s, est. speed input: 113663.19 toks/s, output: 111.00 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 100.01it/s, est. speed input: 114331.41 toks/s, output: 111.65 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 111.65it/s, est. speed input: 114331.41 toks/s, output: 111.65 toks/s]
[rank0]:[W125 18:30:26.846398761 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-25 18:30:27
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:30:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=305356) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=305356) WARNING 01-25 18:30:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 95.18 requests/s, 97558.62 total tokens/s, 95.18 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-25 18:30:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:30:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:30:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:30:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:30:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:30:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:30:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:30:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:30:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:30:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:30:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:30:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:30:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:30:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:30:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:30:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:30:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:30:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:30:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=305356) [2026-01-25 18:30:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=305356) [2026-01-25 18:30:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=305356) [2026-01-25 18:30:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=305356) [2026-01-25 18:30:40] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=305356) [2026-01-25 18:30:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=305356) [2026-01-25 18:30:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=305356) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=305356) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.81it/s]
(EngineCore_DP0 pid=305356) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.80it/s]
(EngineCore_DP0 pid=305356) 
(EngineCore_DP0 pid=305356) [2026-01-25 18:30:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=305356) [2026-01-25 18:30:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=305356) [2026-01-25 18:30:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=305356) [2026-01-25 18:30:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=305356) [2026-01-25 18:30:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=305356) [2026-01-25 18:30:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=305356) [2026-01-25 18:30:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=305356) [2026-01-25 18:30:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=305356) 2026-01-25 18:30:48,096 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=305356) 2026-01-25 18:30:48,114 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=305356) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 18.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 20.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 21.97it/s]
(EngineCore_DP0 pid=305356) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 21.38it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 69/2048 [00:00<00:02, 686.43it/s]
Adding requests:   7%|▋         | 145/2048 [00:00<00:02, 727.20it/s]
Adding requests:  11%|█         | 220/2048 [00:00<00:02, 735.77it/s]
Adding requests:  15%|█▍        | 297/2048 [00:00<00:02, 747.46it/s]
Adding requests:  18%|█▊        | 374/2048 [00:00<00:02, 752.49it/s]
Adding requests:  22%|██▏       | 450/2048 [00:00<00:02, 754.51it/s]
Adding requests:  26%|██▌       | 526/2048 [00:00<00:02, 745.27it/s]
Adding requests:  29%|██▉       | 604/2048 [00:00<00:01, 755.15it/s]
Adding requests:  33%|███▎      | 683/2048 [00:00<00:01, 765.86it/s]
Adding requests:  37%|███▋      | 760/2048 [00:01<00:01, 750.32it/s]
Adding requests:  41%|████      | 836/2048 [00:01<00:01, 737.33it/s]
Adding requests:  45%|████▍     | 914/2048 [00:01<00:01, 747.95it/s]
Adding requests:  48%|████▊     | 991/2048 [00:01<00:01, 753.60it/s]
Adding requests:  52%|█████▏    | 1067/2048 [00:01<00:01, 755.39it/s]
Adding requests:  56%|█████▌    | 1143/2048 [00:01<00:01, 748.04it/s]
Adding requests:  60%|█████▉    | 1224/2048 [00:01<00:01, 764.80it/s]
Adding requests:  64%|██████▎   | 1301/2048 [00:01<00:00, 756.99it/s]
Adding requests:  67%|██████▋   | 1377/2048 [00:01<00:00, 756.10it/s]
Adding requests:  71%|███████   | 1453/2048 [00:01<00:00, 747.42it/s]
Adding requests:  75%|███████▍  | 1531/2048 [00:02<00:00, 756.65it/s]
Adding requests:  79%|███████▊  | 1609/2048 [00:02<00:00, 761.62it/s]
Adding requests:  82%|████████▏ | 1686/2048 [00:02<00:00, 758.84it/s]
Adding requests:  86%|████████▌ | 1762/2048 [00:02<00:00, 756.74it/s]
Adding requests:  90%|████████▉ | 1840/2048 [00:02<00:00, 761.77it/s]
Adding requests:  94%|█████████▎| 1917/2048 [00:02<00:00, 750.62it/s]
Adding requests:  97%|█████████▋| 1994/2048 [00:02<00:00, 753.86it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 753.12it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:00<00:01, 1168.43it/s, est. speed input: 1196588.05 toks/s, output: 1168.46 toks/s]
Processed prompts:  18%|█▊        | 375/2048 [00:01<00:07, 224.95it/s, est. speed input: 276417.61 toks/s, output: 269.94 toks/s]   
Processed prompts:  21%|██        | 430/2048 [00:01<00:08, 184.16it/s, est. speed input: 232610.47 toks/s, output: 227.16 toks/s]
Processed prompts:  23%|██▎       | 465/2048 [00:02<00:09, 164.12it/s, est. speed input: 213674.55 toks/s, output: 208.67 toks/s]
Processed prompts:  24%|██▍       | 490/2048 [00:02<00:11, 140.14it/s, est. speed input: 195472.30 toks/s, output: 190.89 toks/s]
Processed prompts:  25%|██▍       | 509/2048 [00:02<00:11, 135.92it/s, est. speed input: 190616.98 toks/s, output: 186.15 toks/s]
Processed prompts:  26%|██▌       | 526/2048 [00:02<00:11, 129.59it/s, est. speed input: 185616.83 toks/s, output: 181.27 toks/s]
Processed prompts:  26%|██▋       | 541/2048 [00:03<00:12, 121.26it/s, est. speed input: 180497.47 toks/s, output: 176.27 toks/s]
Processed prompts:  27%|██▋       | 554/2048 [00:03<00:13, 111.28it/s, est. speed input: 175281.05 toks/s, output: 171.17 toks/s]
Processed prompts:  28%|██▊       | 566/2048 [00:03<00:14, 101.42it/s, est. speed input: 170242.45 toks/s, output: 166.25 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:03<00:15, 93.62it/s, est. speed input: 165687.20 toks/s, output: 161.80 toks/s] 
Processed prompts:  29%|██▉       | 594/2048 [00:03<00:15, 94.09it/s, est. speed input: 162633.65 toks/s, output: 158.82 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:03<00:15, 94.49it/s, est. speed input: 159854.37 toks/s, output: 156.11 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:04<00:15, 94.77it/s, est. speed input: 157303.02 toks/s, output: 153.62 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:04<00:14, 94.97it/s, est. speed input: 154949.99 toks/s, output: 151.32 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:04<00:14, 95.08it/s, est. speed input: 152769.53 toks/s, output: 149.19 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:04<00:14, 95.17it/s, est. speed input: 150751.25 toks/s, output: 147.22 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:04<00:14, 95.14it/s, est. speed input: 148858.12 toks/s, output: 145.37 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:04<00:14, 95.29it/s, est. speed input: 147123.88 toks/s, output: 143.68 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:05<00:13, 95.35it/s, est. speed input: 145496.92 toks/s, output: 142.09 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:05<00:13, 95.32it/s, est. speed input: 143962.66 toks/s, output: 140.59 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:05<00:13, 95.32it/s, est. speed input: 142526.21 toks/s, output: 139.19 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:05<00:13, 95.30it/s, est. speed input: 141172.51 toks/s, output: 137.86 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:05<00:13, 95.31it/s, est. speed input: 139902.03 toks/s, output: 136.62 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:05<00:13, 95.39it/s, est. speed input: 138714.40 toks/s, output: 135.46 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:06<00:12, 95.40it/s, est. speed input: 137585.31 toks/s, output: 134.36 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:06<00:12, 95.39it/s, est. speed input: 136513.81 toks/s, output: 133.31 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:06<00:12, 95.41it/s, est. speed input: 135504.47 toks/s, output: 132.33 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:06<00:12, 95.36it/s, est. speed input: 134534.62 toks/s, output: 131.38 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:06<00:12, 95.42it/s, est. speed input: 133625.97 toks/s, output: 130.49 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:06<00:12, 95.43it/s, est. speed input: 132757.82 toks/s, output: 129.65 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:07<00:11, 95.43it/s, est. speed input: 131929.41 toks/s, output: 128.84 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:07<00:11, 96.64it/s, est. speed input: 131266.21 toks/s, output: 128.19 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:07<00:11, 96.28it/s, est. speed input: 130508.72 toks/s, output: 127.45 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:07<00:11, 95.86it/s, est. speed input: 129768.69 toks/s, output: 126.73 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:07<00:10, 97.40it/s, est. speed input: 129235.03 toks/s, output: 126.21 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:07<00:10, 96.77it/s, est. speed input: 128564.91 toks/s, output: 125.55 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:08<00:10, 96.33it/s, est. speed input: 127922.11 toks/s, output: 124.92 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:08<00:10, 96.04it/s, est. speed input: 127307.09 toks/s, output: 124.32 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:08<00:10, 95.76it/s, est. speed input: 126709.53 toks/s, output: 123.74 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:08<00:10, 95.61it/s, est. speed input: 126139.12 toks/s, output: 123.18 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:08<00:10, 95.49it/s, est. speed input: 125589.16 toks/s, output: 122.65 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:08<00:10, 95.46it/s, est. speed input: 125064.90 toks/s, output: 122.13 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:09<00:09, 95.52it/s, est. speed input: 124565.94 toks/s, output: 121.65 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:09<00:09, 95.48it/s, est. speed input: 124078.69 toks/s, output: 121.17 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:09<00:09, 95.44it/s, est. speed input: 123608.45 toks/s, output: 120.71 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:09<00:09, 97.11it/s, est. speed input: 123279.33 toks/s, output: 120.39 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:09<00:09, 96.54it/s, est. speed input: 122835.83 toks/s, output: 119.96 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:09<00:08, 96.17it/s, est. speed input: 122408.71 toks/s, output: 119.54 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:10<00:08, 95.98it/s, est. speed input: 122000.69 toks/s, output: 119.14 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:10<00:08, 95.80it/s, est. speed input: 121602.67 toks/s, output: 118.75 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:10<00:08, 95.63it/s, est. speed input: 121215.00 toks/s, output: 118.37 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:10<00:08, 95.31it/s, est. speed input: 120825.71 toks/s, output: 117.99 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:10<00:08, 95.21it/s, est. speed input: 120456.82 toks/s, output: 117.63 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:10<00:08, 95.26it/s, est. speed input: 120107.06 toks/s, output: 117.29 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:11<00:07, 95.28it/s, est. speed input: 119766.48 toks/s, output: 116.96 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:11<00:07, 95.23it/s, est. speed input: 119432.99 toks/s, output: 116.63 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:11<00:07, 95.25it/s, est. speed input: 119111.89 toks/s, output: 116.32 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:11<00:07, 95.24it/s, est. speed input: 118798.74 toks/s, output: 116.01 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:11<00:07, 95.23it/s, est. speed input: 118494.56 toks/s, output: 115.72 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:11<00:07, 95.16it/s, est. speed input: 118195.53 toks/s, output: 115.43 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:12<00:06, 95.22it/s, est. speed input: 117910.70 toks/s, output: 115.15 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:12<00:06, 95.20it/s, est. speed input: 117630.24 toks/s, output: 114.87 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:12<00:06, 95.19it/s, est. speed input: 117357.55 toks/s, output: 114.61 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:12<00:06, 95.13it/s, est. speed input: 117089.25 toks/s, output: 114.34 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:12<00:06, 95.22it/s, est. speed input: 116835.23 toks/s, output: 114.10 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:12<00:06, 95.30it/s, est. speed input: 116588.35 toks/s, output: 113.86 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:13<00:05, 95.34it/s, est. speed input: 116347.28 toks/s, output: 113.62 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:13<00:05, 95.34it/s, est. speed input: 116110.55 toks/s, output: 113.39 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:13<00:05, 95.39it/s, est. speed input: 115882.77 toks/s, output: 113.17 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:13<00:05, 95.33it/s, est. speed input: 115655.28 toks/s, output: 112.94 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:13<00:05, 95.26it/s, est. speed input: 115432.28 toks/s, output: 112.73 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:13<00:05, 95.22it/s, est. speed input: 115215.04 toks/s, output: 112.51 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:14<00:04, 95.24it/s, est. speed input: 115005.16 toks/s, output: 112.31 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:14<00:04, 95.23it/s, est. speed input: 114799.19 toks/s, output: 112.11 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:14<00:04, 95.15it/s, est. speed input: 114594.69 toks/s, output: 111.91 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:14<00:04, 95.20it/s, est. speed input: 114399.48 toks/s, output: 111.72 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:14<00:04, 95.23it/s, est. speed input: 114208.70 toks/s, output: 111.53 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:14<00:04, 95.30it/s, est. speed input: 114024.31 toks/s, output: 111.35 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:15<00:03, 95.33it/s, est. speed input: 113843.30 toks/s, output: 111.17 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:15<00:03, 95.34it/s, est. speed input: 113665.51 toks/s, output: 111.00 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:15<00:03, 95.33it/s, est. speed input: 113491.12 toks/s, output: 110.83 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:15<00:03, 95.36it/s, est. speed input: 113322.02 toks/s, output: 110.67 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:15<00:03, 95.31it/s, est. speed input: 113153.41 toks/s, output: 110.50 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:15<00:03, 95.25it/s, est. speed input: 112987.38 toks/s, output: 110.34 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:16<00:02, 95.28it/s, est. speed input: 112827.74 toks/s, output: 110.18 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:16<00:02, 95.28it/s, est. speed input: 112670.47 toks/s, output: 110.03 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:16<00:02, 95.23it/s, est. speed input: 112514.42 toks/s, output: 109.88 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:16<00:02, 95.19it/s, est. speed input: 112361.42 toks/s, output: 109.73 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:16<00:02, 95.24it/s, est. speed input: 112214.71 toks/s, output: 109.58 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:16<00:01, 95.25it/s, est. speed input: 112069.67 toks/s, output: 109.44 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:17<00:01, 96.94it/s, est. speed input: 111991.12 toks/s, output: 109.37 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:17<00:01, 96.43it/s, est. speed input: 111850.89 toks/s, output: 109.23 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:17<00:01, 96.06it/s, est. speed input: 111712.84 toks/s, output: 109.09 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:17<00:01, 95.88it/s, est. speed input: 111580.02 toks/s, output: 108.96 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:17<00:01, 95.66it/s, est. speed input: 111446.75 toks/s, output: 108.83 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:17<00:00, 97.19it/s, est. speed input: 111375.56 toks/s, output: 108.77 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:18<00:00, 96.58it/s, est. speed input: 111246.56 toks/s, output: 108.64 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:18<00:00, 96.15it/s, est. speed input: 111119.89 toks/s, output: 108.52 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:18<00:00, 95.83it/s, est. speed input: 110994.47 toks/s, output: 108.39 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:18<00:00, 95.62it/s, est. speed input: 110871.95 toks/s, output: 108.27 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:18<00:00, 97.11it/s, est. speed input: 110806.97 toks/s, output: 108.21 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:18<00:00, 97.11it/s, est. speed input: 111568.72 toks/s, output: 108.95 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:18<00:00, 108.95it/s, est. speed input: 111568.72 toks/s, output: 108.95 toks/s]
[rank0]:[W125 18:31:11.062515020 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-25 18:31:12
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:31:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=306344) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=306344) WARNING 01-25 18:31:35 [backends.py:609] Failed to read file <frozen os>
Throughput: 94.83 requests/s, 97202.62 total tokens/s, 94.83 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-25 18:31:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:31:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:31:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:31:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:31:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:31:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:31:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:31:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:31:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:31:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:31:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:31:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:31:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:31:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:31:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:31:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:31:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:31:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:31:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:31:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:31:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:31:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:31:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:31:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:31:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:31:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:31:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:31:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=306344) [2026-01-25 18:31:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=306344) [2026-01-25 18:31:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=306344) [2026-01-25 18:31:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=306344) [2026-01-25 18:31:32] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=306344) [2026-01-25 18:31:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=306344) [2026-01-25 18:31:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=306344) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=306344) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.03it/s]
(EngineCore_DP0 pid=306344) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.02it/s]
(EngineCore_DP0 pid=306344) 
(EngineCore_DP0 pid=306344) [2026-01-25 18:31:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=306344) [2026-01-25 18:31:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=306344) [2026-01-25 18:31:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=306344) [2026-01-25 18:31:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=306344) [2026-01-25 18:31:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=306344) [2026-01-25 18:31:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=306344) [2026-01-25 18:31:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=306344) [2026-01-25 18:31:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=306344) [rank0]:W0125 18:31:36.971000 306344 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=306344) [rank0]:W0125 18:31:37.012000 306344 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=306344) [rank0]:W0125 18:31:37.563000 306344 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=306344) [rank0]:W0125 18:31:37.621000 306344 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=306344) 2026-01-25 18:31:39,215 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=306344) 2026-01-25 18:31:39,269 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=306344) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  9.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 24.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 29.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 27.60it/s]
(EngineCore_DP0 pid=306344) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  6.98it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 25.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 24.36it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 71/4096 [00:00<00:05, 702.51it/s]
Adding requests:   4%|▎         | 146/4096 [00:00<00:05, 728.40it/s]
Adding requests:   5%|▌         | 221/4096 [00:00<00:05, 736.99it/s]
Adding requests:   7%|▋         | 298/4096 [00:00<00:05, 749.49it/s]
Adding requests:   9%|▉         | 373/4096 [00:00<00:05, 743.27it/s]
Adding requests:  11%|█         | 448/4096 [00:00<00:04, 743.01it/s]
Adding requests:  13%|█▎        | 523/4096 [00:00<00:04, 734.93it/s]
Adding requests:  15%|█▍        | 600/4096 [00:00<00:04, 740.07it/s]
Adding requests:  17%|█▋        | 676/4096 [00:00<00:04, 743.62it/s]
Adding requests:  18%|█▊        | 751/4096 [00:01<00:04, 725.03it/s]
Adding requests:  20%|██        | 824/4096 [00:01<00:04, 699.52it/s]
Adding requests:  22%|██▏       | 901/4096 [00:01<00:04, 718.60it/s]
Adding requests:  24%|██▍       | 977/4096 [00:01<00:04, 729.14it/s]
Adding requests:  26%|██▌       | 1054/4096 [00:01<00:04, 739.62it/s]
Adding requests:  28%|██▊       | 1129/4096 [00:01<00:04, 722.12it/s]
Adding requests:  29%|██▉       | 1206/4096 [00:01<00:03, 734.64it/s]
Adding requests:  31%|███▏      | 1280/4096 [00:01<00:03, 732.24it/s]
Adding requests:  33%|███▎      | 1358/4096 [00:01<00:03, 744.15it/s]
Adding requests:  35%|███▌      | 1436/4096 [00:01<00:03, 752.48it/s]
Adding requests:  37%|███▋      | 1513/4096 [00:02<00:03, 757.50it/s]
Adding requests:  39%|███▉      | 1591/4096 [00:02<00:03, 762.25it/s]
Adding requests:  41%|████      | 1668/4096 [00:02<00:03, 762.85it/s]
Adding requests:  43%|████▎     | 1745/4096 [00:02<00:03, 761.84it/s]
Adding requests:  44%|████▍     | 1822/4096 [00:02<00:02, 763.76it/s]
Adding requests:  46%|████▋     | 1899/4096 [00:02<00:02, 763.70it/s]
Adding requests:  48%|████▊     | 1976/4096 [00:02<00:02, 763.48it/s]
Adding requests:  50%|█████     | 2055/4096 [00:02<00:02, 767.95it/s]
Adding requests:  52%|█████▏    | 2132/4096 [00:02<00:02, 761.94it/s]
Adding requests:  54%|█████▍    | 2209/4096 [00:02<00:02, 750.55it/s]
Adding requests:  56%|█████▌    | 2286/4096 [00:03<00:02, 752.82it/s]
Adding requests:  58%|█████▊    | 2362/4096 [00:03<00:02, 731.53it/s]
Adding requests:  60%|█████▉    | 2438/4096 [00:03<00:02, 739.19it/s]
Adding requests:  61%|██████▏   | 2515/4096 [00:03<00:02, 745.38it/s]
Adding requests:  63%|██████▎   | 2593/4096 [00:03<00:01, 753.03it/s]
Adding requests:  65%|██████▌   | 2670/4096 [00:03<00:01, 756.27it/s]
Adding requests:  67%|██████▋   | 2746/4096 [00:03<00:01, 753.34it/s]
Adding requests:  69%|██████▉   | 2822/4096 [00:03<00:01, 753.29it/s]
Adding requests:  71%|███████   | 2900/4096 [00:03<00:01, 758.75it/s]
Adding requests:  73%|███████▎  | 2976/4096 [00:03<00:01, 758.61it/s]
Adding requests:  75%|███████▍  | 3052/4096 [00:04<00:01, 749.23it/s]
Adding requests:  76%|███████▋  | 3127/4096 [00:04<00:01, 748.02it/s]
Adding requests:  78%|███████▊  | 3202/4096 [00:04<00:01, 747.58it/s]
Adding requests:  80%|████████  | 3280/4096 [00:04<00:01, 755.39it/s]
Adding requests:  82%|████████▏ | 3357/4096 [00:04<00:00, 757.81it/s]
Adding requests:  84%|████████▍ | 3433/4096 [00:04<00:00, 758.07it/s]
Adding requests:  86%|████████▌ | 3509/4096 [00:04<00:00, 743.67it/s]
Adding requests:  88%|████████▊ | 3585/4096 [00:04<00:00, 746.47it/s]
Adding requests:  89%|████████▉ | 3660/4096 [00:04<00:00, 733.16it/s]
Adding requests:  91%|█████████ | 3736/4096 [00:05<00:00, 740.77it/s]
Adding requests:  93%|█████████▎| 3814/4096 [00:05<00:00, 752.16it/s]
Adding requests:  95%|█████████▍| 3890/4096 [00:05<00:00, 751.91it/s]
Adding requests:  97%|█████████▋| 3966/4096 [00:05<00:00, 752.43it/s]
Adding requests:  99%|█████████▊| 4042/4096 [00:05<00:00, 750.87it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 747.06it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:00<00:02, 1596.74it/s, est. speed input: 1635124.90 toks/s, output: 1596.76 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:02<00:12, 272.22it/s, est. speed input: 344054.68 toks/s, output: 335.99 toks/s]   
Processed prompts:  18%|█▊        | 746/4096 [00:02<00:15, 216.75it/s, est. speed input: 285005.86 toks/s, output: 278.33 toks/s]
Processed prompts:  19%|█▉        | 790/4096 [00:03<00:16, 200.14it/s, est. speed input: 268136.39 toks/s, output: 261.85 toks/s]
Processed prompts:  20%|██        | 822/4096 [00:03<00:18, 177.42it/s, est. speed input: 250986.98 toks/s, output: 245.10 toks/s]
Processed prompts:  21%|██        | 846/4096 [00:03<00:21, 152.32it/s, est. speed input: 234712.51 toks/s, output: 229.21 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:04<00:25, 128.90it/s, est. speed input: 220172.55 toks/s, output: 215.01 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:04<00:26, 119.92it/s, est. speed input: 210686.66 toks/s, output: 205.75 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:04<00:27, 113.87it/s, est. speed input: 202929.35 toks/s, output: 198.17 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:05<00:28, 109.42it/s, est. speed input: 196233.58 toks/s, output: 191.63 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:05<00:29, 105.22it/s, est. speed input: 189999.77 toks/s, output: 185.55 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:05<00:30, 102.20it/s, est. speed input: 184506.20 toks/s, output: 180.18 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:06<00:30, 100.07it/s, est. speed input: 179629.36 toks/s, output: 175.42 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:06<00:30, 98.53it/s, est. speed input: 175261.68 toks/s, output: 171.15 toks/s] 
Processed prompts:  27%|██▋       | 1122/4096 [00:06<00:30, 97.45it/s, est. speed input: 171334.45 toks/s, output: 167.32 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:07<00:30, 97.48it/s, est. speed input: 168002.27 toks/s, output: 164.06 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:07<00:30, 96.70it/s, est. speed input: 164762.79 toks/s, output: 160.90 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:07<00:29, 96.15it/s, est. speed input: 161802.30 toks/s, output: 158.01 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:08<00:29, 95.78it/s, est. speed input: 159095.62 toks/s, output: 155.37 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:08<00:29, 95.49it/s, est. speed input: 156599.94 toks/s, output: 152.93 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:08<00:29, 95.30it/s, est. speed input: 154298.67 toks/s, output: 150.68 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:09<00:28, 95.22it/s, est. speed input: 152180.50 toks/s, output: 148.61 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:09<00:28, 95.15it/s, est. speed input: 150210.72 toks/s, output: 146.69 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:09<00:28, 95.08it/s, est. speed input: 148374.50 toks/s, output: 144.90 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:10<00:27, 95.06it/s, est. speed input: 146665.02 toks/s, output: 143.23 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:10<00:27, 94.98it/s, est. speed input: 145056.76 toks/s, output: 141.66 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:10<00:27, 94.98it/s, est. speed input: 143557.06 toks/s, output: 140.19 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:11<00:26, 94.89it/s, est. speed input: 142135.74 toks/s, output: 138.80 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:11<00:26, 94.91it/s, est. speed input: 140810.02 toks/s, output: 137.51 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:11<00:26, 94.88it/s, est. speed input: 139555.08 toks/s, output: 136.28 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:12<00:25, 94.93it/s, est. speed input: 138379.12 toks/s, output: 135.14 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:12<00:25, 94.90it/s, est. speed input: 137258.33 toks/s, output: 134.04 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:12<00:25, 94.90it/s, est. speed input: 136200.39 toks/s, output: 133.01 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:13<00:24, 94.90it/s, est. speed input: 135195.75 toks/s, output: 132.03 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:13<00:24, 94.88it/s, est. speed input: 134239.87 toks/s, output: 131.09 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:13<00:24, 94.87it/s, est. speed input: 133330.60 toks/s, output: 130.21 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:14<00:23, 94.87it/s, est. speed input: 132465.95 toks/s, output: 129.36 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:14<00:23, 95.70it/s, est. speed input: 131730.77 toks/s, output: 128.64 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:14<00:23, 95.43it/s, est. speed input: 130939.69 toks/s, output: 127.87 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:15<00:22, 95.21it/s, est. speed input: 130180.45 toks/s, output: 127.13 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:15<00:22, 95.07it/s, est. speed input: 129456.00 toks/s, output: 126.42 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:15<00:22, 95.03it/s, est. speed input: 128767.24 toks/s, output: 125.75 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:16<00:21, 94.99it/s, est. speed input: 128107.20 toks/s, output: 125.10 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:16<00:21, 94.95it/s, est. speed input: 127472.28 toks/s, output: 124.48 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:16<00:21, 94.90it/s, est. speed input: 126861.53 toks/s, output: 123.89 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:17<00:20, 94.69it/s, est. speed input: 126259.04 toks/s, output: 123.30 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:17<00:20, 94.74it/s, est. speed input: 125696.77 toks/s, output: 122.75 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:17<00:20, 94.73it/s, est. speed input: 125152.26 toks/s, output: 122.22 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:18<00:19, 94.75it/s, est. speed input: 124630.67 toks/s, output: 121.71 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:18<00:19, 94.75it/s, est. speed input: 124126.78 toks/s, output: 121.22 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:18<00:19, 95.57it/s, est. speed input: 123703.99 toks/s, output: 120.80 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:19<00:18, 95.36it/s, est. speed input: 123236.71 toks/s, output: 120.35 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:19<00:18, 95.19it/s, est. speed input: 122783.40 toks/s, output: 119.91 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:19<00:18, 95.02it/s, est. speed input: 122342.51 toks/s, output: 119.48 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:20<00:17, 94.95it/s, est. speed input: 121919.55 toks/s, output: 119.06 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:20<00:17, 94.88it/s, est. speed input: 121508.91 toks/s, output: 118.66 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:20<00:17, 94.83it/s, est. speed input: 121111.47 toks/s, output: 118.27 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:21<00:16, 95.65it/s, est. speed input: 120784.18 toks/s, output: 117.95 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:21<00:16, 95.35it/s, est. speed input: 120409.55 toks/s, output: 117.59 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:21<00:16, 95.18it/s, est. speed input: 120048.92 toks/s, output: 117.24 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:22<00:15, 95.08it/s, est. speed input: 119700.25 toks/s, output: 116.89 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:22<00:15, 94.97it/s, est. speed input: 119359.73 toks/s, output: 116.56 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:22<00:15, 94.95it/s, est. speed input: 119032.37 toks/s, output: 116.24 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:23<00:14, 94.85it/s, est. speed input: 118709.71 toks/s, output: 115.93 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:23<00:14, 94.79it/s, est. speed input: 118396.95 toks/s, output: 115.62 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:23<00:14, 94.76it/s, est. speed input: 118093.68 toks/s, output: 115.33 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:24<00:13, 94.75it/s, est. speed input: 117799.32 toks/s, output: 115.04 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:24<00:13, 94.76it/s, est. speed input: 117514.23 toks/s, output: 114.76 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:24<00:13, 94.74it/s, est. speed input: 117235.48 toks/s, output: 114.49 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:25<00:12, 94.74it/s, est. speed input: 116964.36 toks/s, output: 114.22 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:25<00:12, 94.73it/s, est. speed input: 116700.34 toks/s, output: 113.97 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:25<00:12, 94.70it/s, est. speed input: 116441.74 toks/s, output: 113.71 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:26<00:11, 94.68it/s, est. speed input: 116190.31 toks/s, output: 113.47 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:26<00:11, 94.69it/s, est. speed input: 115945.95 toks/s, output: 113.23 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:26<00:11, 94.68it/s, est. speed input: 115707.39 toks/s, output: 113.00 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:27<00:10, 94.71it/s, est. speed input: 115476.05 toks/s, output: 112.77 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:27<00:10, 94.70it/s, est. speed input: 115249.22 toks/s, output: 112.55 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:27<00:10, 94.71it/s, est. speed input: 115028.52 toks/s, output: 112.33 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:28<00:09, 94.72it/s, est. speed input: 114813.51 toks/s, output: 112.12 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:28<00:09, 94.68it/s, est. speed input: 114600.93 toks/s, output: 111.91 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:28<00:09, 94.68it/s, est. speed input: 114394.79 toks/s, output: 111.71 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:29<00:08, 94.67it/s, est. speed input: 114193.39 toks/s, output: 111.52 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:29<00:08, 94.67it/s, est. speed input: 113996.64 toks/s, output: 111.32 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:29<00:08, 94.73it/s, est. speed input: 113806.79 toks/s, output: 111.14 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:30<00:07, 94.70it/s, est. speed input: 113618.28 toks/s, output: 110.96 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:30<00:07, 94.67it/s, est. speed input: 113433.32 toks/s, output: 110.77 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:30<00:07, 94.70it/s, est. speed input: 113254.72 toks/s, output: 110.60 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:31<00:06, 94.70it/s, est. speed input: 113078.75 toks/s, output: 110.43 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:31<00:06, 94.68it/s, est. speed input: 112906.27 toks/s, output: 110.26 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:31<00:06, 94.67it/s, est. speed input: 112737.13 toks/s, output: 110.09 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:32<00:05, 94.66it/s, est. speed input: 112571.73 toks/s, output: 109.93 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:32<00:05, 94.65it/s, est. speed input: 112409.27 toks/s, output: 109.77 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [00:33<00:05, 94.62it/s, est. speed input: 112249.64 toks/s, output: 109.62 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [00:33<00:04, 94.63it/s, est. speed input: 112094.06 toks/s, output: 109.47 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [00:33<00:04, 94.62it/s, est. speed input: 111941.22 toks/s, output: 109.32 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [00:34<00:04, 95.47it/s, est. speed input: 111824.37 toks/s, output: 109.20 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [00:34<00:03, 95.26it/s, est. speed input: 111679.38 toks/s, output: 109.06 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [00:34<00:03, 95.08it/s, est. speed input: 111535.63 toks/s, output: 108.92 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [00:35<00:03, 94.96it/s, est. speed input: 111394.95 toks/s, output: 108.78 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [00:35<00:02, 94.84it/s, est. speed input: 111255.65 toks/s, output: 108.65 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [00:35<00:02, 94.81it/s, est. speed input: 111120.79 toks/s, output: 108.52 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [00:36<00:02, 94.73it/s, est. speed input: 110986.51 toks/s, output: 108.39 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [00:36<00:01, 94.71it/s, est. speed input: 110856.01 toks/s, output: 108.26 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [00:36<00:01, 94.69it/s, est. speed input: 110727.61 toks/s, output: 108.13 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [00:37<00:00, 94.68it/s, est. speed input: 110601.59 toks/s, output: 108.01 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [00:37<00:00, 95.48it/s, est. speed input: 110506.10 toks/s, output: 107.92 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [00:37<00:00, 96.16it/s, est. speed input: 110416.25 toks/s, output: 107.83 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:37<00:00, 96.16it/s, est. speed input: 111230.13 toks/s, output: 108.62 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:37<00:00, 108.62it/s, est. speed input: 111230.13 toks/s, output: 108.62 toks/s]
[rank0]:[W125 18:32:24.061974300 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-25 18:32:25
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:32:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=307770) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=307770) WARNING 01-25 18:32:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     kernel_warmup(self)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     runner._dummy_run(
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 412, in forward
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     def forward(
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     raise e
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "<eval_with_key>.34", line 184, in forward
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = None
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/tmp/torchinductor_root/qn/cqnl4fiwje5ivh666fijm4wlfrd72zlqfdl3xiugglo4synqqybo.py", line 983, in call
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 16384), (16384, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Llama3.2-1B-INT8')
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Llama3.2-1B.py", line 118, in dequant_bias_triton
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) ERROR 01-25 18:33:03 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 15.46 GiB of which 1.79 GiB is free. Including non-PyTorch memory, this process has 13.45 GiB memory in use. Of the allocated memory 10.56 GiB is allocated by PyTorch, and 2.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 18:32:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:32:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:32:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:32:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:32:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:32:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:32:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:32:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:32:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:32:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:32:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:32:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:32:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:32:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:32:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:32:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:32:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:32:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:32:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:32:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:32:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:32:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:32:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:32:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:32:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:32:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:32:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:32:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=307770) [2026-01-25 18:32:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=307770) [2026-01-25 18:32:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=307770) [2026-01-25 18:32:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=307770) [2026-01-25 18:32:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=307770) [2026-01-25 18:32:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=307770) [2026-01-25 18:32:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=307770) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=307770) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.96it/s]
(EngineCore_DP0 pid=307770) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.95it/s]
(EngineCore_DP0 pid=307770) 
(EngineCore_DP0 pid=307770) [2026-01-25 18:32:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=307770) [2026-01-25 18:32:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=307770) [2026-01-25 18:32:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=307770) [2026-01-25 18:32:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=307770) [2026-01-25 18:32:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=307770) [2026-01-25 18:32:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=307770) [2026-01-25 18:32:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=307770) [2026-01-25 18:32:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=307770) [rank0]:W0125 18:33:00.934000 307770 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=307770) [rank0]:W0125 18:33:01.540000 307770 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=307770) 2026-01-25 18:33:03,291 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=307770) 2026-01-25 18:33:03,381 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=307770) Process EngineCore_DP0:
(EngineCore_DP0 pid=307770) Traceback (most recent call last):
(EngineCore_DP0 pid=307770)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=307770)     self.run()
(EngineCore_DP0 pid=307770)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=307770)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=307770)     raise e
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=307770)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=307770)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=307770)     super().__init__(
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=307770)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=307770)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=307770)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=307770)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=307770)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=307770)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=307770)     return func(*args, **kwargs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=307770)     kernel_warmup(self)
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=307770)     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=307770)     runner._dummy_run(
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=307770)     return func(*args, **kwargs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=307770)     outputs = self.model(
(EngineCore_DP0 pid=307770)               ^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=307770)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=307770)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=307770)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=307770)     model_output = self.model(
(EngineCore_DP0 pid=307770)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=307770)     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=307770)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=307770)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 412, in forward
(EngineCore_DP0 pid=307770)     def forward(
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=307770)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=307770)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=307770)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=307770)     raise e
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=307770)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=307770)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=307770)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "<eval_with_key>.34", line 184, in forward
(EngineCore_DP0 pid=307770)     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = None
(EngineCore_DP0 pid=307770)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=307770)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=307770)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=307770)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=307770)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=307770)     return compiled_fn(full_args)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=307770)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=307770)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=307770)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=307770)                             ^^^^^^^
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=307770)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=307770)     return self.current_callable(inputs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=307770)     out = model(new_inputs)
(EngineCore_DP0 pid=307770)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/tmp/torchinductor_root/qn/cqnl4fiwje5ivh666fijm4wlfrd72zlqfdl3xiugglo4synqqybo.py", line 983, in call
(EngineCore_DP0 pid=307770)     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 16384), (16384, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Llama3.2-1B-INT8')
(EngineCore_DP0 pid=307770)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=307770)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=307770)     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=307770)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770)   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Llama3.2-1B.py", line 118, in dequant_bias_triton
(EngineCore_DP0 pid=307770)     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=307770)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=307770) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 15.46 GiB of which 1.79 GiB is free. Including non-PyTorch memory, this process has 13.45 GiB memory in use. Of the allocated memory 10.56 GiB is allocated by PyTorch, and 2.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 18:33:03.635835164 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-25 19:32:39
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:32:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=378702) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=378702) WARNING 01-25 19:32:52 [backends.py:609] Failed to read file <frozen os>
Throughput: 56.40 requests/s, 28935.64 total tokens/s, 56.40 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-25 19:32:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:32:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:32:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:32:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:32:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:32:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:32:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:32:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:32:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:32:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:32:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:32:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:32:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:32:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:32:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:32:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:32:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:32:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:47] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:47] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:47] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:47] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:47] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=378702) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=378702) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.15s/it]
(EngineCore_DP0 pid=378702) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.15s/it]
(EngineCore_DP0 pid=378702) 
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=378702) 2026-01-25 19:32:58,752 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=378702) 2026-01-25 19:32:58,766 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=378702) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.09it/s]
(EngineCore_DP0 pid=378702) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.24it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.24it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  95%|█████████▍| 121/128 [00:00<00:00, 1209.21it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1215.88it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:21,  5.84it/s, est. speed input: 2989.47 toks/s, output: 5.84 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 33.11it/s, est. speed input: 14425.88 toks/s, output: 28.17 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 45.38it/s, est. speed input: 19483.32 toks/s, output: 38.05 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 51.80it/s, est. speed input: 22277.82 toks/s, output: 43.51 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 55.62it/s, est. speed input: 24068.02 toks/s, output: 47.01 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 58.39it/s, est. speed input: 25385.81 toks/s, output: 49.58 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 59.88it/s, est. speed input: 26299.24 toks/s, output: 51.37 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 61.17it/s, est. speed input: 27048.57 toks/s, output: 52.83 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 62.12it/s, est. speed input: 27652.36 toks/s, output: 54.01 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 62.79it/s, est. speed input: 28146.44 toks/s, output: 54.97 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:00, 63.28it/s, est. speed input: 28558.53 toks/s, output: 55.78 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:00, 63.55it/s, est. speed input: 28897.80 toks/s, output: 56.44 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 63.72it/s, est. speed input: 29184.81 toks/s, output: 57.00 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 63.80it/s, est. speed input: 29429.37 toks/s, output: 57.48 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 63.85it/s, est. speed input: 29641.58 toks/s, output: 57.89 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 63.99it/s, est. speed input: 29838.57 toks/s, output: 58.28 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 64.01it/s, est. speed input: 30005.92 toks/s, output: 58.61 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 64.05it/s, est. speed input: 30157.79 toks/s, output: 58.90 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 63.84it/s, est. speed input: 30274.93 toks/s, output: 59.13 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 63.84it/s, est. speed input: 30293.42 toks/s, output: 59.17 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 59.16it/s, est. speed input: 30293.42 toks/s, output: 59.17 toks/s]
[rank0]:[W125 19:33:02.523130380 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-25 19:33:03
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:33:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=379384) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=379384) WARNING 01-25 19:33:17 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.41 requests/s, 39374.71 total tokens/s, 38.41 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-25 19:33:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:33:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:33:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:33:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:33:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:33:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:33:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:33:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:33:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:33:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:33:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:33:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:33:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:33:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:12] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=379384) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=379384) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]
(EngineCore_DP0 pid=379384) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]
(EngineCore_DP0 pid=379384) 
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=379384) 2026-01-25 19:33:23,081 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=379384) 2026-01-25 19:33:23,095 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=379384) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 24.90it/s]
(EngineCore_DP0 pid=379384) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.58it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.57it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  52%|█████▏    | 67/128 [00:00<00:00, 666.43it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 666.13it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 16.71it/s, est. speed input: 17113.88 toks/s, output: 16.71 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:03, 31.48it/s, est. speed input: 29967.41 toks/s, output: 29.26 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:03, 36.01it/s, est. speed input: 34117.57 toks/s, output: 33.32 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 38.29it/s, est. speed input: 36283.91 toks/s, output: 35.43 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 39.53it/s, est. speed input: 37575.16 toks/s, output: 36.69 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 40.33it/s, est. speed input: 38455.62 toks/s, output: 37.55 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 40.80it/s, est. speed input: 39073.95 toks/s, output: 38.16 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 41.12it/s, est. speed input: 39537.72 toks/s, output: 38.61 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 41.35it/s, est. speed input: 39907.19 toks/s, output: 38.97 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:01, 41.52it/s, est. speed input: 40205.59 toks/s, output: 39.26 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 41.61it/s, est. speed input: 40442.39 toks/s, output: 39.49 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 41.65it/s, est. speed input: 40633.81 toks/s, output: 39.68 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 41.73it/s, est. speed input: 40809.60 toks/s, output: 39.85 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 41.74it/s, est. speed input: 40949.63 toks/s, output: 39.99 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 41.76it/s, est. speed input: 41073.62 toks/s, output: 40.11 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 41.79it/s, est. speed input: 41185.72 toks/s, output: 40.22 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 41.82it/s, est. speed input: 41285.19 toks/s, output: 40.32 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:00, 41.80it/s, est. speed input: 41367.66 toks/s, output: 40.40 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 41.84it/s, est. speed input: 41450.03 toks/s, output: 40.48 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 41.73it/s, est. speed input: 41501.92 toks/s, output: 40.53 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 41.49it/s, est. speed input: 41522.21 toks/s, output: 40.55 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 41.60it/s, est. speed input: 41583.50 toks/s, output: 40.61 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 41.68it/s, est. speed input: 41638.72 toks/s, output: 40.66 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 41.73it/s, est. speed input: 41689.47 toks/s, output: 40.71 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 41.66it/s, est. speed input: 41721.16 toks/s, output: 40.74 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 41.68it/s, est. speed input: 41760.26 toks/s, output: 40.78 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.68it/s, est. speed input: 41768.27 toks/s, output: 40.79 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.79it/s, est. speed input: 41768.27 toks/s, output: 40.79 toks/s]
[rank0]:[W125 19:33:27.524715008 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-25 19:33:29
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:33:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=380033) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=380033) WARNING 01-25 19:33:42 [backends.py:609] Failed to read file <frozen os>
Throughput: 44.14 requests/s, 45244.05 total tokens/s, 44.14 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-25 19:33:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:33:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:33:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:33:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:33:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:33:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:33:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:33:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:33:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:33:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:33:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:33:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:33:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:33:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=380033) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=380033) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]
(EngineCore_DP0 pid=380033) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]
(EngineCore_DP0 pid=380033) 
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=380033) 2026-01-25 19:33:48,618 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=380033) 2026-01-25 19:33:48,633 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=380033) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 24.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 24.54it/s]
(EngineCore_DP0 pid=380033) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 12.36it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  25%|██▌       | 64/256 [00:00<00:00, 639.16it/s]
Adding requests:  54%|█████▍    | 139/256 [00:00<00:00, 703.38it/s]
Adding requests:  83%|████████▎ | 212/256 [00:00<00:00, 714.53it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 710.83it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:00<00:01, 144.40it/s, est. speed input: 147870.32 toks/s, output: 144.40 toks/s]
Processed prompts:  13%|█▎        | 33/256 [00:00<00:03, 68.96it/s, est. speed input: 77220.64 toks/s, output: 75.41 toks/s]   
Processed prompts:  16%|█▋        | 42/256 [00:00<00:03, 56.16it/s, est. speed input: 64993.29 toks/s, output: 63.47 toks/s]
Processed prompts:  19%|█▉        | 49/256 [00:00<00:03, 55.02it/s, est. speed input: 63013.21 toks/s, output: 61.54 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:00<00:04, 49.54it/s, est. speed input: 58795.37 toks/s, output: 57.42 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:04, 48.17it/s, est. speed input: 57182.60 toks/s, output: 55.84 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:01<00:03, 47.15it/s, est. speed input: 55929.49 toks/s, output: 54.62 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:03, 46.36it/s, est. speed input: 54901.61 toks/s, output: 53.61 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:03, 45.84it/s, est. speed input: 54078.35 toks/s, output: 52.81 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:01<00:03, 45.42it/s, est. speed input: 53371.69 toks/s, output: 52.12 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:01<00:03, 45.22it/s, est. speed input: 52805.05 toks/s, output: 51.57 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:01<00:03, 45.10it/s, est. speed input: 52320.84 toks/s, output: 51.09 toks/s]
Processed prompts:  41%|████      | 104/256 [00:02<00:03, 44.92it/s, est. speed input: 51877.68 toks/s, output: 50.66 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:03, 44.75it/s, est. speed input: 51479.23 toks/s, output: 50.27 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:02<00:03, 44.77it/s, est. speed input: 51155.32 toks/s, output: 49.96 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:02<00:02, 44.75it/s, est. speed input: 50861.03 toks/s, output: 49.67 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:02<00:02, 44.71it/s, est. speed input: 50592.61 toks/s, output: 49.41 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:02<00:02, 44.69it/s, est. speed input: 50352.32 toks/s, output: 49.17 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:02<00:02, 44.66it/s, est. speed input: 50132.59 toks/s, output: 48.96 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:02<00:02, 44.68it/s, est. speed input: 49938.22 toks/s, output: 48.77 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:03<00:02, 44.68it/s, est. speed input: 49758.13 toks/s, output: 48.59 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:03<00:02, 44.71it/s, est. speed input: 49598.43 toks/s, output: 48.44 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:03<00:02, 44.71it/s, est. speed input: 49447.47 toks/s, output: 48.29 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:03<00:01, 44.63it/s, est. speed input: 49297.00 toks/s, output: 48.14 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:03<00:01, 44.65it/s, est. speed input: 49167.63 toks/s, output: 48.02 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:03<00:01, 44.61it/s, est. speed input: 49041.32 toks/s, output: 47.89 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:03<00:01, 44.66it/s, est. speed input: 48932.79 toks/s, output: 47.79 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:04<00:01, 44.60it/s, est. speed input: 48820.09 toks/s, output: 47.68 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:04<00:01, 44.57it/s, est. speed input: 48715.66 toks/s, output: 47.57 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:04<00:01, 44.50it/s, est. speed input: 48611.94 toks/s, output: 47.47 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:04<00:00, 44.53it/s, est. speed input: 48523.82 toks/s, output: 47.39 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:04<00:00, 44.58it/s, est. speed input: 48443.18 toks/s, output: 47.31 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:04<00:00, 44.61it/s, est. speed input: 48367.12 toks/s, output: 47.23 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:04<00:00, 44.61it/s, est. speed input: 48293.07 toks/s, output: 47.16 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:05<00:00, 44.64it/s, est. speed input: 48225.74 toks/s, output: 47.10 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:05<00:00, 44.65it/s, est. speed input: 48160.84 toks/s, output: 47.03 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:05<00:00, 44.66it/s, est. speed input: 48099.56 toks/s, output: 46.97 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:05<00:00, 44.67it/s, est. speed input: 48042.24 toks/s, output: 46.92 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 44.67it/s, est. speed input: 48209.38 toks/s, output: 47.08 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 47.08it/s, est. speed input: 48209.38 toks/s, output: 47.08 toks/s]
[rank0]:[W125 19:33:55.596163472 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-25 19:33:56
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:34:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=380699) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=380699) WARNING 01-25 19:34:11 [backends.py:609] Failed to read file <frozen os>
Throughput: 42.10 requests/s, 43151.59 total tokens/s, 42.10 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-25 19:34:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:34:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:34:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:34:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:34:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:34:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:34:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:34:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:34:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:34:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:34:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:34:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:34:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:34:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:06] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:06] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:06] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:06] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:06] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=380699) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=380699) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]
(EngineCore_DP0 pid=380699) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]
(EngineCore_DP0 pid=380699) 
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=380699) 2026-01-25 19:34:16,935 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=380699) 2026-01-25 19:34:16,950 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=380699) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 28.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 26.07it/s]
(EngineCore_DP0 pid=380699) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 15.74it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 73/512 [00:00<00:00, 723.00it/s]
Adding requests:  29%|██▉       | 149/512 [00:00<00:00, 742.80it/s]
Adding requests:  44%|████▍     | 225/512 [00:00<00:00, 748.90it/s]
Adding requests:  59%|█████▊    | 300/512 [00:00<00:00, 748.57it/s]
Adding requests:  74%|███████▎  | 377/512 [00:00<00:00, 754.70it/s]
Adding requests:  88%|████████▊ | 453/512 [00:00<00:00, 753.38it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 749.33it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 30/512 [00:00<00:02, 197.70it/s, est. speed input: 202468.32 toks/s, output: 197.70 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:00<00:06, 70.93it/s, est. speed input: 82108.43 toks/s, output: 80.18 toks/s]   
Processed prompts:  12%|█▏        | 61/512 [00:00<00:06, 66.87it/s, est. speed input: 76869.23 toks/s, output: 75.07 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:01<00:08, 52.61it/s, est. speed input: 65380.57 toks/s, output: 63.85 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:01<00:08, 49.79it/s, est. speed input: 62167.79 toks/s, output: 60.71 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:01<00:08, 47.70it/s, est. speed input: 59777.49 toks/s, output: 58.38 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:01<00:09, 46.19it/s, est. speed input: 57931.17 toks/s, output: 56.57 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:01<00:09, 45.06it/s, est. speed input: 56442.49 toks/s, output: 55.12 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:02<00:09, 44.25it/s, est. speed input: 55225.82 toks/s, output: 53.93 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:02<00:09, 43.65it/s, est. speed input: 54208.80 toks/s, output: 52.94 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:02<00:08, 43.25it/s, est. speed input: 53355.15 toks/s, output: 52.10 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:02<00:08, 42.97it/s, est. speed input: 52629.18 toks/s, output: 51.40 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:02<00:08, 42.79it/s, est. speed input: 52003.74 toks/s, output: 50.78 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:02<00:08, 42.65it/s, est. speed input: 51455.95 toks/s, output: 50.25 toks/s]
Processed prompts:  31%|███       | 158/512 [00:03<00:08, 42.54it/s, est. speed input: 50968.32 toks/s, output: 49.77 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:03<00:08, 42.50it/s, est. speed input: 50545.49 toks/s, output: 49.36 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:03<00:07, 42.48it/s, est. speed input: 50168.15 toks/s, output: 48.99 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:03<00:07, 42.45it/s, est. speed input: 49826.06 toks/s, output: 48.66 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:03<00:07, 42.43it/s, est. speed input: 49517.95 toks/s, output: 48.36 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:04<00:07, 42.42it/s, est. speed input: 49238.44 toks/s, output: 48.08 toks/s]
Processed prompts:  40%|████      | 206/512 [00:04<00:07, 42.37it/s, est. speed input: 48975.43 toks/s, output: 47.83 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:04<00:07, 42.32it/s, est. speed input: 48733.64 toks/s, output: 47.59 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:04<00:06, 42.35it/s, est. speed input: 48519.26 toks/s, output: 47.38 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:04<00:06, 42.33it/s, est. speed input: 48317.24 toks/s, output: 47.18 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:05<00:06, 42.30it/s, est. speed input: 48126.98 toks/s, output: 47.00 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:05<00:06, 42.27it/s, est. speed input: 47950.32 toks/s, output: 46.83 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:05<00:06, 42.21it/s, est. speed input: 47780.68 toks/s, output: 46.66 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:05<00:05, 42.24it/s, est. speed input: 47630.60 toks/s, output: 46.51 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:05<00:05, 42.28it/s, est. speed input: 47493.10 toks/s, output: 46.38 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:06<00:05, 42.30it/s, est. speed input: 47363.42 toks/s, output: 46.25 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:06<00:05, 42.32it/s, est. speed input: 47242.55 toks/s, output: 46.14 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:06<00:05, 42.30it/s, est. speed input: 47123.96 toks/s, output: 46.02 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:06<00:04, 42.29it/s, est. speed input: 47013.44 toks/s, output: 45.91 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:06<00:04, 42.29it/s, est. speed input: 46910.13 toks/s, output: 45.81 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:06<00:04, 42.33it/s, est. speed input: 46816.10 toks/s, output: 45.72 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:07<00:04, 42.29it/s, est. speed input: 46720.68 toks/s, output: 45.63 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:07<00:04, 42.28it/s, est. speed input: 46631.68 toks/s, output: 45.54 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:07<00:04, 42.29it/s, est. speed input: 46548.79 toks/s, output: 45.46 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:07<00:03, 42.30it/s, est. speed input: 46469.90 toks/s, output: 45.38 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:07<00:03, 42.28it/s, est. speed input: 46392.27 toks/s, output: 45.30 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:08<00:03, 42.28it/s, est. speed input: 46319.70 toks/s, output: 45.23 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:08<00:03, 42.28it/s, est. speed input: 46250.55 toks/s, output: 45.17 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:08<00:03, 42.27it/s, est. speed input: 46183.88 toks/s, output: 45.10 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:08<00:02, 42.26it/s, est. speed input: 46119.49 toks/s, output: 45.04 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:08<00:02, 42.28it/s, est. speed input: 46060.50 toks/s, output: 44.98 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:09<00:02, 42.31it/s, est. speed input: 46004.90 toks/s, output: 44.93 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:09<00:02, 42.28it/s, est. speed input: 45947.54 toks/s, output: 44.87 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:09<00:02, 42.28it/s, est. speed input: 45894.36 toks/s, output: 44.82 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:09<00:01, 42.30it/s, est. speed input: 45844.81 toks/s, output: 44.77 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:09<00:01, 42.31it/s, est. speed input: 45796.28 toks/s, output: 44.72 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:09<00:01, 42.33it/s, est. speed input: 45751.07 toks/s, output: 44.68 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:10<00:01, 42.35it/s, est. speed input: 45707.45 toks/s, output: 44.64 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:10<00:01, 42.33it/s, est. speed input: 45663.59 toks/s, output: 44.59 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:10<00:00, 42.31it/s, est. speed input: 45620.61 toks/s, output: 44.55 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:10<00:00, 42.25it/s, est. speed input: 45576.78 toks/s, output: 44.51 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:10<00:00, 42.27it/s, est. speed input: 45537.76 toks/s, output: 44.47 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:11<00:00, 42.29it/s, est. speed input: 45500.32 toks/s, output: 44.43 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:11<00:00, 42.26it/s, est. speed input: 45462.23 toks/s, output: 44.40 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:11<00:00, 43.57it/s, est. speed input: 45501.21 toks/s, output: 44.43 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 43.57it/s, est. speed input: 45679.28 toks/s, output: 44.61 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 44.61it/s, est. speed input: 45679.28 toks/s, output: 44.61 toks/s]
[rank0]:[W125 19:34:30.480596960 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-25 19:34:31
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:34:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=381469) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=381469) WARNING 01-25 19:34:47 [backends.py:609] Failed to read file <frozen os>
Throughput: 40.55 requests/s, 41566.37 total tokens/s, 40.55 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-25 19:34:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:34:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:34:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:34:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:34:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:34:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:34:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:34:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:34:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:34:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:34:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:34:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:34:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:34:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=381469) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=381469) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=381469) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=381469) 
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=381469) 2026-01-25 19:34:53,672 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=381469) 2026-01-25 19:34:53,686 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=381469) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  6.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 16.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 14.31it/s]
(EngineCore_DP0 pid=381469) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  5.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 13.57it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 72/1024 [00:00<00:01, 712.86it/s]
Adding requests:  14%|█▍        | 148/1024 [00:00<00:01, 740.36it/s]
Adding requests:  22%|██▏       | 224/1024 [00:00<00:01, 748.24it/s]
Adding requests:  29%|██▉       | 299/1024 [00:00<00:00, 746.42it/s]
Adding requests:  37%|███▋      | 374/1024 [00:00<00:00, 746.90it/s]
Adding requests:  44%|████▍     | 450/1024 [00:00<00:00, 749.39it/s]
Adding requests:  51%|█████▏    | 525/1024 [00:00<00:00, 738.76it/s]
Adding requests:  59%|█████▊    | 600/1024 [00:00<00:00, 737.43it/s]
Adding requests:  66%|██████▋   | 679/1024 [00:00<00:00, 751.15it/s]
Adding requests:  74%|███████▍  | 757/1024 [00:01<00:00, 758.30it/s]
Adding requests:  81%|████████▏ | 833/1024 [00:01<00:00, 747.20it/s]
Adding requests:  89%|████████▉ | 911/1024 [00:01<00:00, 756.93it/s]
Adding requests:  97%|█████████▋| 989/1024 [00:01<00:00, 760.82it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 751.62it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 58/1024 [00:00<00:04, 202.36it/s, est. speed input: 207235.20 toks/s, output: 202.37 toks/s]
Processed prompts:   8%|▊         | 79/1024 [00:00<00:09, 103.76it/s, est. speed input: 119024.75 toks/s, output: 116.23 toks/s]
Processed prompts:   9%|▉         | 91/1024 [00:01<00:13, 68.14it/s, est. speed input: 86840.31 toks/s, output: 84.80 toks/s]   
Processed prompts:  10%|▉         | 99/1024 [00:01<00:15, 61.08it/s, est. speed input: 79855.63 toks/s, output: 77.98 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:01<00:16, 54.20it/s, est. speed input: 74024.80 toks/s, output: 72.29 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:01<00:18, 50.44it/s, est. speed input: 70190.95 toks/s, output: 68.55 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:01<00:18, 47.67it/s, est. speed input: 67177.32 toks/s, output: 65.60 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:02<00:19, 45.69it/s, est. speed input: 64752.08 toks/s, output: 63.23 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:02<00:20, 44.22it/s, est. speed input: 62735.67 toks/s, output: 61.26 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:02<00:20, 43.19it/s, est. speed input: 61045.10 toks/s, output: 59.61 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:02<00:20, 42.48it/s, est. speed input: 59615.64 toks/s, output: 58.22 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:02<00:20, 41.95it/s, est. speed input: 58375.20 toks/s, output: 57.01 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:03<00:20, 41.58it/s, est. speed input: 57295.55 toks/s, output: 55.95 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:03<00:20, 41.31it/s, est. speed input: 56344.10 toks/s, output: 55.02 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:03<00:20, 41.12it/s, est. speed input: 55502.36 toks/s, output: 54.20 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:03<00:20, 40.98it/s, est. speed input: 54749.29 toks/s, output: 53.47 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:03<00:20, 40.88it/s, est. speed input: 54073.32 toks/s, output: 52.81 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:04<00:19, 40.83it/s, est. speed input: 53469.27 toks/s, output: 52.22 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:04<00:19, 40.79it/s, est. speed input: 52920.11 toks/s, output: 51.68 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:04<00:19, 40.77it/s, est. speed input: 52420.92 toks/s, output: 51.19 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:04<00:19, 40.75it/s, est. speed input: 51962.28 toks/s, output: 50.74 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:04<00:19, 40.74it/s, est. speed input: 51542.87 toks/s, output: 50.33 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:05<00:19, 40.72it/s, est. speed input: 51153.88 toks/s, output: 49.95 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:05<00:18, 40.70it/s, est. speed input: 50793.19 toks/s, output: 49.60 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:05<00:18, 40.69it/s, est. speed input: 50459.43 toks/s, output: 49.28 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:05<00:18, 40.69it/s, est. speed input: 50150.74 toks/s, output: 48.98 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:05<00:18, 40.67it/s, est. speed input: 49860.40 toks/s, output: 48.69 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:05<00:18, 40.68it/s, est. speed input: 49592.17 toks/s, output: 48.43 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:06<00:17, 40.69it/s, est. speed input: 49340.54 toks/s, output: 48.18 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:06<00:17, 40.69it/s, est. speed input: 49103.71 toks/s, output: 47.95 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:06<00:17, 40.67it/s, est. speed input: 48879.38 toks/s, output: 47.73 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:06<00:17, 40.68it/s, est. speed input: 48670.42 toks/s, output: 47.53 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:06<00:17, 40.69it/s, est. speed input: 48473.50 toks/s, output: 47.34 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:07<00:16, 40.64it/s, est. speed input: 48281.62 toks/s, output: 47.15 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:07<00:16, 40.66it/s, est. speed input: 48106.01 toks/s, output: 46.98 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:07<00:16, 40.65it/s, est. speed input: 47936.86 toks/s, output: 46.81 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:07<00:16, 40.67it/s, est. speed input: 47778.15 toks/s, output: 46.66 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:07<00:16, 40.66it/s, est. speed input: 47625.58 toks/s, output: 46.51 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:08<00:15, 40.65it/s, est. speed input: 47480.50 toks/s, output: 46.37 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:08<00:15, 40.69it/s, est. speed input: 47346.32 toks/s, output: 46.24 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:08<00:15, 40.69it/s, est. speed input: 47215.59 toks/s, output: 46.11 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:08<00:15, 40.72it/s, est. speed input: 47092.97 toks/s, output: 45.99 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:08<00:15, 40.70it/s, est. speed input: 46973.07 toks/s, output: 45.87 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:09<00:14, 40.68it/s, est. speed input: 46857.02 toks/s, output: 45.76 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:09<00:14, 40.68it/s, est. speed input: 46747.21 toks/s, output: 45.65 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:09<00:14, 40.67it/s, est. speed input: 46641.87 toks/s, output: 45.55 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:09<00:14, 40.66it/s, est. speed input: 46539.96 toks/s, output: 45.45 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:09<00:14, 40.64it/s, est. speed input: 46440.81 toks/s, output: 45.35 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:10<00:13, 40.63it/s, est. speed input: 46346.22 toks/s, output: 45.26 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:10<00:13, 40.65it/s, est. speed input: 46257.72 toks/s, output: 45.17 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:10<00:13, 40.68it/s, est. speed input: 46173.11 toks/s, output: 45.09 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:10<00:13, 40.69it/s, est. speed input: 46090.39 toks/s, output: 45.01 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:10<00:13, 40.70it/s, est. speed input: 46011.49 toks/s, output: 44.93 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:11<00:12, 40.68it/s, est. speed input: 45933.12 toks/s, output: 44.86 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:11<00:12, 40.67it/s, est. speed input: 45858.02 toks/s, output: 44.78 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:11<00:12, 40.67it/s, est. speed input: 45785.83 toks/s, output: 44.71 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:11<00:12, 40.66it/s, est. speed input: 45715.97 toks/s, output: 44.64 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:11<00:12, 40.65it/s, est. speed input: 45647.41 toks/s, output: 44.58 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:12<00:11, 40.64it/s, est. speed input: 45581.79 toks/s, output: 44.51 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:12<00:11, 40.64it/s, est. speed input: 45517.73 toks/s, output: 44.45 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:12<00:11, 40.63it/s, est. speed input: 45456.03 toks/s, output: 44.39 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:12<00:11, 40.65it/s, est. speed input: 45397.27 toks/s, output: 44.33 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:12<00:11, 40.64it/s, est. speed input: 45339.20 toks/s, output: 44.28 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:13<00:10, 40.63it/s, est. speed input: 45282.42 toks/s, output: 44.22 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:13<00:10, 40.61it/s, est. speed input: 45226.78 toks/s, output: 44.17 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:13<00:10, 40.64it/s, est. speed input: 45174.97 toks/s, output: 44.12 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:13<00:10, 40.64it/s, est. speed input: 45123.84 toks/s, output: 44.07 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:13<00:10, 40.63it/s, est. speed input: 45073.64 toks/s, output: 44.02 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:14<00:09, 40.63it/s, est. speed input: 45024.71 toks/s, output: 43.97 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:14<00:09, 40.60it/s, est. speed input: 44976.27 toks/s, output: 43.92 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:14<00:09, 40.63it/s, est. speed input: 44931.20 toks/s, output: 43.88 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:14<00:09, 40.64it/s, est. speed input: 44886.85 toks/s, output: 43.83 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:14<00:09, 40.66it/s, est. speed input: 44844.48 toks/s, output: 43.79 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:15<00:09, 40.65it/s, est. speed input: 44802.17 toks/s, output: 43.75 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:15<00:08, 40.62it/s, est. speed input: 44759.41 toks/s, output: 43.71 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:15<00:08, 40.60it/s, est. speed input: 44718.35 toks/s, output: 43.67 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:15<00:08, 40.62it/s, est. speed input: 44679.48 toks/s, output: 43.63 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:15<00:08, 40.62it/s, est. speed input: 44641.05 toks/s, output: 43.59 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:16<00:08, 40.60it/s, est. speed input: 44603.00 toks/s, output: 43.56 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:16<00:07, 40.59it/s, est. speed input: 44565.77 toks/s, output: 43.52 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:16<00:07, 40.60it/s, est. speed input: 44530.22 toks/s, output: 43.49 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:16<00:07, 40.61it/s, est. speed input: 44495.61 toks/s, output: 43.45 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:16<00:07, 40.63it/s, est. speed input: 44462.29 toks/s, output: 43.42 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:17<00:07, 40.64it/s, est. speed input: 44429.60 toks/s, output: 43.39 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:17<00:06, 40.62it/s, est. speed input: 44396.70 toks/s, output: 43.36 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:17<00:06, 40.62it/s, est. speed input: 44364.91 toks/s, output: 43.33 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:17<00:06, 40.62it/s, est. speed input: 44334.15 toks/s, output: 43.30 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:17<00:06, 40.61it/s, est. speed input: 44303.47 toks/s, output: 43.27 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:17<00:06, 40.60it/s, est. speed input: 44273.14 toks/s, output: 43.24 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:18<00:05, 40.57it/s, est. speed input: 44242.75 toks/s, output: 43.21 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:18<00:05, 40.57it/s, est. speed input: 44213.90 toks/s, output: 43.18 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:18<00:05, 40.59it/s, est. speed input: 44186.09 toks/s, output: 43.15 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:18<00:05, 40.58it/s, est. speed input: 44158.20 toks/s, output: 43.12 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:18<00:05, 40.57it/s, est. speed input: 44130.94 toks/s, output: 43.10 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:19<00:04, 40.55it/s, est. speed input: 44103.54 toks/s, output: 43.07 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:19<00:04, 40.57it/s, est. speed input: 44077.89 toks/s, output: 43.04 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:19<00:04, 40.57it/s, est. speed input: 44052.52 toks/s, output: 43.02 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:19<00:04, 40.58it/s, est. speed input: 44027.63 toks/s, output: 43.00 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:19<00:04, 40.57it/s, est. speed input: 44002.87 toks/s, output: 42.97 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:20<00:03, 40.56it/s, est. speed input: 43978.56 toks/s, output: 42.95 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:20<00:03, 40.56it/s, est. speed input: 43954.99 toks/s, output: 42.92 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:20<00:03, 40.59it/s, est. speed input: 43932.60 toks/s, output: 42.90 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:20<00:03, 40.60it/s, est. speed input: 43910.47 toks/s, output: 42.88 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:20<00:03, 40.60it/s, est. speed input: 43888.53 toks/s, output: 42.86 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:21<00:02, 40.58it/s, est. speed input: 43866.34 toks/s, output: 42.84 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:21<00:02, 40.56it/s, est. speed input: 43844.25 toks/s, output: 42.82 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:21<00:02, 40.55it/s, est. speed input: 43822.93 toks/s, output: 42.80 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:21<00:02, 40.57it/s, est. speed input: 43802.47 toks/s, output: 42.78 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:21<00:02, 40.55it/s, est. speed input: 43781.66 toks/s, output: 42.76 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:22<00:01, 40.53it/s, est. speed input: 43760.80 toks/s, output: 42.74 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:22<00:01, 40.52it/s, est. speed input: 43740.71 toks/s, output: 42.72 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:22<00:01, 40.55it/s, est. speed input: 43721.85 toks/s, output: 42.70 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:22<00:01, 40.58it/s, est. speed input: 43703.63 toks/s, output: 42.68 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:22<00:01, 40.59it/s, est. speed input: 43685.42 toks/s, output: 42.66 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:23<00:00, 40.56it/s, est. speed input: 43666.46 toks/s, output: 42.64 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:23<00:00, 40.56it/s, est. speed input: 43648.46 toks/s, output: 42.63 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:23<00:00, 40.58it/s, est. speed input: 43631.19 toks/s, output: 42.61 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:23<00:00, 40.58it/s, est. speed input: 43613.91 toks/s, output: 42.59 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:23<00:00, 42.07it/s, est. speed input: 43639.51 toks/s, output: 42.62 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:23<00:00, 42.07it/s, est. speed input: 43896.49 toks/s, output: 42.87 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:23<00:00, 42.87it/s, est. speed input: 43896.49 toks/s, output: 42.87 toks/s]
[rank0]:[W125 19:35:20.498544376 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-25 19:35:21
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:35:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=382490) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=382490) WARNING 01-25 19:35:40 [backends.py:609] Failed to read file <frozen os>
Throughput: 40.05 requests/s, 41049.98 total tokens/s, 40.05 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-25 19:35:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:35:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:35:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:35:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:35:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:35:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:35:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:35:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:35:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:35:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:35:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:35:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:35:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:35:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:35:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:35:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:35:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:35:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:35] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:35] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=382490) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=382490) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=382490) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=382490) 
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=382490) 2026-01-25 19:35:46,287 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=382490) 2026-01-25 19:35:46,303 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=382490) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  5.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 14.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 19.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 17.02it/s]
(EngineCore_DP0 pid=382490) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.56it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 21.90it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.66it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 71/2048 [00:00<00:02, 704.29it/s]
Adding requests:   7%|▋         | 145/2048 [00:00<00:02, 723.85it/s]
Adding requests:  11%|█         | 218/2048 [00:00<00:02, 725.69it/s]
Adding requests:  14%|█▍        | 293/2048 [00:00<00:02, 733.50it/s]
Adding requests:  18%|█▊        | 367/2048 [00:00<00:02, 731.49it/s]
Adding requests:  22%|██▏       | 441/2048 [00:00<00:02, 728.72it/s]
Adding requests:  25%|██▌       | 516/2048 [00:00<00:02, 735.13it/s]
Adding requests:  29%|██▉       | 593/2048 [00:00<00:01, 743.15it/s]
Adding requests:  33%|███▎      | 670/2048 [00:00<00:01, 749.63it/s]
Adding requests:  37%|███▋      | 748/2048 [00:01<00:01, 757.16it/s]
Adding requests:  40%|████      | 824/2048 [00:01<00:01, 732.86it/s]
Adding requests:  44%|████▍     | 902/2048 [00:01<00:01, 743.98it/s]
Adding requests:  48%|████▊     | 979/2048 [00:01<00:01, 751.51it/s]
Adding requests:  52%|█████▏    | 1057/2048 [00:01<00:01, 759.38it/s]
Adding requests:  55%|█████▌    | 1134/2048 [00:01<00:01, 758.95it/s]
Adding requests:  59%|█████▉    | 1215/2048 [00:01<00:01, 773.39it/s]
Adding requests:  63%|██████▎   | 1293/2048 [00:01<00:00, 761.09it/s]
Adding requests:  67%|██████▋   | 1372/2048 [00:01<00:00, 766.73it/s]
Adding requests:  71%|███████   | 1449/2048 [00:01<00:00, 765.95it/s]
Adding requests:  75%|███████▍  | 1527/2048 [00:02<00:00, 769.69it/s]
Adding requests:  78%|███████▊  | 1606/2048 [00:02<00:00, 773.98it/s]
Adding requests:  82%|████████▏ | 1684/2048 [00:02<00:00, 765.99it/s]
Adding requests:  86%|████████▌ | 1761/2048 [00:02<00:00, 765.40it/s]
Adding requests:  90%|████████▉ | 1839/2048 [00:02<00:00, 769.05it/s]
Adding requests:  94%|█████████▎| 1916/2048 [00:02<00:00, 758.49it/s]
Adding requests:  97%|█████████▋| 1992/2048 [00:02<00:00, 745.12it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 752.75it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:00<00:03, 581.47it/s, est. speed input: 595466.65 toks/s, output: 581.48 toks/s]
Processed prompts:   8%|▊         | 157/2048 [00:01<00:19, 97.28it/s, est. speed input: 118019.77 toks/s, output: 115.25 toks/s]
Processed prompts:   9%|▉         | 184/2048 [00:02<00:27, 67.87it/s, est. speed input: 87311.81 toks/s, output: 85.27 toks/s]  
Processed prompts:  10%|▉         | 201/2048 [00:02<00:29, 61.59it/s, est. speed input: 80515.84 toks/s, output: 78.63 toks/s]
Processed prompts:  10%|█         | 213/2048 [00:02<00:34, 53.36it/s, est. speed input: 73842.59 toks/s, output: 72.11 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:03<00:38, 47.71it/s, est. speed input: 69045.50 toks/s, output: 67.43 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:03<00:39, 45.60it/s, est. speed input: 66081.45 toks/s, output: 64.53 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:04<00:40, 44.06it/s, est. speed input: 63690.80 toks/s, output: 62.20 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:04<00:41, 42.95it/s, est. speed input: 61723.67 toks/s, output: 60.28 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:04<00:41, 42.15it/s, est. speed input: 60070.29 toks/s, output: 58.66 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:05<00:41, 41.57it/s, est. speed input: 58659.82 toks/s, output: 57.28 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:05<00:42, 41.07it/s, est. speed input: 57417.82 toks/s, output: 56.07 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:06<00:41, 40.80it/s, est. speed input: 56359.68 toks/s, output: 55.04 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:06<00:41, 40.61it/s, est. speed input: 55433.22 toks/s, output: 54.13 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:06<00:41, 40.48it/s, est. speed input: 54612.27 toks/s, output: 53.33 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:07<00:41, 40.38it/s, est. speed input: 53879.20 toks/s, output: 52.62 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:07<00:40, 40.32it/s, est. speed input: 53223.14 toks/s, output: 51.98 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:08<00:40, 40.28it/s, est. speed input: 52631.09 toks/s, output: 51.40 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:08<00:40, 40.25it/s, est. speed input: 52094.50 toks/s, output: 50.87 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:08<00:39, 40.22it/s, est. speed input: 51605.26 toks/s, output: 50.40 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:09<00:39, 40.20it/s, est. speed input: 51157.00 toks/s, output: 49.96 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:09<00:38, 40.19it/s, est. speed input: 50745.83 toks/s, output: 49.56 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:10<00:38, 40.17it/s, est. speed input: 50365.53 toks/s, output: 49.19 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:10<00:38, 40.16it/s, est. speed input: 50015.33 toks/s, output: 48.84 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:10<00:37, 40.17it/s, est. speed input: 49691.97 toks/s, output: 48.53 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:11<00:37, 40.15it/s, est. speed input: 49388.29 toks/s, output: 48.23 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:11<00:37, 40.14it/s, est. speed input: 49105.91 toks/s, output: 47.95 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:12<00:36, 40.14it/s, est. speed input: 48841.83 toks/s, output: 47.70 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:12<00:36, 40.14it/s, est. speed input: 48596.10 toks/s, output: 47.46 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:12<00:35, 40.14it/s, est. speed input: 48364.67 toks/s, output: 47.23 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:13<00:35, 40.13it/s, est. speed input: 48146.01 toks/s, output: 47.02 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:13<00:35, 40.13it/s, est. speed input: 47941.15 toks/s, output: 46.82 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:14<00:34, 40.13it/s, est. speed input: 47747.77 toks/s, output: 46.63 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:14<00:34, 40.11it/s, est. speed input: 47562.32 toks/s, output: 46.45 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:14<00:33, 40.13it/s, est. speed input: 47391.11 toks/s, output: 46.28 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:15<00:33, 40.12it/s, est. speed input: 47226.25 toks/s, output: 46.12 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:15<00:33, 40.12it/s, est. speed input: 47070.32 toks/s, output: 45.97 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:16<00:32, 40.11it/s, est. speed input: 46921.28 toks/s, output: 45.82 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:16<00:32, 40.11it/s, est. speed input: 46779.33 toks/s, output: 45.68 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:16<00:31, 40.11it/s, est. speed input: 46644.73 toks/s, output: 45.55 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:17<00:31, 40.11it/s, est. speed input: 46516.20 toks/s, output: 45.43 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:17<00:31, 40.11it/s, est. speed input: 46393.65 toks/s, output: 45.31 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:18<00:30, 40.11it/s, est. speed input: 46276.11 toks/s, output: 45.19 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:18<00:30, 40.10it/s, est. speed input: 46163.38 toks/s, output: 45.08 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:18<00:29, 40.10it/s, est. speed input: 46055.61 toks/s, output: 44.98 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:19<00:29, 40.09it/s, est. speed input: 45951.64 toks/s, output: 44.87 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:19<00:29, 40.09it/s, est. speed input: 45852.53 toks/s, output: 44.78 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:20<00:28, 40.10it/s, est. speed input: 45757.59 toks/s, output: 44.69 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:20<00:28, 40.09it/s, est. speed input: 45665.36 toks/s, output: 44.60 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:20<00:27, 40.10it/s, est. speed input: 45578.08 toks/s, output: 44.51 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:21<00:27, 40.10it/s, est. speed input: 45493.21 toks/s, output: 44.43 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:21<00:27, 40.10it/s, est. speed input: 45411.76 toks/s, output: 44.35 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:22<00:26, 40.10it/s, est. speed input: 45333.19 toks/s, output: 44.27 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:22<00:26, 40.09it/s, est. speed input: 45256.97 toks/s, output: 44.20 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:22<00:25, 40.09it/s, est. speed input: 45183.86 toks/s, output: 44.12 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:23<00:25, 40.08it/s, est. speed input: 45112.01 toks/s, output: 44.05 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:23<00:25, 40.08it/s, est. speed input: 45043.32 toks/s, output: 43.99 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:24<00:24, 40.09it/s, est. speed input: 44977.54 toks/s, output: 43.92 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:24<00:24, 40.08it/s, est. speed input: 44913.40 toks/s, output: 43.86 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:24<00:23, 40.09it/s, est. speed input: 44851.46 toks/s, output: 43.80 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:25<00:23, 40.07it/s, est. speed input: 44790.76 toks/s, output: 43.74 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:25<00:23, 40.08it/s, est. speed input: 44732.67 toks/s, output: 43.68 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:26<00:22, 40.09it/s, est. speed input: 44676.99 toks/s, output: 43.63 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:26<00:22, 40.07it/s, est. speed input: 44621.14 toks/s, output: 43.58 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:26<00:21, 40.08it/s, est. speed input: 44568.14 toks/s, output: 43.52 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:27<00:21, 40.07it/s, est. speed input: 44516.31 toks/s, output: 43.47 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:27<00:21, 40.07it/s, est. speed input: 44466.05 toks/s, output: 43.42 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:28<00:20, 40.07it/s, est. speed input: 44417.04 toks/s, output: 43.38 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:28<00:20, 40.06it/s, est. speed input: 44369.08 toks/s, output: 43.33 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:28<00:19, 40.08it/s, est. speed input: 44323.58 toks/s, output: 43.28 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:29<00:19, 40.05it/s, est. speed input: 44277.60 toks/s, output: 43.24 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:29<00:19, 40.06it/s, est. speed input: 44233.99 toks/s, output: 43.20 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:30<00:18, 40.06it/s, est. speed input: 44191.59 toks/s, output: 43.16 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:30<00:18, 40.06it/s, est. speed input: 44149.87 toks/s, output: 43.12 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:30<00:17, 40.06it/s, est. speed input: 44109.37 toks/s, output: 43.08 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:31<00:17, 40.04it/s, est. speed input: 44069.09 toks/s, output: 43.04 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:31<00:17, 40.06it/s, est. speed input: 44031.13 toks/s, output: 43.00 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:32<00:16, 40.06it/s, est. speed input: 43993.59 toks/s, output: 42.96 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:32<00:16, 40.05it/s, est. speed input: 43956.70 toks/s, output: 42.93 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:32<00:15, 40.05it/s, est. speed input: 43921.06 toks/s, output: 42.89 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:33<00:15, 40.05it/s, est. speed input: 43885.83 toks/s, output: 42.86 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:33<00:15, 40.05it/s, est. speed input: 43851.83 toks/s, output: 42.82 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:34<00:14, 40.05it/s, est. speed input: 43818.59 toks/s, output: 42.79 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:34<00:14, 40.04it/s, est. speed input: 43785.77 toks/s, output: 42.76 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:34<00:13, 40.05it/s, est. speed input: 43754.23 toks/s, output: 42.73 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:35<00:13, 40.04it/s, est. speed input: 43722.56 toks/s, output: 42.70 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:35<00:13, 40.04it/s, est. speed input: 43692.17 toks/s, output: 42.67 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:36<00:12, 40.05it/s, est. speed input: 43662.64 toks/s, output: 42.64 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:36<00:12, 40.04it/s, est. speed input: 43633.45 toks/s, output: 42.61 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:36<00:11, 40.05it/s, est. speed input: 43605.02 toks/s, output: 42.58 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:37<00:11, 40.04it/s, est. speed input: 43576.82 toks/s, output: 42.56 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:37<00:11, 40.03it/s, est. speed input: 43549.10 toks/s, output: 42.53 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:38<00:10, 40.03it/s, est. speed input: 43522.49 toks/s, output: 42.50 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:38<00:10, 40.03it/s, est. speed input: 43496.06 toks/s, output: 42.48 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:38<00:09, 40.04it/s, est. speed input: 43470.70 toks/s, output: 42.45 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:39<00:09, 40.04it/s, est. speed input: 43445.41 toks/s, output: 42.43 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:39<00:09, 40.04it/s, est. speed input: 43420.84 toks/s, output: 42.40 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:40<00:08, 40.03it/s, est. speed input: 43396.50 toks/s, output: 42.38 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:40<00:08, 40.03it/s, est. speed input: 43372.81 toks/s, output: 42.36 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:40<00:07, 40.04it/s, est. speed input: 43349.83 toks/s, output: 42.33 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:41<00:07, 40.04it/s, est. speed input: 43327.02 toks/s, output: 42.31 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:41<00:07, 40.04it/s, est. speed input: 43304.58 toks/s, output: 42.29 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:42<00:06, 40.03it/s, est. speed input: 43282.60 toks/s, output: 42.27 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:42<00:06, 40.03it/s, est. speed input: 43260.90 toks/s, output: 42.25 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:42<00:05, 40.02it/s, est. speed input: 43239.52 toks/s, output: 42.23 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:43<00:05, 40.03it/s, est. speed input: 43218.78 toks/s, output: 42.21 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:43<00:05, 40.03it/s, est. speed input: 43198.37 toks/s, output: 42.19 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:44<00:04, 40.03it/s, est. speed input: 43178.34 toks/s, output: 42.17 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:44<00:04, 40.68it/s, est. speed input: 43179.53 toks/s, output: 42.17 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:44<00:03, 40.49it/s, est. speed input: 43160.25 toks/s, output: 42.15 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:45<00:03, 40.34it/s, est. speed input: 43140.81 toks/s, output: 42.13 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:45<00:03, 40.25it/s, est. speed input: 43121.92 toks/s, output: 42.11 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:46<00:02, 40.18it/s, est. speed input: 43103.30 toks/s, output: 42.09 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:46<00:02, 40.13it/s, est. speed input: 43085.03 toks/s, output: 42.08 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:46<00:01, 40.11it/s, est. speed input: 43067.44 toks/s, output: 42.06 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:47<00:01, 40.08it/s, est. speed input: 43049.59 toks/s, output: 42.04 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:47<00:01, 40.06it/s, est. speed input: 43032.35 toks/s, output: 42.02 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:48<00:00, 40.05it/s, est. speed input: 43015.24 toks/s, output: 42.01 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:48<00:00, 40.76it/s, est. speed input: 43019.40 toks/s, output: 42.01 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:48<00:00, 40.76it/s, est. speed input: 43315.34 toks/s, output: 42.30 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:48<00:00, 42.30it/s, est. speed input: 43315.34 toks/s, output: 42.30 toks/s]
[rank0]:[W125 19:36:39.013425212 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-25 19:36:40
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:36:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=383925) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=383925) WARNING 01-25 19:37:05 [backends.py:609] Failed to read file <frozen os>
Throughput: 40.12 requests/s, 41123.46 total tokens/s, 40.12 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-25 19:36:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:36:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:36:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:36:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:36:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:36:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:36:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:36:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:36:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:36:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:36:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:36:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:36:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:36:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:36:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:36:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:36:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:36:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:00] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=383925) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=383925) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=383925) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=383925) 
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=383925) [rank0]:W0125 19:37:07.784000 383925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=383925) [rank0]:W0125 19:37:07.834000 383925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=383925) [rank0]:W0125 19:37:08.466000 383925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=383925) [rank0]:W0125 19:37:08.538000 383925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=383925) 2026-01-25 19:37:10,776 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=383925) 2026-01-25 19:37:10,792 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=383925) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:05,  1.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  7.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 12.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 17.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 12.33it/s]
(EngineCore_DP0 pid=383925) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.45it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 16.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 20.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 18.12it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 72/4096 [00:00<00:05, 716.96it/s]
Adding requests:   4%|▎         | 149/4096 [00:00<00:05, 743.74it/s]
Adding requests:   5%|▌         | 225/4096 [00:00<00:05, 749.36it/s]
Adding requests:   7%|▋         | 301/4096 [00:00<00:05, 753.03it/s]
Adding requests:   9%|▉         | 379/4096 [00:00<00:04, 759.90it/s]
Adding requests:  11%|█         | 455/4096 [00:00<00:04, 756.41it/s]
Adding requests:  13%|█▎        | 531/4096 [00:00<00:04, 740.55it/s]
Adding requests:  15%|█▍        | 609/4096 [00:00<00:04, 751.42it/s]
Adding requests:  17%|█▋        | 689/4096 [00:00<00:04, 764.18it/s]
Adding requests:  19%|█▊        | 766/4096 [00:01<00:04, 765.13it/s]
Adding requests:  21%|██        | 843/4096 [00:01<00:04, 744.96it/s]
Adding requests:  22%|██▏       | 918/4096 [00:01<00:04, 742.31it/s]
Adding requests:  24%|██▍       | 994/4096 [00:01<00:04, 746.82it/s]
Adding requests:  26%|██▌       | 1072/4096 [00:01<00:04, 754.35it/s]
Adding requests:  28%|██▊       | 1148/4096 [00:01<00:03, 742.17it/s]
Adding requests:  30%|██▉       | 1227/4096 [00:01<00:03, 754.85it/s]
Adding requests:  32%|███▏      | 1303/4096 [00:01<00:03, 752.33it/s]
Adding requests:  34%|███▎      | 1382/4096 [00:01<00:03, 761.47it/s]
Adding requests:  36%|███▌      | 1459/4096 [00:01<00:03, 761.39it/s]
Adding requests:  38%|███▊      | 1538/4096 [00:02<00:03, 767.27it/s]
Adding requests:  40%|███▉      | 1618/4096 [00:02<00:03, 774.50it/s]
Adding requests:  41%|████▏     | 1696/4096 [00:02<00:03, 769.99it/s]
Adding requests:  43%|████▎     | 1774/4096 [00:02<00:03, 767.56it/s]
Adding requests:  45%|████▌     | 1851/4096 [00:02<00:02, 765.25it/s]
Adding requests:  47%|████▋     | 1928/4096 [00:02<00:02, 758.58it/s]
Adding requests:  49%|████▉     | 2007/4096 [00:02<00:02, 765.09it/s]
Adding requests:  51%|█████     | 2085/4096 [00:02<00:02, 767.51it/s]
Adding requests:  53%|█████▎    | 2162/4096 [00:02<00:02, 758.49it/s]
Adding requests:  55%|█████▍    | 2241/4096 [00:02<00:02, 766.03it/s]
Adding requests:  57%|█████▋    | 2318/4096 [00:03<00:02, 760.81it/s]
Adding requests:  58%|█████▊    | 2395/4096 [00:03<00:02, 754.52it/s]
Adding requests:  60%|██████    | 2471/4096 [00:03<00:02, 754.97it/s]
Adding requests:  62%|██████▏   | 2548/4096 [00:03<00:02, 758.17it/s]
Adding requests:  64%|██████▍   | 2627/4096 [00:03<00:01, 766.17it/s]
Adding requests:  66%|██████▌   | 2705/4096 [00:03<00:01, 769.34it/s]
Adding requests:  68%|██████▊   | 2783/4096 [00:03<00:01, 770.81it/s]
Adding requests:  70%|██████▉   | 2861/4096 [00:03<00:01, 771.51it/s]
Adding requests:  72%|███████▏  | 2939/4096 [00:03<00:01, 768.48it/s]
Adding requests:  74%|███████▎  | 3016/4096 [00:03<00:01, 768.14it/s]
Adding requests:  76%|███████▌  | 3094/4096 [00:04<00:01, 770.26it/s]
Adding requests:  77%|███████▋  | 3172/4096 [00:04<00:01, 771.84it/s]
Adding requests:  79%|███████▉  | 3252/4096 [00:04<00:01, 778.93it/s]
Adding requests:  81%|████████▏ | 3330/4096 [00:04<00:00, 778.38it/s]
Adding requests:  83%|████████▎ | 3409/4096 [00:04<00:00, 780.90it/s]
Adding requests:  85%|████████▌ | 3488/4096 [00:04<00:00, 771.11it/s]
Adding requests:  87%|████████▋ | 3567/4096 [00:04<00:00, 776.54it/s]
Adding requests:  89%|████████▉ | 3645/4096 [00:04<00:00, 769.17it/s]
Adding requests:  91%|█████████ | 3722/4096 [00:04<00:00, 751.41it/s]
Adding requests:  93%|█████████▎| 3801/4096 [00:04<00:00, 760.80it/s]
Adding requests:  95%|█████████▍| 3881/4096 [00:05<00:00, 767.01it/s]
Adding requests:  97%|█████████▋| 3959/4096 [00:05<00:00, 769.14it/s]
Adding requests:  99%|█████████▊| 4036/4096 [00:05<00:00, 760.17it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 761.88it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:00<00:05, 740.49it/s, est. speed input: 758309.50 toks/s, output: 740.51 toks/s]
Processed prompts:   7%|▋         | 269/4096 [00:01<00:32, 118.91it/s, est. speed input: 148790.75 toks/s, output: 145.30 toks/s]
Processed prompts:   7%|▋         | 303/4096 [00:02<00:42, 89.21it/s, est. speed input: 117280.79 toks/s, output: 114.53 toks/s] 
Processed prompts:   8%|▊         | 324/4096 [00:03<00:56, 66.71it/s, est. speed input: 96424.26 toks/s, output: 94.16 toks/s]  
Processed prompts:   9%|▊         | 354/4096 [00:04<01:05, 56.91it/s, est. speed input: 85584.31 toks/s, output: 83.58 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:05<01:12, 51.48it/s, est. speed input: 78572.78 toks/s, output: 76.73 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:05<01:16, 47.91it/s, est. speed input: 73471.09 toks/s, output: 71.75 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:06<01:20, 45.51it/s, est. speed input: 69591.58 toks/s, output: 67.96 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:07<01:22, 43.87it/s, est. speed input: 66544.07 toks/s, output: 64.98 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:08<01:23, 42.77it/s, est. speed input: 64091.56 toks/s, output: 62.59 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:09<01:24, 41.99it/s, est. speed input: 62068.54 toks/s, output: 60.61 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:09<01:24, 41.45it/s, est. speed input: 60371.09 toks/s, output: 58.96 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:10<01:24, 41.07it/s, est. speed input: 58930.13 toks/s, output: 57.55 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:11<01:24, 40.82it/s, est. speed input: 57691.75 toks/s, output: 56.34 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:12<01:24, 40.63it/s, est. speed input: 56612.36 toks/s, output: 55.29 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:12<01:23, 40.50it/s, est. speed input: 55666.07 toks/s, output: 54.36 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:13<01:23, 40.41it/s, est. speed input: 54828.51 toks/s, output: 53.54 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:14<01:22, 40.34it/s, est. speed input: 54080.06 toks/s, output: 52.81 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:15<01:21, 40.29it/s, est. speed input: 53410.11 toks/s, output: 52.16 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:16<01:21, 40.26it/s, est. speed input: 52807.00 toks/s, output: 51.57 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:16<01:20, 40.24it/s, est. speed input: 52259.47 toks/s, output: 51.03 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:17<01:19, 40.22it/s, est. speed input: 51761.11 toks/s, output: 50.55 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:18<01:18, 40.20it/s, est. speed input: 51305.05 toks/s, output: 50.10 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:19<01:17, 40.20it/s, est. speed input: 50887.70 toks/s, output: 49.69 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:20<01:17, 40.20it/s, est. speed input: 50503.13 toks/s, output: 49.32 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:20<01:16, 40.19it/s, est. speed input: 50146.69 toks/s, output: 48.97 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:21<01:15, 40.18it/s, est. speed input: 49815.87 toks/s, output: 48.65 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:22<01:14, 40.17it/s, est. speed input: 49508.85 toks/s, output: 48.35 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:23<01:14, 40.17it/s, est. speed input: 49223.18 toks/s, output: 48.07 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:24<01:13, 40.17it/s, est. speed input: 48955.25 toks/s, output: 47.81 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:24<01:12, 40.16it/s, est. speed input: 48704.38 toks/s, output: 47.56 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:25<01:11, 40.16it/s, est. speed input: 48469.28 toks/s, output: 47.33 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:26<01:10, 40.12it/s, est. speed input: 48244.39 toks/s, output: 47.11 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:27<01:10, 40.13it/s, est. speed input: 48036.79 toks/s, output: 46.91 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:28<01:09, 40.14it/s, est. speed input: 47840.41 toks/s, output: 46.72 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:28<01:08, 40.14it/s, est. speed input: 47654.70 toks/s, output: 46.54 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:29<01:07, 40.14it/s, est. speed input: 47479.09 toks/s, output: 46.37 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:30<01:06, 40.14it/s, est. speed input: 47312.73 toks/s, output: 46.20 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:31<01:06, 40.14it/s, est. speed input: 47154.54 toks/s, output: 46.05 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:32<01:05, 40.14it/s, est. speed input: 47004.37 toks/s, output: 45.90 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:32<01:04, 40.13it/s, est. speed input: 46860.84 toks/s, output: 45.76 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:33<01:03, 40.13it/s, est. speed input: 46724.30 toks/s, output: 45.63 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:34<01:02, 40.14it/s, est. speed input: 46594.65 toks/s, output: 45.50 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:35<01:02, 40.15it/s, est. speed input: 46471.46 toks/s, output: 45.38 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:36<01:01, 40.13it/s, est. speed input: 46351.40 toks/s, output: 45.27 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:36<01:00, 40.13it/s, est. speed input: 46237.96 toks/s, output: 45.15 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:37<00:59, 40.13it/s, est. speed input: 46129.03 toks/s, output: 45.05 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:38<00:58, 40.12it/s, est. speed input: 46024.33 toks/s, output: 44.95 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:39<00:58, 40.13it/s, est. speed input: 45924.21 toks/s, output: 44.85 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:40<00:57, 40.12it/s, est. speed input: 45827.60 toks/s, output: 44.75 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:40<00:56, 40.12it/s, est. speed input: 45734.91 toks/s, output: 44.66 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:41<00:55, 40.46it/s, est. speed input: 45670.10 toks/s, output: 44.60 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:42<00:54, 40.35it/s, est. speed input: 45583.53 toks/s, output: 44.52 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:43<00:53, 40.27it/s, est. speed input: 45500.18 toks/s, output: 44.43 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:44<00:53, 40.23it/s, est. speed input: 45420.19 toks/s, output: 44.36 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:44<00:52, 40.19it/s, est. speed input: 45342.65 toks/s, output: 44.28 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:45<00:51, 40.17it/s, est. speed input: 45268.16 toks/s, output: 44.21 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:46<00:50, 40.15it/s, est. speed input: 45196.03 toks/s, output: 44.14 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:47<00:50, 40.14it/s, est. speed input: 45126.49 toks/s, output: 44.07 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:48<00:49, 40.13it/s, est. speed input: 45059.05 toks/s, output: 44.00 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:48<00:48, 40.12it/s, est. speed input: 44993.51 toks/s, output: 43.94 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:49<00:47, 40.11it/s, est. speed input: 44930.35 toks/s, output: 43.88 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:50<00:47, 40.10it/s, est. speed input: 44868.79 toks/s, output: 43.82 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:51<00:46, 40.11it/s, est. speed input: 44809.79 toks/s, output: 43.76 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:52<00:45, 40.10it/s, est. speed input: 44751.99 toks/s, output: 43.70 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:52<00:44, 40.10it/s, est. speed input: 44696.40 toks/s, output: 43.65 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:53<00:43, 40.10it/s, est. speed input: 44642.11 toks/s, output: 43.60 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:54<00:43, 40.10it/s, est. speed input: 44589.55 toks/s, output: 43.54 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:55<00:42, 40.10it/s, est. speed input: 44538.63 toks/s, output: 43.49 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:56<00:41, 40.10it/s, est. speed input: 44489.13 toks/s, output: 43.45 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:56<00:40, 40.09it/s, est. speed input: 44440.77 toks/s, output: 43.40 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:57<00:39, 40.09it/s, est. speed input: 44393.61 toks/s, output: 43.35 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:58<00:39, 40.09it/s, est. speed input: 44348.11 toks/s, output: 43.31 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:59<00:38, 40.10it/s, est. speed input: 44303.97 toks/s, output: 43.27 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [01:00<00:37, 40.09it/s, est. speed input: 44260.59 toks/s, output: 43.22 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [01:00<00:36, 40.09it/s, est. speed input: 44218.25 toks/s, output: 43.18 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [01:01<00:35, 40.09it/s, est. speed input: 44177.11 toks/s, output: 43.14 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [01:02<00:35, 40.08it/s, est. speed input: 44136.96 toks/s, output: 43.10 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [01:03<00:34, 40.09it/s, est. speed input: 44098.06 toks/s, output: 43.06 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [01:04<00:33, 40.08it/s, est. speed input: 44059.92 toks/s, output: 43.03 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [01:04<00:32, 40.08it/s, est. speed input: 44022.62 toks/s, output: 42.99 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [01:05<00:31, 40.08it/s, est. speed input: 43986.25 toks/s, output: 42.96 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [01:06<00:31, 40.07it/s, est. speed input: 43950.63 toks/s, output: 42.92 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [01:07<00:30, 40.08it/s, est. speed input: 43916.05 toks/s, output: 42.89 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [01:07<00:29, 40.07it/s, est. speed input: 43882.13 toks/s, output: 42.85 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [01:08<00:28, 40.08it/s, est. speed input: 43849.21 toks/s, output: 42.82 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:09<00:27, 40.07it/s, est. speed input: 43816.65 toks/s, output: 42.79 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:10<00:27, 40.07it/s, est. speed input: 43785.02 toks/s, output: 42.76 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:11<00:26, 40.08it/s, est. speed input: 43754.42 toks/s, output: 42.73 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:11<00:25, 40.07it/s, est. speed input: 43724.06 toks/s, output: 42.70 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:12<00:24, 40.07it/s, est. speed input: 43694.35 toks/s, output: 42.67 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:13<00:23, 40.06it/s, est. speed input: 43665.30 toks/s, output: 42.64 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:14<00:23, 40.06it/s, est. speed input: 43636.94 toks/s, output: 42.61 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:15<00:22, 40.06it/s, est. speed input: 43609.09 toks/s, output: 42.59 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:15<00:21, 40.06it/s, est. speed input: 43581.83 toks/s, output: 42.56 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:16<00:20, 40.06it/s, est. speed input: 43555.20 toks/s, output: 42.53 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:17<00:19, 40.06it/s, est. speed input: 43529.09 toks/s, output: 42.51 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:18<00:19, 40.06it/s, est. speed input: 43503.45 toks/s, output: 42.48 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:19<00:18, 40.06it/s, est. speed input: 43478.46 toks/s, output: 42.46 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:19<00:17, 40.06it/s, est. speed input: 43453.91 toks/s, output: 42.44 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:20<00:16, 40.06it/s, est. speed input: 43429.77 toks/s, output: 42.41 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:21<00:15, 40.05it/s, est. speed input: 43406.00 toks/s, output: 42.39 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:22<00:15, 40.06it/s, est. speed input: 43382.90 toks/s, output: 42.37 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:23<00:14, 40.06it/s, est. speed input: 43360.19 toks/s, output: 42.34 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:23<00:13, 40.06it/s, est. speed input: 43337.90 toks/s, output: 42.32 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:24<00:12, 40.05it/s, est. speed input: 43315.84 toks/s, output: 42.30 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:25<00:11, 40.05it/s, est. speed input: 43294.31 toks/s, output: 42.28 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:26<00:11, 40.04it/s, est. speed input: 43272.96 toks/s, output: 42.26 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:27<00:10, 40.05it/s, est. speed input: 43252.24 toks/s, output: 42.24 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:27<00:09, 40.04it/s, est. speed input: 43231.73 toks/s, output: 42.22 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:28<00:08, 40.04it/s, est. speed input: 43211.69 toks/s, output: 42.20 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:29<00:07, 40.04it/s, est. speed input: 43191.91 toks/s, output: 42.18 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:30<00:07, 40.03it/s, est. speed input: 43172.30 toks/s, output: 42.16 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:31<00:06, 40.04it/s, est. speed input: 43153.57 toks/s, output: 42.14 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:31<00:05, 40.04it/s, est. speed input: 43134.91 toks/s, output: 42.12 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:32<00:04, 40.04it/s, est. speed input: 43116.49 toks/s, output: 42.11 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:33<00:03, 40.04it/s, est. speed input: 43098.44 toks/s, output: 42.09 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:34<00:03, 40.04it/s, est. speed input: 43080.76 toks/s, output: 42.07 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:35<00:02, 40.04it/s, est. speed input: 43063.34 toks/s, output: 42.05 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:35<00:01, 40.39it/s, est. speed input: 43056.47 toks/s, output: 42.05 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:36<00:00, 40.66it/s, est. speed input: 43050.23 toks/s, output: 42.04 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:36<00:00, 40.66it/s, est. speed input: 43367.62 toks/s, output: 42.35 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:36<00:00, 42.35it/s, est. speed input: 43367.62 toks/s, output: 42.35 toks/s]
[rank0]:[W125 19:38:55.115518319 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-25 19:38:56
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:39:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=386226) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=386226) WARNING 01-25 19:39:32 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     kernel_warmup(self)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     runner._dummy_run(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 412, in forward
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     def forward(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     raise e
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "<eval_with_key>.58", line 304, in forward
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = None
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/tmp/torchinductor_root/rk/crkxlvzuy365gv62vpjxi3gkogstlgibexososupxgmbv5wc564q.py", line 983, in call
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 16384), (16384, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Llama3.2-3B-INT8')
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Llama3.2-3B.py", line 112, in dequant_bias_triton
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 15.46 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 13.27 GiB memory in use. Of the allocated memory 10.68 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 19:39:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:39:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:39:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:39:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:39:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:39:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:39:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:39:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:39:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:39:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:39:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:39:26] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:39:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:39:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:39:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:39:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:39:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:39:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:27] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:27] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:27] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:27] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:27] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=386226) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=386226) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.14it/s]
(EngineCore_DP0 pid=386226) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.14it/s]
(EngineCore_DP0 pid=386226) 
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=386226) [rank0]:W0125 19:39:34.802000 386226 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=386226) [rank0]:W0125 19:39:35.398000 386226 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=386226) 2026-01-25 19:39:38,340 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=386226) 2026-01-25 19:39:38,427 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=386226) Process EngineCore_DP0:
(EngineCore_DP0 pid=386226) Traceback (most recent call last):
(EngineCore_DP0 pid=386226)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=386226)     self.run()
(EngineCore_DP0 pid=386226)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=386226)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=386226)     raise e
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=386226)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=386226)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=386226)     super().__init__(
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=386226)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=386226)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=386226)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=386226)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=386226)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=386226)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=386226)     return func(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=386226)     kernel_warmup(self)
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=386226)     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=386226)     runner._dummy_run(
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=386226)     return func(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=386226)     outputs = self.model(
(EngineCore_DP0 pid=386226)               ^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=386226)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=386226)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=386226)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=386226)     model_output = self.model(
(EngineCore_DP0 pid=386226)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=386226)     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=386226)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=386226)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 412, in forward
(EngineCore_DP0 pid=386226)     def forward(
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=386226)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=386226)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=386226)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=386226)     raise e
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=386226)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=386226)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=386226)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "<eval_with_key>.58", line 304, in forward
(EngineCore_DP0 pid=386226)     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = None
(EngineCore_DP0 pid=386226)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=386226)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=386226)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=386226)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=386226)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=386226)     return compiled_fn(full_args)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=386226)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=386226)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=386226)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=386226)                             ^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=386226)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=386226)     return self.current_callable(inputs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=386226)     out = model(new_inputs)
(EngineCore_DP0 pid=386226)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/tmp/torchinductor_root/rk/crkxlvzuy365gv62vpjxi3gkogstlgibexososupxgmbv5wc564q.py", line 983, in call
(EngineCore_DP0 pid=386226)     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 16384), (16384, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Llama3.2-3B-INT8')
(EngineCore_DP0 pid=386226)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=386226)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=386226)     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Llama3.2-3B.py", line 112, in dequant_bias_triton
(EngineCore_DP0 pid=386226)     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=386226)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 15.46 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 13.27 GiB memory in use. Of the allocated memory 10.68 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 19:39:38.720725389 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-25 20:57:39
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 20:57:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=472785) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=472785) WARNING 01-25 20:57:56 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.18 requests/s, 18559.30 total tokens/s, 36.18 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-25 20:57:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 20:57:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:57:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 20:57:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:57:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:57:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:57:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:57:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:57:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:57:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 20:57:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 20:57:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 20:57:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 20:57:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 20:57:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 20:57:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:57:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 20:57:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:57:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:57:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:57:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:57:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:57:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:57:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 20:57:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 20:57:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 20:57:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 20:57:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=472785) [2026-01-25 20:57:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=472785) [2026-01-25 20:57:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=472785) [2026-01-25 20:57:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=472785) [2026-01-25 20:57:48] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=472785) [2026-01-25 20:57:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=472785) [2026-01-25 20:57:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=472785) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=472785) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.68s/it]
(EngineCore_DP0 pid=472785) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  2.01s/it]
(EngineCore_DP0 pid=472785) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.96s/it]
(EngineCore_DP0 pid=472785) 
(EngineCore_DP0 pid=472785) [2026-01-25 20:57:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=472785) [2026-01-25 20:57:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=472785) [2026-01-25 20:57:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=472785) [2026-01-25 20:57:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=472785) [2026-01-25 20:57:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=472785) [2026-01-25 20:57:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=472785) [2026-01-25 20:57:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=472785) [2026-01-25 20:57:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=472785) 2026-01-25 20:58:02,427 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=472785) 2026-01-25 20:58:02,441 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=472785) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  6.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.19it/s]
(EngineCore_DP0 pid=472785) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.38it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  65%|██████▍   | 83/128 [00:00<00:00, 823.51it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 873.14it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:18,  7.05it/s, est. speed input: 3611.20 toks/s, output: 7.05 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:05, 23.24it/s, est. speed input: 10460.58 toks/s, output: 20.43 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 29.90it/s, est. speed input: 13278.90 toks/s, output: 25.93 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 33.38it/s, est. speed input: 14822.70 toks/s, output: 28.95 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 35.39it/s, est. speed input: 15791.20 toks/s, output: 30.84 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 36.55it/s, est. speed input: 16435.64 toks/s, output: 32.10 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 37.39it/s, est. speed input: 16924.14 toks/s, output: 33.05 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 37.92it/s, est. speed input: 17291.74 toks/s, output: 33.77 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 38.27it/s, est. speed input: 17579.12 toks/s, output: 34.33 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 38.55it/s, est. speed input: 17817.59 toks/s, output: 34.80 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 38.71it/s, est. speed input: 18010.53 toks/s, output: 35.18 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 38.83it/s, est. speed input: 18172.57 toks/s, output: 35.49 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 38.96it/s, est. speed input: 18315.81 toks/s, output: 35.77 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 38.98it/s, est. speed input: 18431.32 toks/s, output: 36.00 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 39.00it/s, est. speed input: 18533.00 toks/s, output: 36.20 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 38.99it/s, est. speed input: 18619.63 toks/s, output: 36.37 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 39.03it/s, est. speed input: 18700.78 toks/s, output: 36.52 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 39.06it/s, est. speed input: 18773.36 toks/s, output: 36.67 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 39.12it/s, est. speed input: 18841.53 toks/s, output: 36.80 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 39.17it/s, est. speed input: 18903.65 toks/s, output: 36.92 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 39.18it/s, est. speed input: 18958.36 toks/s, output: 37.03 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 39.15it/s, est. speed input: 19004.99 toks/s, output: 37.12 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:00, 39.12it/s, est. speed input: 19047.26 toks/s, output: 37.20 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 39.09it/s, est. speed input: 19085.46 toks/s, output: 37.28 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 39.11it/s, est. speed input: 19123.68 toks/s, output: 37.35 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 39.07it/s, est. speed input: 19154.88 toks/s, output: 37.41 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 39.05it/s, est. speed input: 19184.86 toks/s, output: 37.47 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 39.09it/s, est. speed input: 19215.61 toks/s, output: 37.53 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 39.10it/s, est. speed input: 19243.31 toks/s, output: 37.58 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 39.10it/s, est. speed input: 19268.96 toks/s, output: 37.63 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 39.13it/s, est. speed input: 19294.35 toks/s, output: 37.68 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 39.19it/s, est. speed input: 19320.38 toks/s, output: 37.74 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.19it/s, est. speed input: 19336.28 toks/s, output: 37.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.77it/s, est. speed input: 19336.28 toks/s, output: 37.77 toks/s]
[rank0]:[W125 20:58:07.463967402 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-25 20:58:08
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 20:58:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=473510) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=473510) WARNING 01-25 20:58:23 [backends.py:609] Failed to read file <frozen os>
Throughput: 20.03 requests/s, 20531.52 total tokens/s, 20.03 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-25 20:58:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 20:58:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:58:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:58:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 20:58:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 20:58:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 20:58:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 20:58:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 20:58:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 20:58:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:58:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:58:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 20:58:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 20:58:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 20:58:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 20:58:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=473510) [2026-01-25 20:58:17] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=473510) [2026-01-25 20:58:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=473510) [2026-01-25 20:58:17] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=473510) [2026-01-25 20:58:17] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=473510) [2026-01-25 20:58:17] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=473510) [2026-01-25 20:58:17] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=473510) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=473510) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.37it/s]
(EngineCore_DP0 pid=473510) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.76it/s]
(EngineCore_DP0 pid=473510) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.83it/s]
(EngineCore_DP0 pid=473510) 
(EngineCore_DP0 pid=473510) [2026-01-25 20:58:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=473510) [2026-01-25 20:58:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=473510) [2026-01-25 20:58:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=473510) [2026-01-25 20:58:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=473510) [2026-01-25 20:58:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=473510) [2026-01-25 20:58:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=473510) [2026-01-25 20:58:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=473510) [2026-01-25 20:58:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=473510) 2026-01-25 20:58:29,038 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=473510) 2026-01-25 20:58:29,053 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=473510) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 25.24it/s]
(EngineCore_DP0 pid=473510) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.50it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.49it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  38%|███▊      | 49/128 [00:00<00:00, 486.39it/s]
Adding requests:  81%|████████▏ | 104/128 [00:00<00:00, 521.65it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 520.91it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:03, 34.15it/s, est. speed input: 34975.19 toks/s, output: 34.15 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 25.21it/s, est. speed input: 26994.85 toks/s, output: 26.36 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:04, 23.29it/s, est. speed input: 25210.88 toks/s, output: 24.62 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:05, 22.25it/s, est. speed input: 24233.00 toks/s, output: 23.66 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:05, 21.63it/s, est. speed input: 23615.34 toks/s, output: 23.06 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:05, 21.25it/s, est. speed input: 23199.81 toks/s, output: 22.66 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:04, 21.02it/s, est. speed input: 22905.22 toks/s, output: 22.37 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:04, 20.89it/s, est. speed input: 22688.65 toks/s, output: 22.16 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:04, 20.75it/s, est. speed input: 22499.08 toks/s, output: 21.97 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:04, 20.69it/s, est. speed input: 22360.25 toks/s, output: 21.84 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:04, 20.66it/s, est. speed input: 22246.54 toks/s, output: 21.73 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:04, 20.61it/s, est. speed input: 22144.28 toks/s, output: 21.63 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:04, 20.57it/s, est. speed input: 22056.07 toks/s, output: 21.54 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 20.56it/s, est. speed input: 21984.74 toks/s, output: 21.47 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:03, 20.56it/s, est. speed input: 21924.14 toks/s, output: 21.41 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:03, 20.55it/s, est. speed input: 21868.59 toks/s, output: 21.36 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:02<00:03, 20.51it/s, est. speed input: 21813.31 toks/s, output: 21.30 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:02<00:03, 20.52it/s, est. speed input: 21770.67 toks/s, output: 21.26 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:02<00:03, 20.49it/s, est. speed input: 21726.09 toks/s, output: 21.22 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:02<00:03, 20.51it/s, est. speed input: 21692.51 toks/s, output: 21.18 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 20.49it/s, est. speed input: 21656.29 toks/s, output: 21.15 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:03<00:02, 20.49it/s, est. speed input: 21627.01 toks/s, output: 21.12 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:03<00:02, 20.48it/s, est. speed input: 21597.01 toks/s, output: 21.09 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:03<00:02, 20.50it/s, est. speed input: 21574.68 toks/s, output: 21.07 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:03<00:02, 20.51it/s, est. speed input: 21552.80 toks/s, output: 21.05 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:03<00:02, 20.51it/s, est. speed input: 21532.54 toks/s, output: 21.03 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:03<00:02, 20.50it/s, est. speed input: 21511.50 toks/s, output: 21.01 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:04<00:01, 20.51it/s, est. speed input: 21494.66 toks/s, output: 20.99 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:04<00:01, 20.51it/s, est. speed input: 21477.83 toks/s, output: 20.97 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:04<00:01, 20.51it/s, est. speed input: 21462.26 toks/s, output: 20.96 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:04<00:01, 20.52it/s, est. speed input: 21449.08 toks/s, output: 20.95 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:04<00:01, 20.52it/s, est. speed input: 21434.99 toks/s, output: 20.93 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:04<00:01, 20.50it/s, est. speed input: 21420.67 toks/s, output: 20.92 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:05<00:01, 20.51it/s, est. speed input: 21409.43 toks/s, output: 20.91 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:05<00:00, 20.52it/s, est. speed input: 21398.92 toks/s, output: 20.90 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:05<00:00, 20.53it/s, est. speed input: 21389.30 toks/s, output: 20.89 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:05<00:00, 20.51it/s, est. speed input: 21377.04 toks/s, output: 20.88 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:05<00:00, 20.51it/s, est. speed input: 21367.46 toks/s, output: 20.87 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:05<00:00, 20.52it/s, est. speed input: 21359.35 toks/s, output: 20.86 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:05<00:00, 20.53it/s, est. speed input: 21351.29 toks/s, output: 20.85 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:06<00:00, 20.49it/s, est. speed input: 21340.22 toks/s, output: 20.84 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 20.49it/s, est. speed input: 21334.35 toks/s, output: 20.83 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 20.83it/s, est. speed input: 21334.35 toks/s, output: 20.83 toks/s]
[rank0]:[W125 20:58:36.562065467 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-25 20:58:38
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 20:58:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=474218) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=474218) WARNING 01-25 20:58:53 [backends.py:609] Failed to read file <frozen os>
Throughput: 20.64 requests/s, 21159.63 total tokens/s, 20.64 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-25 20:58:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 20:58:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:58:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:58:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 20:58:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 20:58:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 20:58:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 20:58:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 20:58:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 20:58:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:58:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:58:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:58:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 20:58:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 20:58:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 20:58:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 20:58:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=474218) [2026-01-25 20:58:47] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=474218) [2026-01-25 20:58:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=474218) [2026-01-25 20:58:47] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=474218) [2026-01-25 20:58:47] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=474218) [2026-01-25 20:58:47] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=474218) [2026-01-25 20:58:47] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=474218) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=474218) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.38it/s]
(EngineCore_DP0 pid=474218) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.72it/s]
(EngineCore_DP0 pid=474218) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.79it/s]
(EngineCore_DP0 pid=474218) 
(EngineCore_DP0 pid=474218) [2026-01-25 20:58:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=474218) [2026-01-25 20:58:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=474218) [2026-01-25 20:58:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=474218) [2026-01-25 20:58:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=474218) [2026-01-25 20:58:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=474218) [2026-01-25 20:58:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=474218) [2026-01-25 20:58:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=474218) [2026-01-25 20:58:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=474218) 2026-01-25 20:58:58,750 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=474218) 2026-01-25 20:58:58,764 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=474218) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 23.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 23.71it/s]
(EngineCore_DP0 pid=474218) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.57it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 12.31it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  18%|█▊        | 45/256 [00:00<00:00, 446.28it/s]
Adding requests:  39%|███▉      | 101/256 [00:00<00:00, 507.91it/s]
Adding requests:  62%|██████▏   | 159/256 [00:00<00:00, 540.53it/s]
Adding requests:  85%|████████▍ | 217/256 [00:00<00:00, 554.80it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 535.20it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 10/256 [00:00<00:03, 68.78it/s, est. speed input: 70437.13 toks/s, output: 68.78 toks/s]
Processed prompts:   7%|▋         | 17/256 [00:00<00:06, 35.87it/s, est. speed input: 40113.93 toks/s, output: 39.17 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:00<00:08, 26.38it/s, est. speed input: 31211.84 toks/s, output: 30.48 toks/s]
Processed prompts:  10%|█         | 26/256 [00:00<00:09, 24.55it/s, est. speed input: 29130.34 toks/s, output: 28.45 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:01<00:09, 23.37it/s, est. speed input: 27783.06 toks/s, output: 27.13 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:01<00:09, 22.58it/s, est. speed input: 26828.79 toks/s, output: 26.20 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:01<00:09, 22.02it/s, est. speed input: 26113.93 toks/s, output: 25.50 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:01<00:09, 21.66it/s, est. speed input: 25568.22 toks/s, output: 24.97 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:01<00:09, 21.43it/s, est. speed input: 25140.96 toks/s, output: 24.55 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:02<00:09, 21.25it/s, est. speed input: 24790.28 toks/s, output: 24.21 toks/s]
Processed prompts:  21%|██        | 54/256 [00:02<00:09, 21.13it/s, est. speed input: 24495.67 toks/s, output: 23.92 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:02<00:09, 21.03it/s, est. speed input: 24245.98 toks/s, output: 23.68 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:02<00:09, 20.97it/s, est. speed input: 24033.33 toks/s, output: 23.47 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:02<00:09, 20.92it/s, est. speed input: 23848.44 toks/s, output: 23.29 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:03<00:08, 20.87it/s, est. speed input: 23684.22 toks/s, output: 23.13 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:03<00:08, 20.86it/s, est. speed input: 23543.58 toks/s, output: 22.99 toks/s]
Processed prompts:  30%|███       | 78/256 [00:03<00:08, 20.85it/s, est. speed input: 23418.12 toks/s, output: 22.87 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:03<00:08, 20.83it/s, est. speed input: 23305.07 toks/s, output: 22.76 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:03<00:08, 20.84it/s, est. speed input: 23205.81 toks/s, output: 22.66 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:03<00:07, 20.84it/s, est. speed input: 23116.24 toks/s, output: 22.57 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:04<00:07, 20.85it/s, est. speed input: 23035.90 toks/s, output: 22.50 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:04<00:07, 20.83it/s, est. speed input: 22958.72 toks/s, output: 22.42 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:04<00:07, 20.83it/s, est. speed input: 22890.38 toks/s, output: 22.35 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:04<00:07, 20.85it/s, est. speed input: 22829.62 toks/s, output: 22.29 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:04<00:07, 20.84it/s, est. speed input: 22771.69 toks/s, output: 22.24 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:05<00:06, 20.82it/s, est. speed input: 22715.03 toks/s, output: 22.18 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:05<00:06, 20.82it/s, est. speed input: 22665.09 toks/s, output: 22.13 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:05<00:06, 20.80it/s, est. speed input: 22615.94 toks/s, output: 22.09 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:05<00:06, 20.80it/s, est. speed input: 22571.05 toks/s, output: 22.04 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:05<00:06, 20.80it/s, est. speed input: 22530.02 toks/s, output: 22.00 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:06<00:05, 20.78it/s, est. speed input: 22489.32 toks/s, output: 21.96 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:06<00:05, 20.80it/s, est. speed input: 22453.78 toks/s, output: 21.93 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:06<00:05, 20.80it/s, est. speed input: 22420.12 toks/s, output: 21.89 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:06<00:05, 20.80it/s, est. speed input: 22387.68 toks/s, output: 21.86 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:06<00:05, 20.82it/s, est. speed input: 22359.19 toks/s, output: 21.84 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:07<00:04, 20.81it/s, est. speed input: 22330.06 toks/s, output: 21.81 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:07<00:04, 20.80it/s, est. speed input: 22302.03 toks/s, output: 21.78 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:07<00:04, 20.81it/s, est. speed input: 22277.45 toks/s, output: 21.76 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:07<00:04, 20.82it/s, est. speed input: 22253.65 toks/s, output: 21.73 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:07<00:04, 20.80it/s, est. speed input: 22229.44 toks/s, output: 21.71 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:08<00:03, 20.82it/s, est. speed input: 22208.83 toks/s, output: 21.69 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:08<00:03, 20.81it/s, est. speed input: 22187.31 toks/s, output: 21.67 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:08<00:03, 20.80it/s, est. speed input: 22166.31 toks/s, output: 21.65 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:08<00:03, 20.80it/s, est. speed input: 22146.50 toks/s, output: 21.63 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:08<00:03, 20.79it/s, est. speed input: 22127.06 toks/s, output: 21.61 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:08<00:02, 20.80it/s, est. speed input: 22109.96 toks/s, output: 21.59 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:09<00:02, 20.77it/s, est. speed input: 22090.62 toks/s, output: 21.57 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:09<00:02, 20.77it/s, est. speed input: 22073.54 toks/s, output: 21.56 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:09<00:02, 20.79it/s, est. speed input: 22058.90 toks/s, output: 21.54 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:09<00:02, 20.80it/s, est. speed input: 22044.43 toks/s, output: 21.53 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:09<00:02, 20.80it/s, est. speed input: 22030.18 toks/s, output: 21.51 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:10<00:01, 20.81it/s, est. speed input: 22017.22 toks/s, output: 21.50 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:10<00:01, 20.80it/s, est. speed input: 22003.38 toks/s, output: 21.49 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:10<00:01, 20.81it/s, est. speed input: 21990.81 toks/s, output: 21.48 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:10<00:01, 20.79it/s, est. speed input: 21977.73 toks/s, output: 21.46 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:10<00:01, 20.80it/s, est. speed input: 21965.97 toks/s, output: 21.45 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:11<00:00, 20.80it/s, est. speed input: 21954.58 toks/s, output: 21.44 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:11<00:00, 20.78it/s, est. speed input: 21941.83 toks/s, output: 21.43 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:11<00:00, 20.79it/s, est. speed input: 21931.27 toks/s, output: 21.42 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:11<00:00, 20.79it/s, est. speed input: 21920.95 toks/s, output: 21.41 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:11<00:00, 20.78it/s, est. speed input: 21909.95 toks/s, output: 21.40 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:11<00:00, 20.78it/s, est. speed input: 21991.84 toks/s, output: 21.48 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:11<00:00, 21.48it/s, est. speed input: 21991.84 toks/s, output: 21.48 toks/s]
[rank0]:[W125 20:59:12.352300699 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-25 20:59:13
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 20:59:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=474986) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=474986) WARNING 01-25 20:59:29 [backends.py:609] Failed to read file <frozen os>
Throughput: 20.83 requests/s, 21354.27 total tokens/s, 20.83 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-25 20:59:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 20:59:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:59:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 20:59:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:59:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:59:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:59:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:59:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:59:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:59:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 20:59:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 20:59:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 20:59:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 20:59:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 20:59:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 20:59:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:59:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 20:59:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:59:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:59:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:59:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:59:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 20:59:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 20:59:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 20:59:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 20:59:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 20:59:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 20:59:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=474986) [2026-01-25 20:59:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=474986) [2026-01-25 20:59:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=474986) [2026-01-25 20:59:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=474986) [2026-01-25 20:59:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=474986) [2026-01-25 20:59:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=474986) [2026-01-25 20:59:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=474986) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=474986) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.34it/s]
(EngineCore_DP0 pid=474986) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.74it/s]
(EngineCore_DP0 pid=474986) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.81it/s]
(EngineCore_DP0 pid=474986) 
(EngineCore_DP0 pid=474986) [2026-01-25 20:59:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=474986) [2026-01-25 20:59:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=474986) [2026-01-25 20:59:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=474986) [2026-01-25 20:59:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=474986) [2026-01-25 20:59:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=474986) [2026-01-25 20:59:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=474986) [2026-01-25 20:59:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=474986) [2026-01-25 20:59:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=474986) 2026-01-25 20:59:35,324 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=474986) 2026-01-25 20:59:35,338 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=474986) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 28.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 25.97it/s]
(EngineCore_DP0 pid=474986) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  6.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 14.03it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  10%|█         | 52/512 [00:00<00:00, 511.28it/s]
Adding requests:  21%|██▏       | 110/512 [00:00<00:00, 547.74it/s]
Adding requests:  33%|███▎      | 169/512 [00:00<00:00, 565.85it/s]
Adding requests:  45%|████▍     | 229/512 [00:00<00:00, 576.78it/s]
Adding requests:  56%|█████▌    | 287/512 [00:00<00:00, 573.68it/s]
Adding requests:  68%|██████▊   | 348/512 [00:00<00:00, 585.50it/s]
Adding requests:  80%|███████▉  | 408/512 [00:00<00:00, 588.59it/s]
Adding requests:  92%|█████████▏| 470/512 [00:00<00:00, 595.26it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 585.25it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▎         | 18/512 [00:00<00:05, 98.68it/s, est. speed input: 101054.88 toks/s, output: 98.68 toks/s]
Processed prompts:   5%|▌         | 28/512 [00:00<00:10, 44.35it/s, est. speed input: 50807.16 toks/s, output: 49.62 toks/s] 
Processed prompts:   7%|▋         | 34/512 [00:00<00:16, 29.55it/s, est. speed input: 36800.41 toks/s, output: 35.94 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:01<00:17, 27.21it/s, est. speed input: 34212.47 toks/s, output: 33.41 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:01<00:18, 25.45it/s, est. speed input: 32365.82 toks/s, output: 31.61 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:01<00:19, 24.16it/s, est. speed input: 30984.75 toks/s, output: 30.26 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:01<00:19, 23.21it/s, est. speed input: 29909.91 toks/s, output: 29.21 toks/s]
Processed prompts:  11%|█         | 54/512 [00:01<00:20, 22.54it/s, est. speed input: 29058.80 toks/s, output: 28.38 toks/s]
Processed prompts:  11%|█▏        | 58/512 [00:02<00:20, 22.07it/s, est. speed input: 28363.25 toks/s, output: 27.70 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:02<00:20, 21.73it/s, est. speed input: 27784.06 toks/s, output: 27.13 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:02<00:20, 21.49it/s, est. speed input: 27293.04 toks/s, output: 26.65 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:02<00:20, 21.33it/s, est. speed input: 26873.98 toks/s, output: 26.24 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:02<00:20, 21.21it/s, est. speed input: 26511.62 toks/s, output: 25.89 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:03<00:20, 21.14it/s, est. speed input: 26195.18 toks/s, output: 25.58 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:03<00:20, 21.06it/s, est. speed input: 25909.85 toks/s, output: 25.30 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:03<00:20, 21.02it/s, est. speed input: 25661.76 toks/s, output: 25.06 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:03<00:20, 21.00it/s, est. speed input: 25440.14 toks/s, output: 24.84 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:03<00:19, 20.97it/s, est. speed input: 25237.43 toks/s, output: 24.65 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:04<00:19, 20.96it/s, est. speed input: 25056.16 toks/s, output: 24.47 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:04<00:19, 20.95it/s, est. speed input: 24890.22 toks/s, output: 24.31 toks/s]
Processed prompts:  21%|██        | 106/512 [00:04<00:19, 20.94it/s, est. speed input: 24739.02 toks/s, output: 24.16 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:04<00:19, 20.93it/s, est. speed input: 24599.50 toks/s, output: 24.02 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:04<00:19, 20.93it/s, est. speed input: 24473.13 toks/s, output: 23.90 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:04<00:18, 20.93it/s, est. speed input: 24355.56 toks/s, output: 23.78 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:05<00:18, 20.91it/s, est. speed input: 24244.82 toks/s, output: 23.68 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:05<00:18, 20.91it/s, est. speed input: 24143.65 toks/s, output: 23.58 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:05<00:18, 20.92it/s, est. speed input: 24049.95 toks/s, output: 23.49 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:05<00:18, 20.92it/s, est. speed input: 23962.31 toks/s, output: 23.40 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:05<00:17, 20.92it/s, est. speed input: 23879.80 toks/s, output: 23.32 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:06<00:17, 20.92it/s, est. speed input: 23802.82 toks/s, output: 23.24 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:06<00:17, 20.91it/s, est. speed input: 23729.69 toks/s, output: 23.17 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:06<00:17, 20.90it/s, est. speed input: 23660.35 toks/s, output: 23.11 toks/s]
Processed prompts:  30%|███       | 154/512 [00:06<00:17, 20.90it/s, est. speed input: 23595.89 toks/s, output: 23.04 toks/s]
Processed prompts:  31%|███       | 158/512 [00:06<00:16, 20.90it/s, est. speed input: 23534.84 toks/s, output: 22.98 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:07<00:16, 20.90it/s, est. speed input: 23476.85 toks/s, output: 22.93 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:07<00:16, 20.89it/s, est. speed input: 23421.11 toks/s, output: 22.87 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:07<00:16, 20.88it/s, est. speed input: 23368.33 toks/s, output: 22.82 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:07<00:16, 20.88it/s, est. speed input: 23318.62 toks/s, output: 22.77 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:07<00:15, 20.90it/s, est. speed input: 23272.72 toks/s, output: 22.73 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:08<00:15, 20.90it/s, est. speed input: 23228.02 toks/s, output: 22.68 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:08<00:15, 20.89it/s, est. speed input: 23184.67 toks/s, output: 22.64 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:08<00:15, 20.90it/s, est. speed input: 23144.71 toks/s, output: 22.60 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:08<00:15, 20.90it/s, est. speed input: 23106.11 toks/s, output: 22.56 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:08<00:15, 20.90it/s, est. speed input: 23068.80 toks/s, output: 22.53 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:08<00:14, 20.90it/s, est. speed input: 23033.50 toks/s, output: 22.49 toks/s]
Processed prompts:  40%|████      | 206/512 [00:09<00:14, 20.89it/s, est. speed input: 22998.75 toks/s, output: 22.46 toks/s]
Processed prompts:  41%|████      | 210/512 [00:09<00:14, 20.89it/s, est. speed input: 22965.72 toks/s, output: 22.43 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:09<00:14, 20.89it/s, est. speed input: 22934.25 toks/s, output: 22.40 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:09<00:14, 20.90it/s, est. speed input: 22904.91 toks/s, output: 22.37 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:09<00:13, 20.91it/s, est. speed input: 22876.16 toks/s, output: 22.34 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:10<00:13, 20.90it/s, est. speed input: 22847.77 toks/s, output: 22.31 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:10<00:13, 20.89it/s, est. speed input: 22820.74 toks/s, output: 22.29 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:10<00:13, 20.89it/s, est. speed input: 22794.26 toks/s, output: 22.26 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:10<00:13, 20.89it/s, est. speed input: 22769.32 toks/s, output: 22.24 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:10<00:12, 20.88it/s, est. speed input: 22744.73 toks/s, output: 22.21 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:11<00:12, 20.88it/s, est. speed input: 22721.29 toks/s, output: 22.19 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:11<00:12, 20.87it/s, est. speed input: 22697.83 toks/s, output: 22.17 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:11<00:12, 20.87it/s, est. speed input: 22675.76 toks/s, output: 22.14 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:11<00:12, 20.88it/s, est. speed input: 22654.71 toks/s, output: 22.12 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:11<00:11, 20.89it/s, est. speed input: 22634.99 toks/s, output: 22.10 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:12<00:11, 20.89it/s, est. speed input: 22615.18 toks/s, output: 22.09 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:12<00:11, 20.88it/s, est. speed input: 22595.13 toks/s, output: 22.07 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:12<00:11, 20.87it/s, est. speed input: 22576.24 toks/s, output: 22.05 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:12<00:11, 20.89it/s, est. speed input: 22558.76 toks/s, output: 22.03 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:12<00:11, 20.88it/s, est. speed input: 22541.12 toks/s, output: 22.01 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:13<00:10, 20.88it/s, est. speed input: 22523.66 toks/s, output: 22.00 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:13<00:10, 20.88it/s, est. speed input: 22506.96 toks/s, output: 21.98 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:13<00:10, 20.87it/s, est. speed input: 22490.48 toks/s, output: 21.96 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:13<00:10, 20.87it/s, est. speed input: 22474.57 toks/s, output: 21.95 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:13<00:10, 20.86it/s, est. speed input: 22458.64 toks/s, output: 21.93 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:13<00:09, 20.86it/s, est. speed input: 22443.76 toks/s, output: 21.92 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:14<00:09, 20.85it/s, est. speed input: 22428.42 toks/s, output: 21.90 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:14<00:09, 20.85it/s, est. speed input: 22413.92 toks/s, output: 21.89 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:14<00:09, 20.86it/s, est. speed input: 22400.31 toks/s, output: 21.88 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:14<00:09, 20.87it/s, est. speed input: 22387.46 toks/s, output: 21.86 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:14<00:08, 20.87it/s, est. speed input: 22374.45 toks/s, output: 21.85 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:15<00:08, 20.87it/s, est. speed input: 22361.77 toks/s, output: 21.84 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:15<00:08, 20.85it/s, est. speed input: 22348.54 toks/s, output: 21.82 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:15<00:08, 20.85it/s, est. speed input: 22336.16 toks/s, output: 21.81 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:15<00:08, 20.85it/s, est. speed input: 22324.06 toks/s, output: 21.80 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:15<00:07, 20.86it/s, est. speed input: 22312.66 toks/s, output: 21.79 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:16<00:07, 20.87it/s, est. speed input: 22301.57 toks/s, output: 21.78 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:16<00:07, 20.86it/s, est. speed input: 22290.18 toks/s, output: 21.77 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:16<00:07, 20.85it/s, est. speed input: 22279.10 toks/s, output: 21.76 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:16<00:07, 20.86it/s, est. speed input: 22268.54 toks/s, output: 21.75 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:16<00:06, 20.86it/s, est. speed input: 22258.37 toks/s, output: 21.74 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:17<00:06, 20.86it/s, est. speed input: 22248.16 toks/s, output: 21.73 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:17<00:06, 20.84it/s, est. speed input: 22237.74 toks/s, output: 21.72 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:17<00:06, 20.85it/s, est. speed input: 22228.05 toks/s, output: 21.71 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:17<00:06, 20.86it/s, est. speed input: 22218.75 toks/s, output: 21.70 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:17<00:06, 20.85it/s, est. speed input: 22209.39 toks/s, output: 21.69 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:17<00:05, 20.86it/s, est. speed input: 22200.49 toks/s, output: 21.68 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:18<00:05, 20.86it/s, est. speed input: 22191.69 toks/s, output: 21.67 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:18<00:05, 20.85it/s, est. speed input: 22182.67 toks/s, output: 21.66 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:18<00:05, 20.85it/s, est. speed input: 22174.20 toks/s, output: 21.65 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:18<00:05, 20.86it/s, est. speed input: 22166.10 toks/s, output: 21.65 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:18<00:04, 20.86it/s, est. speed input: 22157.80 toks/s, output: 21.64 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:19<00:04, 20.85it/s, est. speed input: 22149.67 toks/s, output: 21.63 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:19<00:04, 20.85it/s, est. speed input: 22141.52 toks/s, output: 21.62 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:19<00:04, 20.85it/s, est. speed input: 22133.81 toks/s, output: 21.62 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:19<00:04, 20.85it/s, est. speed input: 22126.33 toks/s, output: 21.61 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:19<00:03, 20.85it/s, est. speed input: 22118.62 toks/s, output: 21.60 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:20<00:03, 20.84it/s, est. speed input: 22111.07 toks/s, output: 21.59 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:20<00:03, 20.83it/s, est. speed input: 22103.44 toks/s, output: 21.59 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:20<00:03, 20.84it/s, est. speed input: 22096.40 toks/s, output: 21.58 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:20<00:03, 20.85it/s, est. speed input: 22089.66 toks/s, output: 21.57 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:20<00:02, 20.85it/s, est. speed input: 22083.06 toks/s, output: 21.57 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:21<00:02, 20.86it/s, est. speed input: 22076.55 toks/s, output: 21.56 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:21<00:02, 20.83it/s, est. speed input: 22069.35 toks/s, output: 21.55 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:21<00:02, 20.84it/s, est. speed input: 22063.05 toks/s, output: 21.55 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:21<00:02, 20.86it/s, est. speed input: 22057.19 toks/s, output: 21.54 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:21<00:02, 20.86it/s, est. speed input: 22051.03 toks/s, output: 21.53 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:22<00:01, 20.86it/s, est. speed input: 22044.97 toks/s, output: 21.53 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:22<00:01, 20.84it/s, est. speed input: 22038.61 toks/s, output: 21.52 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:22<00:01, 20.85it/s, est. speed input: 22032.96 toks/s, output: 21.52 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:22<00:01, 20.85it/s, est. speed input: 22027.01 toks/s, output: 21.51 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:22<00:01, 20.84it/s, est. speed input: 22020.95 toks/s, output: 21.50 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:22<00:00, 20.82it/s, est. speed input: 22014.88 toks/s, output: 21.50 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:23<00:00, 20.82it/s, est. speed input: 22009.04 toks/s, output: 21.49 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:23<00:00, 20.82it/s, est. speed input: 22003.45 toks/s, output: 21.49 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:23<00:00, 20.84it/s, est. speed input: 21998.25 toks/s, output: 21.48 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:23<00:00, 22.43it/s, est. speed input: 22035.18 toks/s, output: 21.52 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:23<00:00, 22.43it/s, est. speed input: 22121.52 toks/s, output: 21.60 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:23<00:00, 21.60it/s, est. speed input: 22121.52 toks/s, output: 21.60 toks/s]
[rank0]:[W125 21:00:01.242901089 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-25 21:00:02
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:00:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=475979) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=475979) WARNING 01-25 21:00:20 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     kernel_warmup(self)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     runner._dummy_run(
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     def forward(
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     raise e
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/tmp/torchinductor_root/si/csiatjsx2i27zyfwf3d6jrlepws7ewu7omi6myhvh4c4sb5r7dj6.py", line 958, in call
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) ERROR 01-25 21:00:26 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 592.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 410.94 MiB is free. Including non-PyTorch memory, this process has 14.84 GiB memory in use. Of the allocated memory 11.77 GiB is allocated by PyTorch, and 2.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 21:00:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:00:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:00:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:00:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:00:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:00:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:00:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:00:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:00:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:00:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:00:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:00:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:00:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:00:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:00:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:00:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=475979) [2026-01-25 21:00:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=475979) [2026-01-25 21:00:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=475979) [2026-01-25 21:00:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=475979) [2026-01-25 21:00:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=475979) [2026-01-25 21:00:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=475979) [2026-01-25 21:00:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=475979) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=475979) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=475979) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.68it/s]
(EngineCore_DP0 pid=475979) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.75it/s]
(EngineCore_DP0 pid=475979) 
(EngineCore_DP0 pid=475979) [2026-01-25 21:00:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=475979) [2026-01-25 21:00:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=475979) [2026-01-25 21:00:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=475979) [2026-01-25 21:00:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=475979) [2026-01-25 21:00:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=475979) [2026-01-25 21:00:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=475979) [2026-01-25 21:00:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=475979) [2026-01-25 21:00:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=475979) 2026-01-25 21:00:26,344 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=475979) 2026-01-25 21:00:26,370 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=475979) Process EngineCore_DP0:
(EngineCore_DP0 pid=475979) Traceback (most recent call last):
(EngineCore_DP0 pid=475979)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=475979)     self.run()
(EngineCore_DP0 pid=475979)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=475979)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=475979)     raise e
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=475979)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=475979)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=475979)     super().__init__(
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=475979)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=475979)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=475979)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=475979)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=475979)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=475979)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=475979)     return func(*args, **kwargs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=475979)     kernel_warmup(self)
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=475979)     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=475979)     runner._dummy_run(
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=475979)     return func(*args, **kwargs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=475979)     outputs = self.model(
(EngineCore_DP0 pid=475979)               ^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=475979)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=475979)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=475979)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=475979)     hidden_states = self.model(
(EngineCore_DP0 pid=475979)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=475979)     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=475979)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=475979)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=475979)     def forward(
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=475979)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=475979)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=475979)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=475979)     raise e
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=475979)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=475979)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=475979)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "<eval_with_key>.58", line 332, in forward
(EngineCore_DP0 pid=475979)     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=475979)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=475979)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=475979)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=475979)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=475979)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=475979)     return compiled_fn(full_args)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=475979)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=475979)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=475979)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=475979)                             ^^^^^^^
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=475979)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=475979)     return self.current_callable(inputs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=475979)     out = model(new_inputs)
(EngineCore_DP0 pid=475979)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/tmp/torchinductor_root/si/csiatjsx2i27zyfwf3d6jrlepws7ewu7omi6myhvh4c4sb5r7dj6.py", line 958, in call
(EngineCore_DP0 pid=475979)     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
(EngineCore_DP0 pid=475979)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=475979)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=475979)     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=475979)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979)   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
(EngineCore_DP0 pid=475979)     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=475979)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=475979) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 592.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 410.94 MiB is free. Including non-PyTorch memory, this process has 14.84 GiB memory in use. Of the allocated memory 11.77 GiB is allocated by PyTorch, and 2.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 21:00:26.691079467 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=8192

========== M=16384 ==========
Time: 2026-01-25 21:00:28
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:00:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=476636) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=476636) WARNING 01-25 21:00:49 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     kernel_warmup(self)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     runner._dummy_run(
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     def forward(
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     raise e
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "<eval_with_key>.58", line 332, in forward
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/tmp/torchinductor_root/h3/ch3z4j5zv4cgls3cstbg4tginq7yjwyo6voq2a2nhcqcn3cyotn5.py", line 1078, in call
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) ERROR 01-25 21:00:55 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.46 GiB of which 454.94 MiB is free. Including non-PyTorch memory, this process has 14.79 GiB memory in use. Of the allocated memory 11.25 GiB is allocated by PyTorch, and 3.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 21:00:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:00:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:00:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:00:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:00:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:00:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:00:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:00:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:00:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:00:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:00:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:00:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:00:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:00:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:00:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:00:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:00:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=476636) [2026-01-25 21:00:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=476636) [2026-01-25 21:00:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=476636) [2026-01-25 21:00:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=476636) [2026-01-25 21:00:44] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=476636) [2026-01-25 21:00:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=476636) [2026-01-25 21:00:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=476636) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=476636) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.30it/s]
(EngineCore_DP0 pid=476636) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.73it/s]
(EngineCore_DP0 pid=476636) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.79it/s]
(EngineCore_DP0 pid=476636) 
(EngineCore_DP0 pid=476636) [2026-01-25 21:00:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=476636) [2026-01-25 21:00:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=476636) [2026-01-25 21:00:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=476636) [2026-01-25 21:00:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=476636) [2026-01-25 21:00:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=476636) [2026-01-25 21:00:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=476636) [2026-01-25 21:00:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=476636) [2026-01-25 21:00:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=476636) [rank0]:W0125 21:00:52.532000 476636 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=476636) [rank0]:W0125 21:00:52.581000 476636 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=476636) [rank0]:W0125 21:00:53.151000 476636 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=476636) [rank0]:W0125 21:00:53.228000 476636 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=476636) 2026-01-25 21:00:55,501 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=476636) 2026-01-25 21:00:55,546 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=476636) Process EngineCore_DP0:
(EngineCore_DP0 pid=476636) Traceback (most recent call last):
(EngineCore_DP0 pid=476636)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=476636)     self.run()
(EngineCore_DP0 pid=476636)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=476636)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=476636)     raise e
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=476636)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=476636)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=476636)     super().__init__(
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=476636)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=476636)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=476636)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=476636)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=476636)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=476636)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=476636)     return func(*args, **kwargs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=476636)     kernel_warmup(self)
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=476636)     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=476636)     runner._dummy_run(
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=476636)     return func(*args, **kwargs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=476636)     outputs = self.model(
(EngineCore_DP0 pid=476636)               ^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=476636)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=476636)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=476636)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=476636)     hidden_states = self.model(
(EngineCore_DP0 pid=476636)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=476636)     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=476636)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=476636)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=476636)     def forward(
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=476636)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=476636)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=476636)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=476636)     raise e
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=476636)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=476636)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=476636)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "<eval_with_key>.58", line 332, in forward
(EngineCore_DP0 pid=476636)     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=476636)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=476636)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=476636)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=476636)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=476636)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=476636)     return compiled_fn(full_args)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=476636)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=476636)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=476636)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=476636)                             ^^^^^^^
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=476636)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=476636)     return self.current_callable(inputs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=476636)     out = model(new_inputs)
(EngineCore_DP0 pid=476636)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/tmp/torchinductor_root/h3/ch3z4j5zv4cgls3cstbg4tginq7yjwyo6voq2a2nhcqcn3cyotn5.py", line 1078, in call
(EngineCore_DP0 pid=476636)     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
(EngineCore_DP0 pid=476636)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=476636)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=476636)     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=476636)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636)   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
(EngineCore_DP0 pid=476636)     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=476636)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=476636) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.46 GiB of which 454.94 MiB is free. Including non-PyTorch memory, this process has 14.79 GiB memory in use. Of the allocated memory 11.25 GiB is allocated by PyTorch, and 3.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 21:00:56.864581286 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16384

========== M=32768 ==========
Time: 2026-01-25 21:00:57
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:01:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=477390) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=477390) WARNING 01-25 21:01:25 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     compiled_module = graph.compile_to_module()
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     return self._compile_to_module()
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]                                                              ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     return self._generate(is_inference)
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     self.generate_and_run_autotune_block()
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
(EngineCore_DP0 pid=477390) ERROR 01-25 21:01:29 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.46 GiB of which 586.94 MiB is free. Including non-PyTorch memory, this process has 14.67 GiB memory in use. Of the allocated memory 11.63 GiB is allocated by PyTorch, and 2.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 21:01:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:01:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:01:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 21:01:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:01:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:01:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:01:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:01:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:01:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:01:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:01:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:01:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:01:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:01:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:01:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:01:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:01:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 21:01:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:01:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:01:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:01:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:01:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:01:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:01:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:01:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:01:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:01:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:01:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=477390) [2026-01-25 21:01:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=477390) [2026-01-25 21:01:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=477390) [2026-01-25 21:01:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=477390) [2026-01-25 21:01:20] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=477390) [2026-01-25 21:01:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=477390) [2026-01-25 21:01:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=477390) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=477390) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.31it/s]
(EngineCore_DP0 pid=477390) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.73it/s]
(EngineCore_DP0 pid=477390) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.79it/s]
(EngineCore_DP0 pid=477390) 
(EngineCore_DP0 pid=477390) [2026-01-25 21:01:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=477390) [2026-01-25 21:01:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=477390) [2026-01-25 21:01:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=477390) [2026-01-25 21:01:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=477390) [2026-01-25 21:01:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=477390) [2026-01-25 21:01:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=477390) [2026-01-25 21:01:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=477390) [2026-01-25 21:01:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=477390) [rank0]:W0125 21:01:28.575000 477390 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=477390) [rank0]:W0125 21:01:28.625000 477390 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=477390) [rank0]:W0125 21:01:29.174000 477390 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=477390) [rank0]:W0125 21:01:29.250000 477390 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=477390) Process EngineCore_DP0:
(EngineCore_DP0 pid=477390) Traceback (most recent call last):
(EngineCore_DP0 pid=477390)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=477390)     self.run()
(EngineCore_DP0 pid=477390)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=477390)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=477390)     raise e
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=477390)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=477390)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=477390)     super().__init__(
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=477390)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=477390)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=477390)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=477390)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=477390)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=477390)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=477390)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=477390)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=477390)     return func(*args, **kwargs)
(EngineCore_DP0 pid=477390)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=477390)     return func(*args, **kwargs)
(EngineCore_DP0 pid=477390)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=477390)     self.model_runner.profile_run()
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=477390)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=477390)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=477390)     return func(*args, **kwargs)
(EngineCore_DP0 pid=477390)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=477390)     outputs = self.model(
(EngineCore_DP0 pid=477390)               ^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=477390)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=477390)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=477390)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=477390)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=477390)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=477390)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=477390)     hidden_states = self.model(
(EngineCore_DP0 pid=477390)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=477390)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=477390)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=477390)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=477390)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=477390)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=477390)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
(EngineCore_DP0 pid=477390)     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
(EngineCore_DP0 pid=477390)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
(EngineCore_DP0 pid=477390)     raise InductorError(e, currentframe()).with_traceback(
(EngineCore_DP0 pid=477390)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
(EngineCore_DP0 pid=477390)     mb_compiled_graph = fx_codegen_and_compile(
(EngineCore_DP0 pid=477390)                         ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
(EngineCore_DP0 pid=477390)     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
(EngineCore_DP0 pid=477390)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
(EngineCore_DP0 pid=477390)     compiled_module = graph.compile_to_module()
(EngineCore_DP0 pid=477390)                       ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
(EngineCore_DP0 pid=477390)     return self._compile_to_module()
(EngineCore_DP0 pid=477390)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
(EngineCore_DP0 pid=477390)     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
(EngineCore_DP0 pid=477390)                                                              ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
(EngineCore_DP0 pid=477390)     result = self.wrapper_code.generate(self.is_inference)
(EngineCore_DP0 pid=477390)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
(EngineCore_DP0 pid=477390)     return self._generate(is_inference)
(EngineCore_DP0 pid=477390)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=477390)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
(EngineCore_DP0 pid=477390)     self.generate_and_run_autotune_block()
(EngineCore_DP0 pid=477390)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
(EngineCore_DP0 pid=477390)     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
(EngineCore_DP0 pid=477390) torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 15.46 GiB of which 586.94 MiB is free. Including non-PyTorch memory, this process has 14.67 GiB memory in use. Of the allocated memory 11.63 GiB is allocated by PyTorch, and 2.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 21:01:29.583911649 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=32768

========== M=65536 ==========
Time: 2026-01-25 21:01:31
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:02:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=478312) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=478312) WARNING 01-25 21:02:13 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     compiled_module = graph.compile_to_module()
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     return self._compile_to_module()
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]                                                              ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     return self._generate(is_inference)
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     self.generate_and_run_autotune_block()
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
(EngineCore_DP0 pid=478312) ERROR 01-25 21:02:17 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.46 GiB of which 2.23 GiB is free. Including non-PyTorch memory, this process has 13.01 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 21:02:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:02:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:02:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 21:02:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:02:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:02:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:02:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:02:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:02:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:02:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:02:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:02:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:02:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:02:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:02:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:02:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:02:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 21:02:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:02:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:02:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:02:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:02:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 21:02:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 21:02:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:02:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:02:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:02:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:02:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=478312) [2026-01-25 21:02:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=478312) [2026-01-25 21:02:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=478312) [2026-01-25 21:02:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=478312) [2026-01-25 21:02:08] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=478312) [2026-01-25 21:02:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=478312) [2026-01-25 21:02:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=478312) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=478312) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.30it/s]
(EngineCore_DP0 pid=478312) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.72it/s]
(EngineCore_DP0 pid=478312) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.79it/s]
(EngineCore_DP0 pid=478312) 
(EngineCore_DP0 pid=478312) [2026-01-25 21:02:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=478312) [2026-01-25 21:02:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=478312) [2026-01-25 21:02:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=478312) [2026-01-25 21:02:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=478312) [2026-01-25 21:02:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=478312) [2026-01-25 21:02:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=478312) [2026-01-25 21:02:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=478312) [2026-01-25 21:02:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=478312) [rank0]:W0125 21:02:16.666000 478312 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=478312) [rank0]:W0125 21:02:17.394000 478312 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=478312) Process EngineCore_DP0:
(EngineCore_DP0 pid=478312) Traceback (most recent call last):
(EngineCore_DP0 pid=478312)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=478312)     self.run()
(EngineCore_DP0 pid=478312)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=478312)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=478312)     raise e
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=478312)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=478312)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=478312)     super().__init__(
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=478312)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=478312)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=478312)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=478312)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=478312)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=478312)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=478312)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=478312)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=478312)     return func(*args, **kwargs)
(EngineCore_DP0 pid=478312)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=478312)     return func(*args, **kwargs)
(EngineCore_DP0 pid=478312)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=478312)     self.model_runner.profile_run()
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=478312)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=478312)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=478312)     return func(*args, **kwargs)
(EngineCore_DP0 pid=478312)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=478312)     outputs = self.model(
(EngineCore_DP0 pid=478312)               ^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=478312)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=478312)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=478312)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=478312)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=478312)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=478312)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=478312)     hidden_states = self.model(
(EngineCore_DP0 pid=478312)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=478312)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=478312)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=478312)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=478312)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=478312)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=478312)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
(EngineCore_DP0 pid=478312)     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
(EngineCore_DP0 pid=478312)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
(EngineCore_DP0 pid=478312)     raise InductorError(e, currentframe()).with_traceback(
(EngineCore_DP0 pid=478312)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
(EngineCore_DP0 pid=478312)     mb_compiled_graph = fx_codegen_and_compile(
(EngineCore_DP0 pid=478312)                         ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
(EngineCore_DP0 pid=478312)     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
(EngineCore_DP0 pid=478312)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
(EngineCore_DP0 pid=478312)     compiled_module = graph.compile_to_module()
(EngineCore_DP0 pid=478312)                       ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
(EngineCore_DP0 pid=478312)     return self._compile_to_module()
(EngineCore_DP0 pid=478312)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
(EngineCore_DP0 pid=478312)     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
(EngineCore_DP0 pid=478312)                                                              ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
(EngineCore_DP0 pid=478312)     result = self.wrapper_code.generate(self.is_inference)
(EngineCore_DP0 pid=478312)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
(EngineCore_DP0 pid=478312)     return self._generate(is_inference)
(EngineCore_DP0 pid=478312)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=478312)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
(EngineCore_DP0 pid=478312)     self.generate_and_run_autotune_block()
(EngineCore_DP0 pid=478312)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
(EngineCore_DP0 pid=478312)     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
(EngineCore_DP0 pid=478312) torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.46 GiB of which 2.23 GiB is free. Including non-PyTorch memory, this process has 13.01 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 21:02:18.810688447 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-25 22:02:10
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:02:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [gpu_model_runner.py:3657] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 184.94 MiB is free. Including non-PyTorch memory, this process has 15.06 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 109.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     raise e
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     model = initialize_model(
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     self.model = Qwen2Model(
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]                  ^^^^^^^^^^^
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     old_init(self, **kwargs)
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]                ^^^^^^^^^
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 85, in __init__
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     self.gate_up_proj = MergedColumnParallelLinear(
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 631, in __init__
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 484, in __init__
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     self.quant_method.create_weights(
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     layer.scheme.create_weights(
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     data=torch.empty(
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]          ^^^^^^^^^^^^
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830) ERROR 01-25 22:02:19 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 184.94 MiB is free. Including non-PyTorch memory, this process has 15.06 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 109.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 22:02:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:02:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:02:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:02:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:02:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:02:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:02:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:02:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:02:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:02:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:02:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:02:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=548830) [2026-01-25 22:02:19] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=548830) [2026-01-25 22:02:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=548830) [2026-01-25 22:02:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=548830) [2026-01-25 22:02:19] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=548830) [2026-01-25 22:02:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=548830) [2026-01-25 22:02:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=548830) Process EngineCore_DP0:
(EngineCore_DP0 pid=548830) Traceback (most recent call last):
(EngineCore_DP0 pid=548830)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=548830)     self.run()
(EngineCore_DP0 pid=548830)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=548830)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=548830)     raise e
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=548830)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=548830)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=548830)     super().__init__(
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=548830)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=548830)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=548830)     self._init_executor()
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=548830)     self.driver_worker.load_model()
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=548830)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=548830)     raise e
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=548830)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=548830)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=548830)     model = initialize_model(
(EngineCore_DP0 pid=548830)             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=548830)     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=548830)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=548830)     self.model = Qwen2Model(
(EngineCore_DP0 pid=548830)                  ^^^^^^^^^^^
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=548830)     old_init(self, **kwargs)
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=548830)     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=548830)                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=548830)     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=548830)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=548830)     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=548830)                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=548830)     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=548830)                ^^^^^^^^^
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 85, in __init__
(EngineCore_DP0 pid=548830)     self.gate_up_proj = MergedColumnParallelLinear(
(EngineCore_DP0 pid=548830)                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 631, in __init__
(EngineCore_DP0 pid=548830)     super().__init__(
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 484, in __init__
(EngineCore_DP0 pid=548830)     self.quant_method.create_weights(
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=548830)     layer.scheme.create_weights(
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=548830)     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=548830)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=548830)     data=torch.empty(
(EngineCore_DP0 pid=548830)          ^^^^^^^^^^^^
(EngineCore_DP0 pid=548830)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=548830)     return func(*args, **kwargs)
(EngineCore_DP0 pid=548830)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=548830) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 184.94 MiB is free. Including non-PyTorch memory, this process has 15.06 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 109.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 22:02:19.676120780 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=512

========== M=1024 ==========
Time: 2026-01-25 22:02:21
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:02:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [gpu_model_runner.py:3657] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 172.94 MiB is free. Including non-PyTorch memory, this process has 15.07 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 116.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     raise e
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     model = initialize_model(
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     self.model = Qwen2Model(
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]                  ^^^^^^^^^^^
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     old_init(self, **kwargs)
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]                ^^^^^^^^^
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 85, in __init__
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     self.gate_up_proj = MergedColumnParallelLinear(
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 631, in __init__
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 484, in __init__
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     self.quant_method.create_weights(
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     layer.scheme.create_weights(
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     data=torch.empty(
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]          ^^^^^^^^^^^^
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220) ERROR 01-25 22:02:30 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 172.94 MiB is free. Including non-PyTorch memory, this process has 15.07 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 116.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 22:02:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:02:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:02:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:02:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:02:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:02:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:02:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:02:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:02:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:02:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:02:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:02:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=549220) [2026-01-25 22:02:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=549220) [2026-01-25 22:02:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=549220) [2026-01-25 22:02:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=549220) [2026-01-25 22:02:30] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=549220) [2026-01-25 22:02:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=549220) [2026-01-25 22:02:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=549220) Process EngineCore_DP0:
(EngineCore_DP0 pid=549220) Traceback (most recent call last):
(EngineCore_DP0 pid=549220)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=549220)     self.run()
(EngineCore_DP0 pid=549220)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=549220)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=549220)     raise e
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=549220)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=549220)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=549220)     super().__init__(
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=549220)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=549220)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=549220)     self._init_executor()
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=549220)     self.driver_worker.load_model()
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=549220)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=549220)     raise e
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=549220)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=549220)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=549220)     model = initialize_model(
(EngineCore_DP0 pid=549220)             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=549220)     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=549220)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=549220)     self.model = Qwen2Model(
(EngineCore_DP0 pid=549220)                  ^^^^^^^^^^^
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=549220)     old_init(self, **kwargs)
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=549220)     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=549220)                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=549220)     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=549220)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=549220)     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=549220)                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=549220)     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=549220)                ^^^^^^^^^
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 85, in __init__
(EngineCore_DP0 pid=549220)     self.gate_up_proj = MergedColumnParallelLinear(
(EngineCore_DP0 pid=549220)                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 631, in __init__
(EngineCore_DP0 pid=549220)     super().__init__(
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 484, in __init__
(EngineCore_DP0 pid=549220)     self.quant_method.create_weights(
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=549220)     layer.scheme.create_weights(
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=549220)     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=549220)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=549220)     data=torch.empty(
(EngineCore_DP0 pid=549220)          ^^^^^^^^^^^^
(EngineCore_DP0 pid=549220)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=549220)     return func(*args, **kwargs)
(EngineCore_DP0 pid=549220)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549220) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 172.94 MiB is free. Including non-PyTorch memory, this process has 15.07 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 116.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 22:02:31.973642369 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=1024

========== M=2048 ==========
Time: 2026-01-25 22:02:32
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:02:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [gpu_model_runner.py:3657] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 164.94 MiB is free. Including non-PyTorch memory, this process has 15.08 GiB memory in use. Of the allocated memory 14.60 GiB is allocated by PyTorch, and 114.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     raise e
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     model = initialize_model(
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     self.model = Qwen2Model(
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]                  ^^^^^^^^^^^
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     old_init(self, **kwargs)
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]                ^^^^^^^^^
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 85, in __init__
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     self.gate_up_proj = MergedColumnParallelLinear(
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 631, in __init__
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 484, in __init__
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     self.quant_method.create_weights(
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     layer.scheme.create_weights(
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     data=torch.empty(
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]          ^^^^^^^^^^^^
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634) ERROR 01-25 22:02:42 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 164.94 MiB is free. Including non-PyTorch memory, this process has 15.08 GiB memory in use. Of the allocated memory 14.60 GiB is allocated by PyTorch, and 114.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 22:02:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:02:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:02:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:02:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:02:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:02:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:02:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:02:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:02:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:02:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:02:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:02:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=549634) [2026-01-25 22:02:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=549634) [2026-01-25 22:02:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=549634) [2026-01-25 22:02:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=549634) [2026-01-25 22:02:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=549634) [2026-01-25 22:02:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=549634) [2026-01-25 22:02:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=549634) Process EngineCore_DP0:
(EngineCore_DP0 pid=549634) Traceback (most recent call last):
(EngineCore_DP0 pid=549634)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=549634)     self.run()
(EngineCore_DP0 pid=549634)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=549634)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=549634)     raise e
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=549634)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=549634)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=549634)     super().__init__(
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=549634)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=549634)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=549634)     self._init_executor()
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=549634)     self.driver_worker.load_model()
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=549634)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=549634)     raise e
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=549634)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=549634)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=549634)     model = initialize_model(
(EngineCore_DP0 pid=549634)             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=549634)     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=549634)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=549634)     self.model = Qwen2Model(
(EngineCore_DP0 pid=549634)                  ^^^^^^^^^^^
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=549634)     old_init(self, **kwargs)
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=549634)     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=549634)                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=549634)     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=549634)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=549634)     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=549634)                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=549634)     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=549634)                ^^^^^^^^^
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 85, in __init__
(EngineCore_DP0 pid=549634)     self.gate_up_proj = MergedColumnParallelLinear(
(EngineCore_DP0 pid=549634)                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 631, in __init__
(EngineCore_DP0 pid=549634)     super().__init__(
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 484, in __init__
(EngineCore_DP0 pid=549634)     self.quant_method.create_weights(
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=549634)     layer.scheme.create_weights(
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=549634)     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=549634)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=549634)     data=torch.empty(
(EngineCore_DP0 pid=549634)          ^^^^^^^^^^^^
(EngineCore_DP0 pid=549634)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=549634)     return func(*args, **kwargs)
(EngineCore_DP0 pid=549634)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=549634) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 164.94 MiB is free. Including non-PyTorch memory, this process has 15.08 GiB memory in use. Of the allocated memory 14.60 GiB is allocated by PyTorch, and 114.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 22:02:42.574792264 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=2048

========== M=4096 ==========
Time: 2026-01-25 22:02:44
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:02:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [gpu_model_runner.py:3657] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 144.94 MiB is free. Including non-PyTorch memory, this process has 15.10 GiB memory in use. Of the allocated memory 14.62 GiB is allocated by PyTorch, and 114.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     raise e
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     model = initialize_model(
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     self.model = Qwen2Model(
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]                  ^^^^^^^^^^^
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     old_init(self, **kwargs)
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]                ^^^^^^^^^
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 85, in __init__
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     self.gate_up_proj = MergedColumnParallelLinear(
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 631, in __init__
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 484, in __init__
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     self.quant_method.create_weights(
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     layer.scheme.create_weights(
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     data=torch.empty(
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]          ^^^^^^^^^^^^
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046) ERROR 01-25 22:02:54 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 144.94 MiB is free. Including non-PyTorch memory, this process has 15.10 GiB memory in use. Of the allocated memory 14.62 GiB is allocated by PyTorch, and 114.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 22:02:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:02:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:02:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:02:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:02:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:02:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:02:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:02:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:02:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:02:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:02:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:02:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:02:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:02:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=550046) [2026-01-25 22:02:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=550046) [2026-01-25 22:02:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=550046) [2026-01-25 22:02:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=550046) [2026-01-25 22:02:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=550046) [2026-01-25 22:02:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=550046) [2026-01-25 22:02:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=550046) Process EngineCore_DP0:
(EngineCore_DP0 pid=550046) Traceback (most recent call last):
(EngineCore_DP0 pid=550046)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=550046)     self.run()
(EngineCore_DP0 pid=550046)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=550046)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=550046)     raise e
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=550046)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=550046)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=550046)     super().__init__(
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=550046)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=550046)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=550046)     self._init_executor()
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=550046)     self.driver_worker.load_model()
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=550046)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=550046)     raise e
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=550046)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=550046)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=550046)     model = initialize_model(
(EngineCore_DP0 pid=550046)             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=550046)     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=550046)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=550046)     self.model = Qwen2Model(
(EngineCore_DP0 pid=550046)                  ^^^^^^^^^^^
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=550046)     old_init(self, **kwargs)
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=550046)     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=550046)                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=550046)     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=550046)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=550046)     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=550046)                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=550046)     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=550046)                ^^^^^^^^^
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 85, in __init__
(EngineCore_DP0 pid=550046)     self.gate_up_proj = MergedColumnParallelLinear(
(EngineCore_DP0 pid=550046)                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 631, in __init__
(EngineCore_DP0 pid=550046)     super().__init__(
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 484, in __init__
(EngineCore_DP0 pid=550046)     self.quant_method.create_weights(
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=550046)     layer.scheme.create_weights(
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=550046)     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=550046)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=550046)     data=torch.empty(
(EngineCore_DP0 pid=550046)          ^^^^^^^^^^^^
(EngineCore_DP0 pid=550046)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=550046)     return func(*args, **kwargs)
(EngineCore_DP0 pid=550046)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550046) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 144.94 MiB is free. Including non-PyTorch memory, this process has 15.10 GiB memory in use. Of the allocated memory 14.62 GiB is allocated by PyTorch, and 114.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 22:02:55.913462164 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=4096

========== M=8192 ==========
Time: 2026-01-25 22:02:56
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:03:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [gpu_model_runner.py:3657] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 104.94 MiB is free. Including non-PyTorch memory, this process has 15.14 GiB memory in use. Of the allocated memory 14.66 GiB is allocated by PyTorch, and 114.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     raise e
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     model = initialize_model(
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     self.model = Qwen2Model(
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]                  ^^^^^^^^^^^
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     old_init(self, **kwargs)
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]                ^^^^^^^^^
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 85, in __init__
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     self.gate_up_proj = MergedColumnParallelLinear(
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 631, in __init__
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 484, in __init__
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     self.quant_method.create_weights(
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     layer.scheme.create_weights(
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     data=torch.empty(
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]          ^^^^^^^^^^^^
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487) ERROR 01-25 22:03:08 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 104.94 MiB is free. Including non-PyTorch memory, this process has 15.14 GiB memory in use. Of the allocated memory 14.66 GiB is allocated by PyTorch, and 114.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 22:03:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:03:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:03:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:03:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:03:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:03:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:03:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:03:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:03:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:03:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:03:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:03:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:03:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:03:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:03:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:03:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=550487) [2026-01-25 22:03:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=550487) [2026-01-25 22:03:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=550487) [2026-01-25 22:03:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=550487) [2026-01-25 22:03:08] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=550487) [2026-01-25 22:03:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=550487) [2026-01-25 22:03:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=550487) Process EngineCore_DP0:
(EngineCore_DP0 pid=550487) Traceback (most recent call last):
(EngineCore_DP0 pid=550487)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=550487)     self.run()
(EngineCore_DP0 pid=550487)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=550487)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=550487)     raise e
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=550487)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=550487)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=550487)     super().__init__(
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=550487)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=550487)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=550487)     self._init_executor()
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=550487)     self.driver_worker.load_model()
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=550487)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=550487)     raise e
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=550487)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=550487)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=550487)     model = initialize_model(
(EngineCore_DP0 pid=550487)             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=550487)     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=550487)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=550487)     self.model = Qwen2Model(
(EngineCore_DP0 pid=550487)                  ^^^^^^^^^^^
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=550487)     old_init(self, **kwargs)
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=550487)     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=550487)                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=550487)     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=550487)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=550487)     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=550487)                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=550487)     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=550487)                ^^^^^^^^^
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 85, in __init__
(EngineCore_DP0 pid=550487)     self.gate_up_proj = MergedColumnParallelLinear(
(EngineCore_DP0 pid=550487)                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 631, in __init__
(EngineCore_DP0 pid=550487)     super().__init__(
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 484, in __init__
(EngineCore_DP0 pid=550487)     self.quant_method.create_weights(
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=550487)     layer.scheme.create_weights(
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=550487)     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=550487)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=550487)     data=torch.empty(
(EngineCore_DP0 pid=550487)          ^^^^^^^^^^^^
(EngineCore_DP0 pid=550487)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=550487)     return func(*args, **kwargs)
(EngineCore_DP0 pid=550487)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550487) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 104.94 MiB is free. Including non-PyTorch memory, this process has 15.14 GiB memory in use. Of the allocated memory 14.66 GiB is allocated by PyTorch, and 114.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 22:03:09.922989522 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=8192

========== M=16384 ==========
Time: 2026-01-25 22:03:10
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:03:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [gpu_model_runner.py:3657] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 24.94 MiB is free. Including non-PyTorch memory, this process has 15.22 GiB memory in use. Of the allocated memory 14.74 GiB is allocated by PyTorch, and 114.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     raise e
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     model = initialize_model(
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     self.model = Qwen2Model(
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]                  ^^^^^^^^^^^
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     old_init(self, **kwargs)
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]                ^^^^^^^^^
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 85, in __init__
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     self.gate_up_proj = MergedColumnParallelLinear(
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 631, in __init__
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 484, in __init__
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     self.quant_method.create_weights(
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     layer.scheme.create_weights(
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     data=torch.empty(
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]          ^^^^^^^^^^^^
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987) ERROR 01-25 22:03:26 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 24.94 MiB is free. Including non-PyTorch memory, this process has 15.22 GiB memory in use. Of the allocated memory 14.74 GiB is allocated by PyTorch, and 114.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 22:03:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:03:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:03:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:03:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:03:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:03:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:03:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:03:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:03:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:03:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:03:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:03:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:03:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:03:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:03:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:03:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=550987) [2026-01-25 22:03:26] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=550987) [2026-01-25 22:03:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=550987) [2026-01-25 22:03:26] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=550987) [2026-01-25 22:03:26] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=550987) [2026-01-25 22:03:26] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=550987) [2026-01-25 22:03:26] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=550987) Process EngineCore_DP0:
(EngineCore_DP0 pid=550987) Traceback (most recent call last):
(EngineCore_DP0 pid=550987)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=550987)     self.run()
(EngineCore_DP0 pid=550987)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=550987)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=550987)     raise e
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=550987)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=550987)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=550987)     super().__init__(
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=550987)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=550987)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=550987)     self._init_executor()
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=550987)     self.driver_worker.load_model()
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=550987)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=550987)     raise e
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=550987)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=550987)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=550987)     model = initialize_model(
(EngineCore_DP0 pid=550987)             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=550987)     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=550987)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=550987)     self.model = Qwen2Model(
(EngineCore_DP0 pid=550987)                  ^^^^^^^^^^^
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=550987)     old_init(self, **kwargs)
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=550987)     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=550987)                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=550987)     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=550987)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=550987)     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=550987)                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=550987)     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=550987)                ^^^^^^^^^
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 85, in __init__
(EngineCore_DP0 pid=550987)     self.gate_up_proj = MergedColumnParallelLinear(
(EngineCore_DP0 pid=550987)                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 631, in __init__
(EngineCore_DP0 pid=550987)     super().__init__(
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 484, in __init__
(EngineCore_DP0 pid=550987)     self.quant_method.create_weights(
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=550987)     layer.scheme.create_weights(
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=550987)     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=550987)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=550987)     data=torch.empty(
(EngineCore_DP0 pid=550987)          ^^^^^^^^^^^^
(EngineCore_DP0 pid=550987)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=550987)     return func(*args, **kwargs)
(EngineCore_DP0 pid=550987)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=550987) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 24.94 MiB is free. Including non-PyTorch memory, this process has 15.22 GiB memory in use. Of the allocated memory 14.74 GiB is allocated by PyTorch, and 114.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 22:03:26.759215901 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16384

========== M=32768 ==========
Time: 2026-01-25 22:03:28
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:03:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [gpu_model_runner.py:3657] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 36.94 MiB is free. Including non-PyTorch memory, this process has 15.21 GiB memory in use. Of the allocated memory 14.73 GiB is allocated by PyTorch, and 113.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     raise e
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     model = initialize_model(
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     self.model = Qwen2Model(
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]                  ^^^^^^^^^^^
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     old_init(self, **kwargs)
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]                ^^^^^^^^^
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 92, in __init__
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     self.down_proj = RowParallelLinear(
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]                      ^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 1312, in __init__
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     self.quant_method.create_weights(
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     layer.scheme.create_weights(
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     data=torch.empty(
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]          ^^^^^^^^^^^^
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579) ERROR 01-25 22:03:51 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 36.94 MiB is free. Including non-PyTorch memory, this process has 15.21 GiB memory in use. Of the allocated memory 14.73 GiB is allocated by PyTorch, and 113.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 22:03:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:03:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:03:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:03:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:03:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:03:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:03:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:03:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:03:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:03:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:03:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:03:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:03:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:03:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:03:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:03:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:03:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=551579) [2026-01-25 22:03:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=551579) [2026-01-25 22:03:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=551579) [2026-01-25 22:03:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=551579) [2026-01-25 22:03:51] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=551579) [2026-01-25 22:03:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=551579) [2026-01-25 22:03:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=551579) Process EngineCore_DP0:
(EngineCore_DP0 pid=551579) Traceback (most recent call last):
(EngineCore_DP0 pid=551579)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=551579)     self.run()
(EngineCore_DP0 pid=551579)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=551579)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=551579)     raise e
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=551579)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=551579)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=551579)     super().__init__(
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=551579)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=551579)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=551579)     self._init_executor()
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=551579)     self.driver_worker.load_model()
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=551579)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=551579)     raise e
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=551579)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=551579)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=551579)     model = initialize_model(
(EngineCore_DP0 pid=551579)             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=551579)     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=551579)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=551579)     self.model = Qwen2Model(
(EngineCore_DP0 pid=551579)                  ^^^^^^^^^^^
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=551579)     old_init(self, **kwargs)
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=551579)     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=551579)                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=551579)     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=551579)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=551579)     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=551579)                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=551579)     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=551579)                ^^^^^^^^^
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 92, in __init__
(EngineCore_DP0 pid=551579)     self.down_proj = RowParallelLinear(
(EngineCore_DP0 pid=551579)                      ^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 1312, in __init__
(EngineCore_DP0 pid=551579)     self.quant_method.create_weights(
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=551579)     layer.scheme.create_weights(
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=551579)     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=551579)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=551579)     data=torch.empty(
(EngineCore_DP0 pid=551579)          ^^^^^^^^^^^^
(EngineCore_DP0 pid=551579)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=551579)     return func(*args, **kwargs)
(EngineCore_DP0 pid=551579)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=551579) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 36.94 MiB is free. Including non-PyTorch memory, this process has 15.21 GiB memory in use. Of the allocated memory 14.73 GiB is allocated by PyTorch, and 113.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 22:03:52.883580099 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=32768

========== M=65536 ==========
Time: 2026-01-25 22:03:53
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:04:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [gpu_model_runner.py:3657] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 68.94 MiB is free. Including non-PyTorch memory, this process has 15.17 GiB memory in use. Of the allocated memory 14.70 GiB is allocated by PyTorch, and 110.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     raise e
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     model = initialize_model(
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     self.model = Qwen2Model(
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]                  ^^^^^^^^^^^
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     old_init(self, **kwargs)
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]                ^^^^^^^^^
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 92, in __init__
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     self.down_proj = RowParallelLinear(
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]                      ^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 1312, in __init__
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     self.quant_method.create_weights(
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     layer.scheme.create_weights(
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     data=torch.empty(
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]          ^^^^^^^^^^^^
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358) ERROR 01-25 22:04:30 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 68.94 MiB is free. Including non-PyTorch memory, this process has 15.17 GiB memory in use. Of the allocated memory 14.70 GiB is allocated by PyTorch, and 110.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 22:04:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:04:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:04:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:04:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:04:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:04:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:04:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:04:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:04:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:04:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:04:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:04:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:04:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:04:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:04:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:04:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:04:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-25 22:04:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:04:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:04:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:04:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:04:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-25 22:04:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-25 22:04:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:04:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:04:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:04:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:04:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=552358) [2026-01-25 22:04:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=552358) [2026-01-25 22:04:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=552358) [2026-01-25 22:04:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=552358) [2026-01-25 22:04:30] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=552358) [2026-01-25 22:04:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=552358) [2026-01-25 22:04:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=552358) Process EngineCore_DP0:
(EngineCore_DP0 pid=552358) Traceback (most recent call last):
(EngineCore_DP0 pid=552358)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=552358)     self.run()
(EngineCore_DP0 pid=552358)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=552358)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=552358)     raise e
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=552358)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=552358)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=552358)     super().__init__(
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=552358)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=552358)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=552358)     self._init_executor()
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=552358)     self.driver_worker.load_model()
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=552358)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=552358)     raise e
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=552358)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=552358)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=552358)     model = initialize_model(
(EngineCore_DP0 pid=552358)             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=552358)     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=552358)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 543, in __init__
(EngineCore_DP0 pid=552358)     self.model = Qwen2Model(
(EngineCore_DP0 pid=552358)                  ^^^^^^^^^^^
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/compilation/decorators.py", line 291, in __init__
(EngineCore_DP0 pid=552358)     old_init(self, **kwargs)
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 394, in __init__
(EngineCore_DP0 pid=552358)     self.start_layer, self.end_layer, self.layers = make_layers(
(EngineCore_DP0 pid=552358)                                                     ^^^^^^^^^^^^
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/model_executor/models/utils.py", line 606, in make_layers
(EngineCore_DP0 pid=552358)     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
(EngineCore_DP0 pid=552358)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 396, in <lambda>
(EngineCore_DP0 pid=552358)     lambda prefix: decoder_layer_type(
(EngineCore_DP0 pid=552358)                    ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 272, in __init__
(EngineCore_DP0 pid=552358)     self.mlp = Qwen2MLP(
(EngineCore_DP0 pid=552358)                ^^^^^^^^^
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 92, in __init__
(EngineCore_DP0 pid=552358)     self.down_proj = RowParallelLinear(
(EngineCore_DP0 pid=552358)                      ^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 1312, in __init__
(EngineCore_DP0 pid=552358)     self.quant_method.create_weights(
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 932, in create_weights
(EngineCore_DP0 pid=552358)     layer.scheme.create_weights(
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 655, in create_weights
(EngineCore_DP0 pid=552358)     return self.original_scheme.create_weights(
(EngineCore_DP0 pid=552358)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py", line 67, in create_weights
(EngineCore_DP0 pid=552358)     data=torch.empty(
(EngineCore_DP0 pid=552358)          ^^^^^^^^^^^^
(EngineCore_DP0 pid=552358)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=552358)     return func(*args, **kwargs)
(EngineCore_DP0 pid=552358)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=552358) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 68.94 MiB is free. Including non-PyTorch memory, this process has 15.17 GiB memory in use. Of the allocated memory 14.70 GiB is allocated by PyTorch, and 110.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 22:04:31.120124368 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=65536 ==========
Time: 2026-01-26 01:12:24
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.65 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 01:12:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=767955) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=767955) WARNING 01-26 01:12:57 [backends.py:609] Failed to read file <frozen os>
Throughput: 95.30 requests/s, 97679.89 total tokens/s, 95.30 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 01:12:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 01:12:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 01:12:50] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 01:12:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 01:12:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 01:12:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 01:12:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 01:12:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 01:12:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 01:12:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 01:12:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 01:12:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 01:12:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 01:12:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 01:12:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 01:12:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 01:12:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 01:12:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 01:12:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 01:12:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 01:12:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 01:12:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 01:12:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 01:12:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 01:12:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 01:12:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 01:12:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 01:12:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=767955) [2026-01-26 01:12:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=767955) [2026-01-26 01:12:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=767955) [2026-01-26 01:12:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=767955) [2026-01-26 01:12:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=767955) [2026-01-26 01:12:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=767955) [2026-01-26 01:12:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=767955) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=767955) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.55it/s]
(EngineCore_DP0 pid=767955) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.54it/s]
(EngineCore_DP0 pid=767955) 
(EngineCore_DP0 pid=767955) [2026-01-26 01:12:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=767955) [2026-01-26 01:12:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=767955) [2026-01-26 01:12:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=767955) [2026-01-26 01:12:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=767955) [2026-01-26 01:12:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=767955) [2026-01-26 01:12:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=767955) [2026-01-26 01:12:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=767955) [2026-01-26 01:12:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=767955) 2026-01-26 01:13:00,696 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=767955) 2026-01-26 01:13:00,713 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=767955) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:08,  2.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01, 10.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:00, 17.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:00<00:00, 22.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:00<00:00, 26.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 19.46it/s]
(EngineCore_DP0 pid=767955) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.04it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 26.00it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 33.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 29.04it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   1%|          | 72/8192 [00:00<00:11, 717.28it/s]
Adding requests:   2%|▏         | 147/8192 [00:00<00:10, 731.96it/s]
Adding requests:   3%|▎         | 222/8192 [00:00<00:10, 737.13it/s]
Adding requests:   4%|▎         | 298/8192 [00:00<00:10, 744.34it/s]
Adding requests:   5%|▍         | 373/8192 [00:00<00:10, 736.65it/s]
Adding requests:   5%|▌         | 448/8192 [00:00<00:10, 740.10it/s]
Adding requests:   6%|▋         | 523/8192 [00:00<00:10, 725.85it/s]
Adding requests:   7%|▋         | 599/8192 [00:00<00:10, 734.23it/s]
Adding requests:   8%|▊         | 676/8192 [00:00<00:10, 744.85it/s]
Adding requests:   9%|▉         | 751/8192 [00:01<00:09, 746.23it/s]
Adding requests:  10%|█         | 826/8192 [00:01<00:10, 735.10it/s]
Adding requests:  11%|█         | 905/8192 [00:01<00:09, 749.43it/s]
Adding requests:  12%|█▏        | 983/8192 [00:01<00:09, 756.33it/s]
Adding requests:  13%|█▎        | 1060/8192 [00:01<00:09, 759.45it/s]
Adding requests:  14%|█▍        | 1136/8192 [00:01<00:09, 723.58it/s]
Adding requests:  15%|█▍        | 1215/8192 [00:01<00:09, 739.95it/s]
Adding requests:  16%|█▌        | 1290/8192 [00:01<00:09, 741.95it/s]
Adding requests:  17%|█▋        | 1370/8192 [00:01<00:09, 756.66it/s]
Adding requests:  18%|█▊        | 1448/8192 [00:01<00:08, 762.57it/s]
Adding requests:  19%|█▊        | 1526/8192 [00:02<00:08, 767.20it/s]
Adding requests:  20%|█▉        | 1605/8192 [00:02<00:08, 773.49it/s]
Adding requests:  21%|██        | 1683/8192 [00:02<00:08, 752.62it/s]
Adding requests:  21%|██▏       | 1760/8192 [00:02<00:08, 757.24it/s]
Adding requests:  22%|██▏       | 1839/8192 [00:02<00:08, 764.62it/s]
Adding requests:  23%|██▎       | 1916/8192 [00:02<00:08, 765.41it/s]
Adding requests:  24%|██▍       | 1993/8192 [00:02<00:08, 763.57it/s]
Adding requests:  25%|██▌       | 2073/8192 [00:02<00:07, 773.00it/s]
Adding requests:  26%|██▋       | 2151/8192 [00:02<00:07, 761.40it/s]
Adding requests:  27%|██▋       | 2228/8192 [00:02<00:07, 757.60it/s]
Adding requests:  28%|██▊       | 2304/8192 [00:03<00:07, 758.17it/s]
Adding requests:  29%|██▉       | 2381/8192 [00:03<00:07, 760.94it/s]
Adding requests:  30%|███       | 2459/8192 [00:03<00:07, 765.11it/s]
Adding requests:  31%|███       | 2536/8192 [00:03<00:07, 766.02it/s]
Adding requests:  32%|███▏      | 2613/8192 [00:03<00:07, 760.15it/s]
Adding requests:  33%|███▎      | 2690/8192 [00:03<00:07, 757.61it/s]
Adding requests:  34%|███▍      | 2766/8192 [00:03<00:07, 753.88it/s]
Adding requests:  35%|███▍      | 2843/8192 [00:03<00:07, 756.04it/s]
Adding requests:  36%|███▌      | 2921/8192 [00:03<00:06, 761.63it/s]
Adding requests:  37%|███▋      | 2998/8192 [00:03<00:06, 746.16it/s]
Adding requests:  38%|███▊      | 3074/8192 [00:04<00:06, 749.34it/s]
Adding requests:  38%|███▊      | 3151/8192 [00:04<00:06, 753.54it/s]
Adding requests:  39%|███▉      | 3230/8192 [00:04<00:06, 762.12it/s]
Adding requests:  40%|████      | 3310/8192 [00:04<00:06, 771.17it/s]
Adding requests:  41%|████▏     | 3389/8192 [00:04<00:06, 776.34it/s]
Adding requests:  42%|████▏     | 3467/8192 [00:04<00:06, 771.43it/s]
Adding requests:  43%|████▎     | 3546/8192 [00:04<00:06, 774.27it/s]
Adding requests:  44%|████▍     | 3624/8192 [00:04<00:05, 767.55it/s]
Adding requests:  45%|████▌     | 3703/8192 [00:04<00:05, 772.27it/s]
Adding requests:  46%|████▌     | 3782/8192 [00:04<00:05, 777.29it/s]
Adding requests:  47%|████▋     | 3861/8192 [00:05<00:05, 778.02it/s]
Adding requests:  48%|████▊     | 3939/8192 [00:05<00:05, 776.53it/s]
Adding requests:  49%|████▉     | 4019/8192 [00:05<00:05, 782.48it/s]
Adding requests:  50%|█████     | 4098/8192 [00:05<00:05, 779.80it/s]
Adding requests:  51%|█████     | 4178/8192 [00:05<00:05, 783.47it/s]
Adding requests:  52%|█████▏    | 4257/8192 [00:05<00:05, 784.85it/s]
Adding requests:  53%|█████▎    | 4336/8192 [00:05<00:05, 760.72it/s]
Adding requests:  54%|█████▍    | 4415/8192 [00:05<00:04, 768.85it/s]
Adding requests:  55%|█████▍    | 4493/8192 [00:05<00:04, 768.64it/s]
Adding requests:  56%|█████▌    | 4570/8192 [00:06<00:04, 767.69it/s]
Adding requests:  57%|█████▋    | 4649/8192 [00:06<00:04, 774.11it/s]
Adding requests:  58%|█████▊    | 4727/8192 [00:06<00:04, 768.84it/s]
Adding requests:  59%|█████▊    | 4805/8192 [00:06<00:04, 769.79it/s]
Adding requests:  60%|█████▉    | 4884/8192 [00:06<00:04, 775.08it/s]
Adding requests:  61%|██████    | 4962/8192 [00:06<00:04, 770.09it/s]
Adding requests:  62%|██████▏   | 5042/8192 [00:06<00:04, 776.16it/s]
Adding requests:  63%|██████▎   | 5124/8192 [00:06<00:03, 789.00it/s]
Adding requests:  64%|██████▎   | 5203/8192 [00:06<00:03, 783.97it/s]
Adding requests:  64%|██████▍   | 5282/8192 [00:06<00:03, 775.19it/s]
Adding requests:  65%|██████▌   | 5363/8192 [00:07<00:03, 783.85it/s]
Adding requests:  66%|██████▋   | 5442/8192 [00:07<00:03, 778.53it/s]
Adding requests:  67%|██████▋   | 5520/8192 [00:07<00:03, 776.27it/s]
Adding requests:  68%|██████▊   | 5598/8192 [00:07<00:03, 775.40it/s]
Adding requests:  69%|██████▉   | 5676/8192 [00:07<00:03, 761.18it/s]
Adding requests:  70%|███████   | 5754/8192 [00:07<00:03, 765.83it/s]
Adding requests:  71%|███████   | 5832/8192 [00:07<00:03, 767.61it/s]
Adding requests:  72%|███████▏  | 5910/8192 [00:07<00:02, 771.00it/s]
Adding requests:  73%|███████▎  | 5988/8192 [00:07<00:02, 771.29it/s]
Adding requests:  74%|███████▍  | 6066/8192 [00:07<00:02, 771.96it/s]
Adding requests:  75%|███████▌  | 6144/8192 [00:08<00:02, 772.89it/s]
Adding requests:  76%|███████▌  | 6225/8192 [00:08<00:02, 782.95it/s]
Adding requests:  77%|███████▋  | 6307/8192 [00:08<00:02, 791.41it/s]
Adding requests:  78%|███████▊  | 6389/8192 [00:08<00:02, 798.69it/s]
Adding requests:  79%|███████▉  | 6472/8192 [00:08<00:02, 806.15it/s]
Adding requests:  80%|████████  | 6554/8192 [00:08<00:02, 808.94it/s]
Adding requests:  81%|████████  | 6635/8192 [00:08<00:01, 804.29it/s]
Adding requests:  82%|████████▏ | 6716/8192 [00:08<00:01, 805.55it/s]
Adding requests:  83%|████████▎ | 6797/8192 [00:08<00:01, 806.42it/s]
Adding requests:  84%|████████▍ | 6878/8192 [00:08<00:01, 804.74it/s]
Adding requests:  85%|████████▍ | 6959/8192 [00:09<00:01, 798.14it/s]
Adding requests:  86%|████████▌ | 7039/8192 [00:09<00:01, 775.37it/s]
Adding requests:  87%|████████▋ | 7121/8192 [00:09<00:01, 785.60it/s]
Adding requests:  88%|████████▊ | 7200/8192 [00:09<00:01, 783.97it/s]
Adding requests:  89%|████████▉ | 7280/8192 [00:09<00:01, 787.96it/s]
Adding requests:  90%|████████▉ | 7359/8192 [00:09<00:01, 781.62it/s]
Adding requests:  91%|█████████ | 7442/8192 [00:09<00:00, 794.13it/s]
Adding requests:  92%|█████████▏| 7522/8192 [00:09<00:00, 793.31it/s]
Adding requests:  93%|█████████▎| 7602/8192 [00:09<00:00, 784.91it/s]
Adding requests:  94%|█████████▍| 7684/8192 [00:09<00:00, 792.45it/s]
Adding requests:  95%|█████████▍| 7764/8192 [00:10<00:00, 788.51it/s]
Adding requests:  96%|█████████▌| 7843/8192 [00:10<00:00, 783.62it/s]
Adding requests:  97%|█████████▋| 7922/8192 [00:10<00:00, 780.30it/s]
Adding requests:  98%|█████████▊| 8001/8192 [00:10<00:00, 782.70it/s]
Adding requests:  99%|█████████▊| 8080/8192 [00:10<00:00, 783.71it/s]
Adding requests: 100%|█████████▉| 8161/8192 [00:10<00:00, 791.03it/s]
Adding requests: 100%|██████████| 8192/8192 [00:10<00:00, 770.72it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 958/8192 [00:00<00:01, 5371.45it/s, est. speed input: 5500935.41 toks/s, output: 5371.63 toks/s]
Processed prompts:  18%|█▊        | 1496/8192 [00:05<00:30, 220.75it/s, est. speed input: 277090.04 toks/s, output: 270.60 toks/s]  
Processed prompts:  21%|██        | 1724/8192 [00:07<00:35, 183.85it/s, est. speed input: 234198.39 toks/s, output: 228.71 toks/s]
Processed prompts:  23%|██▎       | 1853/8192 [00:08<00:39, 162.24it/s, est. speed input: 213733.23 toks/s, output: 208.72 toks/s]
Processed prompts:  24%|██▎       | 1935/8192 [00:10<00:45, 136.15it/s, est. speed input: 194103.23 toks/s, output: 189.55 toks/s]
Processed prompts:  24%|██▍       | 1990/8192 [00:10<00:48, 127.65it/s, est. speed input: 187324.59 toks/s, output: 182.93 toks/s]
Processed prompts:  25%|██▍       | 2046/8192 [00:11<00:51, 119.57it/s, est. speed input: 181422.28 toks/s, output: 177.17 toks/s]
Processed prompts:  26%|██▌       | 2110/8192 [00:12<00:53, 114.57it/s, est. speed input: 176833.30 toks/s, output: 172.69 toks/s]
Processed prompts:  27%|██▋       | 2174/8192 [00:12<00:54, 110.19it/s, est. speed input: 172717.64 toks/s, output: 168.67 toks/s]
Processed prompts:  27%|██▋       | 2238/8192 [00:13<00:55, 106.57it/s, est. speed input: 169013.75 toks/s, output: 165.05 toks/s]
Processed prompts:  28%|██▊       | 2302/8192 [00:14<00:56, 103.66it/s, est. speed input: 165651.57 toks/s, output: 161.77 toks/s]
Processed prompts:  29%|██▉       | 2366/8192 [00:14<00:57, 101.44it/s, est. speed input: 162598.03 toks/s, output: 158.79 toks/s]
Processed prompts:  30%|██▉       | 2430/8192 [00:15<00:57, 99.78it/s, est. speed input: 159806.77 toks/s, output: 156.06 toks/s] 
Processed prompts:  30%|███       | 2494/8192 [00:16<00:57, 98.93it/s, est. speed input: 157334.39 toks/s, output: 153.65 toks/s]
Processed prompts:  31%|███       | 2558/8192 [00:16<00:57, 97.93it/s, est. speed input: 154971.76 toks/s, output: 151.34 toks/s]
Processed prompts:  32%|███▏      | 2622/8192 [00:17<00:57, 97.19it/s, est. speed input: 152784.91 toks/s, output: 149.20 toks/s]
Processed prompts:  33%|███▎      | 2686/8192 [00:18<00:56, 96.67it/s, est. speed input: 150761.02 toks/s, output: 147.23 toks/s]
Processed prompts:  34%|███▎      | 2750/8192 [00:18<00:56, 96.29it/s, est. speed input: 148877.96 toks/s, output: 145.39 toks/s]
Processed prompts:  34%|███▍      | 2814/8192 [00:19<00:56, 96.03it/s, est. speed input: 147126.32 toks/s, output: 143.68 toks/s]
Processed prompts:  35%|███▌      | 2878/8192 [00:20<00:55, 95.82it/s, est. speed input: 145484.62 toks/s, output: 142.07 toks/s]
Processed prompts:  36%|███▌      | 2942/8192 [00:20<00:54, 95.69it/s, est. speed input: 143952.18 toks/s, output: 140.58 toks/s]
Processed prompts:  37%|███▋      | 3006/8192 [00:21<00:54, 95.61it/s, est. speed input: 142516.09 toks/s, output: 139.18 toks/s]
Processed prompts:  37%|███▋      | 3070/8192 [00:22<00:53, 95.53it/s, est. speed input: 141163.39 toks/s, output: 137.85 toks/s]
Processed prompts:  38%|███▊      | 3134/8192 [00:22<00:52, 95.49it/s, est. speed input: 139891.24 toks/s, output: 136.61 toks/s]
Processed prompts:  39%|███▉      | 3198/8192 [00:23<00:52, 95.46it/s, est. speed input: 138691.45 toks/s, output: 135.44 toks/s]
Processed prompts:  40%|███▉      | 3262/8192 [00:24<00:51, 95.43it/s, est. speed input: 137557.99 toks/s, output: 134.33 toks/s]
Processed prompts:  41%|████      | 3326/8192 [00:24<00:50, 95.44it/s, est. speed input: 136487.70 toks/s, output: 133.29 toks/s]
Processed prompts:  41%|████▏     | 3390/8192 [00:25<00:50, 95.42it/s, est. speed input: 135471.46 toks/s, output: 132.30 toks/s]
Processed prompts:  42%|████▏     | 3454/8192 [00:26<00:49, 95.40it/s, est. speed input: 134505.90 toks/s, output: 131.35 toks/s]
Processed prompts:  43%|████▎     | 3518/8192 [00:26<00:48, 95.41it/s, est. speed input: 133590.56 toks/s, output: 130.46 toks/s]
Processed prompts:  44%|████▎     | 3582/8192 [00:27<00:48, 95.38it/s, est. speed input: 132716.89 toks/s, output: 129.61 toks/s]
Processed prompts:  45%|████▍     | 3646/8192 [00:28<00:47, 95.37it/s, est. speed input: 131884.65 toks/s, output: 128.79 toks/s]
Processed prompts:  45%|████▌     | 3710/8192 [00:28<00:47, 95.36it/s, est. speed input: 131091.23 toks/s, output: 128.02 toks/s]
Processed prompts:  46%|████▌     | 3774/8192 [00:29<00:46, 95.36it/s, est. speed input: 130334.04 toks/s, output: 127.28 toks/s]
Processed prompts:  47%|████▋     | 3838/8192 [00:30<00:45, 95.35it/s, est. speed input: 129610.08 toks/s, output: 126.57 toks/s]
Processed prompts:  48%|████▊     | 3902/8192 [00:30<00:44, 95.37it/s, est. speed input: 128919.40 toks/s, output: 125.90 toks/s]
Processed prompts:  48%|████▊     | 3966/8192 [00:31<00:44, 95.34it/s, est. speed input: 128254.25 toks/s, output: 125.25 toks/s]
Processed prompts:  49%|████▉     | 4030/8192 [00:32<00:43, 95.74it/s, est. speed input: 127655.06 toks/s, output: 124.66 toks/s]
Processed prompts:  50%|████▉     | 4094/8192 [00:32<00:42, 95.61it/s, est. speed input: 127043.57 toks/s, output: 124.07 toks/s]
Processed prompts:  51%|█████     | 4158/8192 [00:33<00:42, 95.53it/s, est. speed input: 126456.73 toks/s, output: 123.49 toks/s]
Processed prompts:  52%|█████▏    | 4222/8192 [00:34<00:41, 95.88it/s, est. speed input: 125927.99 toks/s, output: 122.98 toks/s]
Processed prompts:  52%|█████▏    | 4286/8192 [00:35<00:40, 95.69it/s, est. speed input: 125383.25 toks/s, output: 122.44 toks/s]
Processed prompts:  53%|█████▎    | 4350/8192 [00:35<00:40, 95.57it/s, est. speed input: 124859.62 toks/s, output: 121.93 toks/s]
Processed prompts:  54%|█████▍    | 4414/8192 [00:36<00:39, 95.49it/s, est. speed input: 124356.00 toks/s, output: 121.44 toks/s]
Processed prompts:  55%|█████▍    | 4478/8192 [00:37<00:38, 95.44it/s, est. speed input: 123870.73 toks/s, output: 120.97 toks/s]
Processed prompts:  55%|█████▌    | 4542/8192 [00:37<00:38, 95.40it/s, est. speed input: 123402.92 toks/s, output: 120.51 toks/s]
Processed prompts:  56%|█████▌    | 4606/8192 [00:38<00:37, 95.36it/s, est. speed input: 122950.12 toks/s, output: 120.07 toks/s]
Processed prompts:  57%|█████▋    | 4670/8192 [00:39<00:36, 95.34it/s, est. speed input: 122513.77 toks/s, output: 119.64 toks/s]
Processed prompts:  58%|█████▊    | 4734/8192 [00:39<00:36, 95.33it/s, est. speed input: 122092.26 toks/s, output: 119.23 toks/s]
Processed prompts:  59%|█████▊    | 4798/8192 [00:40<00:35, 95.33it/s, est. speed input: 121684.96 toks/s, output: 118.83 toks/s]
Processed prompts:  59%|█████▉    | 4862/8192 [00:41<00:34, 95.31it/s, est. speed input: 121290.09 toks/s, output: 118.45 toks/s]
Processed prompts:  60%|██████    | 4926/8192 [00:41<00:34, 95.33it/s, est. speed input: 120909.81 toks/s, output: 118.08 toks/s]
Processed prompts:  61%|██████    | 4990/8192 [00:42<00:33, 95.72it/s, est. speed input: 120567.21 toks/s, output: 117.74 toks/s]
Processed prompts:  62%|██████▏   | 5054/8192 [00:43<00:32, 95.54it/s, est. speed input: 120205.11 toks/s, output: 117.39 toks/s]
Processed prompts:  62%|██████▏   | 5118/8192 [00:43<00:32, 95.46it/s, est. speed input: 119857.47 toks/s, output: 117.05 toks/s]
Processed prompts:  63%|██████▎   | 5182/8192 [00:44<00:31, 95.41it/s, est. speed input: 119520.02 toks/s, output: 116.72 toks/s]
Processed prompts:  64%|██████▍   | 5246/8192 [00:45<00:30, 95.36it/s, est. speed input: 119192.23 toks/s, output: 116.40 toks/s]
Processed prompts:  65%|██████▍   | 5310/8192 [00:45<00:30, 95.34it/s, est. speed input: 118874.83 toks/s, output: 116.09 toks/s]
Processed prompts:  66%|██████▌   | 5374/8192 [00:46<00:29, 95.32it/s, est. speed input: 118566.29 toks/s, output: 115.79 toks/s]
Processed prompts:  66%|██████▋   | 5438/8192 [00:47<00:28, 95.27it/s, est. speed input: 118264.86 toks/s, output: 115.49 toks/s]
Processed prompts:  67%|██████▋   | 5502/8192 [00:47<00:28, 95.27it/s, est. speed input: 117973.11 toks/s, output: 115.21 toks/s]
Processed prompts:  68%|██████▊   | 5566/8192 [00:48<00:27, 95.26it/s, est. speed input: 117689.77 toks/s, output: 114.93 toks/s]
Processed prompts:  69%|██████▊   | 5630/8192 [00:49<00:26, 95.26it/s, est. speed input: 117413.84 toks/s, output: 114.66 toks/s]
Processed prompts:  70%|██████▉   | 5694/8192 [00:49<00:26, 95.25it/s, est. speed input: 117145.46 toks/s, output: 114.40 toks/s]
Processed prompts:  70%|███████   | 5758/8192 [00:50<00:25, 95.25it/s, est. speed input: 116883.95 toks/s, output: 114.14 toks/s]
Processed prompts:  71%|███████   | 5822/8192 [00:51<00:24, 95.26it/s, est. speed input: 116630.06 toks/s, output: 113.90 toks/s]
Processed prompts:  72%|███████▏  | 5886/8192 [00:51<00:24, 95.25it/s, est. speed input: 116381.95 toks/s, output: 113.65 toks/s]
Processed prompts:  73%|███████▎  | 5950/8192 [00:52<00:23, 95.26it/s, est. speed input: 116141.16 toks/s, output: 113.42 toks/s]
Processed prompts:  73%|███████▎  | 6014/8192 [00:53<00:22, 95.25it/s, est. speed input: 115905.38 toks/s, output: 113.19 toks/s]
Processed prompts:  74%|███████▍  | 6078/8192 [00:53<00:22, 95.22it/s, est. speed input: 115674.66 toks/s, output: 112.96 toks/s]
Processed prompts:  75%|███████▍  | 6142/8192 [00:54<00:21, 95.29it/s, est. speed input: 115454.04 toks/s, output: 112.75 toks/s]
Processed prompts:  76%|███████▌  | 6206/8192 [00:55<00:20, 95.24it/s, est. speed input: 115233.70 toks/s, output: 112.53 toks/s]
Processed prompts:  77%|███████▋  | 6270/8192 [00:55<00:20, 95.21it/s, est. speed input: 115019.21 toks/s, output: 112.32 toks/s]
Processed prompts:  77%|███████▋  | 6334/8192 [00:56<00:19, 95.21it/s, est. speed input: 114810.52 toks/s, output: 112.12 toks/s]
Processed prompts:  78%|███████▊  | 6398/8192 [00:57<00:18, 95.23it/s, est. speed input: 114607.78 toks/s, output: 111.92 toks/s]
Processed prompts:  79%|███████▉  | 6462/8192 [00:57<00:18, 95.22it/s, est. speed input: 114409.01 toks/s, output: 111.73 toks/s]
Processed prompts:  80%|███████▉  | 6526/8192 [00:58<00:17, 95.24it/s, est. speed input: 114215.49 toks/s, output: 111.54 toks/s]
Processed prompts:  80%|████████  | 6590/8192 [00:59<00:16, 95.63it/s, est. speed input: 114043.77 toks/s, output: 111.37 toks/s]
Processed prompts:  81%|████████  | 6654/8192 [00:59<00:16, 95.91it/s, est. speed input: 113875.91 toks/s, output: 111.21 toks/s]
Processed prompts:  82%|████████▏ | 6718/8192 [01:00<00:15, 95.70it/s, est. speed input: 113693.83 toks/s, output: 111.03 toks/s]
Processed prompts:  83%|████████▎ | 6782/8192 [01:01<00:14, 95.62it/s, est. speed input: 113518.73 toks/s, output: 110.86 toks/s]
Processed prompts:  84%|████████▎ | 6846/8192 [01:01<00:14, 95.43it/s, est. speed input: 113341.91 toks/s, output: 110.69 toks/s]
Processed prompts:  84%|████████▍ | 6910/8192 [01:02<00:13, 95.35it/s, est. speed input: 113170.90 toks/s, output: 110.52 toks/s]
Processed prompts:  85%|████████▌ | 6974/8192 [01:03<00:12, 95.31it/s, est. speed input: 113004.09 toks/s, output: 110.36 toks/s]
Processed prompts:  86%|████████▌ | 7038/8192 [01:03<00:12, 95.28it/s, est. speed input: 112841.00 toks/s, output: 110.20 toks/s]
Processed prompts:  87%|████████▋ | 7102/8192 [01:04<00:11, 95.27it/s, est. speed input: 112681.55 toks/s, output: 110.04 toks/s]
Processed prompts:  87%|████████▋ | 7166/8192 [01:05<00:10, 95.27it/s, est. speed input: 112525.70 toks/s, output: 109.89 toks/s]
Processed prompts:  88%|████████▊ | 7230/8192 [01:05<00:10, 95.20it/s, est. speed input: 112370.35 toks/s, output: 109.74 toks/s]
Processed prompts:  89%|████████▉ | 7294/8192 [01:06<00:09, 95.20it/s, est. speed input: 112219.83 toks/s, output: 109.59 toks/s]
Processed prompts:  90%|████████▉ | 7358/8192 [01:07<00:08, 95.19it/s, est. speed input: 112071.99 toks/s, output: 109.45 toks/s]
Processed prompts:  91%|█████████ | 7422/8192 [01:07<00:08, 95.18it/s, est. speed input: 111927.36 toks/s, output: 109.30 toks/s]
Processed prompts:  91%|█████████▏| 7486/8192 [01:08<00:07, 95.18it/s, est. speed input: 111785.54 toks/s, output: 109.17 toks/s]
Processed prompts:  92%|█████████▏| 7550/8192 [01:09<00:06, 95.19it/s, est. speed input: 111646.84 toks/s, output: 109.03 toks/s]
Processed prompts:  93%|█████████▎| 7614/8192 [01:09<00:06, 95.18it/s, est. speed input: 111510.29 toks/s, output: 108.90 toks/s]
Processed prompts:  94%|█████████▎| 7678/8192 [01:10<00:05, 95.17it/s, est. speed input: 111376.19 toks/s, output: 108.77 toks/s]
Processed prompts:  95%|█████████▍| 7742/8192 [01:11<00:04, 95.18it/s, est. speed input: 111244.94 toks/s, output: 108.64 toks/s]
Processed prompts:  95%|█████████▌| 7806/8192 [01:11<00:04, 95.17it/s, est. speed input: 111115.82 toks/s, output: 108.51 toks/s]
Processed prompts:  96%|█████████▌| 7870/8192 [01:12<00:03, 95.18it/s, est. speed input: 110989.70 toks/s, output: 108.39 toks/s]
Processed prompts:  97%|█████████▋| 7934/8192 [01:13<00:02, 95.18it/s, est. speed input: 110865.60 toks/s, output: 108.27 toks/s]
Processed prompts:  98%|█████████▊| 7998/8192 [01:13<00:02, 95.17it/s, est. speed input: 110743.52 toks/s, output: 108.15 toks/s]
Processed prompts:  98%|█████████▊| 8062/8192 [01:14<00:01, 95.59it/s, est. speed input: 110638.40 toks/s, output: 108.05 toks/s]
Processed prompts:  99%|█████████▉| 8126/8192 [01:15<00:00, 95.62it/s, est. speed input: 110526.00 toks/s, output: 107.94 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:15<00:00, 95.62it/s, est. speed input: 111355.75 toks/s, output: 108.75 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:15<00:00, 108.75it/s, est. speed input: 111355.75 toks/s, output: 108.75 toks/s]
[rank0]:[W126 01:14:29.967384995 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 01:29:23
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.65 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 01:29:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=784616) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=784616) WARNING 01-26 01:30:00 [backends.py:609] Failed to read file <frozen os>
Throughput: 40.26 requests/s, 41269.73 total tokens/s, 40.26 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 01:29:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 01:29:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 01:29:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 01:29:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 01:29:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 01:29:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 01:29:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 01:29:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 01:29:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 01:29:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 01:29:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 01:29:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 01:29:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 01:29:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 01:29:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 01:29:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 01:29:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 01:29:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 01:29:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 01:29:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 01:29:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 01:29:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 01:29:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 01:29:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 01:29:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 01:29:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 01:29:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 01:29:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=784616) [2026-01-26 01:29:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=784616) [2026-01-26 01:29:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=784616) [2026-01-26 01:29:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=784616) [2026-01-26 01:29:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=784616) [2026-01-26 01:29:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=784616) [2026-01-26 01:29:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=784616) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=784616) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.19s/it]
(EngineCore_DP0 pid=784616) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.19s/it]
(EngineCore_DP0 pid=784616) 
(EngineCore_DP0 pid=784616) [2026-01-26 01:29:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=784616) [2026-01-26 01:29:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=784616) [2026-01-26 01:29:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=784616) [2026-01-26 01:29:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=784616) [2026-01-26 01:29:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=784616) [2026-01-26 01:29:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=784616) [2026-01-26 01:29:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=784616) [2026-01-26 01:29:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=784616) 2026-01-26 01:30:04,443 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=784616) 2026-01-26 01:30:04,466 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=784616) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:01<00:22,  1.25s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:01<00:03,  3.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:02,  5.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  7.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:00,  9.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00, 12.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00, 16.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00, 18.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.09it/s]
(EngineCore_DP0 pid=784616) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.33it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 18.92it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 25.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 24.35it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   1%|          | 72/8192 [00:00<00:11, 716.51it/s]
Adding requests:   2%|▏         | 147/8192 [00:00<00:10, 733.91it/s]
Adding requests:   3%|▎         | 223/8192 [00:00<00:10, 744.87it/s]
Adding requests:   4%|▎         | 298/8192 [00:00<00:10, 746.46it/s]
Adding requests:   5%|▍         | 373/8192 [00:00<00:10, 743.28it/s]
Adding requests:   5%|▌         | 449/8192 [00:00<00:10, 745.84it/s]
Adding requests:   6%|▋         | 524/8192 [00:00<00:10, 715.42it/s]
Adding requests:   7%|▋         | 599/8192 [00:00<00:10, 725.86it/s]
Adding requests:   8%|▊         | 673/8192 [00:00<00:10, 727.81it/s]
Adding requests:   9%|▉         | 749/8192 [00:01<00:10, 736.14it/s]
Adding requests:  10%|█         | 823/8192 [00:01<00:10, 726.70it/s]
Adding requests:  11%|█         | 899/8192 [00:01<00:09, 734.90it/s]
Adding requests:  12%|█▏        | 976/8192 [00:01<00:09, 743.03it/s]
Adding requests:  13%|█▎        | 1053/8192 [00:01<00:09, 750.89it/s]
Adding requests:  14%|█▍        | 1129/8192 [00:01<00:09, 743.15it/s]
Adding requests:  15%|█▍        | 1208/8192 [00:01<00:09, 756.35it/s]
Adding requests:  16%|█▌        | 1284/8192 [00:01<00:09, 752.67it/s]
Adding requests:  17%|█▋        | 1361/8192 [00:01<00:09, 756.40it/s]
Adding requests:  18%|█▊        | 1437/8192 [00:01<00:08, 757.19it/s]
Adding requests:  19%|█▊        | 1517/8192 [00:02<00:08, 766.98it/s]
Adding requests:  19%|█▉        | 1597/8192 [00:02<00:08, 776.42it/s]
Adding requests:  20%|██        | 1675/8192 [00:02<00:08, 773.94it/s]
Adding requests:  21%|██▏       | 1753/8192 [00:02<00:08, 769.48it/s]
Adding requests:  22%|██▏       | 1830/8192 [00:02<00:08, 750.86it/s]
Adding requests:  23%|██▎       | 1907/8192 [00:02<00:08, 754.64it/s]
Adding requests:  24%|██▍       | 1984/8192 [00:02<00:08, 758.15it/s]
Adding requests:  25%|██▌       | 2063/8192 [00:02<00:07, 766.71it/s]
Adding requests:  26%|██▌       | 2140/8192 [00:02<00:08, 752.43it/s]
Adding requests:  27%|██▋       | 2216/8192 [00:02<00:08, 746.54it/s]
Adding requests:  28%|██▊       | 2294/8192 [00:03<00:07, 755.81it/s]
Adding requests:  29%|██▉       | 2370/8192 [00:03<00:07, 756.58it/s]
Adding requests:  30%|██▉       | 2447/8192 [00:03<00:07, 759.14it/s]
Adding requests:  31%|███       | 2525/8192 [00:03<00:07, 764.19it/s]
Adding requests:  32%|███▏      | 2602/8192 [00:03<00:07, 765.50it/s]
Adding requests:  33%|███▎      | 2681/8192 [00:03<00:07, 771.75it/s]
Adding requests:  34%|███▎      | 2759/8192 [00:03<00:07, 771.19it/s]
Adding requests:  35%|███▍      | 2837/8192 [00:03<00:06, 766.91it/s]
Adding requests:  36%|███▌      | 2916/8192 [00:03<00:06, 771.35it/s]
Adding requests:  37%|███▋      | 2994/8192 [00:03<00:06, 768.67it/s]
Adding requests:  37%|███▋      | 3071/8192 [00:04<00:06, 757.55it/s]
Adding requests:  38%|███▊      | 3147/8192 [00:04<00:06, 742.10it/s]
Adding requests:  39%|███▉      | 3224/8192 [00:04<00:06, 749.05it/s]
Adding requests:  40%|████      | 3302/8192 [00:04<00:06, 757.49it/s]
Adding requests:  41%|████      | 3379/8192 [00:04<00:06, 759.36it/s]
Adding requests:  42%|████▏     | 3455/8192 [00:04<00:06, 750.69it/s]
Adding requests:  43%|████▎     | 3531/8192 [00:04<00:06, 752.49it/s]
Adding requests:  44%|████▍     | 3609/8192 [00:04<00:06, 758.47it/s]
Adding requests:  45%|████▍     | 3685/8192 [00:04<00:05, 751.71it/s]
Adding requests:  46%|████▌     | 3761/8192 [00:04<00:05, 752.27it/s]
Adding requests:  47%|████▋     | 3840/8192 [00:05<00:05, 761.83it/s]
Adding requests:  48%|████▊     | 3917/8192 [00:05<00:05, 763.53it/s]
Adding requests:  49%|████▉     | 3994/8192 [00:05<00:05, 764.05it/s]
Adding requests:  50%|████▉     | 4072/8192 [00:05<00:05, 766.89it/s]
Adding requests:  51%|█████     | 4150/8192 [00:05<00:05, 769.42it/s]
Adding requests:  52%|█████▏    | 4230/8192 [00:05<00:05, 776.90it/s]
Adding requests:  53%|█████▎    | 4309/8192 [00:05<00:04, 780.13it/s]
Adding requests:  54%|█████▎    | 4389/8192 [00:05<00:04, 783.73it/s]
Adding requests:  55%|█████▍    | 4468/8192 [00:05<00:04, 769.31it/s]
Adding requests:  55%|█████▌    | 4545/8192 [00:06<00:04, 748.85it/s]
Adding requests:  56%|█████▋    | 4622/8192 [00:06<00:04, 753.94it/s]
Adding requests:  57%|█████▋    | 4700/8192 [00:06<00:04, 759.96it/s]
Adding requests:  58%|█████▊    | 4779/8192 [00:06<00:04, 767.12it/s]
Adding requests:  59%|█████▉    | 4857/8192 [00:06<00:04, 769.02it/s]
Adding requests:  60%|██████    | 4934/8192 [00:06<00:04, 767.01it/s]
Adding requests:  61%|██████    | 5014/8192 [00:06<00:04, 774.43it/s]
Adding requests:  62%|██████▏   | 5095/8192 [00:06<00:03, 782.39it/s]
Adding requests:  63%|██████▎   | 5175/8192 [00:06<00:03, 787.53it/s]
Adding requests:  64%|██████▍   | 5254/8192 [00:06<00:03, 768.99it/s]
Adding requests:  65%|██████▌   | 5332/8192 [00:07<00:03, 759.84it/s]
Adding requests:  66%|██████▌   | 5410/8192 [00:07<00:03, 763.34it/s]
Adding requests:  67%|██████▋   | 5487/8192 [00:07<00:03, 758.67it/s]
Adding requests:  68%|██████▊   | 5564/8192 [00:07<00:03, 760.53it/s]
Adding requests:  69%|██████▉   | 5642/8192 [00:07<00:03, 765.62it/s]
Adding requests:  70%|██████▉   | 5719/8192 [00:07<00:03, 758.63it/s]
Adding requests:  71%|███████   | 5797/8192 [00:07<00:03, 762.44it/s]
Adding requests:  72%|███████▏  | 5874/8192 [00:07<00:03, 750.18it/s]
Adding requests:  73%|███████▎  | 5953/8192 [00:07<00:02, 761.38it/s]
Adding requests:  74%|███████▎  | 6033/8192 [00:07<00:02, 771.79it/s]
Adding requests:  75%|███████▍  | 6112/8192 [00:08<00:02, 774.11it/s]
Adding requests:  76%|███████▌  | 6192/8192 [00:08<00:02, 778.98it/s]
Adding requests:  77%|███████▋  | 6275/8192 [00:08<00:02, 791.88it/s]
Adding requests:  78%|███████▊  | 6356/8192 [00:08<00:02, 795.77it/s]
Adding requests:  79%|███████▊  | 6438/8192 [00:08<00:02, 800.49it/s]
Adding requests:  80%|███████▉  | 6519/8192 [00:08<00:02, 800.82it/s]
Adding requests:  81%|████████  | 6600/8192 [00:08<00:02, 792.70it/s]
Adding requests:  82%|████████▏ | 6680/8192 [00:08<00:01, 791.89it/s]
Adding requests:  83%|████████▎ | 6760/8192 [00:08<00:01, 794.21it/s]
Adding requests:  84%|████████▎ | 6841/8192 [00:08<00:01, 797.38it/s]
Adding requests:  84%|████████▍ | 6921/8192 [00:09<00:01, 792.15it/s]
Adding requests:  85%|████████▌ | 7001/8192 [00:09<00:01, 784.65it/s]
Adding requests:  86%|████████▋ | 7080/8192 [00:09<00:01, 784.07it/s]
Adding requests:  87%|████████▋ | 7159/8192 [00:09<00:01, 781.34it/s]
Adding requests:  88%|████████▊ | 7238/8192 [00:09<00:01, 755.00it/s]
Adding requests:  89%|████████▉ | 7318/8192 [00:09<00:01, 767.88it/s]
Adding requests:  90%|█████████ | 7398/8192 [00:09<00:01, 777.03it/s]
Adding requests:  91%|█████████▏| 7480/8192 [00:09<00:00, 788.63it/s]
Adding requests:  92%|█████████▏| 7561/8192 [00:09<00:00, 794.60it/s]
Adding requests:  93%|█████████▎| 7641/8192 [00:09<00:00, 795.92it/s]
Adding requests:  94%|█████████▍| 7721/8192 [00:10<00:00, 793.43it/s]
Adding requests:  95%|█████████▌| 7801/8192 [00:10<00:00, 787.63it/s]
Adding requests:  96%|█████████▌| 7880/8192 [00:10<00:00, 785.99it/s]
Adding requests:  97%|█████████▋| 7959/8192 [00:10<00:00, 782.79it/s]
Adding requests:  98%|█████████▊| 8038/8192 [00:10<00:00, 779.97it/s]
Adding requests:  99%|█████████▉| 8117/8192 [00:10<00:00, 780.81it/s]
Adding requests: 100%|██████████| 8192/8192 [00:10<00:00, 765.98it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 434/8192 [00:00<00:06, 1141.95it/s, est. speed input: 1169401.71 toks/s, output: 1141.96 toks/s]
Processed prompts:   7%|▋         | 549/8192 [00:03<00:53, 142.72it/s, est. speed input: 184414.94 toks/s, output: 180.09 toks/s]   
Processed prompts:   7%|▋         | 599/8192 [00:04<01:11, 105.90it/s, est. speed input: 144844.71 toks/s, output: 141.45 toks/s]
Processed prompts:   8%|▊         | 628/8192 [00:05<01:28, 85.24it/s, est. speed input: 125486.87 toks/s, output: 122.55 toks/s] 
Processed prompts:   8%|▊         | 647/8192 [00:05<01:31, 82.73it/s, est. speed input: 122208.12 toks/s, output: 119.34 toks/s]
Processed prompts:   8%|▊         | 662/8192 [00:06<01:50, 68.21it/s, est. speed input: 112702.78 toks/s, output: 110.06 toks/s]
Processed prompts:   8%|▊         | 674/8192 [00:06<01:57, 63.87it/s, est. speed input: 109297.95 toks/s, output: 106.74 toks/s]
Processed prompts:   8%|▊         | 686/8192 [00:06<02:05, 59.67it/s, est. speed input: 106251.88 toks/s, output: 103.76 toks/s]
Processed prompts:   9%|▊         | 698/8192 [00:06<02:14, 55.76it/s, est. speed input: 103466.66 toks/s, output: 101.04 toks/s]
Processed prompts:   9%|▊         | 710/8192 [00:07<02:23, 52.30it/s, est. speed input: 100910.68 toks/s, output: 98.55 toks/s] 
Processed prompts:   9%|▉         | 722/8192 [00:07<02:31, 49.42it/s, est. speed input: 98559.56 toks/s, output: 96.25 toks/s] 
Processed prompts:   9%|▉         | 734/8192 [00:07<02:38, 47.12it/s, est. speed input: 96388.56 toks/s, output: 94.13 toks/s]
Processed prompts:   9%|▉         | 746/8192 [00:08<02:44, 45.32it/s, est. speed input: 94374.24 toks/s, output: 92.16 toks/s]
Processed prompts:   9%|▉         | 758/8192 [00:08<02:49, 43.96it/s, est. speed input: 92500.99 toks/s, output: 90.33 toks/s]
Processed prompts:   9%|▉         | 770/8192 [00:08<02:52, 42.95it/s, est. speed input: 90753.19 toks/s, output: 88.63 toks/s]
Processed prompts:  10%|▉         | 782/8192 [00:08<02:55, 42.21it/s, est. speed input: 89119.94 toks/s, output: 87.03 toks/s]
Processed prompts:  10%|▉         | 794/8192 [00:09<02:57, 41.69it/s, est. speed input: 87593.68 toks/s, output: 85.54 toks/s]
Processed prompts:  10%|▉         | 806/8192 [00:09<02:58, 41.32it/s, est. speed input: 86162.66 toks/s, output: 84.14 toks/s]
Processed prompts:  10%|▉         | 818/8192 [00:09<02:59, 41.07it/s, est. speed input: 84820.58 toks/s, output: 82.83 toks/s]
Processed prompts:  10%|█         | 830/8192 [00:10<03:00, 40.88it/s, est. speed input: 83553.88 toks/s, output: 81.60 toks/s]
Processed prompts:  10%|█         | 842/8192 [00:10<03:00, 40.75it/s, est. speed input: 82359.08 toks/s, output: 80.43 toks/s]
Processed prompts:  10%|█         | 854/8192 [00:10<03:00, 40.66it/s, est. speed input: 81231.12 toks/s, output: 79.33 toks/s]
Processed prompts:  11%|█         | 866/8192 [00:11<03:00, 40.58it/s, est. speed input: 80160.99 toks/s, output: 78.28 toks/s]
Processed prompts:  11%|█         | 878/8192 [00:11<03:00, 40.53it/s, est. speed input: 79146.97 toks/s, output: 77.29 toks/s]
Processed prompts:  11%|█         | 890/8192 [00:11<03:00, 40.50it/s, est. speed input: 78185.08 toks/s, output: 76.35 toks/s]
Processed prompts:  11%|█         | 902/8192 [00:11<03:00, 40.47it/s, est. speed input: 77270.30 toks/s, output: 75.46 toks/s]
Processed prompts:  11%|█         | 914/8192 [00:12<02:59, 40.44it/s, est. speed input: 76398.82 toks/s, output: 74.61 toks/s]
Processed prompts:  11%|█▏        | 926/8192 [00:12<02:59, 40.43it/s, est. speed input: 75569.33 toks/s, output: 73.80 toks/s]
Processed prompts:  11%|█▏        | 938/8192 [00:12<02:59, 40.44it/s, est. speed input: 74781.10 toks/s, output: 73.03 toks/s]
Processed prompts:  12%|█▏        | 950/8192 [00:13<02:59, 40.41it/s, est. speed input: 74023.84 toks/s, output: 72.29 toks/s]
Processed prompts:  12%|█▏        | 962/8192 [00:13<02:58, 40.42it/s, est. speed input: 73303.33 toks/s, output: 71.59 toks/s]
Processed prompts:  12%|█▏        | 974/8192 [00:13<02:58, 40.43it/s, est. speed input: 72614.88 toks/s, output: 70.91 toks/s]
Processed prompts:  12%|█▏        | 986/8192 [00:14<02:58, 40.43it/s, est. speed input: 71955.26 toks/s, output: 70.27 toks/s]
Processed prompts:  12%|█▏        | 998/8192 [00:14<02:57, 40.43it/s, est. speed input: 71322.29 toks/s, output: 69.65 toks/s]
Processed prompts:  12%|█▏        | 1010/8192 [00:14<02:57, 40.44it/s, est. speed input: 70715.46 toks/s, output: 69.06 toks/s]
Processed prompts:  12%|█▏        | 1022/8192 [00:14<02:57, 40.42it/s, est. speed input: 70130.70 toks/s, output: 68.49 toks/s]
Processed prompts:  13%|█▎        | 1034/8192 [00:15<02:57, 40.42it/s, est. speed input: 69569.67 toks/s, output: 67.94 toks/s]
Processed prompts:  13%|█▎        | 1046/8192 [00:15<02:56, 40.42it/s, est. speed input: 69030.56 toks/s, output: 67.41 toks/s]
Processed prompts:  13%|█▎        | 1058/8192 [00:15<02:56, 40.43it/s, est. speed input: 68512.43 toks/s, output: 66.91 toks/s]
Processed prompts:  13%|█▎        | 1070/8192 [00:16<02:56, 40.41it/s, est. speed input: 68011.37 toks/s, output: 66.42 toks/s]
Processed prompts:  13%|█▎        | 1082/8192 [00:16<02:55, 40.41it/s, est. speed input: 67529.57 toks/s, output: 65.95 toks/s]
Processed prompts:  13%|█▎        | 1094/8192 [00:16<02:55, 40.41it/s, est. speed input: 67064.42 toks/s, output: 65.49 toks/s]
Processed prompts:  14%|█▎        | 1106/8192 [00:17<02:55, 40.42it/s, est. speed input: 66616.47 toks/s, output: 65.06 toks/s]
Processed prompts:  14%|█▎        | 1118/8192 [00:17<02:55, 40.40it/s, est. speed input: 66181.90 toks/s, output: 64.63 toks/s]
Processed prompts:  14%|█▍        | 1130/8192 [00:17<02:54, 40.41it/s, est. speed input: 65763.71 toks/s, output: 64.22 toks/s]
Processed prompts:  14%|█▍        | 1142/8192 [00:17<02:54, 40.39it/s, est. speed input: 65357.66 toks/s, output: 63.83 toks/s]
Processed prompts:  14%|█▍        | 1154/8192 [00:18<02:54, 40.39it/s, est. speed input: 64965.90 toks/s, output: 63.44 toks/s]
Processed prompts:  14%|█▍        | 1166/8192 [00:18<02:53, 40.40it/s, est. speed input: 64587.46 toks/s, output: 63.07 toks/s]
Processed prompts:  14%|█▍        | 1178/8192 [00:18<02:53, 40.40it/s, est. speed input: 64220.40 toks/s, output: 62.72 toks/s]
Processed prompts:  15%|█▍        | 1190/8192 [00:19<02:53, 40.40it/s, est. speed input: 63864.68 toks/s, output: 62.37 toks/s]
Processed prompts:  15%|█▍        | 1202/8192 [00:19<02:53, 40.40it/s, est. speed input: 63519.85 toks/s, output: 62.03 toks/s]
Processed prompts:  15%|█▍        | 1214/8192 [00:19<02:52, 40.41it/s, est. speed input: 63186.06 toks/s, output: 61.71 toks/s]
Processed prompts:  15%|█▍        | 1226/8192 [00:19<02:52, 40.41it/s, est. speed input: 62861.52 toks/s, output: 61.39 toks/s]
Processed prompts:  15%|█▌        | 1238/8192 [00:20<02:52, 40.39it/s, est. speed input: 62545.42 toks/s, output: 61.08 toks/s]
Processed prompts:  15%|█▌        | 1250/8192 [00:20<02:51, 40.39it/s, est. speed input: 62239.44 toks/s, output: 60.78 toks/s]
Processed prompts:  15%|█▌        | 1262/8192 [00:20<02:51, 40.39it/s, est. speed input: 61941.79 toks/s, output: 60.49 toks/s]
Processed prompts:  16%|█▌        | 1274/8192 [00:21<02:51, 40.39it/s, est. speed input: 61653.32 toks/s, output: 60.21 toks/s]
Processed prompts:  16%|█▌        | 1286/8192 [00:21<02:50, 40.39it/s, est. speed input: 61371.96 toks/s, output: 59.93 toks/s]
Processed prompts:  16%|█▌        | 1298/8192 [00:21<02:50, 40.39it/s, est. speed input: 61098.54 toks/s, output: 59.67 toks/s]
Processed prompts:  16%|█▌        | 1310/8192 [00:22<02:50, 40.38it/s, est. speed input: 60832.28 toks/s, output: 59.41 toks/s]
Processed prompts:  16%|█▌        | 1322/8192 [00:22<02:50, 40.37it/s, est. speed input: 60572.59 toks/s, output: 59.15 toks/s]
Processed prompts:  16%|█▋        | 1334/8192 [00:22<02:49, 40.38it/s, est. speed input: 60320.86 toks/s, output: 58.91 toks/s]
Processed prompts:  16%|█▋        | 1346/8192 [00:22<02:49, 40.38it/s, est. speed input: 60075.07 toks/s, output: 58.67 toks/s]
Processed prompts:  17%|█▋        | 1358/8192 [00:23<02:49, 40.39it/s, est. speed input: 59835.98 toks/s, output: 58.43 toks/s]
Processed prompts:  17%|█▋        | 1370/8192 [00:23<02:48, 40.38it/s, est. speed input: 59602.30 toks/s, output: 58.21 toks/s]
Processed prompts:  17%|█▋        | 1382/8192 [00:23<02:48, 40.38it/s, est. speed input: 59374.53 toks/s, output: 57.98 toks/s]
Processed prompts:  17%|█▋        | 1394/8192 [00:24<02:48, 40.37it/s, est. speed input: 59151.95 toks/s, output: 57.77 toks/s]
Processed prompts:  17%|█▋        | 1406/8192 [00:24<02:48, 40.38it/s, est. speed input: 58935.76 toks/s, output: 57.55 toks/s]
Processed prompts:  17%|█▋        | 1418/8192 [00:24<02:47, 40.38it/s, est. speed input: 58724.29 toks/s, output: 57.35 toks/s]
Processed prompts:  17%|█▋        | 1430/8192 [00:25<02:47, 40.38it/s, est. speed input: 58518.12 toks/s, output: 57.15 toks/s]
Processed prompts:  18%|█▊        | 1442/8192 [00:25<02:47, 40.40it/s, est. speed input: 58317.43 toks/s, output: 56.95 toks/s]
Processed prompts:  18%|█▊        | 1454/8192 [00:25<02:46, 40.39it/s, est. speed input: 58120.48 toks/s, output: 56.76 toks/s]
Processed prompts:  18%|█▊        | 1466/8192 [00:25<02:46, 40.40it/s, est. speed input: 57928.83 toks/s, output: 56.57 toks/s]
Processed prompts:  18%|█▊        | 1478/8192 [00:26<02:46, 40.40it/s, est. speed input: 57741.19 toks/s, output: 56.39 toks/s]
Processed prompts:  18%|█▊        | 1490/8192 [00:26<02:45, 40.39it/s, est. speed input: 57557.28 toks/s, output: 56.21 toks/s]
Processed prompts:  18%|█▊        | 1502/8192 [00:26<02:45, 40.38it/s, est. speed input: 57377.41 toks/s, output: 56.03 toks/s]
Processed prompts:  18%|█▊        | 1514/8192 [00:27<02:45, 40.37it/s, est. speed input: 57200.99 toks/s, output: 55.86 toks/s]
Processed prompts:  19%|█▊        | 1526/8192 [00:27<02:45, 40.36it/s, est. speed input: 57028.68 toks/s, output: 55.69 toks/s]
Processed prompts:  19%|█▉        | 1538/8192 [00:27<02:44, 40.38it/s, est. speed input: 56860.74 toks/s, output: 55.53 toks/s]
Processed prompts:  19%|█▉        | 1550/8192 [00:27<02:44, 40.37it/s, est. speed input: 56695.90 toks/s, output: 55.37 toks/s]
Processed prompts:  19%|█▉        | 1562/8192 [00:28<02:44, 40.37it/s, est. speed input: 56534.44 toks/s, output: 55.21 toks/s]
Processed prompts:  19%|█▉        | 1574/8192 [00:28<02:43, 40.37it/s, est. speed input: 56376.59 toks/s, output: 55.06 toks/s]
Processed prompts:  19%|█▉        | 1586/8192 [00:28<02:43, 40.38it/s, est. speed input: 56222.14 toks/s, output: 54.90 toks/s]
Processed prompts:  20%|█▉        | 1598/8192 [00:29<02:43, 40.38it/s, est. speed input: 56070.52 toks/s, output: 54.76 toks/s]
Processed prompts:  20%|█▉        | 1610/8192 [00:29<02:42, 40.39it/s, est. speed input: 55922.66 toks/s, output: 54.61 toks/s]
Processed prompts:  20%|█▉        | 1622/8192 [00:29<02:42, 40.36it/s, est. speed input: 55776.00 toks/s, output: 54.47 toks/s]
Processed prompts:  20%|█▉        | 1634/8192 [00:30<02:42, 40.36it/s, est. speed input: 55633.10 toks/s, output: 54.33 toks/s]
Processed prompts:  20%|██        | 1646/8192 [00:30<02:42, 40.36it/s, est. speed input: 55493.24 toks/s, output: 54.19 toks/s]
Processed prompts:  20%|██        | 1658/8192 [00:30<02:41, 40.36it/s, est. speed input: 55355.97 toks/s, output: 54.06 toks/s]
Processed prompts:  20%|██        | 1670/8192 [00:30<02:41, 40.37it/s, est. speed input: 55221.56 toks/s, output: 53.93 toks/s]
Processed prompts:  21%|██        | 1682/8192 [00:31<02:41, 40.37it/s, est. speed input: 55089.58 toks/s, output: 53.80 toks/s]
Processed prompts:  21%|██        | 1694/8192 [00:31<02:40, 40.37it/s, est. speed input: 54960.07 toks/s, output: 53.67 toks/s]
Processed prompts:  21%|██        | 1706/8192 [00:31<02:40, 40.35it/s, est. speed input: 54832.13 toks/s, output: 53.55 toks/s]
Processed prompts:  21%|██        | 1718/8192 [00:32<02:40, 40.35it/s, est. speed input: 54707.16 toks/s, output: 53.42 toks/s]
Processed prompts:  21%|██        | 1730/8192 [00:32<02:40, 40.36it/s, est. speed input: 54584.87 toks/s, output: 53.31 toks/s]
Processed prompts:  21%|██▏       | 1742/8192 [00:32<02:39, 40.35it/s, est. speed input: 54464.31 toks/s, output: 53.19 toks/s]
Processed prompts:  21%|██▏       | 1754/8192 [00:33<02:39, 40.33it/s, est. speed input: 54345.24 toks/s, output: 53.07 toks/s]
Processed prompts:  22%|██▏       | 1766/8192 [00:33<02:39, 40.35it/s, est. speed input: 54229.45 toks/s, output: 52.96 toks/s]
Processed prompts:  22%|██▏       | 1778/8192 [00:33<02:38, 40.36it/s, est. speed input: 54115.76 toks/s, output: 52.85 toks/s]
Processed prompts:  22%|██▏       | 1790/8192 [00:33<02:38, 40.35it/s, est. speed input: 54003.22 toks/s, output: 52.74 toks/s]
Processed prompts:  22%|██▏       | 1802/8192 [00:34<02:38, 40.35it/s, est. speed input: 53893.07 toks/s, output: 52.63 toks/s]
Processed prompts:  22%|██▏       | 1814/8192 [00:34<02:38, 40.34it/s, est. speed input: 53784.60 toks/s, output: 52.52 toks/s]
Processed prompts:  22%|██▏       | 1826/8192 [00:34<02:37, 40.34it/s, est. speed input: 53677.82 toks/s, output: 52.42 toks/s]
Processed prompts:  22%|██▏       | 1838/8192 [00:35<02:37, 40.34it/s, est. speed input: 53573.29 toks/s, output: 52.32 toks/s]
Processed prompts:  23%|██▎       | 1850/8192 [00:35<02:37, 40.35it/s, est. speed input: 53470.65 toks/s, output: 52.22 toks/s]
Processed prompts:  23%|██▎       | 1862/8192 [00:35<02:36, 40.34it/s, est. speed input: 53369.03 toks/s, output: 52.12 toks/s]
Processed prompts:  23%|██▎       | 1874/8192 [00:36<02:36, 40.34it/s, est. speed input: 53269.63 toks/s, output: 52.02 toks/s]
Processed prompts:  23%|██▎       | 1886/8192 [00:36<02:36, 40.33it/s, est. speed input: 53171.28 toks/s, output: 51.93 toks/s]
Processed prompts:  23%|██▎       | 1898/8192 [00:36<02:36, 40.34it/s, est. speed input: 53075.03 toks/s, output: 51.83 toks/s]
Processed prompts:  23%|██▎       | 1910/8192 [00:36<02:35, 40.35it/s, est. speed input: 52980.59 toks/s, output: 51.74 toks/s]
Processed prompts:  23%|██▎       | 1922/8192 [00:37<02:35, 40.32it/s, est. speed input: 52886.39 toks/s, output: 51.65 toks/s]
Processed prompts:  24%|██▎       | 1934/8192 [00:37<02:35, 40.35it/s, est. speed input: 52795.36 toks/s, output: 51.56 toks/s]
Processed prompts:  24%|██▍       | 1946/8192 [00:37<02:34, 40.35it/s, est. speed input: 52705.18 toks/s, output: 51.47 toks/s]
Processed prompts:  24%|██▍       | 1958/8192 [00:38<02:34, 40.35it/s, est. speed input: 52616.13 toks/s, output: 51.38 toks/s]
Processed prompts:  24%|██▍       | 1970/8192 [00:38<02:34, 40.36it/s, est. speed input: 52528.92 toks/s, output: 51.30 toks/s]
Processed prompts:  24%|██▍       | 1982/8192 [00:38<02:33, 40.35it/s, est. speed input: 52442.50 toks/s, output: 51.21 toks/s]
Processed prompts:  24%|██▍       | 1994/8192 [00:38<02:33, 40.33it/s, est. speed input: 52357.08 toks/s, output: 51.13 toks/s]
Processed prompts:  24%|██▍       | 2006/8192 [00:39<02:33, 40.33it/s, est. speed input: 52273.24 toks/s, output: 51.05 toks/s]
Processed prompts:  25%|██▍       | 2018/8192 [00:39<02:33, 40.34it/s, est. speed input: 52191.01 toks/s, output: 50.97 toks/s]
Processed prompts:  25%|██▍       | 2030/8192 [00:39<02:32, 40.33it/s, est. speed input: 52109.62 toks/s, output: 50.89 toks/s]
Processed prompts:  25%|██▍       | 2042/8192 [00:40<02:32, 40.32it/s, est. speed input: 52029.27 toks/s, output: 50.81 toks/s]
Processed prompts:  25%|██▌       | 2054/8192 [00:40<02:32, 40.33it/s, est. speed input: 51950.75 toks/s, output: 50.73 toks/s]
Processed prompts:  25%|██▌       | 2066/8192 [00:40<02:31, 40.33it/s, est. speed input: 51872.97 toks/s, output: 50.66 toks/s]
Processed prompts:  25%|██▌       | 2078/8192 [00:41<02:31, 40.34it/s, est. speed input: 51796.78 toks/s, output: 50.58 toks/s]
Processed prompts:  26%|██▌       | 2090/8192 [00:41<02:31, 40.33it/s, est. speed input: 51721.14 toks/s, output: 50.51 toks/s]
Processed prompts:  26%|██▌       | 2102/8192 [00:41<02:31, 40.31it/s, est. speed input: 51646.07 toks/s, output: 50.44 toks/s]
Processed prompts:  26%|██▌       | 2114/8192 [00:41<02:30, 40.30it/s, est. speed input: 51572.16 toks/s, output: 50.36 toks/s]
Processed prompts:  26%|██▌       | 2126/8192 [00:42<02:30, 40.31it/s, est. speed input: 51499.87 toks/s, output: 50.29 toks/s]
Processed prompts:  26%|██▌       | 2138/8192 [00:42<02:30, 40.31it/s, est. speed input: 51428.30 toks/s, output: 50.22 toks/s]
Processed prompts:  26%|██▌       | 2150/8192 [00:42<02:29, 40.31it/s, est. speed input: 51357.85 toks/s, output: 50.15 toks/s]
Processed prompts:  26%|██▋       | 2162/8192 [00:43<02:29, 40.32it/s, est. speed input: 51288.64 toks/s, output: 50.09 toks/s]
Processed prompts:  27%|██▋       | 2174/8192 [00:43<02:29, 40.33it/s, est. speed input: 51220.37 toks/s, output: 50.02 toks/s]
Processed prompts:  27%|██▋       | 2186/8192 [00:43<02:28, 40.33it/s, est. speed input: 51153.02 toks/s, output: 49.95 toks/s]
Processed prompts:  27%|██▋       | 2198/8192 [00:44<02:28, 40.33it/s, est. speed input: 51086.33 toks/s, output: 49.89 toks/s]
Processed prompts:  27%|██▋       | 2210/8192 [00:44<02:28, 40.32it/s, est. speed input: 51020.53 toks/s, output: 49.82 toks/s]
Processed prompts:  27%|██▋       | 2222/8192 [00:44<02:28, 40.33it/s, est. speed input: 50955.76 toks/s, output: 49.76 toks/s]
Processed prompts:  27%|██▋       | 2234/8192 [00:44<02:27, 40.31it/s, est. speed input: 50891.26 toks/s, output: 49.70 toks/s]
Processed prompts:  27%|██▋       | 2246/8192 [00:45<02:27, 40.31it/s, est. speed input: 50828.12 toks/s, output: 49.64 toks/s]
Processed prompts:  28%|██▊       | 2258/8192 [00:45<02:27, 40.32it/s, est. speed input: 50765.86 toks/s, output: 49.58 toks/s]
Processed prompts:  28%|██▊       | 2270/8192 [00:45<02:26, 40.33it/s, est. speed input: 50704.52 toks/s, output: 49.52 toks/s]
Processed prompts:  28%|██▊       | 2282/8192 [00:46<02:26, 40.32it/s, est. speed input: 50643.81 toks/s, output: 49.46 toks/s]
Processed prompts:  28%|██▊       | 2294/8192 [00:46<02:26, 40.33it/s, est. speed input: 50583.92 toks/s, output: 49.40 toks/s]
Processed prompts:  28%|██▊       | 2306/8192 [00:46<02:25, 40.32it/s, est. speed input: 50524.63 toks/s, output: 49.34 toks/s]
Processed prompts:  28%|██▊       | 2318/8192 [00:47<02:25, 40.31it/s, est. speed input: 50465.89 toks/s, output: 49.28 toks/s]
Processed prompts:  28%|██▊       | 2330/8192 [00:47<02:25, 40.30it/s, est. speed input: 50407.90 toks/s, output: 49.23 toks/s]
Processed prompts:  29%|██▊       | 2342/8192 [00:47<02:25, 40.30it/s, est. speed input: 50350.75 toks/s, output: 49.17 toks/s]
Processed prompts:  29%|██▊       | 2354/8192 [00:47<02:24, 40.29it/s, est. speed input: 50293.94 toks/s, output: 49.12 toks/s]
Processed prompts:  29%|██▉       | 2366/8192 [00:48<02:24, 40.29it/s, est. speed input: 50238.16 toks/s, output: 49.06 toks/s]
Processed prompts:  29%|██▉       | 2378/8192 [00:48<02:24, 40.31it/s, est. speed input: 50183.48 toks/s, output: 49.01 toks/s]
Processed prompts:  29%|██▉       | 2390/8192 [00:48<02:23, 40.29it/s, est. speed input: 50128.81 toks/s, output: 48.95 toks/s]
Processed prompts:  29%|██▉       | 2402/8192 [00:49<02:23, 40.29it/s, est. speed input: 50075.05 toks/s, output: 48.90 toks/s]
Processed prompts:  29%|██▉       | 2414/8192 [00:49<02:23, 40.31it/s, est. speed input: 50022.24 toks/s, output: 48.85 toks/s]
Processed prompts:  30%|██▉       | 2426/8192 [00:49<02:23, 40.32it/s, est. speed input: 49970.07 toks/s, output: 48.80 toks/s]
Processed prompts:  30%|██▉       | 2438/8192 [00:50<02:22, 40.32it/s, est. speed input: 49918.39 toks/s, output: 48.75 toks/s]
Processed prompts:  30%|██▉       | 2450/8192 [00:50<02:22, 40.32it/s, est. speed input: 49867.31 toks/s, output: 48.70 toks/s]
Processed prompts:  30%|███       | 2462/8192 [00:50<02:22, 40.32it/s, est. speed input: 49816.82 toks/s, output: 48.65 toks/s]
Processed prompts:  30%|███       | 2474/8192 [00:50<02:21, 40.30it/s, est. speed input: 49766.46 toks/s, output: 48.60 toks/s]
Processed prompts:  30%|███       | 2486/8192 [00:51<02:21, 40.31it/s, est. speed input: 49717.37 toks/s, output: 48.55 toks/s]
Processed prompts:  30%|███       | 2498/8192 [00:51<02:21, 40.31it/s, est. speed input: 49668.58 toks/s, output: 48.50 toks/s]
Processed prompts:  31%|███       | 2510/8192 [00:51<02:20, 40.32it/s, est. speed input: 49620.59 toks/s, output: 48.46 toks/s]
Processed prompts:  31%|███       | 2522/8192 [00:52<02:20, 40.31it/s, est. speed input: 49572.67 toks/s, output: 48.41 toks/s]
Processed prompts:  31%|███       | 2534/8192 [00:52<02:20, 40.31it/s, est. speed input: 49525.52 toks/s, output: 48.36 toks/s]
Processed prompts:  31%|███       | 2546/8192 [00:52<02:20, 40.31it/s, est. speed input: 49478.92 toks/s, output: 48.32 toks/s]
Processed prompts:  31%|███       | 2558/8192 [00:52<02:19, 40.31it/s, est. speed input: 49432.98 toks/s, output: 48.27 toks/s]
Processed prompts:  31%|███▏      | 2570/8192 [00:53<02:19, 40.31it/s, est. speed input: 49387.23 toks/s, output: 48.23 toks/s]
Processed prompts:  32%|███▏      | 2582/8192 [00:53<02:19, 40.31it/s, est. speed input: 49342.18 toks/s, output: 48.19 toks/s]
Processed prompts:  32%|███▏      | 2594/8192 [00:53<02:18, 40.29it/s, est. speed input: 49297.31 toks/s, output: 48.14 toks/s]
Processed prompts:  32%|███▏      | 2606/8192 [00:54<02:18, 40.29it/s, est. speed input: 49252.99 toks/s, output: 48.10 toks/s]
Processed prompts:  32%|███▏      | 2618/8192 [00:54<02:18, 40.29it/s, est. speed input: 49209.37 toks/s, output: 48.06 toks/s]
Processed prompts:  32%|███▏      | 2630/8192 [00:54<02:18, 40.29it/s, est. speed input: 49166.07 toks/s, output: 48.01 toks/s]
Processed prompts:  32%|███▏      | 2642/8192 [00:55<02:17, 40.29it/s, est. speed input: 49123.24 toks/s, output: 47.97 toks/s]
Processed prompts:  32%|███▏      | 2654/8192 [00:55<02:17, 40.28it/s, est. speed input: 49080.85 toks/s, output: 47.93 toks/s]
Processed prompts:  33%|███▎      | 2666/8192 [00:55<02:17, 40.29it/s, est. speed input: 49039.13 toks/s, output: 47.89 toks/s]
Processed prompts:  33%|███▎      | 2678/8192 [00:55<02:16, 40.30it/s, est. speed input: 48997.90 toks/s, output: 47.85 toks/s]
Processed prompts:  33%|███▎      | 2690/8192 [00:56<02:16, 40.31it/s, est. speed input: 48957.17 toks/s, output: 47.81 toks/s]
Processed prompts:  33%|███▎      | 2702/8192 [00:56<02:16, 40.30it/s, est. speed input: 48916.58 toks/s, output: 47.77 toks/s]
Processed prompts:  33%|███▎      | 2714/8192 [00:56<02:15, 40.29it/s, est. speed input: 48876.22 toks/s, output: 47.73 toks/s]
Processed prompts:  33%|███▎      | 2726/8192 [00:57<02:15, 40.29it/s, est. speed input: 48836.53 toks/s, output: 47.69 toks/s]
Processed prompts:  33%|███▎      | 2738/8192 [00:57<02:15, 40.30it/s, est. speed input: 48797.39 toks/s, output: 47.65 toks/s]
Processed prompts:  34%|███▎      | 2750/8192 [00:57<02:15, 40.30it/s, est. speed input: 48758.67 toks/s, output: 47.62 toks/s]
Processed prompts:  34%|███▎      | 2762/8192 [00:58<02:14, 40.30it/s, est. speed input: 48720.26 toks/s, output: 47.58 toks/s]
Processed prompts:  34%|███▍      | 2774/8192 [00:58<02:14, 40.31it/s, est. speed input: 48682.37 toks/s, output: 47.54 toks/s]
Processed prompts:  34%|███▍      | 2786/8192 [00:58<02:14, 40.29it/s, est. speed input: 48644.49 toks/s, output: 47.50 toks/s]
Processed prompts:  34%|███▍      | 2798/8192 [00:58<02:13, 40.30it/s, est. speed input: 48607.30 toks/s, output: 47.47 toks/s]
Processed prompts:  34%|███▍      | 2810/8192 [00:59<02:13, 40.29it/s, est. speed input: 48570.21 toks/s, output: 47.43 toks/s]
Processed prompts:  34%|███▍      | 2822/8192 [00:59<02:13, 40.29it/s, est. speed input: 48533.66 toks/s, output: 47.40 toks/s]
Processed prompts:  35%|███▍      | 2834/8192 [00:59<02:13, 40.28it/s, est. speed input: 48497.20 toks/s, output: 47.36 toks/s]
Processed prompts:  35%|███▍      | 2846/8192 [01:00<02:12, 40.28it/s, est. speed input: 48461.32 toks/s, output: 47.33 toks/s]
Processed prompts:  35%|███▍      | 2858/8192 [01:00<02:12, 40.29it/s, est. speed input: 48425.97 toks/s, output: 47.29 toks/s]
Processed prompts:  35%|███▌      | 2870/8192 [01:00<02:12, 40.28it/s, est. speed input: 48390.63 toks/s, output: 47.26 toks/s]
Processed prompts:  35%|███▌      | 2882/8192 [01:01<02:11, 40.28it/s, est. speed input: 48355.76 toks/s, output: 47.22 toks/s]
Processed prompts:  35%|███▌      | 2894/8192 [01:01<02:11, 40.29it/s, est. speed input: 48321.37 toks/s, output: 47.19 toks/s]
Processed prompts:  35%|███▌      | 2906/8192 [01:01<02:11, 40.29it/s, est. speed input: 48287.27 toks/s, output: 47.16 toks/s]
Processed prompts:  36%|███▌      | 2918/8192 [01:01<02:10, 40.29it/s, est. speed input: 48253.51 toks/s, output: 47.12 toks/s]
Processed prompts:  36%|███▌      | 2930/8192 [01:02<02:10, 40.29it/s, est. speed input: 48220.05 toks/s, output: 47.09 toks/s]
Processed prompts:  36%|███▌      | 2942/8192 [01:02<02:10, 40.31it/s, est. speed input: 48187.28 toks/s, output: 47.06 toks/s]
Processed prompts:  36%|███▌      | 2954/8192 [01:02<02:09, 40.30it/s, est. speed input: 48154.23 toks/s, output: 47.03 toks/s]
Processed prompts:  36%|███▌      | 2966/8192 [01:03<02:09, 40.28it/s, est. speed input: 48121.41 toks/s, output: 46.99 toks/s]
Processed prompts:  36%|███▋      | 2978/8192 [01:03<02:09, 40.28it/s, est. speed input: 48089.05 toks/s, output: 46.96 toks/s]
Processed prompts:  36%|███▋      | 2990/8192 [01:03<02:09, 40.26it/s, est. speed input: 48056.78 toks/s, output: 46.93 toks/s]
Processed prompts:  37%|███▋      | 3002/8192 [01:04<02:08, 40.29it/s, est. speed input: 48025.42 toks/s, output: 46.90 toks/s]
Processed prompts:  37%|███▋      | 3014/8192 [01:04<02:08, 40.29it/s, est. speed input: 47994.16 toks/s, output: 46.87 toks/s]
Processed prompts:  37%|███▋      | 3026/8192 [01:04<02:08, 40.29it/s, est. speed input: 47963.16 toks/s, output: 46.84 toks/s]
Processed prompts:  37%|███▋      | 3038/8192 [01:04<02:07, 40.28it/s, est. speed input: 47932.14 toks/s, output: 46.81 toks/s]
Processed prompts:  37%|███▋      | 3050/8192 [01:05<02:07, 40.28it/s, est. speed input: 47901.53 toks/s, output: 46.78 toks/s]
Processed prompts:  37%|███▋      | 3062/8192 [01:05<02:07, 40.27it/s, est. speed input: 47871.11 toks/s, output: 46.75 toks/s]
Processed prompts:  38%|███▊      | 3074/8192 [01:05<02:07, 40.28it/s, est. speed input: 47841.18 toks/s, output: 46.72 toks/s]
Processed prompts:  38%|███▊      | 3086/8192 [01:06<02:06, 40.29it/s, est. speed input: 47811.61 toks/s, output: 46.69 toks/s]
Processed prompts:  38%|███▊      | 3098/8192 [01:06<02:06, 40.29it/s, est. speed input: 47782.25 toks/s, output: 46.66 toks/s]
Processed prompts:  38%|███▊      | 3110/8192 [01:06<02:06, 40.29it/s, est. speed input: 47753.10 toks/s, output: 46.63 toks/s]
Processed prompts:  38%|███▊      | 3122/8192 [01:06<02:05, 40.30it/s, est. speed input: 47724.38 toks/s, output: 46.61 toks/s]
Processed prompts:  38%|███▊      | 3134/8192 [01:07<02:05, 40.29it/s, est. speed input: 47695.72 toks/s, output: 46.58 toks/s]
Processed prompts:  38%|███▊      | 3146/8192 [01:07<02:05, 40.30it/s, est. speed input: 47667.48 toks/s, output: 46.55 toks/s]
Processed prompts:  39%|███▊      | 3158/8192 [01:07<02:04, 40.31it/s, est. speed input: 47639.58 toks/s, output: 46.52 toks/s]
Processed prompts:  39%|███▊      | 3170/8192 [01:08<02:04, 40.30it/s, est. speed input: 47611.57 toks/s, output: 46.50 toks/s]
Processed prompts:  39%|███▉      | 3182/8192 [01:08<02:04, 40.30it/s, est. speed input: 47584.08 toks/s, output: 46.47 toks/s]
Processed prompts:  39%|███▉      | 3194/8192 [01:08<02:04, 40.29it/s, est. speed input: 47556.56 toks/s, output: 46.44 toks/s]
Processed prompts:  39%|███▉      | 3206/8192 [01:09<02:03, 40.28it/s, est. speed input: 47529.17 toks/s, output: 46.42 toks/s]
Processed prompts:  39%|███▉      | 3218/8192 [01:09<02:03, 40.27it/s, est. speed input: 47502.11 toks/s, output: 46.39 toks/s]
Processed prompts:  39%|███▉      | 3230/8192 [01:09<02:03, 40.29it/s, est. speed input: 47475.60 toks/s, output: 46.36 toks/s]
Processed prompts:  40%|███▉      | 3242/8192 [01:09<02:02, 40.29it/s, est. speed input: 47449.12 toks/s, output: 46.34 toks/s]
Processed prompts:  40%|███▉      | 3254/8192 [01:10<02:02, 40.29it/s, est. speed input: 47422.85 toks/s, output: 46.31 toks/s]
Processed prompts:  40%|███▉      | 3266/8192 [01:10<02:02, 40.30it/s, est. speed input: 47396.94 toks/s, output: 46.29 toks/s]
Processed prompts:  40%|████      | 3278/8192 [01:10<02:01, 40.30it/s, est. speed input: 47371.27 toks/s, output: 46.26 toks/s]
Processed prompts:  40%|████      | 3290/8192 [01:11<02:01, 40.30it/s, est. speed input: 47345.63 toks/s, output: 46.24 toks/s]
Processed prompts:  40%|████      | 3302/8192 [01:11<02:01, 40.29it/s, est. speed input: 47320.21 toks/s, output: 46.21 toks/s]
Processed prompts:  40%|████      | 3314/8192 [01:11<02:01, 40.28it/s, est. speed input: 47294.82 toks/s, output: 46.19 toks/s]
Processed prompts:  41%|████      | 3326/8192 [01:12<02:00, 40.26it/s, est. speed input: 47269.47 toks/s, output: 46.16 toks/s]
Processed prompts:  41%|████      | 3338/8192 [01:12<02:00, 40.26it/s, est. speed input: 47244.60 toks/s, output: 46.14 toks/s]
Processed prompts:  41%|████      | 3350/8192 [01:12<02:00, 40.28it/s, est. speed input: 47220.16 toks/s, output: 46.11 toks/s]
Processed prompts:  41%|████      | 3362/8192 [01:12<01:59, 40.29it/s, est. speed input: 47195.93 toks/s, output: 46.09 toks/s]
Processed prompts:  41%|████      | 3374/8192 [01:13<01:59, 40.27it/s, est. speed input: 47171.55 toks/s, output: 46.07 toks/s]
Processed prompts:  41%|████▏     | 3386/8192 [01:13<01:59, 40.27it/s, est. speed input: 47147.49 toks/s, output: 46.04 toks/s]
Processed prompts:  41%|████▏     | 3398/8192 [01:13<01:59, 40.27it/s, est. speed input: 47123.65 toks/s, output: 46.02 toks/s]
Processed prompts:  42%|████▏     | 3410/8192 [01:14<01:58, 40.28it/s, est. speed input: 47100.18 toks/s, output: 46.00 toks/s]
Processed prompts:  42%|████▏     | 3422/8192 [01:14<01:58, 40.29it/s, est. speed input: 47076.82 toks/s, output: 45.97 toks/s]
Processed prompts:  42%|████▏     | 3434/8192 [01:14<01:58, 40.28it/s, est. speed input: 47053.49 toks/s, output: 45.95 toks/s]
Processed prompts:  42%|████▏     | 3446/8192 [01:15<01:57, 40.28it/s, est. speed input: 47030.45 toks/s, output: 45.93 toks/s]
Processed prompts:  42%|████▏     | 3458/8192 [01:15<01:57, 40.27it/s, est. speed input: 47007.43 toks/s, output: 45.91 toks/s]
Processed prompts:  42%|████▏     | 3470/8192 [01:15<01:57, 40.28it/s, est. speed input: 46984.79 toks/s, output: 45.88 toks/s]
Processed prompts:  43%|████▎     | 3482/8192 [01:15<01:56, 40.28it/s, est. speed input: 46962.34 toks/s, output: 45.86 toks/s]
Processed prompts:  43%|████▎     | 3494/8192 [01:16<01:56, 40.28it/s, est. speed input: 46939.91 toks/s, output: 45.84 toks/s]
Processed prompts:  43%|████▎     | 3506/8192 [01:16<01:56, 40.28it/s, est. speed input: 46917.83 toks/s, output: 45.82 toks/s]
Processed prompts:  43%|████▎     | 3518/8192 [01:16<01:56, 40.29it/s, est. speed input: 46895.89 toks/s, output: 45.80 toks/s]
Processed prompts:  43%|████▎     | 3530/8192 [01:17<01:55, 40.29it/s, est. speed input: 46874.13 toks/s, output: 45.78 toks/s]
Processed prompts:  43%|████▎     | 3542/8192 [01:17<01:55, 40.26it/s, est. speed input: 46852.13 toks/s, output: 45.75 toks/s]
Processed prompts:  43%|████▎     | 3554/8192 [01:17<01:55, 40.26it/s, est. speed input: 46830.59 toks/s, output: 45.73 toks/s]
Processed prompts:  44%|████▎     | 3566/8192 [01:18<01:54, 40.24it/s, est. speed input: 46808.84 toks/s, output: 45.71 toks/s]
Processed prompts:  44%|████▎     | 3578/8192 [01:18<01:54, 40.26it/s, est. speed input: 46787.77 toks/s, output: 45.69 toks/s]
Processed prompts:  44%|████▍     | 3590/8192 [01:18<01:54, 40.27it/s, est. speed input: 46766.85 toks/s, output: 45.67 toks/s]
Processed prompts:  44%|████▍     | 3602/8192 [01:18<01:53, 40.27it/s, est. speed input: 46745.96 toks/s, output: 45.65 toks/s]
Processed prompts:  44%|████▍     | 3614/8192 [01:19<01:53, 40.28it/s, est. speed input: 46725.43 toks/s, output: 45.63 toks/s]
Processed prompts:  44%|████▍     | 3626/8192 [01:19<01:53, 40.26it/s, est. speed input: 46704.63 toks/s, output: 45.61 toks/s]
Processed prompts:  44%|████▍     | 3638/8192 [01:19<01:53, 40.27it/s, est. speed input: 46684.23 toks/s, output: 45.59 toks/s]
Processed prompts:  45%|████▍     | 3650/8192 [01:20<01:52, 40.27it/s, est. speed input: 46663.96 toks/s, output: 45.57 toks/s]
Processed prompts:  45%|████▍     | 3662/8192 [01:20<01:52, 40.26it/s, est. speed input: 46643.75 toks/s, output: 45.55 toks/s]
Processed prompts:  45%|████▍     | 3674/8192 [01:20<01:52, 40.27it/s, est. speed input: 46623.93 toks/s, output: 45.53 toks/s]
Processed prompts:  45%|████▍     | 3686/8192 [01:20<01:51, 40.26it/s, est. speed input: 46603.95 toks/s, output: 45.51 toks/s]
Processed prompts:  45%|████▌     | 3698/8192 [01:21<01:51, 40.27it/s, est. speed input: 46584.38 toks/s, output: 45.49 toks/s]
Processed prompts:  45%|████▌     | 3710/8192 [01:21<01:51, 40.26it/s, est. speed input: 46564.70 toks/s, output: 45.47 toks/s]
Processed prompts:  45%|████▌     | 3722/8192 [01:21<01:50, 40.28it/s, est. speed input: 46545.50 toks/s, output: 45.45 toks/s]
Processed prompts:  46%|████▌     | 3734/8192 [01:22<01:50, 40.27it/s, est. speed input: 46526.18 toks/s, output: 45.44 toks/s]
Processed prompts:  46%|████▌     | 3746/8192 [01:22<01:50, 40.26it/s, est. speed input: 46506.97 toks/s, output: 45.42 toks/s]
Processed prompts:  46%|████▌     | 3758/8192 [01:22<01:50, 40.28it/s, est. speed input: 46488.19 toks/s, output: 45.40 toks/s]
Processed prompts:  46%|████▌     | 3770/8192 [01:23<01:49, 40.28it/s, est. speed input: 46469.38 toks/s, output: 45.38 toks/s]
Processed prompts:  46%|████▌     | 3782/8192 [01:23<01:49, 40.28it/s, est. speed input: 46450.70 toks/s, output: 45.36 toks/s]
Processed prompts:  46%|████▋     | 3794/8192 [01:23<01:49, 40.26it/s, est. speed input: 46431.97 toks/s, output: 45.34 toks/s]
Processed prompts:  46%|████▋     | 3806/8192 [01:23<01:48, 40.25it/s, est. speed input: 46413.29 toks/s, output: 45.33 toks/s]
Processed prompts:  47%|████▋     | 3818/8192 [01:24<01:48, 40.25it/s, est. speed input: 46394.91 toks/s, output: 45.31 toks/s]
Processed prompts:  47%|████▋     | 3830/8192 [01:24<01:48, 40.26it/s, est. speed input: 46376.80 toks/s, output: 45.29 toks/s]
Processed prompts:  47%|████▋     | 3842/8192 [01:24<01:48, 40.27it/s, est. speed input: 46358.86 toks/s, output: 45.27 toks/s]
Processed prompts:  47%|████▋     | 3854/8192 [01:25<01:47, 40.27it/s, est. speed input: 46340.93 toks/s, output: 45.25 toks/s]
Processed prompts:  47%|████▋     | 3866/8192 [01:25<01:47, 40.27it/s, est. speed input: 46323.17 toks/s, output: 45.24 toks/s]
Processed prompts:  47%|████▋     | 3878/8192 [01:25<01:47, 40.27it/s, est. speed input: 46305.46 toks/s, output: 45.22 toks/s]
Processed prompts:  47%|████▋     | 3890/8192 [01:26<01:46, 40.28it/s, est. speed input: 46288.06 toks/s, output: 45.20 toks/s]
Processed prompts:  48%|████▊     | 3902/8192 [01:26<01:46, 40.28it/s, est. speed input: 46270.62 toks/s, output: 45.19 toks/s]
Processed prompts:  48%|████▊     | 3914/8192 [01:26<01:46, 40.27it/s, est. speed input: 46253.23 toks/s, output: 45.17 toks/s]
Processed prompts:  48%|████▊     | 3926/8192 [01:26<01:45, 40.26it/s, est. speed input: 46235.87 toks/s, output: 45.15 toks/s]
Processed prompts:  48%|████▊     | 3938/8192 [01:27<01:45, 40.26it/s, est. speed input: 46218.74 toks/s, output: 45.14 toks/s]
Processed prompts:  48%|████▊     | 3950/8192 [01:27<01:45, 40.25it/s, est. speed input: 46201.68 toks/s, output: 45.12 toks/s]
Processed prompts:  48%|████▊     | 3962/8192 [01:27<01:45, 40.25it/s, est. speed input: 46184.77 toks/s, output: 45.10 toks/s]
Processed prompts:  49%|████▊     | 3974/8192 [01:28<01:44, 40.27it/s, est. speed input: 46168.19 toks/s, output: 45.09 toks/s]
Processed prompts:  49%|████▊     | 3986/8192 [01:28<01:44, 40.28it/s, est. speed input: 46151.66 toks/s, output: 45.07 toks/s]
Processed prompts:  49%|████▉     | 3998/8192 [01:28<01:44, 40.27it/s, est. speed input: 46135.16 toks/s, output: 45.05 toks/s]
Processed prompts:  49%|████▉     | 4010/8192 [01:29<01:43, 40.29it/s, est. speed input: 46118.95 toks/s, output: 45.04 toks/s]
Processed prompts:  49%|████▉     | 4022/8192 [01:29<01:43, 40.28it/s, est. speed input: 46102.68 toks/s, output: 45.02 toks/s]
Processed prompts:  49%|████▉     | 4034/8192 [01:29<01:43, 40.28it/s, est. speed input: 46086.52 toks/s, output: 45.01 toks/s]
Processed prompts:  49%|████▉     | 4046/8192 [01:29<01:42, 40.26it/s, est. speed input: 46070.17 toks/s, output: 44.99 toks/s]
Processed prompts:  50%|████▉     | 4058/8192 [01:30<01:42, 40.27it/s, est. speed input: 46054.28 toks/s, output: 44.97 toks/s]
Processed prompts:  50%|████▉     | 4070/8192 [01:30<01:42, 40.26it/s, est. speed input: 46038.38 toks/s, output: 44.96 toks/s]
Processed prompts:  50%|████▉     | 4082/8192 [01:30<01:42, 40.28it/s, est. speed input: 46022.77 toks/s, output: 44.94 toks/s]
Processed prompts:  50%|████▉     | 4094/8192 [01:31<01:41, 40.28it/s, est. speed input: 46007.23 toks/s, output: 44.93 toks/s]
Processed prompts:  50%|█████     | 4106/8192 [01:31<01:41, 40.28it/s, est. speed input: 45991.70 toks/s, output: 44.91 toks/s]
Processed prompts:  50%|█████     | 4118/8192 [01:31<01:41, 40.26it/s, est. speed input: 45976.04 toks/s, output: 44.90 toks/s]
Processed prompts:  50%|█████     | 4130/8192 [01:32<01:40, 40.26it/s, est. speed input: 45960.66 toks/s, output: 44.88 toks/s]
Processed prompts:  51%|█████     | 4142/8192 [01:32<01:40, 40.25it/s, est. speed input: 45945.26 toks/s, output: 44.87 toks/s]
Processed prompts:  51%|█████     | 4154/8192 [01:32<01:40, 40.27it/s, est. speed input: 45930.26 toks/s, output: 44.85 toks/s]
Processed prompts:  51%|█████     | 4166/8192 [01:32<01:40, 40.24it/s, est. speed input: 45914.88 toks/s, output: 44.84 toks/s]
Processed prompts:  51%|█████     | 4178/8192 [01:33<01:39, 40.23it/s, est. speed input: 45899.68 toks/s, output: 44.82 toks/s]
Processed prompts:  51%|█████     | 4190/8192 [01:33<01:39, 40.25it/s, est. speed input: 45884.86 toks/s, output: 44.81 toks/s]
Processed prompts:  51%|█████▏    | 4202/8192 [01:33<01:39, 40.25it/s, est. speed input: 45870.01 toks/s, output: 44.79 toks/s]
Processed prompts:  51%|█████▏    | 4214/8192 [01:34<01:38, 40.24it/s, est. speed input: 45855.22 toks/s, output: 44.78 toks/s]
Processed prompts:  52%|█████▏    | 4226/8192 [01:34<01:38, 40.25it/s, est. speed input: 45840.65 toks/s, output: 44.77 toks/s]
Processed prompts:  52%|█████▏    | 4238/8192 [01:34<01:38, 40.27it/s, est. speed input: 45826.28 toks/s, output: 44.75 toks/s]
Processed prompts:  52%|█████▏    | 4250/8192 [01:34<01:37, 40.27it/s, est. speed input: 45811.87 toks/s, output: 44.74 toks/s]
Processed prompts:  52%|█████▏    | 4262/8192 [01:35<01:37, 40.27it/s, est. speed input: 45797.56 toks/s, output: 44.72 toks/s]
Processed prompts:  52%|█████▏    | 4274/8192 [01:35<01:37, 40.28it/s, est. speed input: 45783.42 toks/s, output: 44.71 toks/s]
Processed prompts:  52%|█████▏    | 4286/8192 [01:35<01:37, 40.26it/s, est. speed input: 45769.09 toks/s, output: 44.70 toks/s]
Processed prompts:  52%|█████▏    | 4298/8192 [01:36<01:36, 40.26it/s, est. speed input: 45755.03 toks/s, output: 44.68 toks/s]
Processed prompts:  53%|█████▎    | 4310/8192 [01:36<01:36, 40.27it/s, est. speed input: 45741.14 toks/s, output: 44.67 toks/s]
Processed prompts:  53%|█████▎    | 4322/8192 [01:36<01:36, 40.28it/s, est. speed input: 45727.43 toks/s, output: 44.66 toks/s]
Processed prompts:  53%|█████▎    | 4334/8192 [01:37<01:35, 40.27it/s, est. speed input: 45713.58 toks/s, output: 44.64 toks/s]
Processed prompts:  53%|█████▎    | 4346/8192 [01:37<01:35, 40.26it/s, est. speed input: 45699.80 toks/s, output: 44.63 toks/s]
Processed prompts:  53%|█████▎    | 4358/8192 [01:37<01:35, 40.26it/s, est. speed input: 45686.13 toks/s, output: 44.62 toks/s]
Processed prompts:  53%|█████▎    | 4370/8192 [01:37<01:34, 40.26it/s, est. speed input: 45672.52 toks/s, output: 44.60 toks/s]
Processed prompts:  53%|█████▎    | 4382/8192 [01:38<01:34, 40.25it/s, est. speed input: 45658.99 toks/s, output: 44.59 toks/s]
Processed prompts:  54%|█████▎    | 4394/8192 [01:38<01:34, 40.26it/s, est. speed input: 45645.65 toks/s, output: 44.58 toks/s]
Processed prompts:  54%|█████▍    | 4406/8192 [01:38<01:34, 40.23it/s, est. speed input: 45632.02 toks/s, output: 44.56 toks/s]
Processed prompts:  54%|█████▍    | 4418/8192 [01:39<01:33, 40.23it/s, est. speed input: 45618.67 toks/s, output: 44.55 toks/s]
Processed prompts:  54%|█████▍    | 4430/8192 [01:39<01:33, 40.23it/s, est. speed input: 45605.39 toks/s, output: 44.54 toks/s]
Processed prompts:  54%|█████▍    | 4442/8192 [01:39<01:33, 40.23it/s, est. speed input: 45592.23 toks/s, output: 44.52 toks/s]
Processed prompts:  54%|█████▍    | 4454/8192 [01:40<01:32, 40.24it/s, est. speed input: 45579.18 toks/s, output: 44.51 toks/s]
Processed prompts:  55%|█████▍    | 4466/8192 [01:40<01:32, 40.24it/s, est. speed input: 45566.17 toks/s, output: 44.50 toks/s]
Processed prompts:  55%|█████▍    | 4478/8192 [01:40<01:32, 40.25it/s, est. speed input: 45553.37 toks/s, output: 44.49 toks/s]
Processed prompts:  55%|█████▍    | 4490/8192 [01:40<01:31, 40.26it/s, est. speed input: 45540.70 toks/s, output: 44.47 toks/s]
Processed prompts:  55%|█████▍    | 4502/8192 [01:41<01:31, 40.26it/s, est. speed input: 45528.01 toks/s, output: 44.46 toks/s]
Processed prompts:  55%|█████▌    | 4514/8192 [01:41<01:31, 40.25it/s, est. speed input: 45515.29 toks/s, output: 44.45 toks/s]
Processed prompts:  55%|█████▌    | 4526/8192 [01:41<01:31, 40.24it/s, est. speed input: 45502.61 toks/s, output: 44.44 toks/s]
Processed prompts:  55%|█████▌    | 4538/8192 [01:42<01:30, 40.24it/s, est. speed input: 45490.06 toks/s, output: 44.42 toks/s]
Processed prompts:  56%|█████▌    | 4550/8192 [01:42<01:30, 40.24it/s, est. speed input: 45477.54 toks/s, output: 44.41 toks/s]
Processed prompts:  56%|█████▌    | 4562/8192 [01:42<01:30, 40.26it/s, est. speed input: 45465.36 toks/s, output: 44.40 toks/s]
Processed prompts:  56%|█████▌    | 4574/8192 [01:43<01:29, 40.27it/s, est. speed input: 45453.23 toks/s, output: 44.39 toks/s]
Processed prompts:  56%|█████▌    | 4586/8192 [01:43<01:29, 40.26it/s, est. speed input: 45441.02 toks/s, output: 44.38 toks/s]
Processed prompts:  56%|█████▌    | 4598/8192 [01:43<01:29, 40.26it/s, est. speed input: 45428.87 toks/s, output: 44.36 toks/s]
Processed prompts:  56%|█████▋    | 4610/8192 [01:43<01:28, 40.27it/s, est. speed input: 45416.92 toks/s, output: 44.35 toks/s]
Processed prompts:  56%|█████▋    | 4622/8192 [01:44<01:28, 40.26it/s, est. speed input: 45404.88 toks/s, output: 44.34 toks/s]
Processed prompts:  57%|█████▋    | 4634/8192 [01:44<01:28, 40.27it/s, est. speed input: 45393.04 toks/s, output: 44.33 toks/s]
Processed prompts:  57%|█████▋    | 4646/8192 [01:44<01:28, 40.24it/s, est. speed input: 45380.93 toks/s, output: 44.32 toks/s]
Processed prompts:  57%|█████▋    | 4658/8192 [01:45<01:27, 40.22it/s, est. speed input: 45368.91 toks/s, output: 44.31 toks/s]
Processed prompts:  57%|█████▋    | 4670/8192 [01:45<01:27, 40.23it/s, est. speed input: 45357.17 toks/s, output: 44.29 toks/s]
Processed prompts:  57%|█████▋    | 4682/8192 [01:45<01:27, 40.24it/s, est. speed input: 45345.53 toks/s, output: 44.28 toks/s]
Processed prompts:  57%|█████▋    | 4694/8192 [01:46<01:26, 40.25it/s, est. speed input: 45333.97 toks/s, output: 44.27 toks/s]
Processed prompts:  57%|█████▋    | 4706/8192 [01:46<01:26, 40.25it/s, est. speed input: 45322.39 toks/s, output: 44.26 toks/s]
Processed prompts:  58%|█████▊    | 4718/8192 [01:46<01:26, 40.23it/s, est. speed input: 45310.78 toks/s, output: 44.25 toks/s]
Processed prompts:  58%|█████▊    | 4730/8192 [01:46<01:26, 40.24it/s, est. speed input: 45299.33 toks/s, output: 44.24 toks/s]
Processed prompts:  58%|█████▊    | 4742/8192 [01:47<01:25, 40.24it/s, est. speed input: 45288.01 toks/s, output: 44.23 toks/s]
Processed prompts:  58%|█████▊    | 4754/8192 [01:47<01:25, 40.26it/s, est. speed input: 45276.83 toks/s, output: 44.22 toks/s]
Processed prompts:  58%|█████▊    | 4766/8192 [01:47<01:25, 40.25it/s, est. speed input: 45265.57 toks/s, output: 44.20 toks/s]
Processed prompts:  58%|█████▊    | 4778/8192 [01:48<01:24, 40.24it/s, est. speed input: 45254.30 toks/s, output: 44.19 toks/s]
Processed prompts:  58%|█████▊    | 4790/8192 [01:48<01:24, 40.24it/s, est. speed input: 45243.21 toks/s, output: 44.18 toks/s]
Processed prompts:  59%|█████▊    | 4802/8192 [01:48<01:24, 40.24it/s, est. speed input: 45232.06 toks/s, output: 44.17 toks/s]
Processed prompts:  59%|█████▉    | 4814/8192 [01:49<01:23, 40.25it/s, est. speed input: 45221.14 toks/s, output: 44.16 toks/s]
Processed prompts:  59%|█████▉    | 4826/8192 [01:49<01:23, 40.26it/s, est. speed input: 45210.33 toks/s, output: 44.15 toks/s]
Processed prompts:  59%|█████▉    | 4838/8192 [01:49<01:23, 40.26it/s, est. speed input: 45199.50 toks/s, output: 44.14 toks/s]
Processed prompts:  59%|█████▉    | 4850/8192 [01:49<01:23, 40.26it/s, est. speed input: 45188.68 toks/s, output: 44.13 toks/s]
Processed prompts:  59%|█████▉    | 4862/8192 [01:50<01:22, 40.25it/s, est. speed input: 45177.94 toks/s, output: 44.12 toks/s]
Processed prompts:  59%|█████▉    | 4874/8192 [01:50<01:22, 40.26it/s, est. speed input: 45167.32 toks/s, output: 44.11 toks/s]
Processed prompts:  60%|█████▉    | 4886/8192 [01:50<01:22, 40.25it/s, est. speed input: 45156.65 toks/s, output: 44.10 toks/s]
Processed prompts:  60%|█████▉    | 4898/8192 [01:51<01:21, 40.25it/s, est. speed input: 45146.02 toks/s, output: 44.09 toks/s]
Processed prompts:  60%|█████▉    | 4910/8192 [01:51<01:21, 40.25it/s, est. speed input: 45135.51 toks/s, output: 44.08 toks/s]
Processed prompts:  60%|██████    | 4922/8192 [01:51<01:21, 40.25it/s, est. speed input: 45125.07 toks/s, output: 44.07 toks/s]
Processed prompts:  60%|██████    | 4934/8192 [01:51<01:20, 40.25it/s, est. speed input: 45114.66 toks/s, output: 44.06 toks/s]
Processed prompts:  60%|██████    | 4946/8192 [01:52<01:20, 40.25it/s, est. speed input: 45104.30 toks/s, output: 44.05 toks/s]
Processed prompts:  61%|██████    | 4958/8192 [01:52<01:20, 40.25it/s, est. speed input: 45093.98 toks/s, output: 44.04 toks/s]
Processed prompts:  61%|██████    | 4970/8192 [01:52<01:20, 40.25it/s, est. speed input: 45083.77 toks/s, output: 44.03 toks/s]
Processed prompts:  61%|██████    | 4982/8192 [01:53<01:19, 40.25it/s, est. speed input: 45073.61 toks/s, output: 44.02 toks/s]
Processed prompts:  61%|██████    | 4994/8192 [01:53<01:19, 40.26it/s, est. speed input: 45063.50 toks/s, output: 44.01 toks/s]
Processed prompts:  61%|██████    | 5006/8192 [01:53<01:19, 40.24it/s, est. speed input: 45053.27 toks/s, output: 44.00 toks/s]
Processed prompts:  61%|██████▏   | 5018/8192 [01:54<01:18, 40.24it/s, est. speed input: 45043.25 toks/s, output: 43.99 toks/s]
Processed prompts:  61%|██████▏   | 5030/8192 [01:54<01:18, 40.24it/s, est. speed input: 45033.24 toks/s, output: 43.98 toks/s]
Processed prompts:  62%|██████▏   | 5042/8192 [01:54<01:18, 40.24it/s, est. speed input: 45023.30 toks/s, output: 43.97 toks/s]
Processed prompts:  62%|██████▏   | 5054/8192 [01:54<01:17, 40.25it/s, est. speed input: 45013.49 toks/s, output: 43.96 toks/s]
Processed prompts:  62%|██████▏   | 5066/8192 [01:55<01:17, 40.26it/s, est. speed input: 45003.73 toks/s, output: 43.95 toks/s]
Processed prompts:  62%|██████▏   | 5078/8192 [01:55<01:17, 40.25it/s, est. speed input: 44993.94 toks/s, output: 43.94 toks/s]
Processed prompts:  62%|██████▏   | 5090/8192 [01:55<01:17, 40.25it/s, est. speed input: 44984.18 toks/s, output: 43.93 toks/s]
Processed prompts:  62%|██████▏   | 5102/8192 [01:56<01:16, 40.25it/s, est. speed input: 44974.50 toks/s, output: 43.92 toks/s]
Processed prompts:  62%|██████▏   | 5114/8192 [01:56<01:16, 40.25it/s, est. speed input: 44964.93 toks/s, output: 43.91 toks/s]
Processed prompts:  63%|██████▎   | 5126/8192 [01:56<01:16, 40.24it/s, est. speed input: 44955.26 toks/s, output: 43.90 toks/s]
Processed prompts:  63%|██████▎   | 5138/8192 [01:57<01:15, 40.24it/s, est. speed input: 44945.73 toks/s, output: 43.89 toks/s]
Processed prompts:  63%|██████▎   | 5150/8192 [01:57<01:15, 40.24it/s, est. speed input: 44936.22 toks/s, output: 43.88 toks/s]
Processed prompts:  63%|██████▎   | 5162/8192 [01:57<01:15, 40.25it/s, est. speed input: 44926.88 toks/s, output: 43.87 toks/s]
Processed prompts:  63%|██████▎   | 5174/8192 [01:57<01:14, 40.25it/s, est. speed input: 44917.47 toks/s, output: 43.86 toks/s]
Processed prompts:  63%|██████▎   | 5186/8192 [01:58<01:14, 40.25it/s, est. speed input: 44908.16 toks/s, output: 43.86 toks/s]
Processed prompts:  63%|██████▎   | 5198/8192 [01:58<01:14, 40.25it/s, est. speed input: 44898.88 toks/s, output: 43.85 toks/s]
Processed prompts:  64%|██████▎   | 5210/8192 [01:58<01:14, 40.26it/s, est. speed input: 44889.70 toks/s, output: 43.84 toks/s]
Processed prompts:  64%|██████▎   | 5222/8192 [01:59<01:13, 40.26it/s, est. speed input: 44880.59 toks/s, output: 43.83 toks/s]
Processed prompts:  64%|██████▍   | 5234/8192 [01:59<01:13, 40.27it/s, est. speed input: 44871.55 toks/s, output: 43.82 toks/s]
Processed prompts:  64%|██████▍   | 5246/8192 [01:59<01:13, 40.25it/s, est. speed input: 44862.30 toks/s, output: 43.81 toks/s]
Processed prompts:  64%|██████▍   | 5258/8192 [02:00<01:12, 40.25it/s, est. speed input: 44853.23 toks/s, output: 43.80 toks/s]
Processed prompts:  64%|██████▍   | 5270/8192 [02:00<01:12, 40.25it/s, est. speed input: 44844.20 toks/s, output: 43.79 toks/s]
Processed prompts:  64%|██████▍   | 5282/8192 [02:00<01:12, 40.22it/s, est. speed input: 44835.00 toks/s, output: 43.78 toks/s]
Processed prompts:  65%|██████▍   | 5294/8192 [02:00<01:12, 40.23it/s, est. speed input: 44826.06 toks/s, output: 43.78 toks/s]
Processed prompts:  65%|██████▍   | 5306/8192 [02:01<01:11, 40.24it/s, est. speed input: 44817.19 toks/s, output: 43.77 toks/s]
Processed prompts:  65%|██████▍   | 5318/8192 [02:01<01:11, 40.24it/s, est. speed input: 44808.34 toks/s, output: 43.76 toks/s]
Processed prompts:  65%|██████▌   | 5330/8192 [02:01<01:11, 40.25it/s, est. speed input: 44799.58 toks/s, output: 43.75 toks/s]
Processed prompts:  65%|██████▌   | 5342/8192 [02:02<01:10, 40.25it/s, est. speed input: 44790.84 toks/s, output: 43.74 toks/s]
Processed prompts:  65%|██████▌   | 5354/8192 [02:02<01:10, 40.24it/s, est. speed input: 44782.06 toks/s, output: 43.73 toks/s]
Processed prompts:  66%|██████▌   | 5366/8192 [02:02<01:10, 40.24it/s, est. speed input: 44773.41 toks/s, output: 43.72 toks/s]
Processed prompts:  66%|██████▌   | 5378/8192 [02:03<01:09, 40.24it/s, est. speed input: 44764.75 toks/s, output: 43.72 toks/s]
Processed prompts:  66%|██████▌   | 5390/8192 [02:03<01:09, 40.25it/s, est. speed input: 44756.25 toks/s, output: 43.71 toks/s]
Processed prompts:  66%|██████▌   | 5402/8192 [02:03<01:09, 40.26it/s, est. speed input: 44747.77 toks/s, output: 43.70 toks/s]
Processed prompts:  66%|██████▌   | 5414/8192 [02:03<01:09, 40.24it/s, est. speed input: 44739.09 toks/s, output: 43.69 toks/s]
Processed prompts:  66%|██████▌   | 5426/8192 [02:04<01:08, 40.23it/s, est. speed input: 44730.55 toks/s, output: 43.68 toks/s]
Processed prompts:  66%|██████▋   | 5438/8192 [02:04<01:08, 40.25it/s, est. speed input: 44722.22 toks/s, output: 43.67 toks/s]
Processed prompts:  67%|██████▋   | 5450/8192 [02:04<01:08, 40.25it/s, est. speed input: 44713.90 toks/s, output: 43.67 toks/s]
Processed prompts:  67%|██████▋   | 5462/8192 [02:05<01:07, 40.23it/s, est. speed input: 44705.35 toks/s, output: 43.66 toks/s]
Processed prompts:  67%|██████▋   | 5474/8192 [02:05<01:07, 40.24it/s, est. speed input: 44697.10 toks/s, output: 43.65 toks/s]
Processed prompts:  67%|██████▋   | 5486/8192 [02:05<01:07, 40.24it/s, est. speed input: 44688.81 toks/s, output: 43.64 toks/s]
Processed prompts:  67%|██████▋   | 5498/8192 [02:06<01:06, 40.24it/s, est. speed input: 44680.61 toks/s, output: 43.63 toks/s]
Processed prompts:  67%|██████▋   | 5510/8192 [02:06<01:06, 40.24it/s, est. speed input: 44672.34 toks/s, output: 43.63 toks/s]
Processed prompts:  67%|██████▋   | 5522/8192 [02:06<01:06, 40.25it/s, est. speed input: 44664.32 toks/s, output: 43.62 toks/s]
Processed prompts:  68%|██████▊   | 5534/8192 [02:06<01:06, 40.26it/s, est. speed input: 44656.31 toks/s, output: 43.61 toks/s]
Processed prompts:  68%|██████▊   | 5546/8192 [02:07<01:05, 40.23it/s, est. speed input: 44648.00 toks/s, output: 43.60 toks/s]
Processed prompts:  68%|██████▊   | 5558/8192 [02:07<01:05, 40.23it/s, est. speed input: 44639.96 toks/s, output: 43.59 toks/s]
Processed prompts:  68%|██████▊   | 5570/8192 [02:07<01:05, 40.24it/s, est. speed input: 44631.98 toks/s, output: 43.59 toks/s]
Processed prompts:  68%|██████▊   | 5582/8192 [02:08<01:04, 40.25it/s, est. speed input: 44624.05 toks/s, output: 43.58 toks/s]
Processed prompts:  68%|██████▊   | 5594/8192 [02:08<01:04, 40.25it/s, est. speed input: 44616.19 toks/s, output: 43.57 toks/s]
Processed prompts:  68%|██████▊   | 5606/8192 [02:08<01:04, 40.25it/s, est. speed input: 44608.26 toks/s, output: 43.56 toks/s]
Processed prompts:  69%|██████▊   | 5618/8192 [02:08<01:03, 40.24it/s, est. speed input: 44600.39 toks/s, output: 43.56 toks/s]
Processed prompts:  69%|██████▊   | 5630/8192 [02:09<01:03, 40.23it/s, est. speed input: 44592.46 toks/s, output: 43.55 toks/s]
Processed prompts:  69%|██████▉   | 5642/8192 [02:09<01:03, 40.23it/s, est. speed input: 44584.67 toks/s, output: 43.54 toks/s]
Processed prompts:  69%|██████▉   | 5654/8192 [02:09<01:03, 40.21it/s, est. speed input: 44576.71 toks/s, output: 43.53 toks/s]
Processed prompts:  69%|██████▉   | 5666/8192 [02:10<01:02, 40.22it/s, est. speed input: 44568.97 toks/s, output: 43.52 toks/s]
Processed prompts:  69%|██████▉   | 5678/8192 [02:10<01:02, 40.21it/s, est. speed input: 44561.15 toks/s, output: 43.52 toks/s]
Processed prompts:  69%|██████▉   | 5690/8192 [02:10<01:02, 40.22it/s, est. speed input: 44553.54 toks/s, output: 43.51 toks/s]
Processed prompts:  70%|██████▉   | 5702/8192 [02:11<01:01, 40.22it/s, est. speed input: 44545.82 toks/s, output: 43.50 toks/s]
Processed prompts:  70%|██████▉   | 5714/8192 [02:11<01:01, 40.21it/s, est. speed input: 44538.15 toks/s, output: 43.49 toks/s]
Processed prompts:  70%|██████▉   | 5726/8192 [02:11<01:01, 40.23it/s, est. speed input: 44530.65 toks/s, output: 43.49 toks/s]
Processed prompts:  70%|███████   | 5738/8192 [02:11<01:01, 40.22it/s, est. speed input: 44523.04 toks/s, output: 43.48 toks/s]
Processed prompts:  70%|███████   | 5750/8192 [02:12<01:00, 40.19it/s, est. speed input: 44515.30 toks/s, output: 43.47 toks/s]
Processed prompts:  70%|███████   | 5762/8192 [02:12<01:00, 40.19it/s, est. speed input: 44507.73 toks/s, output: 43.46 toks/s]
Processed prompts:  70%|███████   | 5774/8192 [02:12<01:00, 40.22it/s, est. speed input: 44500.44 toks/s, output: 43.46 toks/s]
Processed prompts:  71%|███████   | 5786/8192 [02:13<00:59, 40.24it/s, est. speed input: 44493.15 toks/s, output: 43.45 toks/s]
Processed prompts:  71%|███████   | 5798/8192 [02:13<00:59, 40.23it/s, est. speed input: 44485.71 toks/s, output: 43.44 toks/s]
Processed prompts:  71%|███████   | 5810/8192 [02:13<00:59, 40.21it/s, est. speed input: 44478.19 toks/s, output: 43.44 toks/s]
Processed prompts:  71%|███████   | 5822/8192 [02:14<00:58, 40.23it/s, est. speed input: 44470.99 toks/s, output: 43.43 toks/s]
Processed prompts:  71%|███████   | 5834/8192 [02:14<00:58, 40.20it/s, est. speed input: 44463.53 toks/s, output: 43.42 toks/s]
Processed prompts:  71%|███████▏  | 5846/8192 [02:14<00:58, 40.22it/s, est. speed input: 44456.37 toks/s, output: 43.41 toks/s]
Processed prompts:  72%|███████▏  | 5858/8192 [02:14<00:58, 40.19it/s, est. speed input: 44448.88 toks/s, output: 43.41 toks/s]
Processed prompts:  72%|███████▏  | 5870/8192 [02:15<00:57, 40.22it/s, est. speed input: 44441.84 toks/s, output: 43.40 toks/s]
Processed prompts:  72%|███████▏  | 5882/8192 [02:15<00:57, 40.21it/s, est. speed input: 44434.60 toks/s, output: 43.39 toks/s]
Processed prompts:  72%|███████▏  | 5894/8192 [02:15<00:57, 40.21it/s, est. speed input: 44427.47 toks/s, output: 43.39 toks/s]
Processed prompts:  72%|███████▏  | 5906/8192 [02:16<00:56, 40.23it/s, est. speed input: 44420.47 toks/s, output: 43.38 toks/s]
Processed prompts:  72%|███████▏  | 5918/8192 [02:16<00:56, 40.24it/s, est. speed input: 44413.50 toks/s, output: 43.37 toks/s]
Processed prompts:  72%|███████▏  | 5930/8192 [02:16<00:56, 40.24it/s, est. speed input: 44406.53 toks/s, output: 43.37 toks/s]
Processed prompts:  73%|███████▎  | 5942/8192 [02:17<00:55, 40.25it/s, est. speed input: 44399.60 toks/s, output: 43.36 toks/s]
Processed prompts:  73%|███████▎  | 5954/8192 [02:17<00:55, 40.26it/s, est. speed input: 44392.76 toks/s, output: 43.35 toks/s]
Processed prompts:  73%|███████▎  | 5966/8192 [02:17<00:55, 40.24it/s, est. speed input: 44385.80 toks/s, output: 43.35 toks/s]
Processed prompts:  73%|███████▎  | 5978/8192 [02:17<00:55, 40.23it/s, est. speed input: 44378.84 toks/s, output: 43.34 toks/s]
Processed prompts:  73%|███████▎  | 5990/8192 [02:18<00:54, 40.22it/s, est. speed input: 44371.88 toks/s, output: 43.33 toks/s]
Processed prompts:  73%|███████▎  | 6002/8192 [02:18<00:54, 40.22it/s, est. speed input: 44365.02 toks/s, output: 43.33 toks/s]
Processed prompts:  73%|███████▎  | 6014/8192 [02:18<00:54, 40.21it/s, est. speed input: 44358.12 toks/s, output: 43.32 toks/s]
Processed prompts:  74%|███████▎  | 6026/8192 [02:19<00:53, 40.23it/s, est. speed input: 44351.45 toks/s, output: 43.31 toks/s]
Processed prompts:  74%|███████▎  | 6038/8192 [02:19<00:53, 40.24it/s, est. speed input: 44344.80 toks/s, output: 43.31 toks/s]
Processed prompts:  74%|███████▍  | 6050/8192 [02:19<00:53, 40.23it/s, est. speed input: 44338.01 toks/s, output: 43.30 toks/s]
Processed prompts:  74%|███████▍  | 6062/8192 [02:20<00:52, 40.23it/s, est. speed input: 44331.31 toks/s, output: 43.29 toks/s]
Processed prompts:  74%|███████▍  | 6074/8192 [02:20<00:52, 40.24it/s, est. speed input: 44324.70 toks/s, output: 43.29 toks/s]
Processed prompts:  74%|███████▍  | 6086/8192 [02:20<00:52, 40.23it/s, est. speed input: 44318.02 toks/s, output: 43.28 toks/s]
Processed prompts:  74%|███████▍  | 6098/8192 [02:20<00:52, 40.22it/s, est. speed input: 44311.33 toks/s, output: 43.27 toks/s]
Processed prompts:  75%|███████▍  | 6110/8192 [02:21<00:51, 40.22it/s, est. speed input: 44304.74 toks/s, output: 43.27 toks/s]
Processed prompts:  75%|███████▍  | 6122/8192 [02:21<00:51, 40.24it/s, est. speed input: 44298.32 toks/s, output: 43.26 toks/s]
Processed prompts:  75%|███████▍  | 6134/8192 [02:21<00:51, 40.23it/s, est. speed input: 44291.71 toks/s, output: 43.25 toks/s]
Processed prompts:  75%|███████▌  | 6146/8192 [02:22<00:50, 40.23it/s, est. speed input: 44285.19 toks/s, output: 43.25 toks/s]
Processed prompts:  75%|███████▌  | 6158/8192 [02:22<00:50, 40.23it/s, est. speed input: 44278.73 toks/s, output: 43.24 toks/s]
Processed prompts:  75%|███████▌  | 6170/8192 [02:22<00:50, 40.21it/s, est. speed input: 44272.15 toks/s, output: 43.23 toks/s]
Processed prompts:  75%|███████▌  | 6182/8192 [02:23<00:49, 40.22it/s, est. speed input: 44265.78 toks/s, output: 43.23 toks/s]
Processed prompts:  76%|███████▌  | 6194/8192 [02:23<00:49, 40.22it/s, est. speed input: 44259.38 toks/s, output: 43.22 toks/s]
Processed prompts:  76%|███████▌  | 6206/8192 [02:23<00:49, 40.23it/s, est. speed input: 44253.07 toks/s, output: 43.22 toks/s]
Processed prompts:  76%|███████▌  | 6218/8192 [02:23<00:49, 40.22it/s, est. speed input: 44246.66 toks/s, output: 43.21 toks/s]
Processed prompts:  76%|███████▌  | 6230/8192 [02:24<00:48, 40.22it/s, est. speed input: 44240.29 toks/s, output: 43.20 toks/s]
Processed prompts:  76%|███████▌  | 6242/8192 [02:24<00:48, 40.22it/s, est. speed input: 44234.02 toks/s, output: 43.20 toks/s]
Processed prompts:  76%|███████▋  | 6254/8192 [02:24<00:48, 40.21it/s, est. speed input: 44227.67 toks/s, output: 43.19 toks/s]
Processed prompts:  76%|███████▋  | 6266/8192 [02:25<00:47, 40.23it/s, est. speed input: 44221.50 toks/s, output: 43.19 toks/s]
Processed prompts:  77%|███████▋  | 6278/8192 [02:25<00:47, 40.21it/s, est. speed input: 44215.16 toks/s, output: 43.18 toks/s]
Processed prompts:  77%|███████▋  | 6290/8192 [02:25<00:47, 40.21it/s, est. speed input: 44208.92 toks/s, output: 43.17 toks/s]
Processed prompts:  77%|███████▋  | 6302/8192 [02:25<00:47, 40.21it/s, est. speed input: 44202.70 toks/s, output: 43.17 toks/s]
Processed prompts:  77%|███████▋  | 6314/8192 [02:26<00:46, 40.21it/s, est. speed input: 44196.55 toks/s, output: 43.16 toks/s]
Processed prompts:  77%|███████▋  | 6326/8192 [02:26<00:46, 40.21it/s, est. speed input: 44190.39 toks/s, output: 43.15 toks/s]
Processed prompts:  77%|███████▋  | 6338/8192 [02:26<00:46, 40.18it/s, est. speed input: 44184.05 toks/s, output: 43.15 toks/s]
Processed prompts:  78%|███████▊  | 6350/8192 [02:27<00:45, 40.20it/s, est. speed input: 44178.05 toks/s, output: 43.14 toks/s]
Processed prompts:  78%|███████▊  | 6362/8192 [02:27<00:45, 40.19it/s, est. speed input: 44171.85 toks/s, output: 43.14 toks/s]
Processed prompts:  78%|███████▊  | 6374/8192 [02:27<00:45, 40.20it/s, est. speed input: 44165.87 toks/s, output: 43.13 toks/s]
Processed prompts:  78%|███████▊  | 6386/8192 [02:28<00:44, 40.22it/s, est. speed input: 44159.95 toks/s, output: 43.12 toks/s]
Processed prompts:  78%|███████▊  | 6398/8192 [02:28<00:44, 40.22it/s, est. speed input: 44153.98 toks/s, output: 43.12 toks/s]
Processed prompts:  78%|███████▊  | 6410/8192 [02:28<00:44, 40.23it/s, est. speed input: 44148.12 toks/s, output: 43.11 toks/s]
Processed prompts:  78%|███████▊  | 6422/8192 [02:28<00:44, 40.22it/s, est. speed input: 44142.10 toks/s, output: 43.11 toks/s]
Processed prompts:  79%|███████▊  | 6434/8192 [02:29<00:43, 40.22it/s, est. speed input: 44136.16 toks/s, output: 43.10 toks/s]
Processed prompts:  79%|███████▊  | 6446/8192 [02:29<00:43, 40.20it/s, est. speed input: 44130.17 toks/s, output: 43.10 toks/s]
Processed prompts:  79%|███████▉  | 6458/8192 [02:29<00:43, 40.18it/s, est. speed input: 44124.09 toks/s, output: 43.09 toks/s]
Processed prompts:  79%|███████▉  | 6470/8192 [02:30<00:42, 40.17it/s, est. speed input: 44118.08 toks/s, output: 43.08 toks/s]
Processed prompts:  79%|███████▉  | 6482/8192 [02:30<00:42, 40.15it/s, est. speed input: 44112.05 toks/s, output: 43.08 toks/s]
Processed prompts:  79%|███████▉  | 6494/8192 [02:30<00:42, 40.19it/s, est. speed input: 44106.38 toks/s, output: 43.07 toks/s]
Processed prompts:  79%|███████▉  | 6506/8192 [02:31<00:41, 40.20it/s, est. speed input: 44100.60 toks/s, output: 43.07 toks/s]
Processed prompts:  80%|███████▉  | 6518/8192 [02:31<00:41, 40.21it/s, est. speed input: 44094.92 toks/s, output: 43.06 toks/s]
Processed prompts:  80%|███████▉  | 6530/8192 [02:31<00:41, 40.23it/s, est. speed input: 44089.31 toks/s, output: 43.06 toks/s]
Processed prompts:  80%|███████▉  | 6542/8192 [02:31<00:41, 40.20it/s, est. speed input: 44083.44 toks/s, output: 43.05 toks/s]
Processed prompts:  80%|████████  | 6554/8192 [02:32<00:40, 40.19it/s, est. speed input: 44077.65 toks/s, output: 43.04 toks/s]
Processed prompts:  80%|████████  | 6566/8192 [02:32<00:40, 40.22it/s, est. speed input: 44072.12 toks/s, output: 43.04 toks/s]
Processed prompts:  80%|████████  | 6578/8192 [02:32<00:40, 40.19it/s, est. speed input: 44066.26 toks/s, output: 43.03 toks/s]
Processed prompts:  80%|████████  | 6590/8192 [02:33<00:39, 41.06it/s, est. speed input: 44066.68 toks/s, output: 43.03 toks/s]
Processed prompts:  81%|████████  | 6602/8192 [02:33<00:38, 40.82it/s, est. speed input: 44061.18 toks/s, output: 43.03 toks/s]
Processed prompts:  81%|████████  | 6614/8192 [02:33<00:38, 40.65it/s, est. speed input: 44055.68 toks/s, output: 43.02 toks/s]
Processed prompts:  81%|████████  | 6626/8192 [02:34<00:38, 40.53it/s, est. speed input: 44050.16 toks/s, output: 43.02 toks/s]
Processed prompts:  81%|████████  | 6638/8192 [02:34<00:38, 40.44it/s, est. speed input: 44044.67 toks/s, output: 43.01 toks/s]
Processed prompts:  81%|████████  | 6650/8192 [02:34<00:38, 40.34it/s, est. speed input: 44038.92 toks/s, output: 43.01 toks/s]
Processed prompts:  81%|████████▏ | 6662/8192 [02:34<00:37, 40.30it/s, est. speed input: 44033.39 toks/s, output: 43.00 toks/s]
Processed prompts:  81%|████████▏ | 6674/8192 [02:35<00:37, 40.26it/s, est. speed input: 44027.81 toks/s, output: 43.00 toks/s]
Processed prompts:  82%|████████▏ | 6686/8192 [02:35<00:37, 40.26it/s, est. speed input: 44022.45 toks/s, output: 42.99 toks/s]
Processed prompts:  82%|████████▏ | 6698/8192 [02:35<00:37, 40.24it/s, est. speed input: 44016.98 toks/s, output: 42.99 toks/s]
Processed prompts:  82%|████████▏ | 6710/8192 [02:36<00:36, 40.24it/s, est. speed input: 44011.57 toks/s, output: 42.98 toks/s]
Processed prompts:  82%|████████▏ | 6722/8192 [02:36<00:36, 40.24it/s, est. speed input: 44006.24 toks/s, output: 42.97 toks/s]
Processed prompts:  82%|████████▏ | 6734/8192 [02:36<00:36, 40.19it/s, est. speed input: 44000.53 toks/s, output: 42.97 toks/s]
Processed prompts:  82%|████████▏ | 6746/8192 [02:37<00:35, 40.20it/s, est. speed input: 43995.21 toks/s, output: 42.96 toks/s]
Processed prompts:  82%|████████▏ | 6758/8192 [02:37<00:35, 40.19it/s, est. speed input: 43989.77 toks/s, output: 42.96 toks/s]
Processed prompts:  83%|████████▎ | 6770/8192 [02:37<00:35, 40.23it/s, est. speed input: 43984.65 toks/s, output: 42.95 toks/s]
Processed prompts:  83%|████████▎ | 6782/8192 [02:37<00:35, 40.24it/s, est. speed input: 43979.46 toks/s, output: 42.95 toks/s]
Processed prompts:  83%|████████▎ | 6794/8192 [02:38<00:34, 40.21it/s, est. speed input: 43974.01 toks/s, output: 42.94 toks/s]
Processed prompts:  83%|████████▎ | 6806/8192 [02:38<00:34, 40.20it/s, est. speed input: 43968.72 toks/s, output: 42.94 toks/s]
Processed prompts:  83%|████████▎ | 6818/8192 [02:38<00:34, 40.21it/s, est. speed input: 43963.50 toks/s, output: 42.93 toks/s]
Processed prompts:  83%|████████▎ | 6830/8192 [02:39<00:33, 40.19it/s, est. speed input: 43958.10 toks/s, output: 42.93 toks/s]
Processed prompts:  84%|████████▎ | 6842/8192 [02:39<00:33, 40.18it/s, est. speed input: 43952.82 toks/s, output: 42.92 toks/s]
Processed prompts:  84%|████████▎ | 6854/8192 [02:39<00:33, 40.21it/s, est. speed input: 43947.77 toks/s, output: 42.92 toks/s]
Processed prompts:  84%|████████▍ | 6866/8192 [02:39<00:32, 40.19it/s, est. speed input: 43942.44 toks/s, output: 42.91 toks/s]
Processed prompts:  84%|████████▍ | 6878/8192 [02:40<00:32, 40.20it/s, est. speed input: 43937.30 toks/s, output: 42.91 toks/s]
Processed prompts:  84%|████████▍ | 6890/8192 [02:40<00:32, 40.16it/s, est. speed input: 43931.89 toks/s, output: 42.90 toks/s]
Processed prompts:  84%|████████▍ | 6902/8192 [02:40<00:32, 40.20it/s, est. speed input: 43926.93 toks/s, output: 42.90 toks/s]
Processed prompts:  84%|████████▍ | 6914/8192 [02:41<00:31, 40.21it/s, est. speed input: 43921.90 toks/s, output: 42.89 toks/s]
Processed prompts:  85%|████████▍ | 6926/8192 [02:41<00:31, 40.20it/s, est. speed input: 43916.73 toks/s, output: 42.89 toks/s]
Processed prompts:  85%|████████▍ | 6938/8192 [02:41<00:31, 40.18it/s, est. speed input: 43911.57 toks/s, output: 42.88 toks/s]
Processed prompts:  85%|████████▍ | 6950/8192 [02:42<00:30, 40.19it/s, est. speed input: 43906.54 toks/s, output: 42.88 toks/s]
Processed prompts:  85%|████████▍ | 6962/8192 [02:42<00:30, 40.21it/s, est. speed input: 43901.58 toks/s, output: 42.87 toks/s]
Processed prompts:  85%|████████▌ | 6974/8192 [02:42<00:30, 40.21it/s, est. speed input: 43896.59 toks/s, output: 42.87 toks/s]
Processed prompts:  85%|████████▌ | 6986/8192 [02:42<00:29, 40.22it/s, est. speed input: 43891.66 toks/s, output: 42.86 toks/s]
Processed prompts:  85%|████████▌ | 6998/8192 [02:43<00:29, 40.21it/s, est. speed input: 43886.66 toks/s, output: 42.86 toks/s]
Processed prompts:  86%|████████▌ | 7010/8192 [02:43<00:29, 40.22it/s, est. speed input: 43881.77 toks/s, output: 42.85 toks/s]
Processed prompts:  86%|████████▌ | 7022/8192 [02:43<00:29, 40.23it/s, est. speed input: 43876.91 toks/s, output: 42.85 toks/s]
Processed prompts:  86%|████████▌ | 7034/8192 [02:44<00:28, 40.23it/s, est. speed input: 43872.04 toks/s, output: 42.84 toks/s]
Processed prompts:  86%|████████▌ | 7046/8192 [02:44<00:28, 40.24it/s, est. speed input: 43867.29 toks/s, output: 42.84 toks/s]
Processed prompts:  86%|████████▌ | 7058/8192 [02:44<00:28, 40.21it/s, est. speed input: 43862.23 toks/s, output: 42.83 toks/s]
Processed prompts:  86%|████████▋ | 7070/8192 [02:45<00:27, 40.19it/s, est. speed input: 43857.24 toks/s, output: 42.83 toks/s]
Processed prompts:  86%|████████▋ | 7082/8192 [02:45<00:27, 40.21it/s, est. speed input: 43852.50 toks/s, output: 42.82 toks/s]
Processed prompts:  87%|████████▋ | 7094/8192 [02:45<00:27, 40.22it/s, est. speed input: 43847.78 toks/s, output: 42.82 toks/s]
Processed prompts:  87%|████████▋ | 7106/8192 [02:45<00:26, 40.22it/s, est. speed input: 43842.99 toks/s, output: 42.82 toks/s]
Processed prompts:  87%|████████▋ | 7118/8192 [02:46<00:26, 40.23it/s, est. speed input: 43838.25 toks/s, output: 42.81 toks/s]
Processed prompts:  87%|████████▋ | 7130/8192 [02:46<00:26, 40.22it/s, est. speed input: 43833.46 toks/s, output: 42.81 toks/s]
Processed prompts:  87%|████████▋ | 7142/8192 [02:46<00:26, 40.19it/s, est. speed input: 43828.53 toks/s, output: 42.80 toks/s]
Processed prompts:  87%|████████▋ | 7154/8192 [02:47<00:25, 40.21it/s, est. speed input: 43823.87 toks/s, output: 42.80 toks/s]
Processed prompts:  87%|████████▋ | 7166/8192 [02:47<00:25, 40.19it/s, est. speed input: 43819.05 toks/s, output: 42.79 toks/s]
Processed prompts:  88%|████████▊ | 7178/8192 [02:47<00:25, 40.19it/s, est. speed input: 43814.30 toks/s, output: 42.79 toks/s]
Processed prompts:  88%|████████▊ | 7190/8192 [02:48<00:24, 40.19it/s, est. speed input: 43809.57 toks/s, output: 42.78 toks/s]
Processed prompts:  88%|████████▊ | 7202/8192 [02:48<00:24, 40.19it/s, est. speed input: 43804.87 toks/s, output: 42.78 toks/s]
Processed prompts:  88%|████████▊ | 7214/8192 [02:48<00:24, 40.18it/s, est. speed input: 43800.12 toks/s, output: 42.77 toks/s]
Processed prompts:  88%|████████▊ | 7226/8192 [02:48<00:24, 40.20it/s, est. speed input: 43795.55 toks/s, output: 42.77 toks/s]
Processed prompts:  88%|████████▊ | 7238/8192 [02:49<00:23, 40.19it/s, est. speed input: 43790.85 toks/s, output: 42.76 toks/s]
Processed prompts:  89%|████████▊ | 7250/8192 [02:49<00:23, 40.20it/s, est. speed input: 43786.27 toks/s, output: 42.76 toks/s]
Processed prompts:  89%|████████▊ | 7262/8192 [02:49<00:23, 40.19it/s, est. speed input: 43781.58 toks/s, output: 42.76 toks/s]
Processed prompts:  89%|████████▉ | 7274/8192 [02:50<00:22, 40.21it/s, est. speed input: 43777.14 toks/s, output: 42.75 toks/s]
Processed prompts:  89%|████████▉ | 7286/8192 [02:50<00:22, 40.20it/s, est. speed input: 43772.52 toks/s, output: 42.75 toks/s]
Processed prompts:  89%|████████▉ | 7298/8192 [02:50<00:22, 40.15it/s, est. speed input: 43767.61 toks/s, output: 42.74 toks/s]
Processed prompts:  89%|████████▉ | 7310/8192 [02:51<00:21, 40.15it/s, est. speed input: 43763.00 toks/s, output: 42.74 toks/s]
Processed prompts:  89%|████████▉ | 7322/8192 [02:51<00:21, 40.17it/s, est. speed input: 43758.51 toks/s, output: 42.73 toks/s]
Processed prompts:  90%|████████▉ | 7334/8192 [02:51<00:21, 40.20it/s, est. speed input: 43754.14 toks/s, output: 42.73 toks/s]
Processed prompts:  90%|████████▉ | 7346/8192 [02:51<00:21, 40.20it/s, est. speed input: 43749.60 toks/s, output: 42.72 toks/s]
Processed prompts:  90%|████████▉ | 7358/8192 [02:52<00:20, 40.18it/s, est. speed input: 43745.03 toks/s, output: 42.72 toks/s]
Processed prompts:  90%|████████▉ | 7370/8192 [02:52<00:20, 40.19it/s, est. speed input: 43740.56 toks/s, output: 42.72 toks/s]
Processed prompts:  90%|█████████ | 7382/8192 [02:52<00:20, 40.19it/s, est. speed input: 43736.11 toks/s, output: 42.71 toks/s]
Processed prompts:  90%|█████████ | 7394/8192 [02:53<00:19, 40.19it/s, est. speed input: 43731.69 toks/s, output: 42.71 toks/s]
Processed prompts:  90%|█████████ | 7406/8192 [02:53<00:19, 40.19it/s, est. speed input: 43727.25 toks/s, output: 42.70 toks/s]
Processed prompts:  91%|█████████ | 7418/8192 [02:53<00:19, 40.20it/s, est. speed input: 43722.88 toks/s, output: 42.70 toks/s]
Processed prompts:  91%|█████████ | 7430/8192 [02:54<00:18, 40.21it/s, est. speed input: 43718.56 toks/s, output: 42.69 toks/s]
Processed prompts:  91%|█████████ | 7442/8192 [02:54<00:18, 40.19it/s, est. speed input: 43714.08 toks/s, output: 42.69 toks/s]
Processed prompts:  91%|█████████ | 7454/8192 [02:54<00:18, 40.22it/s, est. speed input: 43709.87 toks/s, output: 42.69 toks/s]
Processed prompts:  91%|█████████ | 7466/8192 [02:54<00:18, 40.19it/s, est. speed input: 43705.39 toks/s, output: 42.68 toks/s]
Processed prompts:  91%|█████████▏| 7478/8192 [02:55<00:17, 40.16it/s, est. speed input: 43700.86 toks/s, output: 42.68 toks/s]
Processed prompts:  91%|█████████▏| 7490/8192 [02:55<00:17, 40.09it/s, est. speed input: 43696.01 toks/s, output: 42.67 toks/s]
Processed prompts:  92%|█████████▏| 7502/8192 [02:55<00:17, 40.09it/s, est. speed input: 43691.54 toks/s, output: 42.67 toks/s]
Processed prompts:  92%|█████████▏| 7514/8192 [02:56<00:16, 40.07it/s, est. speed input: 43686.93 toks/s, output: 42.66 toks/s]
Processed prompts:  92%|█████████▏| 7526/8192 [02:56<00:16, 40.10it/s, est. speed input: 43682.59 toks/s, output: 42.66 toks/s]
Processed prompts:  92%|█████████▏| 7538/8192 [02:56<00:16, 40.12it/s, est. speed input: 43678.27 toks/s, output: 42.65 toks/s]
Processed prompts:  92%|█████████▏| 7550/8192 [02:57<00:16, 40.12it/s, est. speed input: 43673.88 toks/s, output: 42.65 toks/s]
Processed prompts:  92%|█████████▏| 7562/8192 [02:57<00:15, 40.13it/s, est. speed input: 43669.58 toks/s, output: 42.65 toks/s]
Processed prompts:  92%|█████████▏| 7574/8192 [02:57<00:15, 40.13it/s, est. speed input: 43665.24 toks/s, output: 42.64 toks/s]
Processed prompts:  93%|█████████▎| 7586/8192 [02:57<00:15, 40.13it/s, est. speed input: 43660.90 toks/s, output: 42.64 toks/s]
Processed prompts:  93%|█████████▎| 7598/8192 [02:58<00:14, 40.15it/s, est. speed input: 43656.75 toks/s, output: 42.63 toks/s]
Processed prompts:  93%|█████████▎| 7610/8192 [02:58<00:14, 40.19it/s, est. speed input: 43652.70 toks/s, output: 42.63 toks/s]
Processed prompts:  93%|█████████▎| 7622/8192 [02:58<00:14, 40.09it/s, est. speed input: 43647.95 toks/s, output: 42.62 toks/s]
Processed prompts:  93%|█████████▎| 7634/8192 [02:59<00:13, 40.05it/s, est. speed input: 43643.38 toks/s, output: 42.62 toks/s]
Processed prompts:  93%|█████████▎| 7646/8192 [02:59<00:13, 40.06it/s, est. speed input: 43639.04 toks/s, output: 42.62 toks/s]
Processed prompts:  93%|█████████▎| 7658/8192 [02:59<00:13, 40.05it/s, est. speed input: 43634.61 toks/s, output: 42.61 toks/s]
Processed prompts:  94%|█████████▎| 7670/8192 [03:00<00:13, 40.10it/s, est. speed input: 43630.57 toks/s, output: 42.61 toks/s]
Processed prompts:  94%|█████████▍| 7682/8192 [03:00<00:12, 40.08it/s, est. speed input: 43626.15 toks/s, output: 42.60 toks/s]
Processed prompts:  94%|█████████▍| 7694/8192 [03:00<00:12, 40.05it/s, est. speed input: 43621.71 toks/s, output: 42.60 toks/s]
Processed prompts:  94%|█████████▍| 7706/8192 [03:00<00:12, 40.04it/s, est. speed input: 43617.34 toks/s, output: 42.60 toks/s]
Processed prompts:  94%|█████████▍| 7718/8192 [03:01<00:11, 40.08it/s, est. speed input: 43613.21 toks/s, output: 42.59 toks/s]
Processed prompts:  94%|█████████▍| 7730/8192 [03:01<00:11, 40.10it/s, est. speed input: 43609.10 toks/s, output: 42.59 toks/s]
Processed prompts:  95%|█████████▍| 7742/8192 [03:01<00:11, 40.15it/s, est. speed input: 43605.22 toks/s, output: 42.58 toks/s]
Processed prompts:  95%|█████████▍| 7754/8192 [03:02<00:10, 40.11it/s, est. speed input: 43600.88 toks/s, output: 42.58 toks/s]
Processed prompts:  95%|█████████▍| 7766/8192 [03:02<00:10, 40.13it/s, est. speed input: 43596.85 toks/s, output: 42.58 toks/s]
Processed prompts:  95%|█████████▍| 7778/8192 [03:02<00:10, 40.13it/s, est. speed input: 43592.75 toks/s, output: 42.57 toks/s]
Processed prompts:  95%|█████████▌| 7790/8192 [03:03<00:10, 40.11it/s, est. speed input: 43588.55 toks/s, output: 42.57 toks/s]
Processed prompts:  95%|█████████▌| 7802/8192 [03:03<00:09, 40.02it/s, est. speed input: 43583.89 toks/s, output: 42.56 toks/s]
Processed prompts:  95%|█████████▌| 7814/8192 [03:03<00:09, 40.02it/s, est. speed input: 43579.65 toks/s, output: 42.56 toks/s]
Processed prompts:  96%|█████████▌| 7826/8192 [03:03<00:09, 40.00it/s, est. speed input: 43575.28 toks/s, output: 42.55 toks/s]
Processed prompts:  96%|█████████▌| 7838/8192 [03:04<00:08, 40.04it/s, est. speed input: 43571.28 toks/s, output: 42.55 toks/s]
Processed prompts:  96%|█████████▌| 7850/8192 [03:04<00:08, 40.11it/s, est. speed input: 43567.50 toks/s, output: 42.55 toks/s]
Processed prompts:  96%|█████████▌| 7862/8192 [03:04<00:08, 40.15it/s, est. speed input: 43563.69 toks/s, output: 42.54 toks/s]
Processed prompts:  96%|█████████▌| 7874/8192 [03:05<00:07, 40.18it/s, est. speed input: 43559.89 toks/s, output: 42.54 toks/s]
Processed prompts:  96%|█████████▋| 7886/8192 [03:05<00:07, 40.17it/s, est. speed input: 43555.96 toks/s, output: 42.54 toks/s]
Processed prompts:  96%|█████████▋| 7898/8192 [03:05<00:07, 40.17it/s, est. speed input: 43552.06 toks/s, output: 42.53 toks/s]
Processed prompts:  97%|█████████▋| 7910/8192 [03:05<00:07, 40.13it/s, est. speed input: 43547.94 toks/s, output: 42.53 toks/s]
Processed prompts:  97%|█████████▋| 7922/8192 [03:06<00:06, 40.14it/s, est. speed input: 43544.06 toks/s, output: 42.52 toks/s]
Processed prompts:  97%|█████████▋| 7934/8192 [03:06<00:06, 40.12it/s, est. speed input: 43540.06 toks/s, output: 42.52 toks/s]
Processed prompts:  97%|█████████▋| 7946/8192 [03:06<00:06, 40.08it/s, est. speed input: 43535.86 toks/s, output: 42.52 toks/s]
Processed prompts:  97%|█████████▋| 7958/8192 [03:07<00:05, 40.06it/s, est. speed input: 43531.77 toks/s, output: 42.51 toks/s]
Processed prompts:  97%|█████████▋| 7970/8192 [03:07<00:05, 40.04it/s, est. speed input: 43527.64 toks/s, output: 42.51 toks/s]
Processed prompts:  97%|█████████▋| 7982/8192 [03:07<00:05, 40.05it/s, est. speed input: 43523.69 toks/s, output: 42.50 toks/s]
Processed prompts:  98%|█████████▊| 7994/8192 [03:08<00:04, 40.07it/s, est. speed input: 43519.76 toks/s, output: 42.50 toks/s]
Processed prompts:  98%|█████████▊| 8006/8192 [03:08<00:04, 40.05it/s, est. speed input: 43515.72 toks/s, output: 42.50 toks/s]
Processed prompts:  98%|█████████▊| 8018/8192 [03:08<00:04, 40.05it/s, est. speed input: 43511.72 toks/s, output: 42.49 toks/s]
Processed prompts:  98%|█████████▊| 8030/8192 [03:08<00:04, 40.05it/s, est. speed input: 43507.77 toks/s, output: 42.49 toks/s]
Processed prompts:  98%|█████████▊| 8042/8192 [03:09<00:03, 40.08it/s, est. speed input: 43504.01 toks/s, output: 42.48 toks/s]
Processed prompts:  98%|█████████▊| 8054/8192 [03:09<00:03, 40.11it/s, est. speed input: 43500.27 toks/s, output: 42.48 toks/s]
Processed prompts:  98%|█████████▊| 8066/8192 [03:09<00:03, 40.08it/s, est. speed input: 43496.27 toks/s, output: 42.48 toks/s]
Processed prompts:  99%|█████████▊| 8078/8192 [03:10<00:02, 40.13it/s, est. speed input: 43492.71 toks/s, output: 42.47 toks/s]
Processed prompts:  99%|█████████▉| 8090/8192 [03:10<00:02, 40.15it/s, est. speed input: 43489.04 toks/s, output: 42.47 toks/s]
Processed prompts:  99%|█████████▉| 8102/8192 [03:10<00:02, 40.16it/s, est. speed input: 43485.39 toks/s, output: 42.47 toks/s]
Processed prompts:  99%|█████████▉| 8114/8192 [03:11<00:01, 40.17it/s, est. speed input: 43481.77 toks/s, output: 42.46 toks/s]
Processed prompts:  99%|█████████▉| 8126/8192 [03:11<00:01, 40.17it/s, est. speed input: 43478.12 toks/s, output: 42.46 toks/s]
Processed prompts:  99%|█████████▉| 8138/8192 [03:11<00:01, 40.16it/s, est. speed input: 43474.40 toks/s, output: 42.46 toks/s]
Processed prompts:  99%|█████████▉| 8150/8192 [03:11<00:01, 40.10it/s, est. speed input: 43470.39 toks/s, output: 42.45 toks/s]
Processed prompts: 100%|█████████▉| 8162/8192 [03:12<00:00, 40.06it/s, est. speed input: 43466.44 toks/s, output: 42.45 toks/s]
Processed prompts: 100%|█████████▉| 8174/8192 [03:12<00:00, 40.05it/s, est. speed input: 43462.59 toks/s, output: 42.44 toks/s]
Processed prompts: 100%|█████████▉| 8186/8192 [03:12<00:00, 45.44it/s, est. speed input: 43485.49 toks/s, output: 42.47 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [03:12<00:00, 45.44it/s, est. speed input: 43517.34 toks/s, output: 42.50 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [03:12<00:00, 42.50it/s, est. speed input: 43517.34 toks/s, output: 42.50 toks/s]
[rank0]:[W126 01:33:31.449111911 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 02:04:14
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.65 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M8192.json


========== M=8192 ==========
Time: 2026-01-26 02:31:31
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.7 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:31:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=847658) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=847658) WARNING 01-26 02:31:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 20.66 requests/s, 21175.56 total tokens/s, 20.66 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 02:31:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:31:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:31:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:31:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:31:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:31:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:31:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:31:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:31:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:31:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:31:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:31:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:31:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:31:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:31:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:31:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:31:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=847658) [2026-01-26 02:31:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=847658) [2026-01-26 02:31:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=847658) [2026-01-26 02:31:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=847658) [2026-01-26 02:31:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=847658) [2026-01-26 02:31:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=847658) [2026-01-26 02:31:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=847658) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=847658) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.49it/s]
(EngineCore_DP0 pid=847658) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
(EngineCore_DP0 pid=847658) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
(EngineCore_DP0 pid=847658) 
(EngineCore_DP0 pid=847658) [2026-01-26 02:31:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=847658) [2026-01-26 02:31:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=847658) [2026-01-26 02:31:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=847658) [2026-01-26 02:31:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=847658) [2026-01-26 02:31:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=847658) [2026-01-26 02:31:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=847658) [2026-01-26 02:31:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=847658) [2026-01-26 02:31:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=847658) 2026-01-26 02:31:52,759 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=847658) 2026-01-26 02:31:52,774 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=847658) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  5.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 17.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 15.02it/s]
(EngineCore_DP0 pid=847658) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.37it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   5%|▍         | 49/1024 [00:00<00:02, 485.31it/s]
Adding requests:  10%|█         | 104/1024 [00:00<00:01, 517.96it/s]
Adding requests:  16%|█▌        | 162/1024 [00:00<00:01, 544.52it/s]
Adding requests:  22%|██▏       | 222/1024 [00:00<00:01, 561.77it/s]
Adding requests:  27%|██▋       | 279/1024 [00:00<00:01, 564.41it/s]
Adding requests:  33%|███▎      | 339/1024 [00:00<00:01, 575.74it/s]
Adding requests:  39%|███▉      | 401/1024 [00:00<00:01, 589.23it/s]
Adding requests:  45%|████▍     | 460/1024 [00:00<00:00, 589.45it/s]
Adding requests:  52%|█████▏    | 528/1024 [00:00<00:00, 615.21it/s]
Adding requests:  58%|█████▊    | 590/1024 [00:01<00:00, 595.67it/s]
Adding requests:  63%|██████▎   | 650/1024 [00:01<00:00, 592.64it/s]
Adding requests:  69%|██████▉   | 711/1024 [00:01<00:00, 596.09it/s]
Adding requests:  75%|███████▌  | 771/1024 [00:01<00:00, 581.80it/s]
Adding requests:  81%|████████  | 831/1024 [00:01<00:00, 586.58it/s]
Adding requests:  87%|████████▋ | 890/1024 [00:01<00:00, 584.63it/s]
Adding requests:  93%|█████████▎| 949/1024 [00:01<00:00, 579.61it/s]
Adding requests:  99%|█████████▊| 1009/1024 [00:01<00:00, 584.15it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 580.69it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 34/1024 [00:00<00:08, 120.44it/s, est. speed input: 123334.48 toks/s, output: 120.44 toks/s]
Processed prompts:   5%|▍         | 47/1024 [00:00<00:15, 63.20it/s, est. speed input: 72157.67 toks/s, output: 70.47 toks/s]   
Processed prompts:   5%|▌         | 54/1024 [00:01<00:23, 41.35it/s, est. speed input: 52583.26 toks/s, output: 51.35 toks/s]
Processed prompts:   6%|▌         | 59/1024 [00:01<00:32, 29.73it/s, est. speed input: 42059.15 toks/s, output: 41.07 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:01<00:37, 25.47it/s, est. speed input: 37103.56 toks/s, output: 36.23 toks/s]
Processed prompts:   7%|▋         | 74/1024 [00:02<00:39, 23.85it/s, est. speed input: 34342.11 toks/s, output: 33.54 toks/s]
Processed prompts:   8%|▊         | 82/1024 [00:02<00:41, 22.84it/s, est. speed input: 32404.19 toks/s, output: 31.64 toks/s]
Processed prompts:   9%|▉         | 90/1024 [00:02<00:42, 22.17it/s, est. speed input: 30961.15 toks/s, output: 30.24 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:03<00:42, 21.74it/s, est. speed input: 29852.95 toks/s, output: 29.15 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:03<00:42, 21.43it/s, est. speed input: 28969.36 toks/s, output: 28.29 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:04<00:42, 21.23it/s, est. speed input: 28250.41 toks/s, output: 27.59 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:04<00:42, 21.08it/s, est. speed input: 27652.91 toks/s, output: 27.00 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:04<00:42, 20.98it/s, est. speed input: 27150.07 toks/s, output: 26.51 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:05<00:42, 20.91it/s, est. speed input: 26718.17 toks/s, output: 26.09 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:05<00:42, 20.86it/s, est. speed input: 26345.45 toks/s, output: 25.73 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:06<00:41, 20.82it/s, est. speed input: 26019.34 toks/s, output: 25.41 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:06<00:41, 20.79it/s, est. speed input: 25732.18 toks/s, output: 25.13 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:06<00:41, 20.77it/s, est. speed input: 25478.20 toks/s, output: 24.88 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:07<00:40, 20.76it/s, est. speed input: 25251.17 toks/s, output: 24.66 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:07<00:40, 20.75it/s, est. speed input: 25046.60 toks/s, output: 24.46 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:07<00:40, 20.74it/s, est. speed input: 24861.77 toks/s, output: 24.28 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:08<00:39, 20.74it/s, est. speed input: 24693.86 toks/s, output: 24.12 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:08<00:39, 20.73it/s, est. speed input: 24541.26 toks/s, output: 23.97 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:09<00:38, 20.73it/s, est. speed input: 24401.24 toks/s, output: 23.83 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:09<00:38, 20.73it/s, est. speed input: 24272.40 toks/s, output: 23.70 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:09<00:38, 20.73it/s, est. speed input: 24153.61 toks/s, output: 23.59 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:10<00:37, 20.72it/s, est. speed input: 24042.71 toks/s, output: 23.48 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:10<00:37, 20.71it/s, est. speed input: 23939.67 toks/s, output: 23.38 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:11<00:36, 20.71it/s, est. speed input: 23844.70 toks/s, output: 23.29 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:11<00:36, 20.72it/s, est. speed input: 23756.27 toks/s, output: 23.20 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:11<00:36, 20.72it/s, est. speed input: 23673.65 toks/s, output: 23.12 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:12<00:35, 20.71it/s, est. speed input: 23595.54 toks/s, output: 23.04 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:12<00:35, 20.72it/s, est. speed input: 23522.76 toks/s, output: 22.97 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:13<00:35, 20.72it/s, est. speed input: 23454.43 toks/s, output: 22.90 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:13<00:34, 20.71it/s, est. speed input: 23388.59 toks/s, output: 22.84 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:13<00:34, 20.71it/s, est. speed input: 23327.30 toks/s, output: 22.78 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:14<00:33, 20.70it/s, est. speed input: 23268.93 toks/s, output: 22.72 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:14<00:33, 20.70it/s, est. speed input: 23213.66 toks/s, output: 22.67 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:14<00:33, 20.70it/s, est. speed input: 23161.72 toks/s, output: 22.62 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:15<00:32, 20.70it/s, est. speed input: 23111.93 toks/s, output: 22.57 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:15<00:32, 20.70it/s, est. speed input: 23065.06 toks/s, output: 22.52 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:16<00:31, 20.70it/s, est. speed input: 23020.34 toks/s, output: 22.48 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:16<00:31, 20.69it/s, est. speed input: 22976.85 toks/s, output: 22.44 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:16<00:31, 20.70it/s, est. speed input: 22936.09 toks/s, output: 22.40 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:17<00:30, 20.69it/s, est. speed input: 22896.59 toks/s, output: 22.36 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:17<00:30, 20.69it/s, est. speed input: 22859.14 toks/s, output: 22.32 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:18<00:30, 20.69it/s, est. speed input: 22823.59 toks/s, output: 22.29 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:18<00:29, 20.69it/s, est. speed input: 22789.39 toks/s, output: 22.26 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:18<00:29, 20.69it/s, est. speed input: 22756.44 toks/s, output: 22.22 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:19<00:28, 20.69it/s, est. speed input: 22724.89 toks/s, output: 22.19 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:19<00:28, 20.69it/s, est. speed input: 22694.15 toks/s, output: 22.16 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:19<00:28, 20.68it/s, est. speed input: 22664.52 toks/s, output: 22.13 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:20<00:27, 20.68it/s, est. speed input: 22636.31 toks/s, output: 22.11 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:20<00:27, 20.68it/s, est. speed input: 22609.20 toks/s, output: 22.08 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:21<00:26, 20.68it/s, est. speed input: 22583.08 toks/s, output: 22.05 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:21<00:26, 20.68it/s, est. speed input: 22557.60 toks/s, output: 22.03 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:21<00:26, 20.68it/s, est. speed input: 22533.45 toks/s, output: 22.01 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:22<00:25, 20.68it/s, est. speed input: 22510.02 toks/s, output: 21.98 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:22<00:25, 20.68it/s, est. speed input: 22486.96 toks/s, output: 21.96 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:23<00:25, 20.68it/s, est. speed input: 22464.98 toks/s, output: 21.94 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:23<00:24, 20.69it/s, est. speed input: 22444.14 toks/s, output: 21.92 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:23<00:24, 20.69it/s, est. speed input: 22423.61 toks/s, output: 21.90 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:24<00:23, 20.68it/s, est. speed input: 22403.70 toks/s, output: 21.88 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:24<00:23, 20.69it/s, est. speed input: 22384.59 toks/s, output: 21.86 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:24<00:23, 20.68it/s, est. speed input: 22365.91 toks/s, output: 21.84 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:25<00:22, 20.68it/s, est. speed input: 22347.61 toks/s, output: 21.82 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:25<00:22, 20.68it/s, est. speed input: 22329.97 toks/s, output: 21.81 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:26<00:21, 20.68it/s, est. speed input: 22312.83 toks/s, output: 21.79 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:26<00:21, 20.68it/s, est. speed input: 22296.34 toks/s, output: 21.77 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:26<00:21, 20.68it/s, est. speed input: 22280.11 toks/s, output: 21.76 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:27<00:20, 20.67it/s, est. speed input: 22264.17 toks/s, output: 21.74 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:27<00:20, 20.67it/s, est. speed input: 22248.72 toks/s, output: 21.73 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:28<00:20, 20.67it/s, est. speed input: 22233.97 toks/s, output: 21.71 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:28<00:19, 20.67it/s, est. speed input: 22219.45 toks/s, output: 21.70 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:28<00:19, 20.67it/s, est. speed input: 22205.26 toks/s, output: 21.68 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:29<00:18, 20.67it/s, est. speed input: 22191.56 toks/s, output: 21.67 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:29<00:18, 20.67it/s, est. speed input: 22178.16 toks/s, output: 21.66 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:30<00:18, 20.67it/s, est. speed input: 22165.12 toks/s, output: 21.65 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:30<00:17, 20.67it/s, est. speed input: 22152.40 toks/s, output: 21.63 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:30<00:17, 20.66it/s, est. speed input: 22139.77 toks/s, output: 21.62 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:31<00:16, 20.67it/s, est. speed input: 22127.74 toks/s, output: 21.61 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:31<00:16, 20.66it/s, est. speed input: 22115.79 toks/s, output: 21.60 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:31<00:16, 20.66it/s, est. speed input: 22104.03 toks/s, output: 21.59 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:32<00:15, 20.67it/s, est. speed input: 22092.90 toks/s, output: 21.58 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:32<00:15, 20.67it/s, est. speed input: 22081.94 toks/s, output: 21.56 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:33<00:15, 20.66it/s, est. speed input: 22070.99 toks/s, output: 21.55 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:33<00:14, 20.66it/s, est. speed input: 22060.35 toks/s, output: 21.54 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:33<00:14, 20.66it/s, est. speed input: 22049.89 toks/s, output: 21.53 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:34<00:13, 20.66it/s, est. speed input: 22039.74 toks/s, output: 21.52 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:34<00:13, 20.66it/s, est. speed input: 22029.84 toks/s, output: 21.51 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:35<00:13, 20.65it/s, est. speed input: 22019.99 toks/s, output: 21.50 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:35<00:12, 20.65it/s, est. speed input: 22010.41 toks/s, output: 21.49 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:35<00:12, 20.65it/s, est. speed input: 22001.00 toks/s, output: 21.49 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:36<00:11, 20.65it/s, est. speed input: 21992.06 toks/s, output: 21.48 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:36<00:11, 20.66it/s, est. speed input: 21983.22 toks/s, output: 21.47 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:36<00:11, 20.66it/s, est. speed input: 21974.74 toks/s, output: 21.46 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:37<00:10, 20.66it/s, est. speed input: 21966.21 toks/s, output: 21.45 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:37<00:10, 20.66it/s, est. speed input: 21957.75 toks/s, output: 21.44 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:38<00:09, 20.66it/s, est. speed input: 21949.62 toks/s, output: 21.44 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:38<00:09, 20.66it/s, est. speed input: 21941.81 toks/s, output: 21.43 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:38<00:09, 20.66it/s, est. speed input: 21934.00 toks/s, output: 21.42 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:39<00:08, 20.66it/s, est. speed input: 21926.38 toks/s, output: 21.41 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:39<00:08, 20.66it/s, est. speed input: 21918.72 toks/s, output: 21.40 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:40<00:08, 20.66it/s, est. speed input: 21911.40 toks/s, output: 21.40 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:40<00:07, 20.65it/s, est. speed input: 21903.95 toks/s, output: 21.39 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:40<00:07, 20.65it/s, est. speed input: 21896.71 toks/s, output: 21.38 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:41<00:06, 20.65it/s, est. speed input: 21889.71 toks/s, output: 21.38 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:41<00:06, 20.66it/s, est. speed input: 21882.92 toks/s, output: 21.37 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:42<00:06, 20.66it/s, est. speed input: 21876.17 toks/s, output: 21.36 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:42<00:05, 20.65it/s, est. speed input: 21869.40 toks/s, output: 21.36 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:42<00:05, 20.65it/s, est. speed input: 21862.80 toks/s, output: 21.35 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:43<00:04, 20.65it/s, est. speed input: 21856.37 toks/s, output: 21.34 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:43<00:04, 20.64it/s, est. speed input: 21849.76 toks/s, output: 21.34 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:43<00:04, 20.64it/s, est. speed input: 21843.44 toks/s, output: 21.33 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:44<00:03, 20.64it/s, est. speed input: 21837.20 toks/s, output: 21.33 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:44<00:03, 20.64it/s, est. speed input: 21831.23 toks/s, output: 21.32 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:45<00:03, 20.65it/s, est. speed input: 21825.42 toks/s, output: 21.31 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:45<00:02, 20.64it/s, est. speed input: 21819.52 toks/s, output: 21.31 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:45<00:02, 20.64it/s, est. speed input: 21813.71 toks/s, output: 21.30 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:46<00:01, 20.64it/s, est. speed input: 21808.05 toks/s, output: 21.30 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:46<00:01, 20.63it/s, est. speed input: 21802.26 toks/s, output: 21.29 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:47<00:01, 20.63it/s, est. speed input: 21796.72 toks/s, output: 21.29 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:47<00:00, 20.63it/s, est. speed input: 21791.27 toks/s, output: 21.28 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:47<00:00, 21.41it/s, est. speed input: 21807.32 toks/s, output: 21.30 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:47<00:00, 21.41it/s, est. speed input: 21935.80 toks/s, output: 21.42 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:47<00:00, 21.42it/s, est. speed input: 21935.80 toks/s, output: 21.42 toks/s]
[rank0]:[W126 02:32:43.742980244 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 02:32:59
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.6 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:33:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=849361) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=849361) WARNING 01-26 02:33:21 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=849361) ERROR 01-26 02:33:24 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 02:33:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:33:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:33:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:33:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:33:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:33:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:33:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:33:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:33:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:33:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:33:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:33:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:33:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:33:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:33:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:33:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=849361) [2026-01-26 02:33:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=849361) [2026-01-26 02:33:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=849361) [2026-01-26 02:33:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=849361) [2026-01-26 02:33:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=849361) [2026-01-26 02:33:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=849361) [2026-01-26 02:33:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=849361) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=849361) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=849361) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
(EngineCore_DP0 pid=849361) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]
(EngineCore_DP0 pid=849361) 
(EngineCore_DP0 pid=849361) [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=849361) [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=849361) [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=849361) [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=849361) [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=849361) [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=849361) [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=849361) [2026-01-26 02:33:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=849361) Process EngineCore_DP0:
(EngineCore_DP0 pid=849361) Traceback (most recent call last):
(EngineCore_DP0 pid=849361)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=849361)     self.run()
(EngineCore_DP0 pid=849361)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=849361)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=849361)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=849361)     raise e
(EngineCore_DP0 pid=849361)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=849361)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=849361)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=849361)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=849361)     super().__init__(
(EngineCore_DP0 pid=849361)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=849361)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=849361)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=849361)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=849361)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=849361)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=849361)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=849361)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=849361)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=849361)     raise ValueError(
(EngineCore_DP0 pid=849361) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 02:33:25.888069013 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16384

========== M=32768 ==========
Time: 2026-01-26 02:33:41
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.5 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:33:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=850457) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=850457) WARNING 01-26 02:34:10 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     def forward(
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     raise e
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/tmp/torchinductor_root/ly/clyalnwiaizemjabupngf24dmvovjtiqf6jb3aehujiaoncqbsa7.py", line 1078, in call
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) ERROR 01-26 02:34:13 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.51 GiB is free. Including non-PyTorch memory, this process has 13.93 GiB memory in use. Of the allocated memory 11.41 GiB is allocated by PyTorch, and 2.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-26 02:33:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:33:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:33:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:33:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:33:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:33:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:33:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:33:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:33:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:34:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:34:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:34:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:34:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:34:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:34:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:34:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:34:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=850457) [2026-01-26 02:34:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=850457) [2026-01-26 02:34:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=850457) [2026-01-26 02:34:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=850457) [2026-01-26 02:34:04] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=850457) [2026-01-26 02:34:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=850457) [2026-01-26 02:34:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=850457) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=850457) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.49it/s]
(EngineCore_DP0 pid=850457) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
(EngineCore_DP0 pid=850457) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]
(EngineCore_DP0 pid=850457) 
(EngineCore_DP0 pid=850457) [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=850457) [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=850457) [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=850457) [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=850457) [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=850457) [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=850457) [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=850457) [2026-01-26 02:34:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=850457) [rank0]:W0126 02:34:13.481000 850457 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=850457) [rank0]:W0126 02:34:13.576000 850457 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=850457) Process EngineCore_DP0:
(EngineCore_DP0 pid=850457) Traceback (most recent call last):
(EngineCore_DP0 pid=850457)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=850457)     self.run()
(EngineCore_DP0 pid=850457)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=850457)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=850457)     raise e
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=850457)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=850457)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=850457)     super().__init__(
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=850457)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=850457)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=850457)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=850457)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=850457)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=850457)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=850457)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=850457)     return func(*args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=850457)     return func(*args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=850457)     self.model_runner.profile_run()
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=850457)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=850457)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=850457)     return func(*args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=850457)     outputs = self.model(
(EngineCore_DP0 pid=850457)               ^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=850457)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=850457)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=850457)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=850457)     hidden_states = self.model(
(EngineCore_DP0 pid=850457)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=850457)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=850457)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=850457)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=850457)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=850457)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=850457)     def forward(
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=850457)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=850457)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=850457)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=850457)     raise e
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=850457)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=850457)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=850457)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=850457)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=850457)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=850457)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=850457)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=850457)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=850457)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=850457)     return compiled_fn(full_args)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=850457)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=850457)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=850457)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=850457)                             ^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=850457)     outs = compiled_fn(args)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=850457)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=850457)     return self.current_callable(inputs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=850457)     out = model(new_inputs)
(EngineCore_DP0 pid=850457)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/tmp/torchinductor_root/ly/clyalnwiaizemjabupngf24dmvovjtiqf6jb3aehujiaoncqbsa7.py", line 1078, in call
(EngineCore_DP0 pid=850457)     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
(EngineCore_DP0 pid=850457)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=850457)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=850457)     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=850457)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457)   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
(EngineCore_DP0 pid=850457)     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=850457)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=850457) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.51 GiB is free. Including non-PyTorch memory, this process has 13.93 GiB memory in use. Of the allocated memory 11.41 GiB is allocated by PyTorch, and 2.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:34:14.933350716 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=32768

========== M=32768 ==========
Time: 2026-01-26 02:34:25
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.45 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:34:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=851473) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=851473) WARNING 01-26 02:34:53 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     def forward(
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     raise e
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/tmp/torchinductor_root/ly/clyalnwiaizemjabupngf24dmvovjtiqf6jb3aehujiaoncqbsa7.py", line 1078, in call
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) ERROR 01-26 02:34:56 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.45 GiB is free. Including non-PyTorch memory, this process has 13.99 GiB memory in use. Of the allocated memory 11.41 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-26 02:34:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:34:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:34:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:34:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:34:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:34:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:34:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:34:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:34:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:34:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:34:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:34:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:34:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:34:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:34:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:34:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:34:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=851473) [2026-01-26 02:34:47] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=851473) [2026-01-26 02:34:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=851473) [2026-01-26 02:34:47] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=851473) [2026-01-26 02:34:47] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=851473) [2026-01-26 02:34:47] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=851473) [2026-01-26 02:34:47] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=851473) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=851473) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=851473) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
(EngineCore_DP0 pid=851473) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]
(EngineCore_DP0 pid=851473) 
(EngineCore_DP0 pid=851473) [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=851473) [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=851473) [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=851473) [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=851473) [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=851473) [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=851473) [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=851473) [2026-01-26 02:34:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=851473) Process EngineCore_DP0:
(EngineCore_DP0 pid=851473) Traceback (most recent call last):
(EngineCore_DP0 pid=851473)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=851473)     self.run()
(EngineCore_DP0 pid=851473)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=851473)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=851473)     raise e
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=851473)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=851473)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=851473)     super().__init__(
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=851473)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=851473)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=851473)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=851473)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=851473)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=851473)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=851473)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=851473)     return func(*args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=851473)     return func(*args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=851473)     self.model_runner.profile_run()
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=851473)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=851473)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=851473)     return func(*args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=851473)     outputs = self.model(
(EngineCore_DP0 pid=851473)               ^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=851473)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=851473)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=851473)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=851473)     hidden_states = self.model(
(EngineCore_DP0 pid=851473)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=851473)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=851473)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=851473)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=851473)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=851473)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=851473)     def forward(
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=851473)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=851473)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=851473)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=851473)     raise e
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=851473)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=851473)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=851473)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=851473)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=851473)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=851473)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=851473)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=851473)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=851473)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=851473)     return compiled_fn(full_args)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=851473)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=851473)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=851473)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=851473)                             ^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=851473)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=851473)     return self.current_callable(inputs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=851473)     out = model(new_inputs)
(EngineCore_DP0 pid=851473)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/tmp/torchinductor_root/ly/clyalnwiaizemjabupngf24dmvovjtiqf6jb3aehujiaoncqbsa7.py", line 1078, in call
(EngineCore_DP0 pid=851473)     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
(EngineCore_DP0 pid=851473)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=851473)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=851473)     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=851473)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473)   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
(EngineCore_DP0 pid=851473)     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=851473)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=851473) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.45 GiB is free. Including non-PyTorch memory, this process has 13.99 GiB memory in use. Of the allocated memory 11.41 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:34:56.237085113 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=32768

========== M=32768 ==========
Time: 2026-01-26 02:35:07
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.4 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:35:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=852458) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=852458) WARNING 01-26 02:35:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     def forward(
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     raise e
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/tmp/torchinductor_root/ly/clyalnwiaizemjabupngf24dmvovjtiqf6jb3aehujiaoncqbsa7.py", line 1078, in call
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) ERROR 01-26 02:35:38 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.45 GiB is free. Including non-PyTorch memory, this process has 13.99 GiB memory in use. Of the allocated memory 11.41 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-26 02:35:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:35:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:35:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:35:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:35:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:35:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:35:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:35:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:35:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:35:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:35:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:35:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:35:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:35:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:35:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:35:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:35:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=852458) [2026-01-26 02:35:29] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=852458) [2026-01-26 02:35:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=852458) [2026-01-26 02:35:29] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=852458) [2026-01-26 02:35:29] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=852458) [2026-01-26 02:35:29] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=852458) [2026-01-26 02:35:29] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=852458) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=852458) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.49it/s]
(EngineCore_DP0 pid=852458) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]
(EngineCore_DP0 pid=852458) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
(EngineCore_DP0 pid=852458) 
(EngineCore_DP0 pid=852458) [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=852458) [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=852458) [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=852458) [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=852458) [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=852458) [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=852458) [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=852458) [2026-01-26 02:35:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=852458) Process EngineCore_DP0:
(EngineCore_DP0 pid=852458) Traceback (most recent call last):
(EngineCore_DP0 pid=852458)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=852458)     self.run()
(EngineCore_DP0 pid=852458)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=852458)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=852458)     raise e
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=852458)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=852458)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=852458)     super().__init__(
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=852458)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=852458)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=852458)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=852458)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=852458)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=852458)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=852458)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=852458)     return func(*args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=852458)     return func(*args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=852458)     self.model_runner.profile_run()
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=852458)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=852458)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=852458)     return func(*args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=852458)     outputs = self.model(
(EngineCore_DP0 pid=852458)               ^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=852458)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=852458)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=852458)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=852458)     hidden_states = self.model(
(EngineCore_DP0 pid=852458)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=852458)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=852458)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=852458)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=852458)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=852458)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=852458)     def forward(
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=852458)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=852458)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=852458)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=852458)     raise e
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=852458)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=852458)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=852458)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=852458)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=852458)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=852458)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=852458)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=852458)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=852458)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=852458)     return compiled_fn(full_args)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=852458)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=852458)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=852458)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=852458)                             ^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=852458)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=852458)     return self.current_callable(inputs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=852458)     out = model(new_inputs)
(EngineCore_DP0 pid=852458)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/tmp/torchinductor_root/ly/clyalnwiaizemjabupngf24dmvovjtiqf6jb3aehujiaoncqbsa7.py", line 1078, in call
(EngineCore_DP0 pid=852458)     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 37888), (37888, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Qwen2.5-7B-INT8')
(EngineCore_DP0 pid=852458)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=852458)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=852458)     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=852458)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458)   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Qwen2.5-7B.py", line 110, in dequant_bias_triton
(EngineCore_DP0 pid=852458)     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=852458)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=852458) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 15.47 GiB of which 1.45 GiB is free. Including non-PyTorch memory, this process has 13.99 GiB memory in use. Of the allocated memory 11.41 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:35:38.734150720 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=32768

========== M=65536 ==========
Time: 2026-01-26 02:35:55
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.45 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:36:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=853736) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=853736) WARNING 01-26 02:36:38 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     compiled_module = graph.compile_to_module()
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     return self._compile_to_module()
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]                                                              ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     return self._generate(is_inference)
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     self.generate_and_run_autotune_block()
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
(EngineCore_DP0 pid=853736) ERROR 01-26 02:36:41 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 13.12 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-26 02:36:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:36:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:36:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:36:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:36:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:36:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:36:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:36:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:36:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:36:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:36:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:36:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:36:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:36:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:36:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:36:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:36:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=853736) [2026-01-26 02:36:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=853736) [2026-01-26 02:36:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=853736) [2026-01-26 02:36:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=853736) [2026-01-26 02:36:32] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=853736) [2026-01-26 02:36:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=853736) [2026-01-26 02:36:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=853736) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=853736) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=853736) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]
(EngineCore_DP0 pid=853736) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
(EngineCore_DP0 pid=853736) 
(EngineCore_DP0 pid=853736) [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=853736) [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=853736) [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=853736) [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=853736) [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=853736) [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=853736) [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=853736) [2026-01-26 02:36:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=853736) [rank0]:W0126 02:36:41.119000 853736 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=853736) Process EngineCore_DP0:
(EngineCore_DP0 pid=853736) Traceback (most recent call last):
(EngineCore_DP0 pid=853736)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=853736)     self.run()
(EngineCore_DP0 pid=853736)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=853736)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=853736)     raise e
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=853736)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=853736)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=853736)     super().__init__(
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=853736)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=853736)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=853736)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=853736)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=853736)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=853736)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=853736)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=853736)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=853736)     return func(*args, **kwargs)
(EngineCore_DP0 pid=853736)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=853736)     return func(*args, **kwargs)
(EngineCore_DP0 pid=853736)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=853736)     self.model_runner.profile_run()
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=853736)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=853736)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=853736)     return func(*args, **kwargs)
(EngineCore_DP0 pid=853736)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=853736)     outputs = self.model(
(EngineCore_DP0 pid=853736)               ^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=853736)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=853736)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=853736)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=853736)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=853736)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=853736)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=853736)     hidden_states = self.model(
(EngineCore_DP0 pid=853736)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=853736)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=853736)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=853736)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=853736)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=853736)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=853736)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
(EngineCore_DP0 pid=853736)     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
(EngineCore_DP0 pid=853736)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
(EngineCore_DP0 pid=853736)     raise InductorError(e, currentframe()).with_traceback(
(EngineCore_DP0 pid=853736)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
(EngineCore_DP0 pid=853736)     mb_compiled_graph = fx_codegen_and_compile(
(EngineCore_DP0 pid=853736)                         ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
(EngineCore_DP0 pid=853736)     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
(EngineCore_DP0 pid=853736)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
(EngineCore_DP0 pid=853736)     compiled_module = graph.compile_to_module()
(EngineCore_DP0 pid=853736)                       ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
(EngineCore_DP0 pid=853736)     return self._compile_to_module()
(EngineCore_DP0 pid=853736)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
(EngineCore_DP0 pid=853736)     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
(EngineCore_DP0 pid=853736)                                                              ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
(EngineCore_DP0 pid=853736)     result = self.wrapper_code.generate(self.is_inference)
(EngineCore_DP0 pid=853736)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
(EngineCore_DP0 pid=853736)     return self._generate(is_inference)
(EngineCore_DP0 pid=853736)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=853736)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
(EngineCore_DP0 pid=853736)     self.generate_and_run_autotune_block()
(EngineCore_DP0 pid=853736)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
(EngineCore_DP0 pid=853736)     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
(EngineCore_DP0 pid=853736) torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 13.12 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:36:41.524996581 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=65536 ==========
Time: 2026-01-26 02:36:52
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.4 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:37:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=854936) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=854936) WARNING 01-26 02:37:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     compiled_module = graph.compile_to_module()
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     return self._compile_to_module()
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]                                                              ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     return self._generate(is_inference)
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     self.generate_and_run_autotune_block()
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
(EngineCore_DP0 pid=854936) ERROR 01-26 02:37:39 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 13.12 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-26 02:37:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:37:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:37:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:37:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:37:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:37:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:37:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:37:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:37:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:37:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:37:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:37:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:37:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:37:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:37:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:37:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:37:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=854936) [2026-01-26 02:37:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=854936) [2026-01-26 02:37:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=854936) [2026-01-26 02:37:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=854936) [2026-01-26 02:37:30] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=854936) [2026-01-26 02:37:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=854936) [2026-01-26 02:37:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=854936) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=854936) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=854936) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
(EngineCore_DP0 pid=854936) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]
(EngineCore_DP0 pid=854936) 
(EngineCore_DP0 pid=854936) [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=854936) [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=854936) [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=854936) [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=854936) [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=854936) [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=854936) [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=854936) [2026-01-26 02:37:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=854936) [rank0]:W0126 02:37:39.307000 854936 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=854936) Process EngineCore_DP0:
(EngineCore_DP0 pid=854936) Traceback (most recent call last):
(EngineCore_DP0 pid=854936)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=854936)     self.run()
(EngineCore_DP0 pid=854936)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=854936)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=854936)     raise e
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=854936)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=854936)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=854936)     super().__init__(
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=854936)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=854936)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=854936)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=854936)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=854936)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=854936)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=854936)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=854936)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=854936)     return func(*args, **kwargs)
(EngineCore_DP0 pid=854936)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=854936)     return func(*args, **kwargs)
(EngineCore_DP0 pid=854936)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=854936)     self.model_runner.profile_run()
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=854936)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=854936)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=854936)     return func(*args, **kwargs)
(EngineCore_DP0 pid=854936)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=854936)     outputs = self.model(
(EngineCore_DP0 pid=854936)               ^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=854936)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=854936)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=854936)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=854936)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=854936)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=854936)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=854936)     hidden_states = self.model(
(EngineCore_DP0 pid=854936)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=854936)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=854936)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=854936)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=854936)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=854936)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=854936)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
(EngineCore_DP0 pid=854936)     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
(EngineCore_DP0 pid=854936)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
(EngineCore_DP0 pid=854936)     raise InductorError(e, currentframe()).with_traceback(
(EngineCore_DP0 pid=854936)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
(EngineCore_DP0 pid=854936)     mb_compiled_graph = fx_codegen_and_compile(
(EngineCore_DP0 pid=854936)                         ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
(EngineCore_DP0 pid=854936)     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
(EngineCore_DP0 pid=854936)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
(EngineCore_DP0 pid=854936)     compiled_module = graph.compile_to_module()
(EngineCore_DP0 pid=854936)                       ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
(EngineCore_DP0 pid=854936)     return self._compile_to_module()
(EngineCore_DP0 pid=854936)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
(EngineCore_DP0 pid=854936)     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
(EngineCore_DP0 pid=854936)                                                              ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
(EngineCore_DP0 pid=854936)     result = self.wrapper_code.generate(self.is_inference)
(EngineCore_DP0 pid=854936)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
(EngineCore_DP0 pid=854936)     return self._generate(is_inference)
(EngineCore_DP0 pid=854936)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=854936)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
(EngineCore_DP0 pid=854936)     self.generate_and_run_autotune_block()
(EngineCore_DP0 pid=854936)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
(EngineCore_DP0 pid=854936)     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
(EngineCore_DP0 pid=854936) torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 13.12 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:37:39.707966227 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=65536 ==========
Time: 2026-01-26 02:37:51
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.35000000000000003 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:38:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=856153) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=856153) WARNING 01-26 02:38:34 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     raise InductorError(e, currentframe()).with_traceback(
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     mb_compiled_graph = fx_codegen_and_compile(
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]                         ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     compiled_module = graph.compile_to_module()
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     return self._compile_to_module()
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]                                                              ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     result = self.wrapper_code.generate(self.is_inference)
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     return self._generate(is_inference)
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     self.generate_and_run_autotune_block()
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866]     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
(EngineCore_DP0 pid=856153) ERROR 01-26 02:38:37 [core.py:866] torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 13.12 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-26 02:38:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:38:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:38:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:38:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:38:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:38:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:38:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:38:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:38:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 02:38:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:38:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 02:38:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 02:38:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:38:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:38:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:38:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:38:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=856153) [2026-01-26 02:38:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=856153) [2026-01-26 02:38:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=856153) [2026-01-26 02:38:28] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=856153) [2026-01-26 02:38:28] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=856153) [2026-01-26 02:38:28] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=856153) [2026-01-26 02:38:28] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=856153) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=856153) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.49it/s]
(EngineCore_DP0 pid=856153) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]
(EngineCore_DP0 pid=856153) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
(EngineCore_DP0 pid=856153) 
(EngineCore_DP0 pid=856153) [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=856153) [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=856153) [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=856153) [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=856153) [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=856153) [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=856153) [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=856153) [2026-01-26 02:38:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=856153) [rank0]:W0126 02:38:37.458000 856153 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=856153) Process EngineCore_DP0:
(EngineCore_DP0 pid=856153) Traceback (most recent call last):
(EngineCore_DP0 pid=856153)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=856153)     self.run()
(EngineCore_DP0 pid=856153)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=856153)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=856153)     raise e
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=856153)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=856153)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=856153)     super().__init__(
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=856153)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=856153)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=856153)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=856153)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=856153)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=856153)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=856153)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=856153)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=856153)     return func(*args, **kwargs)
(EngineCore_DP0 pid=856153)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=856153)     return func(*args, **kwargs)
(EngineCore_DP0 pid=856153)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=856153)     self.model_runner.profile_run()
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=856153)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=856153)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=856153)     return func(*args, **kwargs)
(EngineCore_DP0 pid=856153)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=856153)     outputs = self.model(
(EngineCore_DP0 pid=856153)               ^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=856153)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=856153)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=856153)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=856153)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=856153)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=856153)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=856153)     hidden_states = self.model(
(EngineCore_DP0 pid=856153)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=856153)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=856153)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=856153)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=856153)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=856153)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=856153)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 845, in compile_wrapper
(EngineCore_DP0 pid=856153)     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
(EngineCore_DP0 pid=856153)     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 990, in _compile_fx_inner
(EngineCore_DP0 pid=856153)     raise InductorError(e, currentframe()).with_traceback(
(EngineCore_DP0 pid=856153)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 974, in _compile_fx_inner
(EngineCore_DP0 pid=856153)     mb_compiled_graph = fx_codegen_and_compile(
(EngineCore_DP0 pid=856153)                         ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1695, in fx_codegen_and_compile
(EngineCore_DP0 pid=856153)     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
(EngineCore_DP0 pid=856153)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 1505, in codegen_and_compile
(EngineCore_DP0 pid=856153)     compiled_module = graph.compile_to_module()
(EngineCore_DP0 pid=856153)                       ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2319, in compile_to_module
(EngineCore_DP0 pid=856153)     return self._compile_to_module()
(EngineCore_DP0 pid=856153)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2325, in _compile_to_module
(EngineCore_DP0 pid=856153)     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()
(EngineCore_DP0 pid=856153)                                                              ^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py", line 2271, in codegen
(EngineCore_DP0 pid=856153)     result = self.wrapper_code.generate(self.is_inference)
(EngineCore_DP0 pid=856153)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1552, in generate
(EngineCore_DP0 pid=856153)     return self._generate(is_inference)
(EngineCore_DP0 pid=856153)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=856153)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1615, in _generate
(EngineCore_DP0 pid=856153)     self.generate_and_run_autotune_block()
(EngineCore_DP0 pid=856153)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/wrapper.py", line 1695, in generate_and_run_autotune_block
(EngineCore_DP0 pid=856153)     raise RuntimeError(f"Failed to run autotuning code block: {e}") from e
(EngineCore_DP0 pid=856153) torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 15.47 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 13.12 GiB memory in use. Of the allocated memory 10.41 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W126 02:38:38.862140672 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-28 00:19:42
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/BitNet-2B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:19:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3471216) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3471216) WARNING 01-28 00:19:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 54.95 requests/s, 28188.79 total tokens/s, 54.95 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-28 00:19:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:19:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:19:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:19:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:19:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:19:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:19:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:19:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:19:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:19:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:19:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:19:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:19:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:19:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:19:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:19:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:19:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:19:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:19:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3471216) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3471216) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3471216) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3471216) 
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3471216) [2026-01-28 00:19:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3471216) 2026-01-28 00:20:01,838 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3471216) 2026-01-28 00:20:01,853 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3471216) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.86it/s]
(EngineCore_DP0 pid=3471216) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.38it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  88%|████████▊ | 112/128 [00:00<00:00, 1112.80it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1132.65it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:22,  5.57it/s, est. speed input: 2850.30 toks/s, output: 5.57 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 32.17it/s, est. speed input: 13966.49 toks/s, output: 27.28 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 44.36it/s, est. speed input: 18963.77 toks/s, output: 37.04 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 51.17it/s, est. speed input: 21839.63 toks/s, output: 42.65 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 55.06it/s, est. speed input: 23652.60 toks/s, output: 46.20 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 57.43it/s, est. speed input: 24900.49 toks/s, output: 48.63 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 58.17it/s, est. speed input: 25590.36 toks/s, output: 49.98 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 59.62it/s, est. speed input: 26354.26 toks/s, output: 51.47 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 60.50it/s, est. speed input: 26944.79 toks/s, output: 52.63 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 60.94it/s, est. speed input: 27401.15 toks/s, output: 53.52 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:00, 61.53it/s, est. speed input: 27814.74 toks/s, output: 54.33 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:00, 61.90it/s, est. speed input: 28158.71 toks/s, output: 55.00 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:00, 62.12it/s, est. speed input: 28446.79 toks/s, output: 55.56 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 62.24it/s, est. speed input: 28693.21 toks/s, output: 56.04 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:01<00:00, 62.43it/s, est. speed input: 28917.65 toks/s, output: 56.48 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 62.39it/s, est. speed input: 29098.56 toks/s, output: 56.83 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:01<00:00, 62.22it/s, est. speed input: 29246.12 toks/s, output: 57.12 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 62.46it/s, est. speed input: 29408.20 toks/s, output: 57.44 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 62.69it/s, est. speed input: 29558.42 toks/s, output: 57.73 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 62.69it/s, est. speed input: 29592.81 toks/s, output: 57.80 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 57.80it/s, est. speed input: 29592.81 toks/s, output: 57.80 toks/s]
[rank0]:[W128 00:20:05.727060062 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-28 00:20:07
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/BitNet-2B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:20:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3471892) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3471892) WARNING 01-28 00:20:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 45.50 requests/s, 46634.28 total tokens/s, 45.50 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-28 00:20:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:20:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:20:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:20:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:20:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:20:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:20:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:20:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:20:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:20:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:20:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:20:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:20:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:20:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:20:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:20:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:20:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:20:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3471892) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3471892) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.83it/s]
(EngineCore_DP0 pid=3471892) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.83it/s]
(EngineCore_DP0 pid=3471892) 
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3471892) [2026-01-28 00:20:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3471892) 2026-01-28 00:20:26,958 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3471892) 2026-01-28 00:20:26,974 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3471892) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.69it/s]
(EngineCore_DP0 pid=3471892) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.57it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.57it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 633.15it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 679.85it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:04, 26.67it/s, est. speed input: 27311.95 toks/s, output: 26.67 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 39.58it/s, est. speed input: 38442.70 toks/s, output: 37.54 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:02, 44.14it/s, est. speed input: 42474.06 toks/s, output: 41.48 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 46.28it/s, est. speed input: 44507.41 toks/s, output: 43.46 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 47.51it/s, est. speed input: 45758.11 toks/s, output: 44.68 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 48.31it/s, est. speed input: 46625.82 toks/s, output: 45.53 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 48.80it/s, est. speed input: 47241.66 toks/s, output: 46.13 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:01, 49.11it/s, est. speed input: 47700.30 toks/s, output: 46.58 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 49.35it/s, est. speed input: 48069.12 toks/s, output: 46.94 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:01, 49.38it/s, est. speed input: 48324.86 toks/s, output: 47.19 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 49.51it/s, est. speed input: 48566.13 toks/s, output: 47.43 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 49.59it/s, est. speed input: 48763.96 toks/s, output: 47.62 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 49.66it/s, est. speed input: 48934.55 toks/s, output: 47.79 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 49.68it/s, est. speed input: 49076.39 toks/s, output: 47.93 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 49.62it/s, est. speed input: 49182.13 toks/s, output: 48.03 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 49.68it/s, est. speed input: 49295.50 toks/s, output: 48.14 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:00, 49.82it/s, est. speed input: 49433.08 toks/s, output: 48.27 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 49.87it/s, est. speed input: 49546.46 toks/s, output: 48.38 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 49.80it/s, est. speed input: 49611.62 toks/s, output: 48.45 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 49.79it/s, est. speed input: 49675.86 toks/s, output: 48.51 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 49.78it/s, est. speed input: 49735.33 toks/s, output: 48.57 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 49.83it/s, est. speed input: 49808.91 toks/s, output: 48.64 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 49.81it/s, est. speed input: 49857.46 toks/s, output: 48.69 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 49.79it/s, est. speed input: 49900.52 toks/s, output: 48.73 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 49.80it/s, est. speed input: 49943.75 toks/s, output: 48.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 49.80it/s, est. speed input: 49965.50 toks/s, output: 48.79 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 48.79it/s, est. speed input: 49965.50 toks/s, output: 48.79 toks/s]
[rank0]:[W128 00:20:31.992028323 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-28 00:20:32
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/BitNet-2B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:20:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3472542) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3472542) WARNING 01-28 00:20:46 [backends.py:609] Failed to read file <frozen os>
Throughput: 49.24 requests/s, 50468.17 total tokens/s, 49.24 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-28 00:20:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:20:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:20:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:20:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:20:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:20:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:20:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:20:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:20:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:20:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:20:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:20:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:20:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:20:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:20:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:20:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:20:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:20:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:20:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3472542) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3472542) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.91it/s]
(EngineCore_DP0 pid=3472542) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.91it/s]
(EngineCore_DP0 pid=3472542) 
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3472542) [2026-01-28 00:20:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3472542) 2026-01-28 00:20:52,153 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3472542) 2026-01-28 00:20:52,168 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3472542) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 22.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 22.88it/s]
(EngineCore_DP0 pid=3472542) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.49it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.96it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:38,  6.63it/s]
Adding requests:  29%|██▉       | 74/256 [00:00<00:00, 358.06it/s]
Adding requests:  58%|█████▊    | 149/256 [00:00<00:00, 516.11it/s]
Adding requests:  87%|████████▋ | 223/256 [00:00<00:00, 597.03it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 513.09it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 171.73it/s, est. speed input: 175867.20 toks/s, output: 171.73 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:00<00:02, 74.07it/s, est. speed input: 83329.30 toks/s, output: 81.38 toks/s]   
Processed prompts:  19%|█▉        | 49/256 [00:00<00:03, 67.09it/s, est. speed input: 75751.76 toks/s, output: 73.98 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:00<00:03, 59.54it/s, est. speed input: 69244.01 toks/s, output: 67.62 toks/s]
Processed prompts:  25%|██▌       | 65/256 [00:00<00:03, 59.64it/s, est. speed input: 68302.20 toks/s, output: 66.70 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:03, 55.10it/s, est. speed input: 65234.75 toks/s, output: 63.71 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:03, 54.20it/s, est. speed input: 64061.86 toks/s, output: 62.56 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:01<00:03, 53.47it/s, est. speed input: 63087.81 toks/s, output: 61.61 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:03, 52.83it/s, est. speed input: 62236.83 toks/s, output: 60.78 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:01<00:03, 52.33it/s, est. speed input: 61503.01 toks/s, output: 60.06 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:01<00:02, 52.02it/s, est. speed input: 60886.45 toks/s, output: 59.46 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:01<00:02, 51.82it/s, est. speed input: 60356.82 toks/s, output: 58.94 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:01<00:02, 51.71it/s, est. speed input: 59895.79 toks/s, output: 58.49 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:02<00:02, 51.60it/s, est. speed input: 59482.61 toks/s, output: 58.09 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:02<00:02, 51.55it/s, est. speed input: 59118.53 toks/s, output: 57.73 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:02<00:02, 51.51it/s, est. speed input: 58788.80 toks/s, output: 57.41 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:02<00:02, 51.51it/s, est. speed input: 58497.92 toks/s, output: 57.13 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:02<00:02, 51.37it/s, est. speed input: 58208.45 toks/s, output: 56.84 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:02<00:02, 51.36it/s, est. speed input: 57960.18 toks/s, output: 56.60 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:02<00:01, 51.32it/s, est. speed input: 57727.18 toks/s, output: 56.37 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:02<00:01, 51.27it/s, est. speed input: 57509.91 toks/s, output: 56.16 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:03<00:01, 51.17it/s, est. speed input: 57300.47 toks/s, output: 55.96 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:03<00:01, 51.24it/s, est. speed input: 57126.15 toks/s, output: 55.79 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:03<00:01, 51.30it/s, est. speed input: 56965.58 toks/s, output: 55.63 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:03<00:01, 51.34it/s, est. speed input: 56816.32 toks/s, output: 55.48 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:03<00:01, 51.30it/s, est. speed input: 56668.63 toks/s, output: 55.34 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:03<00:01, 51.21it/s, est. speed input: 56522.88 toks/s, output: 55.20 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:03<00:01, 51.28it/s, est. speed input: 56401.53 toks/s, output: 55.08 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:03<00:00, 51.24it/s, est. speed input: 56277.51 toks/s, output: 54.96 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:03<00:00, 51.21it/s, est. speed input: 56161.70 toks/s, output: 54.85 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:04<00:00, 51.28it/s, est. speed input: 56060.74 toks/s, output: 54.75 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:04<00:00, 51.32it/s, est. speed input: 55965.50 toks/s, output: 54.65 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:04<00:00, 51.37it/s, est. speed input: 55877.02 toks/s, output: 54.57 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:04<00:00, 51.40it/s, est. speed input: 55793.74 toks/s, output: 54.49 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:04<00:00, 51.29it/s, est. speed input: 55701.61 toks/s, output: 54.40 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:04<00:00, 51.27it/s, est. speed input: 55619.47 toks/s, output: 54.32 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 51.27it/s, est. speed input: 55793.25 toks/s, output: 54.49 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 54.48it/s, est. speed input: 55793.25 toks/s, output: 54.49 toks/s]
[rank0]:[W128 00:20:58.569137193 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-28 00:20:59
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/BitNet-2B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:21:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3473210) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3473210) WARNING 01-28 00:21:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 50.12 requests/s, 51369.33 total tokens/s, 50.12 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-28 00:21:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:21:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:21:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:21:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:21:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:21:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:21:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:21:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:21:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:21:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:21:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:21:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:21:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:21:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:21:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:21:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:21:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:21:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3473210) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3473210) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3473210) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3473210) 
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3473210) [2026-01-28 00:21:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3473210) 2026-01-28 00:21:20,996 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3473210) 2026-01-28 00:21:21,013 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3473210) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 15.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.57it/s]
(EngineCore_DP0 pid=3473210) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 14.33it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  13%|█▎        | 69/512 [00:00<00:00, 683.39it/s]
Adding requests:  28%|██▊       | 143/512 [00:00<00:00, 713.43it/s]
Adding requests:  42%|████▏     | 216/512 [00:00<00:00, 720.80it/s]
Adding requests:  57%|█████▋    | 292/512 [00:00<00:00, 733.10it/s]
Adding requests:  72%|███████▏  | 367/512 [00:00<00:00, 736.08it/s]
Adding requests:  86%|████████▌ | 441/512 [00:00<00:00, 736.63it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 731.18it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 38/512 [00:00<00:02, 219.04it/s, est. speed input: 224308.63 toks/s, output: 219.04 toks/s]
Processed prompts:  12%|█▏        | 60/512 [00:00<00:04, 93.78it/s, est. speed input: 107737.30 toks/s, output: 105.21 toks/s] 
Processed prompts:  14%|█▍        | 73/512 [00:00<00:05, 78.28it/s, est. speed input: 92491.34 toks/s, output: 90.32 toks/s]  
Processed prompts:  16%|█▌        | 83/512 [00:01<00:06, 65.07it/s, est. speed input: 81156.35 toks/s, output: 79.25 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:01<00:06, 61.37it/s, est. speed input: 77306.45 toks/s, output: 75.49 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:01<00:07, 56.70it/s, est. speed input: 73542.38 toks/s, output: 71.82 toks/s]
Processed prompts:  21%|██        | 106/512 [00:01<00:07, 54.98it/s, est. speed input: 71266.37 toks/s, output: 69.60 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:01<00:07, 53.71it/s, est. speed input: 69424.14 toks/s, output: 67.80 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:01<00:07, 52.81it/s, est. speed input: 67907.41 toks/s, output: 66.32 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:01<00:07, 52.16it/s, est. speed input: 66632.69 toks/s, output: 65.07 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:02<00:07, 51.70it/s, est. speed input: 65545.27 toks/s, output: 64.01 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:02<00:07, 51.32it/s, est. speed input: 64591.02 toks/s, output: 63.08 toks/s]
Processed prompts:  30%|███       | 154/512 [00:02<00:07, 51.04it/s, est. speed input: 63755.17 toks/s, output: 62.26 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:02<00:06, 50.87it/s, est. speed input: 63028.38 toks/s, output: 61.55 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:02<00:06, 50.73it/s, est. speed input: 62378.79 toks/s, output: 60.92 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:02<00:06, 50.68it/s, est. speed input: 61809.26 toks/s, output: 60.36 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:03<00:06, 50.57it/s, est. speed input: 61284.06 toks/s, output: 59.85 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:03<00:06, 50.54it/s, est. speed input: 60817.40 toks/s, output: 59.39 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:03<00:06, 50.53it/s, est. speed input: 60396.77 toks/s, output: 58.98 toks/s]
Processed prompts:  41%|████      | 210/512 [00:03<00:05, 50.51it/s, est. speed input: 60011.42 toks/s, output: 58.60 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:03<00:05, 50.48it/s, est. speed input: 59654.90 toks/s, output: 58.26 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:03<00:05, 50.47it/s, est. speed input: 59330.02 toks/s, output: 57.94 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:04<00:05, 50.45it/s, est. speed input: 59029.37 toks/s, output: 57.65 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:04<00:05, 50.45it/s, est. speed input: 58752.01 toks/s, output: 57.37 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:04<00:05, 50.39it/s, est. speed input: 58486.12 toks/s, output: 57.12 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:04<00:05, 50.40it/s, est. speed input: 58245.84 toks/s, output: 56.88 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:04<00:04, 50.41it/s, est. speed input: 58023.82 toks/s, output: 56.66 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:04<00:04, 50.41it/s, est. speed input: 57814.45 toks/s, output: 56.46 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:05<00:04, 50.47it/s, est. speed input: 57624.88 toks/s, output: 56.27 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:05<00:04, 50.44it/s, est. speed input: 57439.12 toks/s, output: 56.09 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:05<00:04, 50.41it/s, est. speed input: 57263.93 toks/s, output: 55.92 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:05<00:04, 50.43it/s, est. speed input: 57102.99 toks/s, output: 55.76 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:05<00:03, 50.45it/s, est. speed input: 56950.95 toks/s, output: 55.62 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:05<00:03, 50.46it/s, est. speed input: 56807.41 toks/s, output: 55.48 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:05<00:03, 50.48it/s, est. speed input: 56673.14 toks/s, output: 55.34 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:06<00:03, 50.50it/s, est. speed input: 56545.57 toks/s, output: 55.22 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:06<00:03, 50.46it/s, est. speed input: 56420.39 toks/s, output: 55.10 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:06<00:03, 50.48it/s, est. speed input: 56304.96 toks/s, output: 54.99 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:06<00:02, 50.47it/s, est. speed input: 56193.67 toks/s, output: 54.88 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:06<00:02, 50.43it/s, est. speed input: 56083.98 toks/s, output: 54.77 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:06<00:02, 50.43it/s, est. speed input: 55982.37 toks/s, output: 54.67 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:07<00:02, 50.44it/s, est. speed input: 55885.89 toks/s, output: 54.58 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:07<00:02, 50.40it/s, est. speed input: 55789.54 toks/s, output: 54.48 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:07<00:02, 50.36it/s, est. speed input: 55696.92 toks/s, output: 54.39 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:07<00:02, 50.40it/s, est. speed input: 55613.02 toks/s, output: 54.31 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:07<00:01, 50.43it/s, est. speed input: 55532.81 toks/s, output: 54.23 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:07<00:01, 50.44it/s, est. speed input: 55455.08 toks/s, output: 54.16 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:08<00:01, 50.46it/s, est. speed input: 55381.31 toks/s, output: 54.08 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:08<00:01, 50.45it/s, est. speed input: 55308.97 toks/s, output: 54.01 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:08<00:01, 50.43it/s, est. speed input: 55238.14 toks/s, output: 53.94 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:08<00:01, 50.46it/s, est. speed input: 55172.70 toks/s, output: 53.88 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:08<00:00, 50.47it/s, est. speed input: 55109.57 toks/s, output: 53.82 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:08<00:00, 50.43it/s, est. speed input: 55045.10 toks/s, output: 53.75 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:08<00:00, 50.46it/s, est. speed input: 54986.63 toks/s, output: 53.70 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:09<00:00, 50.47it/s, est. speed input: 54930.08 toks/s, output: 53.64 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:09<00:00, 50.43it/s, est. speed input: 54872.29 toks/s, output: 53.59 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:09<00:00, 50.42it/s, est. speed input: 54817.53 toks/s, output: 53.53 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 50.42it/s, est. speed input: 55100.81 toks/s, output: 53.81 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 53.81it/s, est. speed input: 55100.81 toks/s, output: 53.81 toks/s]
[rank0]:[W128 00:21:32.597433692 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-28 00:21:33
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/BitNet-2B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:21:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3473980) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3473980) WARNING 01-28 00:21:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 47.70 requests/s, 48890.71 total tokens/s, 47.70 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-28 00:21:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:21:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:21:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:21:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:21:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:21:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:21:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:21:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:21:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:21:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:21:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:21:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:21:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:21:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:21:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:21:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:21:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:21:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:21:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3473980) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3473980) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.87it/s]
(EngineCore_DP0 pid=3473980) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.87it/s]
(EngineCore_DP0 pid=3473980) 
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3473980) [2026-01-28 00:21:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3473980) 2026-01-28 00:21:55,891 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3473980) 2026-01-28 00:21:55,907 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3473980) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  3.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 12.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 11.88it/s]
(EngineCore_DP0 pid=3473980) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.98it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 72/1024 [00:00<00:01, 713.50it/s]
Adding requests:  15%|█▍        | 149/1024 [00:00<00:01, 741.36it/s]
Adding requests:  22%|██▏       | 226/1024 [00:00<00:01, 750.07it/s]
Adding requests:  29%|██▉       | 302/1024 [00:00<00:00, 751.40it/s]
Adding requests:  37%|███▋      | 378/1024 [00:00<00:00, 749.40it/s]
Adding requests:  44%|████▍     | 454/1024 [00:00<00:00, 750.48it/s]
Adding requests:  52%|█████▏    | 530/1024 [00:00<00:00, 738.64it/s]
Adding requests:  59%|█████▉    | 607/1024 [00:00<00:00, 747.49it/s]
Adding requests:  67%|██████▋   | 685/1024 [00:00<00:00, 755.89it/s]
Adding requests:  74%|███████▍  | 761/1024 [00:01<00:00, 754.69it/s]
Adding requests:  82%|████████▏ | 837/1024 [00:01<00:00, 738.13it/s]
Adding requests:  89%|████████▉ | 916/1024 [00:01<00:00, 753.09it/s]
Adding requests:  97%|█████████▋| 993/1024 [00:01<00:00, 757.71it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 751.28it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:00<00:03, 294.85it/s, est. speed input: 301950.86 toks/s, output: 294.86 toks/s]
Processed prompts:   9%|▉         | 96/1024 [00:00<00:08, 115.90it/s, est. speed input: 135661.65 toks/s, output: 132.48 toks/s]
Processed prompts:  11%|█         | 112/1024 [00:01<00:10, 87.33it/s, est. speed input: 108358.11 toks/s, output: 105.82 toks/s]
Processed prompts:  12%|█▏        | 123/1024 [00:01<00:13, 66.91it/s, est. speed input: 90452.23 toks/s, output: 88.33 toks/s]  
Processed prompts:  13%|█▎        | 131/1024 [00:01<00:14, 62.88it/s, est. speed input: 86015.78 toks/s, output: 84.00 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:01<00:15, 58.03it/s, est. speed input: 81854.77 toks/s, output: 79.94 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:01<00:15, 55.51it/s, est. speed input: 78959.86 toks/s, output: 77.11 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:02<00:16, 53.54it/s, est. speed input: 76547.31 toks/s, output: 74.75 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:02<00:16, 52.00it/s, est. speed input: 74484.12 toks/s, output: 72.74 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:02<00:16, 50.86it/s, est. speed input: 72713.42 toks/s, output: 71.01 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:02<00:16, 50.00it/s, est. speed input: 71167.34 toks/s, output: 69.50 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:02<00:16, 49.40it/s, est. speed input: 69814.55 toks/s, output: 68.18 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:02<00:16, 48.93it/s, est. speed input: 68607.97 toks/s, output: 67.00 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:03<00:16, 48.64it/s, est. speed input: 67544.21 toks/s, output: 65.96 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:03<00:16, 48.44it/s, est. speed input: 66592.92 toks/s, output: 65.03 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:03<00:16, 48.27it/s, est. speed input: 65727.51 toks/s, output: 64.19 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:03<00:16, 48.14it/s, est. speed input: 64942.30 toks/s, output: 63.42 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:03<00:16, 48.08it/s, est. speed input: 64233.78 toks/s, output: 62.73 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:03<00:16, 48.01it/s, est. speed input: 63580.07 toks/s, output: 62.09 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:04<00:16, 47.97it/s, est. speed input: 62981.99 toks/s, output: 61.51 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:04<00:15, 47.95it/s, est. speed input: 62432.39 toks/s, output: 60.97 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:04<00:15, 47.92it/s, est. speed input: 61921.72 toks/s, output: 60.47 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:04<00:15, 47.90it/s, est. speed input: 61448.35 toks/s, output: 60.01 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:04<00:15, 47.87it/s, est. speed input: 61006.52 toks/s, output: 59.58 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:04<00:15, 47.88it/s, est. speed input: 60598.96 toks/s, output: 59.18 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:05<00:15, 47.88it/s, est. speed input: 60217.70 toks/s, output: 58.81 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:05<00:14, 47.89it/s, est. speed input: 59861.73 toks/s, output: 58.46 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:05<00:14, 47.89it/s, est. speed input: 59526.81 toks/s, output: 58.13 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:05<00:14, 47.85it/s, est. speed input: 59207.48 toks/s, output: 57.82 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:05<00:14, 47.85it/s, est. speed input: 58910.38 toks/s, output: 57.53 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:05<00:14, 47.88it/s, est. speed input: 58632.57 toks/s, output: 57.26 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:06<00:14, 47.87it/s, est. speed input: 58367.63 toks/s, output: 57.00 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:06<00:13, 47.88it/s, est. speed input: 58117.81 toks/s, output: 56.76 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:06<00:13, 47.88it/s, est. speed input: 57880.51 toks/s, output: 56.52 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:06<00:13, 47.87it/s, est. speed input: 57654.77 toks/s, output: 56.30 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:06<00:13, 47.84it/s, est. speed input: 57437.65 toks/s, output: 56.09 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:06<00:13, 47.87it/s, est. speed input: 57236.11 toks/s, output: 55.89 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:07<00:13, 47.86it/s, est. speed input: 57040.50 toks/s, output: 55.70 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:07<00:12, 47.86it/s, est. speed input: 56855.49 toks/s, output: 55.52 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:07<00:12, 47.86it/s, est. speed input: 56678.67 toks/s, output: 55.35 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:07<00:12, 47.83it/s, est. speed input: 56506.19 toks/s, output: 55.18 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:07<00:12, 47.82it/s, est. speed input: 56343.29 toks/s, output: 55.02 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:07<00:12, 47.85it/s, est. speed input: 56189.28 toks/s, output: 54.87 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:08<00:12, 47.85it/s, est. speed input: 56040.87 toks/s, output: 54.73 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:08<00:11, 47.84it/s, est. speed input: 55897.35 toks/s, output: 54.59 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:08<00:11, 47.84it/s, est. speed input: 55759.61 toks/s, output: 54.45 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:08<00:11, 47.84it/s, est. speed input: 55627.54 toks/s, output: 54.32 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:08<00:11, 47.84it/s, est. speed input: 55501.03 toks/s, output: 54.20 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:08<00:11, 47.87it/s, est. speed input: 55380.39 toks/s, output: 54.08 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:09<00:11, 47.87it/s, est. speed input: 55263.47 toks/s, output: 53.97 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:09<00:10, 47.86it/s, est. speed input: 55149.96 toks/s, output: 53.86 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:09<00:10, 47.82it/s, est. speed input: 55038.34 toks/s, output: 53.75 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:09<00:10, 47.83it/s, est. speed input: 54933.07 toks/s, output: 53.65 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:09<00:10, 47.84it/s, est. speed input: 54831.23 toks/s, output: 53.55 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:09<00:10, 47.85it/s, est. speed input: 54733.43 toks/s, output: 53.45 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:10<00:10, 47.84it/s, est. speed input: 54637.82 toks/s, output: 53.36 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:10<00:09, 47.84it/s, est. speed input: 54545.31 toks/s, output: 53.27 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:10<00:09, 47.84it/s, est. speed input: 54456.52 toks/s, output: 53.18 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:10<00:09, 47.81it/s, est. speed input: 54368.00 toks/s, output: 53.09 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:10<00:09, 47.82it/s, est. speed input: 54284.33 toks/s, output: 53.01 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:10<00:09, 47.85it/s, est. speed input: 54204.69 toks/s, output: 52.93 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:11<00:09, 47.86it/s, est. speed input: 54126.85 toks/s, output: 52.86 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:11<00:08, 47.85it/s, est. speed input: 54050.21 toks/s, output: 52.78 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:11<00:08, 47.86it/s, est. speed input: 53976.80 toks/s, output: 52.71 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:11<00:08, 47.84it/s, est. speed input: 53904.27 toks/s, output: 52.64 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:11<00:08, 47.84it/s, est. speed input: 53834.25 toks/s, output: 52.57 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:11<00:08, 47.86it/s, est. speed input: 53767.06 toks/s, output: 52.51 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:12<00:08, 47.87it/s, est. speed input: 53701.92 toks/s, output: 52.44 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:12<00:07, 47.87it/s, est. speed input: 53638.28 toks/s, output: 52.38 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:12<00:07, 47.86it/s, est. speed input: 53575.56 toks/s, output: 52.32 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:12<00:07, 47.80it/s, est. speed input: 53511.96 toks/s, output: 52.26 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:12<00:07, 47.81it/s, est. speed input: 53452.75 toks/s, output: 52.20 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:12<00:07, 47.84it/s, est. speed input: 53395.63 toks/s, output: 52.14 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:13<00:07, 47.81it/s, est. speed input: 53338.03 toks/s, output: 52.09 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:13<00:06, 47.82it/s, est. speed input: 53283.29 toks/s, output: 52.03 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:13<00:06, 47.84it/s, est. speed input: 53230.63 toks/s, output: 51.98 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:13<00:06, 47.81it/s, est. speed input: 53176.75 toks/s, output: 51.93 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:13<00:06, 47.81it/s, est. speed input: 53125.51 toks/s, output: 51.88 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:13<00:06, 47.83it/s, est. speed input: 53076.15 toks/s, output: 51.83 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:14<00:06, 47.82it/s, est. speed input: 53027.35 toks/s, output: 51.78 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:14<00:05, 47.84it/s, est. speed input: 52980.66 toks/s, output: 51.74 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:14<00:05, 47.84it/s, est. speed input: 52934.23 toks/s, output: 51.69 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:14<00:05, 47.82it/s, est. speed input: 52888.18 toks/s, output: 51.65 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:14<00:05, 47.82it/s, est. speed input: 52843.89 toks/s, output: 51.61 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:14<00:05, 47.82it/s, est. speed input: 52800.47 toks/s, output: 51.56 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:15<00:05, 47.84it/s, est. speed input: 52758.86 toks/s, output: 51.52 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:15<00:04, 47.85it/s, est. speed input: 52717.78 toks/s, output: 51.48 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:15<00:04, 47.86it/s, est. speed input: 52677.79 toks/s, output: 51.44 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:15<00:04, 47.84it/s, est. speed input: 52637.64 toks/s, output: 51.40 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:15<00:04, 47.83it/s, est. speed input: 52598.79 toks/s, output: 51.37 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:15<00:04, 47.81it/s, est. speed input: 52560.02 toks/s, output: 51.33 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:16<00:04, 47.83it/s, est. speed input: 52523.19 toks/s, output: 51.29 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:16<00:03, 47.82it/s, est. speed input: 52486.50 toks/s, output: 51.26 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:16<00:03, 47.80it/s, est. speed input: 52450.06 toks/s, output: 51.22 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:16<00:03, 47.77it/s, est. speed input: 52413.71 toks/s, output: 51.19 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:16<00:03, 47.77it/s, est. speed input: 52378.70 toks/s, output: 51.15 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:16<00:03, 47.77it/s, est. speed input: 52344.38 toks/s, output: 51.12 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:17<00:03, 47.80it/s, est. speed input: 52312.02 toks/s, output: 51.09 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:17<00:02, 47.81it/s, est. speed input: 52279.65 toks/s, output: 51.05 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:17<00:02, 47.78it/s, est. speed input: 52246.77 toks/s, output: 51.02 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:17<00:02, 47.79it/s, est. speed input: 52215.41 toks/s, output: 50.99 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:17<00:02, 47.81it/s, est. speed input: 52185.35 toks/s, output: 50.96 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:17<00:02, 47.80it/s, est. speed input: 52154.88 toks/s, output: 50.93 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:18<00:02, 47.79it/s, est. speed input: 52124.90 toks/s, output: 50.90 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:18<00:01, 47.80it/s, est. speed input: 52095.94 toks/s, output: 50.87 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:18<00:01, 47.77it/s, est. speed input: 52066.31 toks/s, output: 50.85 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:18<00:01, 47.75it/s, est. speed input: 52037.27 toks/s, output: 50.82 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:18<00:01, 47.75it/s, est. speed input: 52009.32 toks/s, output: 50.79 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:18<00:01, 47.76it/s, est. speed input: 51982.28 toks/s, output: 50.76 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:19<00:01, 47.79it/s, est. speed input: 51956.05 toks/s, output: 50.74 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:19<00:00, 47.78it/s, est. speed input: 51929.66 toks/s, output: 50.71 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:19<00:00, 47.77it/s, est. speed input: 51903.52 toks/s, output: 50.69 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:19<00:00, 47.73it/s, est. speed input: 51876.80 toks/s, output: 50.66 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:19<00:00, 47.75it/s, est. speed input: 51851.87 toks/s, output: 50.64 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:19<00:00, 47.76it/s, est. speed input: 51827.54 toks/s, output: 50.61 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:20<00:00, 49.45it/s, est. speed input: 51852.42 toks/s, output: 50.64 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:20<00:00, 49.45it/s, est. speed input: 52157.73 toks/s, output: 50.94 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:20<00:00, 50.94it/s, est. speed input: 52157.73 toks/s, output: 50.94 toks/s]
[rank0]:[W128 00:22:19.950428348 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-28 00:22:20
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/BitNet-2B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:22:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3475003) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3475003) WARNING 01-28 00:22:39 [backends.py:609] Failed to read file <frozen os>
Throughput: 46.59 requests/s, 47756.89 total tokens/s, 46.59 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-28 00:22:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:22:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:22:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:22:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:22:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:22:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:22:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:22:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:22:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:22:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:22:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:22:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:22:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:22:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:22:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:22:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:22:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:22:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:22:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:34] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3475003) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3475003) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]
(EngineCore_DP0 pid=3475003) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]
(EngineCore_DP0 pid=3475003) 
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3475003) [2026-01-28 00:22:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3475003) 2026-01-28 00:22:45,151 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3475003) 2026-01-28 00:22:45,195 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3475003) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  4.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 10.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 17.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.84it/s]
(EngineCore_DP0 pid=3475003) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.66it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.66it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 68/2048 [00:00<00:02, 679.29it/s]
Adding requests:   7%|▋         | 141/2048 [00:00<00:02, 708.45it/s]
Adding requests:  11%|█         | 216/2048 [00:00<00:02, 724.31it/s]
Adding requests:  14%|█▍        | 294/2048 [00:00<00:02, 745.39it/s]
Adding requests:  18%|█▊        | 369/2048 [00:00<00:02, 746.44it/s]
Adding requests:  22%|██▏       | 445/2048 [00:00<00:02, 748.04it/s]
Adding requests:  25%|██▌       | 520/2048 [00:00<00:02, 741.89it/s]
Adding requests:  29%|██▉       | 597/2048 [00:00<00:01, 749.02it/s]
Adding requests:  33%|███▎      | 673/2048 [00:00<00:01, 749.99it/s]
Adding requests:  37%|███▋      | 749/2048 [00:01<00:01, 750.67it/s]
Adding requests:  40%|████      | 825/2048 [00:01<00:01, 739.57it/s]
Adding requests:  44%|████▍     | 902/2048 [00:01<00:01, 747.26it/s]
Adding requests:  48%|████▊     | 977/2048 [00:01<00:01, 721.98it/s]
Adding requests:  51%|█████▏    | 1054/2048 [00:01<00:01, 735.83it/s]
Adding requests:  55%|█████▌    | 1128/2048 [00:01<00:01, 735.44it/s]
Adding requests:  59%|█████▉    | 1206/2048 [00:01<00:01, 747.68it/s]
Adding requests:  63%|██████▎   | 1281/2048 [00:01<00:01, 739.96it/s]
Adding requests:  66%|██████▋   | 1358/2048 [00:01<00:00, 746.21it/s]
Adding requests:  70%|███████   | 1436/2048 [00:01<00:00, 753.88it/s]
Adding requests:  74%|███████▍  | 1517/2048 [00:02<00:00, 767.53it/s]
Adding requests:  78%|███████▊  | 1598/2048 [00:02<00:00, 777.50it/s]
Adding requests:  82%|████████▏ | 1676/2048 [00:02<00:00, 775.02it/s]
Adding requests:  86%|████████▌ | 1754/2048 [00:02<00:00, 769.00it/s]
Adding requests:  89%|████████▉ | 1831/2048 [00:02<00:00, 755.56it/s]
Adding requests:  93%|█████████▎| 1907/2048 [00:02<00:00, 753.20it/s]
Adding requests:  97%|█████████▋| 1984/2048 [00:02<00:00, 756.47it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 749.05it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:00<00:06, 303.63it/s, est. speed input: 310930.37 toks/s, output: 303.64 toks/s]
Processed prompts:   8%|▊         | 161/2048 [00:00<00:09, 189.91it/s, est. speed input: 213868.65 toks/s, output: 208.86 toks/s]
Processed prompts:   9%|▊         | 179/2048 [00:01<00:20, 91.18it/s, est. speed input: 125962.07 toks/s, output: 123.01 toks/s] 
Processed prompts:   9%|▉         | 194/2048 [00:01<00:24, 76.90it/s, est. speed input: 110508.86 toks/s, output: 107.92 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:02<00:27, 67.84it/s, est. speed input: 100498.87 toks/s, output: 98.14 toks/s] 
Processed prompts:  11%|█         | 226/2048 [00:02<00:29, 61.49it/s, est. speed input: 93238.15 toks/s, output: 91.05 toks/s] 
Processed prompts:  12%|█▏        | 242/2048 [00:02<00:31, 57.04it/s, est. speed input: 87727.46 toks/s, output: 85.67 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:03<00:33, 53.95it/s, est. speed input: 83420.91 toks/s, output: 81.47 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:03<00:34, 51.80it/s, est. speed input: 79956.40 toks/s, output: 78.08 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:03<00:34, 50.25it/s, est. speed input: 77091.44 toks/s, output: 75.28 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:04<00:35, 49.19it/s, est. speed input: 74703.16 toks/s, output: 72.95 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:04<00:35, 48.34it/s, est. speed input: 72634.42 toks/s, output: 70.93 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:04<00:35, 47.84it/s, est. speed input: 70888.75 toks/s, output: 69.23 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:05<00:35, 47.50it/s, est. speed input: 69378.47 toks/s, output: 67.75 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:05<00:35, 47.27it/s, est. speed input: 68052.87 toks/s, output: 66.46 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:05<00:35, 47.09it/s, est. speed input: 66877.51 toks/s, output: 65.31 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:06<00:35, 46.98it/s, est. speed input: 65834.43 toks/s, output: 64.29 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:06<00:34, 46.89it/s, est. speed input: 64897.95 toks/s, output: 63.38 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:06<00:34, 46.82it/s, est. speed input: 64051.32 toks/s, output: 62.55 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:07<00:34, 46.80it/s, est. speed input: 63291.55 toks/s, output: 61.81 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:07<00:33, 46.77it/s, est. speed input: 62596.31 toks/s, output: 61.13 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:07<00:33, 46.74it/s, est. speed input: 61959.18 toks/s, output: 60.51 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:08<00:33, 46.73it/s, est. speed input: 61375.46 toks/s, output: 59.94 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:08<00:32, 46.72it/s, est. speed input: 60839.44 toks/s, output: 59.41 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:08<00:32, 46.71it/s, est. speed input: 60341.94 toks/s, output: 58.93 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:09<00:32, 46.70it/s, est. speed input: 59881.76 toks/s, output: 58.48 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:09<00:31, 46.70it/s, est. speed input: 59455.13 toks/s, output: 58.06 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:10<00:31, 46.68it/s, est. speed input: 59054.72 toks/s, output: 57.67 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:10<00:31, 46.68it/s, est. speed input: 58681.76 toks/s, output: 57.31 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:10<00:30, 46.68it/s, est. speed input: 58333.97 toks/s, output: 56.97 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:11<00:30, 46.67it/s, est. speed input: 58005.76 toks/s, output: 56.65 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:11<00:30, 46.67it/s, est. speed input: 57698.99 toks/s, output: 56.35 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:11<00:29, 46.66it/s, est. speed input: 57408.23 toks/s, output: 56.06 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:12<00:29, 46.66it/s, est. speed input: 57135.17 toks/s, output: 55.80 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:12<00:29, 46.67it/s, est. speed input: 56877.72 toks/s, output: 55.54 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:12<00:28, 46.65it/s, est. speed input: 56631.03 toks/s, output: 55.30 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:13<00:28, 46.65it/s, est. speed input: 56399.96 toks/s, output: 55.08 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:13<00:28, 46.65it/s, est. speed input: 56179.83 toks/s, output: 54.86 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:13<00:27, 46.63it/s, est. speed input: 55969.10 toks/s, output: 54.66 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:14<00:27, 46.64it/s, est. speed input: 55769.97 toks/s, output: 54.46 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:14<00:27, 46.64it/s, est. speed input: 55580.66 toks/s, output: 54.28 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:14<00:26, 46.63it/s, est. speed input: 55398.64 toks/s, output: 54.10 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:15<00:26, 46.64it/s, est. speed input: 55226.05 toks/s, output: 53.93 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:15<00:26, 46.64it/s, est. speed input: 55061.41 toks/s, output: 53.77 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:15<00:25, 46.62it/s, est. speed input: 54901.76 toks/s, output: 53.61 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:16<00:25, 46.63it/s, est. speed input: 54750.68 toks/s, output: 53.47 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:16<00:25, 46.63it/s, est. speed input: 54605.31 toks/s, output: 53.33 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:16<00:24, 46.62it/s, est. speed input: 54465.02 toks/s, output: 53.19 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:17<00:24, 46.62it/s, est. speed input: 54331.04 toks/s, output: 53.06 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:17<00:23, 46.62it/s, est. speed input: 54202.30 toks/s, output: 52.93 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:17<00:23, 46.61it/s, est. speed input: 54077.73 toks/s, output: 52.81 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:18<00:23, 46.62it/s, est. speed input: 53958.91 toks/s, output: 52.69 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:18<00:22, 46.62it/s, est. speed input: 53844.00 toks/s, output: 52.58 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:18<00:22, 46.61it/s, est. speed input: 53732.77 toks/s, output: 52.47 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:19<00:22, 46.62it/s, est. speed input: 53626.31 toks/s, output: 52.37 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:19<00:21, 46.61it/s, est. speed input: 53522.77 toks/s, output: 52.27 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:19<00:21, 46.59it/s, est. speed input: 53421.84 toks/s, output: 52.17 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:20<00:21, 46.60it/s, est. speed input: 53325.99 toks/s, output: 52.08 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:20<00:20, 46.59it/s, est. speed input: 53232.29 toks/s, output: 51.98 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:21<00:20, 46.59it/s, est. speed input: 53142.03 toks/s, output: 51.90 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:21<00:20, 46.59it/s, est. speed input: 53054.60 toks/s, output: 51.81 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:21<00:19, 46.60it/s, est. speed input: 52970.50 toks/s, output: 51.73 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:22<00:19, 46.59it/s, est. speed input: 52887.80 toks/s, output: 51.65 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:22<00:18, 47.34it/s, est. speed input: 52851.55 toks/s, output: 51.61 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:22<00:18, 47.11it/s, est. speed input: 52773.69 toks/s, output: 51.54 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:23<00:18, 46.95it/s, est. speed input: 52698.11 toks/s, output: 51.46 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:23<00:18, 46.84it/s, est. speed input: 52624.81 toks/s, output: 51.39 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:23<00:17, 46.77it/s, est. speed input: 52553.70 toks/s, output: 51.32 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:24<00:17, 46.72it/s, est. speed input: 52484.70 toks/s, output: 51.25 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:24<00:17, 46.69it/s, est. speed input: 52417.95 toks/s, output: 51.19 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:24<00:16, 46.64it/s, est. speed input: 52351.93 toks/s, output: 51.12 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:25<00:16, 46.63it/s, est. speed input: 52288.72 toks/s, output: 51.06 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:25<00:16, 46.63it/s, est. speed input: 52227.23 toks/s, output: 51.00 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:25<00:15, 46.60it/s, est. speed input: 52166.08 toks/s, output: 50.94 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:26<00:15, 46.60it/s, est. speed input: 52107.55 toks/s, output: 50.89 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:26<00:15, 46.60it/s, est. speed input: 52050.64 toks/s, output: 50.83 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:26<00:14, 46.59it/s, est. speed input: 51994.59 toks/s, output: 50.78 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:27<00:14, 46.59it/s, est. speed input: 51940.64 toks/s, output: 50.72 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:27<00:14, 46.60it/s, est. speed input: 51888.25 toks/s, output: 50.67 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:27<00:13, 46.58it/s, est. speed input: 51835.94 toks/s, output: 50.62 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:28<00:13, 46.59it/s, est. speed input: 51785.84 toks/s, output: 50.57 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:28<00:13, 46.57it/s, est. speed input: 51736.07 toks/s, output: 50.52 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:28<00:12, 46.57it/s, est. speed input: 51687.80 toks/s, output: 50.48 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:29<00:12, 46.57it/s, est. speed input: 51640.80 toks/s, output: 50.43 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:29<00:11, 46.58it/s, est. speed input: 51595.50 toks/s, output: 50.39 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:29<00:11, 46.58it/s, est. speed input: 51550.65 toks/s, output: 50.34 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:30<00:11, 46.58it/s, est. speed input: 51507.02 toks/s, output: 50.30 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:30<00:10, 46.59it/s, est. speed input: 51464.41 toks/s, output: 50.26 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:30<00:10, 46.57it/s, est. speed input: 51421.95 toks/s, output: 50.22 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:31<00:10, 46.58it/s, est. speed input: 51381.33 toks/s, output: 50.18 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:31<00:09, 46.57it/s, est. speed input: 51341.16 toks/s, output: 50.14 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:31<00:09, 46.56it/s, est. speed input: 51301.51 toks/s, output: 50.10 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:32<00:09, 46.56it/s, est. speed input: 51262.91 toks/s, output: 50.06 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:32<00:08, 46.56it/s, est. speed input: 51225.33 toks/s, output: 50.02 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:33<00:08, 46.56it/s, est. speed input: 51188.16 toks/s, output: 49.99 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:33<00:08, 46.56it/s, est. speed input: 51152.05 toks/s, output: 49.95 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:33<00:07, 46.57it/s, est. speed input: 51117.14 toks/s, output: 49.92 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:34<00:07, 46.56it/s, est. speed input: 51082.07 toks/s, output: 49.88 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:34<00:07, 46.57it/s, est. speed input: 51048.22 toks/s, output: 49.85 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:34<00:06, 46.55it/s, est. speed input: 51014.40 toks/s, output: 49.82 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:35<00:06, 46.55it/s, est. speed input: 50981.71 toks/s, output: 49.79 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:35<00:06, 46.55it/s, est. speed input: 50949.53 toks/s, output: 49.76 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:35<00:05, 46.55it/s, est. speed input: 50917.89 toks/s, output: 49.72 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:36<00:05, 46.56it/s, est. speed input: 50887.16 toks/s, output: 49.69 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:36<00:05, 46.55it/s, est. speed input: 50856.67 toks/s, output: 49.66 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:36<00:04, 46.55it/s, est. speed input: 50826.74 toks/s, output: 49.64 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:37<00:04, 46.55it/s, est. speed input: 50797.48 toks/s, output: 49.61 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:37<00:04, 46.55it/s, est. speed input: 50768.84 toks/s, output: 49.58 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:37<00:03, 47.40it/s, est. speed input: 50768.11 toks/s, output: 49.58 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:38<00:03, 47.15it/s, est. speed input: 50740.37 toks/s, output: 49.55 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:38<00:03, 46.97it/s, est. speed input: 50713.10 toks/s, output: 49.52 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:38<00:02, 46.84it/s, est. speed input: 50685.95 toks/s, output: 49.50 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:39<00:02, 46.76it/s, est. speed input: 50659.67 toks/s, output: 49.47 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:39<00:02, 46.71it/s, est. speed input: 50634.06 toks/s, output: 49.45 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:39<00:01, 46.65it/s, est. speed input: 50608.11 toks/s, output: 49.42 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:40<00:01, 46.62it/s, est. speed input: 50583.02 toks/s, output: 49.40 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:40<00:00, 46.58it/s, est. speed input: 50557.83 toks/s, output: 49.37 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:40<00:00, 46.56it/s, est. speed input: 50533.16 toks/s, output: 49.35 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:41<00:00, 47.21it/s, est. speed input: 50528.52 toks/s, output: 49.34 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:41<00:00, 47.21it/s, est. speed input: 50876.08 toks/s, output: 49.68 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:41<00:00, 49.68it/s, est. speed input: 50876.08 toks/s, output: 49.68 toks/s]
[rank0]:[W128 00:23:31.833944993 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-28 00:23:32
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/json/BitNet-2B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 00:23:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3476461) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3476461) WARNING 01-28 00:23:56 [backends.py:609] Failed to read file <frozen os>
Throughput: 46.51 requests/s, 47671.14 total tokens/s, 46.51 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-28 00:23:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:23:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:23:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:23:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:23:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:23:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:23:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:23:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:23:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 00:23:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 00:23:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 00:23:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 00:23:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 00:23:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 00:23:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 00:23:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 00:23:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 00:23:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 00:23:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3476461) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3476461) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3476461) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3476461) 
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3476461) [2026-01-28 00:23:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=3476461) [rank0]:W0128 00:23:59.859000 3476461 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3476461) [rank0]:W0128 00:23:59.908000 3476461 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3476461) [rank0]:W0128 00:24:00.589000 3476461 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3476461) [rank0]:W0128 00:24:00.665000 3476461 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3476461) 2026-01-28 00:24:02,951 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3476461) 2026-01-28 00:24:02,968 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3476461) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:04,  2.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  7.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 12.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 16.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 12.36it/s]
(EngineCore_DP0 pid=3476461) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.38it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 22.83it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 20.27it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 71/4096 [00:00<00:05, 708.81it/s]
Adding requests:   3%|▎         | 142/4096 [00:00<00:05, 698.62it/s]
Adding requests:   5%|▌         | 212/4096 [00:00<00:05, 697.51it/s]
Adding requests:   7%|▋         | 289/4096 [00:00<00:05, 724.08it/s]
Adding requests:   9%|▉         | 365/4096 [00:00<00:05, 736.42it/s]
Adding requests:  11%|█         | 441/4096 [00:00<00:04, 743.65it/s]
Adding requests:  13%|█▎        | 516/4096 [00:00<00:04, 736.75it/s]
Adding requests:  14%|█▍        | 590/4096 [00:00<00:04, 733.79it/s]
Adding requests:  16%|█▋        | 666/4096 [00:00<00:04, 740.05it/s]
Adding requests:  18%|█▊        | 743/4096 [00:01<00:04, 749.16it/s]
Adding requests:  20%|█▉        | 818/4096 [00:01<00:04, 739.68it/s]
Adding requests:  22%|██▏       | 895/4096 [00:01<00:04, 746.07it/s]
Adding requests:  24%|██▎       | 970/4096 [00:01<00:04, 744.01it/s]
Adding requests:  26%|██▌       | 1045/4096 [00:01<00:04, 732.17it/s]
Adding requests:  27%|██▋       | 1119/4096 [00:01<00:04, 734.27it/s]
Adding requests:  29%|██▉       | 1197/4096 [00:01<00:03, 746.92it/s]
Adding requests:  31%|███       | 1272/4096 [00:01<00:03, 745.11it/s]
Adding requests:  33%|███▎      | 1350/4096 [00:01<00:03, 754.16it/s]
Adding requests:  35%|███▍      | 1429/4096 [00:01<00:03, 762.70it/s]
Adding requests:  37%|███▋      | 1509/4096 [00:02<00:03, 773.57it/s]
Adding requests:  39%|███▉      | 1589/4096 [00:02<00:03, 779.22it/s]
Adding requests:  41%|████      | 1667/4096 [00:02<00:03, 773.27it/s]
Adding requests:  43%|████▎     | 1745/4096 [00:02<00:03, 773.73it/s]
Adding requests:  45%|████▍     | 1823/4096 [00:02<00:02, 769.60it/s]
Adding requests:  46%|████▋     | 1901/4096 [00:02<00:02, 771.85it/s]
Adding requests:  48%|████▊     | 1979/4096 [00:02<00:02, 772.02it/s]
Adding requests:  50%|█████     | 2057/4096 [00:02<00:02, 768.71it/s]
Adding requests:  52%|█████▏    | 2134/4096 [00:02<00:02, 743.20it/s]
Adding requests:  54%|█████▍    | 2209/4096 [00:02<00:02, 730.59it/s]
Adding requests:  56%|█████▌    | 2288/4096 [00:03<00:02, 745.13it/s]
Adding requests:  58%|█████▊    | 2365/4096 [00:03<00:02, 750.24it/s]
Adding requests:  60%|█████▉    | 2442/4096 [00:03<00:02, 754.05it/s]
Adding requests:  61%|██████▏   | 2518/4096 [00:03<00:02, 754.80it/s]
Adding requests:  63%|██████▎   | 2595/4096 [00:03<00:01, 758.45it/s]
Adding requests:  65%|██████▌   | 2673/4096 [00:03<00:01, 763.92it/s]
Adding requests:  67%|██████▋   | 2750/4096 [00:03<00:01, 765.18it/s]
Adding requests:  69%|██████▉   | 2827/4096 [00:03<00:01, 755.60it/s]
Adding requests:  71%|███████   | 2905/4096 [00:03<00:01, 762.73it/s]
Adding requests:  73%|███████▎  | 2982/4096 [00:03<00:01, 759.16it/s]
Adding requests:  75%|███████▍  | 3059/4096 [00:04<00:01, 759.46it/s]
Adding requests:  77%|███████▋  | 3136/4096 [00:04<00:01, 758.71it/s]
Adding requests:  78%|███████▊  | 3212/4096 [00:04<00:01, 757.92it/s]
Adding requests:  80%|████████  | 3288/4096 [00:04<00:01, 757.90it/s]
Adding requests:  82%|████████▏ | 3365/4096 [00:04<00:00, 761.22it/s]
Adding requests:  84%|████████▍ | 3442/4096 [00:04<00:00, 760.64it/s]
Adding requests:  86%|████████▌ | 3519/4096 [00:04<00:00, 756.38it/s]
Adding requests:  88%|████████▊ | 3595/4096 [00:04<00:00, 747.09it/s]
Adding requests:  90%|████████▉ | 3671/4096 [00:04<00:00, 749.77it/s]
Adding requests:  91%|█████████▏| 3747/4096 [00:04<00:00, 752.78it/s]
Adding requests:  93%|█████████▎| 3828/4096 [00:05<00:00, 767.97it/s]
Adding requests:  95%|█████████▌| 3907/4096 [00:05<00:00, 773.09it/s]
Adding requests:  97%|█████████▋| 3985/4096 [00:05<00:00, 767.65it/s]
Adding requests:  99%|█████████▉| 4062/4096 [00:05<00:00, 766.73it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 754.35it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:00<00:02, 1694.23it/s, est. speed input: 1735092.16 toks/s, output: 1694.29 toks/s]
Processed prompts:  10%|▉         | 396/4096 [00:03<00:39, 93.21it/s, est. speed input: 113870.18 toks/s, output: 111.20 toks/s]    
Processed prompts:  11%|█▏        | 469/4096 [00:04<00:45, 78.91it/s, est. speed input: 97355.17 toks/s, output: 95.07 toks/s]  
Processed prompts:  12%|█▏        | 511/4096 [00:05<00:47, 75.31it/s, est. speed input: 93123.21 toks/s, output: 90.94 toks/s]
Processed prompts:  13%|█▎        | 539/4096 [00:06<00:52, 67.54it/s, est. speed input: 87539.22 toks/s, output: 85.49 toks/s]
Processed prompts:  14%|█▎        | 559/4096 [00:06<01:00, 58.20it/s, est. speed input: 81878.05 toks/s, output: 79.96 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:07<01:09, 50.33it/s, est. speed input: 77096.65 toks/s, output: 75.29 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:08<01:10, 49.33it/s, est. speed input: 74686.23 toks/s, output: 72.94 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:09<01:11, 48.57it/s, est. speed input: 72642.44 toks/s, output: 70.94 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:09<01:11, 48.02it/s, est. speed input: 70886.20 toks/s, output: 69.22 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:10<01:11, 47.62it/s, est. speed input: 69363.23 toks/s, output: 67.74 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:11<01:10, 47.32it/s, est. speed input: 68025.66 toks/s, output: 66.43 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:11<01:10, 47.11it/s, est. speed input: 66843.49 toks/s, output: 65.28 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:12<01:10, 46.96it/s, est. speed input: 65792.72 toks/s, output: 64.25 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:13<01:09, 46.85it/s, est. speed input: 64849.58 toks/s, output: 63.33 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:13<01:09, 46.78it/s, est. speed input: 64000.56 toks/s, output: 62.50 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:14<01:08, 46.72it/s, est. speed input: 63231.64 toks/s, output: 61.75 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:15<01:07, 46.69it/s, est. speed input: 62532.32 toks/s, output: 61.07 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:15<01:07, 46.65it/s, est. speed input: 61891.59 toks/s, output: 60.44 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:16<01:06, 46.63it/s, est. speed input: 61304.92 toks/s, output: 59.87 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:17<01:05, 46.61it/s, est. speed input: 60762.54 toks/s, output: 59.34 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:17<01:05, 46.59it/s, est. speed input: 60262.38 toks/s, output: 58.85 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:18<01:04, 46.58it/s, est. speed input: 59799.03 toks/s, output: 58.40 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:19<01:03, 46.58it/s, est. speed input: 59368.79 toks/s, output: 57.98 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:20<01:03, 46.65it/s, est. speed input: 58979.38 toks/s, output: 57.60 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:20<01:02, 46.63it/s, est. speed input: 58605.12 toks/s, output: 57.23 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:21<01:01, 46.61it/s, est. speed input: 58254.16 toks/s, output: 56.89 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:22<01:01, 46.59it/s, est. speed input: 57924.87 toks/s, output: 56.57 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:22<01:00, 46.53it/s, est. speed input: 57609.31 toks/s, output: 56.26 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:23<00:59, 46.55it/s, est. speed input: 57319.26 toks/s, output: 55.98 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:24<00:59, 46.55it/s, est. speed input: 57044.53 toks/s, output: 55.71 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:24<00:58, 46.55it/s, est. speed input: 56785.52 toks/s, output: 55.45 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:25<00:57, 46.56it/s, est. speed input: 56540.70 toks/s, output: 55.22 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:26<00:57, 46.55it/s, est. speed input: 56308.01 toks/s, output: 54.99 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:26<00:56, 46.55it/s, est. speed input: 56086.73 toks/s, output: 54.77 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:27<00:55, 46.55it/s, est. speed input: 55877.58 toks/s, output: 54.57 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:28<00:54, 46.55it/s, est. speed input: 55677.88 toks/s, output: 54.37 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:28<00:54, 46.54it/s, est. speed input: 55486.95 toks/s, output: 54.19 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:29<00:53, 46.54it/s, est. speed input: 55305.68 toks/s, output: 54.01 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:30<00:52, 46.54it/s, est. speed input: 55132.35 toks/s, output: 53.84 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:31<00:52, 46.53it/s, est. speed input: 54966.04 toks/s, output: 53.68 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:31<00:51, 46.54it/s, est. speed input: 54807.78 toks/s, output: 53.52 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:32<00:50, 46.54it/s, est. speed input: 54656.04 toks/s, output: 53.38 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:33<00:50, 46.53it/s, est. speed input: 54509.68 toks/s, output: 53.23 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:33<00:49, 46.52it/s, est. speed input: 54369.66 toks/s, output: 53.10 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:34<00:48, 46.52it/s, est. speed input: 54235.36 toks/s, output: 52.96 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:35<00:48, 46.08it/s, est. speed input: 54072.57 toks/s, output: 52.81 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:35<00:47, 46.21it/s, est. speed input: 53948.96 toks/s, output: 52.68 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:36<00:46, 46.31it/s, est. speed input: 53830.87 toks/s, output: 52.57 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:37<00:46, 46.37it/s, est. speed input: 53716.18 toks/s, output: 52.46 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:37<00:45, 46.41it/s, est. speed input: 53605.38 toks/s, output: 52.35 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:38<00:44, 46.44it/s, est. speed input: 53498.96 toks/s, output: 52.25 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:39<00:44, 46.46it/s, est. speed input: 53396.01 toks/s, output: 52.14 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:40<00:43, 46.47it/s, est. speed input: 53296.86 toks/s, output: 52.05 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:40<00:42, 46.48it/s, est. speed input: 53200.69 toks/s, output: 51.95 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:41<00:41, 46.49it/s, est. speed input: 53108.17 toks/s, output: 51.86 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:42<00:41, 46.49it/s, est. speed input: 53018.18 toks/s, output: 51.78 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:42<00:40, 46.49it/s, est. speed input: 52931.19 toks/s, output: 51.69 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:43<00:39, 46.50it/s, est. speed input: 52847.27 toks/s, output: 51.61 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:44<00:39, 46.49it/s, est. speed input: 52765.23 toks/s, output: 51.53 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:44<00:38, 46.51it/s, est. speed input: 52686.91 toks/s, output: 51.45 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:45<00:37, 46.50it/s, est. speed input: 52609.99 toks/s, output: 51.38 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:46<00:37, 46.50it/s, est. speed input: 52535.76 toks/s, output: 51.30 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:46<00:36, 46.50it/s, est. speed input: 52463.25 toks/s, output: 51.23 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:47<00:35, 46.49it/s, est. speed input: 52392.54 toks/s, output: 51.16 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:48<00:35, 46.49it/s, est. speed input: 52324.15 toks/s, output: 51.10 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:48<00:34, 46.89it/s, est. speed input: 52278.94 toks/s, output: 51.05 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:49<00:33, 46.77it/s, est. speed input: 52213.94 toks/s, output: 50.99 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:50<00:32, 46.69it/s, est. speed input: 52150.98 toks/s, output: 50.93 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:50<00:32, 46.62it/s, est. speed input: 52089.22 toks/s, output: 50.87 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:51<00:31, 46.58it/s, est. speed input: 52029.69 toks/s, output: 50.81 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:52<00:30, 46.55it/s, est. speed input: 51971.34 toks/s, output: 50.75 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:53<00:30, 46.52it/s, est. speed input: 51914.32 toks/s, output: 50.70 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:53<00:29, 46.51it/s, est. speed input: 51858.96 toks/s, output: 50.64 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:54<00:28, 46.49it/s, est. speed input: 51804.70 toks/s, output: 50.59 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:55<00:28, 46.50it/s, est. speed input: 51752.62 toks/s, output: 50.54 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:55<00:27, 46.48it/s, est. speed input: 51700.67 toks/s, output: 50.49 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:56<00:26, 46.49it/s, est. speed input: 51651.12 toks/s, output: 50.44 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:57<00:26, 46.47it/s, est. speed input: 51601.42 toks/s, output: 50.39 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:57<00:25, 46.46it/s, est. speed input: 51553.30 toks/s, output: 50.35 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:58<00:24, 46.46it/s, est. speed input: 51506.69 toks/s, output: 50.30 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:59<00:24, 46.46it/s, est. speed input: 51460.81 toks/s, output: 50.25 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:59<00:23, 46.46it/s, est. speed input: 51416.01 toks/s, output: 50.21 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:00<00:22, 46.46it/s, est. speed input: 51372.67 toks/s, output: 50.17 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:01<00:21, 46.46it/s, est. speed input: 51330.06 toks/s, output: 50.13 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:02<00:21, 46.45it/s, est. speed input: 51287.86 toks/s, output: 50.09 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:02<00:20, 46.46it/s, est. speed input: 51247.15 toks/s, output: 50.05 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:03<00:19, 46.46it/s, est. speed input: 51207.19 toks/s, output: 50.01 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:04<00:19, 46.46it/s, est. speed input: 51168.18 toks/s, output: 49.97 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:04<00:18, 46.46it/s, est. speed input: 51129.94 toks/s, output: 49.93 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:05<00:17, 46.46it/s, est. speed input: 51092.70 toks/s, output: 49.90 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:06<00:17, 46.46it/s, est. speed input: 51056.08 toks/s, output: 49.86 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:06<00:16, 46.46it/s, est. speed input: 51020.18 toks/s, output: 49.82 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:07<00:15, 46.46it/s, est. speed input: 50985.08 toks/s, output: 49.79 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:08<00:15, 46.46it/s, est. speed input: 50950.44 toks/s, output: 49.76 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:08<00:14, 46.44it/s, est. speed input: 50916.24 toks/s, output: 49.72 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:09<00:13, 46.45it/s, est. speed input: 50883.32 toks/s, output: 49.69 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:10<00:13, 46.45it/s, est. speed input: 50850.85 toks/s, output: 49.66 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:10<00:12, 46.45it/s, est. speed input: 50818.86 toks/s, output: 49.63 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:11<00:11, 46.46it/s, est. speed input: 50787.82 toks/s, output: 49.60 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:12<00:10, 46.45it/s, est. speed input: 50757.07 toks/s, output: 49.57 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:13<00:10, 46.45it/s, est. speed input: 50726.89 toks/s, output: 49.54 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:13<00:09, 46.45it/s, est. speed input: 50697.35 toks/s, output: 49.51 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:14<00:08, 46.45it/s, est. speed input: 50668.43 toks/s, output: 49.48 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:15<00:08, 46.45it/s, est. speed input: 50639.81 toks/s, output: 49.45 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:15<00:07, 46.45it/s, est. speed input: 50611.76 toks/s, output: 49.43 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:16<00:06, 46.45it/s, est. speed input: 50584.36 toks/s, output: 49.40 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:17<00:06, 46.45it/s, est. speed input: 50557.30 toks/s, output: 49.37 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:17<00:05, 46.45it/s, est. speed input: 50530.92 toks/s, output: 49.35 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:18<00:04, 46.45it/s, est. speed input: 50504.81 toks/s, output: 49.32 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:19<00:04, 46.44it/s, est. speed input: 50479.01 toks/s, output: 49.30 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:19<00:03, 46.44it/s, est. speed input: 50453.67 toks/s, output: 49.27 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:20<00:02, 46.44it/s, est. speed input: 50428.83 toks/s, output: 49.25 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:21<00:02, 46.44it/s, est. speed input: 50404.45 toks/s, output: 49.22 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:21<00:01, 46.85it/s, est. speed input: 50392.97 toks/s, output: 49.21 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:22<00:00, 47.19it/s, est. speed input: 50382.90 toks/s, output: 49.20 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:22<00:00, 47.19it/s, est. speed input: 50754.47 toks/s, output: 49.56 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:22<00:00, 49.56it/s, est. speed input: 50754.47 toks/s, output: 49.56 toks/s]
[rank0]:[W128 00:25:33.274863278 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

