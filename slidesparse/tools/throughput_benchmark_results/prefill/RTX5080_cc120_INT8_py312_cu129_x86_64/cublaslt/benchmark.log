
========== M=16 ==========
Time: 2026-01-23 03:31:34
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Qwen2.5-0.5B-INT8_M16.json


========== M=128 ==========
Time: 2026-01-23 03:31:34
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Qwen2.5-0.5B-INT8_M128.json


========== M=256 ==========
Time: 2026-01-23 03:31:34
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 385 --max-num-batched-tokens 385 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Qwen2.5-0.5B-INT8_M256.json


========== M=16 ==========
Time: 2026-01-23 04:15:19
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Qwen2.5-0.5B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-23 04:15:22 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-23 04:15:22 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8'}
INFO 01-23 04:15:22 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:15:22 [model.py:1661] Using max model len 145
INFO 01-23 04:15:22 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:26 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:26 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:55073 backend=nccl
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:27 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:27 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8...
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:27 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:27 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:27 [default_loader.py:308] Loading weights took 0.08 seconds
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:28 [gpu_model_runner.py:3659] Model loading took 0.5924 GiB memory and 0.254821 seconds
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:32 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/90ee5c37e8/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:32 [backends.py:703] Dynamo bytecode transform time: 3.92 s
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:35 [backends.py:261] Cache the graph of compile range (1, 145) for later use
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:49 [backends.py:278] Compiling a graph for compile range (1, 145) takes 15.90 s
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:49 [monitor.py:34] torch.compile takes 19.82 s in total
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:50 [gpu_worker.py:375] Available KV cache memory: 11.41 GiB
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:50 [kv_cache_utils.py:1291] GPU KV cache size: 996,976 tokens
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:50 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 6231.10x
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:51 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:51 [core.py:259] init engine (profile, create kv cache, warmup model) took 23.11 seconds
INFO 01-23 04:15:51 [llm.py:360] Supported tasks: ['generate']
Throughput: 124.35 requests/s, 2113.99 total tokens/s, 124.35 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-23 04:15:22] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:15:22] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:15:22] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:15:22] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:15:26] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:15:26] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:15:26] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:15:26] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=41305) [2026-01-23 04:15:27] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=41305) [2026-01-23 04:15:27] INFO gemm_wrapper.py:307: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=41305) [2026-01-23 04:15:27] INFO kernels.py:278: INT8 quant kernel loaded
(EngineCore_DP0 pid=41305) [2026-01-23 04:15:27] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=41305) [2026-01-23 04:15:27] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=41305) [2026-01-23 04:15:27] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=41305) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=41305) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.63it/s]
(EngineCore_DP0 pid=41305) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=41305) 2026-01-23 04:15:50,629 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=41305) 2026-01-23 04:15:50,637 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=41305) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 50.49it/s]
(EngineCore_DP0 pid=41305) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.70it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.70it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 6100.88it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:01, 85.06it/s, est. speed input: 1361.14 toks/s, output: 85.07 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:00, 113.86it/s, est. speed input: 1752.22 toks/s, output: 109.51 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:00, 121.92it/s, est. speed input: 1869.91 toks/s, output: 116.87 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:00<00:00, 126.10it/s, est. speed input: 1932.42 toks/s, output: 120.78 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:00<00:00, 128.04it/s, est. speed input: 1966.87 toks/s, output: 122.93 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:00<00:00, 129.60it/s, est. speed input: 1992.68 toks/s, output: 124.54 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:00<00:00, 130.32it/s, est. speed input: 2009.36 toks/s, output: 125.58 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:00<00:00, 130.57it/s, est. speed input: 2020.48 toks/s, output: 126.28 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:00<00:00, 130.97it/s, est. speed input: 2030.41 toks/s, output: 126.90 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 130.97it/s, est. speed input: 2034.56 toks/s, output: 127.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 127.15it/s, est. speed input: 2034.56 toks/s, output: 127.16 toks/s]
[rank0]:[W123 04:15:53.825384761 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-23 04:15:54
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Qwen2.5-0.5B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-23 04:15:58 [datasets.py:612] Sampling input_len from [128, 128] and output_len from [1, 1]
INFO 01-23 04:15:58 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8'}
INFO 01-23 04:15:58 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:15:58 [model.py:1661] Using max model len 257
INFO 01-23 04:15:58 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:02 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:02 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:60927 backend=nccl
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:02 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:02 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8...
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:02 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:02 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:02 [default_loader.py:308] Loading weights took 0.08 seconds
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:03 [gpu_model_runner.py:3659] Model loading took 0.5924 GiB memory and 0.260811 seconds
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:07 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/1039dc10af/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:07 [backends.py:703] Dynamo bytecode transform time: 3.84 s
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:09 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:23 [backends.py:278] Compiling a graph for compile range (1, 257) takes 14.66 s
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:23 [monitor.py:34] torch.compile takes 18.50 s in total
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:24 [gpu_worker.py:375] Available KV cache memory: 11.41 GiB
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:24 [kv_cache_utils.py:1291] GPU KV cache size: 996,944 tokens
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:24 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 3665.24x
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:25 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.00 GiB
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:25 [core.py:259] init engine (profile, create kv cache, warmup model) took 21.97 seconds
INFO 01-23 04:16:25 [llm.py:360] Supported tasks: ['generate']
Throughput: 120.72 requests/s, 15572.24 total tokens/s, 120.72 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-23 04:15:58] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:15:58] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:15:58] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:15:58] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:16:02] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:16:02] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:16:02] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:16:02] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=42858) [2026-01-23 04:16:02] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=42858) [2026-01-23 04:16:02] INFO gemm_wrapper.py:307: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=42858) [2026-01-23 04:16:02] INFO kernels.py:278: INT8 quant kernel loaded
(EngineCore_DP0 pid=42858) [2026-01-23 04:16:02] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=42858) [2026-01-23 04:16:02] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=42858) [2026-01-23 04:16:02] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=42858) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=42858) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.51it/s]
(EngineCore_DP0 pid=42858) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=42858) 2026-01-23 04:16:24,345 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=42858) 2026-01-23 04:16:24,372 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=42858) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 50.92it/s]
(EngineCore_DP0 pid=42858) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2482.88it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:00, 125.25it/s, est. speed input: 16033.58 toks/s, output: 125.25 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:00, 126.68it/s, est. speed input: 16188.70 toks/s, output: 126.47 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:00, 127.41it/s, est. speed input: 16265.26 toks/s, output: 127.07 toks/s]
Processed prompts:  41%|████      | 52/128 [00:00<00:00, 126.95it/s, est. speed input: 16239.16 toks/s, output: 126.87 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:00<00:00, 126.62it/s, est. speed input: 16217.98 toks/s, output: 126.70 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:00<00:00, 125.66it/s, est. speed input: 16155.21 toks/s, output: 126.21 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:00<00:00, 126.46it/s, est. speed input: 16190.13 toks/s, output: 126.48 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:00<00:00, 126.70it/s, est. speed input: 16201.72 toks/s, output: 126.57 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:00<00:00, 127.32it/s, est. speed input: 16231.53 toks/s, output: 126.81 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 127.32it/s, est. speed input: 16260.15 toks/s, output: 127.03 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 127.02it/s, est. speed input: 16260.15 toks/s, output: 127.03 toks/s]
[rank0]:[W123 04:16:26.583095970 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-23 04:16:28
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 385 --max-num-batched-tokens 385 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Qwen2.5-0.5B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-23 04:16:31 [datasets.py:612] Sampling input_len from [256, 256] and output_len from [1, 1]
INFO 01-23 04:16:32 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', 'max_model_len': 385, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 385, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8'}
INFO 01-23 04:16:32 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:16:32 [model.py:1661] Using max model len 385
INFO 01-23 04:16:32 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=385.
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:35 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=385, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [385], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:36 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:37483 backend=nccl
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:36 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:36 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8...
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:36 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:36 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:37 [default_loader.py:308] Loading weights took 0.08 seconds
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:37 [gpu_model_runner.py:3659] Model loading took 0.5924 GiB memory and 0.258797 seconds
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:41 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/c092e4da1e/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:41 [backends.py:703] Dynamo bytecode transform time: 3.82 s
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:43 [backends.py:261] Cache the graph of compile range (1, 385) for later use
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:57 [backends.py:278] Compiling a graph for compile range (1, 385) takes 14.58 s
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:57 [monitor.py:34] torch.compile takes 18.40 s in total
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:58 [gpu_worker.py:375] Available KV cache memory: 11.41 GiB
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:58 [kv_cache_utils.py:1291] GPU KV cache size: 996,880 tokens
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:58 [kv_cache_utils.py:1296] Maximum concurrency for 385 tokens per request: 2492.20x
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:59 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.01 GiB
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:59 [core.py:259] init engine (profile, create kv cache, warmup model) took 21.80 seconds
INFO 01-23 04:16:59 [llm.py:360] Supported tasks: ['generate']
Throughput: 120.33 requests/s, 30925.37 total tokens/s, 120.33 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-23 04:16:32] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:16:32] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:16:32] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:16:32] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:16:35] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:16:35] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:16:35] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:16:35] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=44216) [2026-01-23 04:16:36] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=44216) [2026-01-23 04:16:36] INFO gemm_wrapper.py:307: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=44216) [2026-01-23 04:16:36] INFO kernels.py:278: INT8 quant kernel loaded
(EngineCore_DP0 pid=44216) [2026-01-23 04:16:36] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=44216) [2026-01-23 04:16:36] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=44216) [2026-01-23 04:16:36] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=44216) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=44216) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.63it/s]
(EngineCore_DP0 pid=44216) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=44216) 2026-01-23 04:16:58,490 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=44216) 2026-01-23 04:16:58,498 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=44216) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 53.72it/s]
(EngineCore_DP0 pid=44216) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.53it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1428.96it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:00, 158.20it/s, est. speed input: 40504.33 toks/s, output: 158.20 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:00, 139.18it/s, est. speed input: 36284.52 toks/s, output: 141.73 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:00, 135.87it/s, est. speed input: 35439.39 toks/s, output: 138.43 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:00<00:00, 134.25it/s, est. speed input: 35020.11 toks/s, output: 136.80 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:00<00:00, 128.70it/s, est. speed input: 34091.14 toks/s, output: 133.17 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:00<00:00, 128.80it/s, est. speed input: 33918.85 toks/s, output: 132.49 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:00<00:00, 128.40it/s, est. speed input: 33749.95 toks/s, output: 131.83 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:00<00:00, 128.81it/s, est. speed input: 33689.33 toks/s, output: 131.60 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 128.81it/s, est. speed input: 33699.61 toks/s, output: 131.64 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 131.63it/s, est. speed input: 33699.61 toks/s, output: 131.64 toks/s]
[rank0]:[W123 04:17:00.709339107 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16 ==========
Time: 2026-01-24 23:35:51
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-24 23:35:54 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [1, 1]
INFO 01-24 23:35:54 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-24 23:35:54 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-24 23:35:54 [model.py:1661] Using max model len 145
INFO 01-24 23:35:55 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=841938) INFO 01-24 23:35:59 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=841938) INFO 01-24 23:35:59 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:44081 backend=nccl
(EngineCore_DP0 pid=841938) INFO 01-24 23:35:59 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=841938) INFO 01-24 23:35:59 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=841938) INFO 01-24 23:35:59 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=841938) INFO 01-24 23:35:59 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=841938) INFO 01-24 23:36:00 [default_loader.py:308] Loading weights took 0.25 seconds
(EngineCore_DP0 pid=841938) INFO 01-24 23:36:00 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.440973 seconds
(EngineCore_DP0 pid=841938) WARNING 01-24 23:36:02 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=841938) INFO 01-24 23:36:02 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/73a128f3ed/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=841938) INFO 01-24 23:36:02 [backends.py:703] Dynamo bytecode transform time: 2.03 s
(EngineCore_DP0 pid=841938) INFO 01-24 23:36:04 [backends.py:261] Cache the graph of compile range (1, 145) for later use
(EngineCore_DP0 pid=841938) INFO 01-24 23:36:07 [backends.py:278] Compiling a graph for compile range (1, 145) takes 3.48 s
(EngineCore_DP0 pid=841938) INFO 01-24 23:36:07 [monitor.py:34] torch.compile takes 5.51 s in total
(EngineCore_DP0 pid=841938) INFO 01-24 23:36:07 [gpu_worker.py:375] Available KV cache memory: 10.39 GiB
(EngineCore_DP0 pid=841938) INFO 01-24 23:36:08 [kv_cache_utils.py:1291] GPU KV cache size: 340,448 tokens
(EngineCore_DP0 pid=841938) INFO 01-24 23:36:08 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 2127.80x
(EngineCore_DP0 pid=841938) INFO 01-24 23:36:09 [gpu_model_runner.py:4587] Graph capturing finished in 2 secs, took 0.01 GiB
(EngineCore_DP0 pid=841938) INFO 01-24 23:36:09 [core.py:259] init engine (profile, create kv cache, warmup model) took 9.43 seconds
INFO 01-24 23:36:10 [llm.py:360] Supported tasks: ['generate']
Throughput: 58.20 requests/s, 989.41 total tokens/s, 58.20 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-24 23:35:55] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-24 23:35:55] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:35:55] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-24 23:35:55] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:35:55] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:35:55] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:35:55] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:35:55] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:35:55] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:35:55] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-24 23:35:55] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-24 23:35:55] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-24 23:35:55] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-24 23:35:55] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-24 23:35:58] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-24 23:35:59] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:35:59] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-24 23:35:59] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:35:59] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:35:59] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:35:59] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:35:59] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:35:59] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:35:59] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-24 23:35:59] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-24 23:35:59] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-24 23:35:59] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-24 23:35:59] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=841938) [2026-01-24 23:35:59] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=841938) [2026-01-24 23:35:59] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=841938) [2026-01-24 23:35:59] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=841938) [2026-01-24 23:35:59] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=841938) [2026-01-24 23:35:59] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=841938) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=841938) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.27it/s]
(EngineCore_DP0 pid=841938) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.27it/s]
(EngineCore_DP0 pid=841938) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=841938) 2026-01-24 23:36:08,101 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=841938) 2026-01-24 23:36:08,115 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=841938) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.79it/s]
(EngineCore_DP0 pid=841938) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 6627.22it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<01:08,  1.86it/s, est. speed input: 29.74 toks/s, output: 1.86 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:06, 18.04it/s, est. speed input: 223.69 toks/s, output: 13.98 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 31.97it/s, est. speed input: 364.23 toks/s, output: 22.76 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 43.38it/s, est. speed input: 470.54 toks/s, output: 29.41 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 52.55it/s, est. speed input: 554.61 toks/s, output: 34.66 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:01, 59.74it/s, est. speed input: 622.96 toks/s, output: 38.93 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:01, 64.45it/s, est. speed input: 677.02 toks/s, output: 42.31 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 68.34it/s, est. speed input: 723.53 toks/s, output: 45.22 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 71.24it/s, est. speed input: 763.29 toks/s, output: 47.71 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 73.48it/s, est. speed input: 797.89 toks/s, output: 49.87 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 75.23it/s, est. speed input: 828.37 toks/s, output: 51.77 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 75.72it/s, est. speed input: 853.41 toks/s, output: 53.34 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 76.24it/s, est. speed input: 875.90 toks/s, output: 54.74 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 77.21it/s, est. speed input: 897.20 toks/s, output: 56.07 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 76.83it/s, est. speed input: 914.16 toks/s, output: 57.13 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 75.54it/s, est. speed input: 927.30 toks/s, output: 57.96 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 75.54it/s, est. speed input: 940.16 toks/s, output: 58.76 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 58.76it/s, est. speed input: 940.16 toks/s, output: 58.76 toks/s]
[rank0]:[W124 23:36:12.417043048 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-24 23:36:13
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-24 23:36:17 [datasets.py:612] Sampling input_len from [127, 127] and output_len from [1, 1]
INFO 01-24 23:36:17 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-24 23:36:17 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-24 23:36:17 [model.py:1661] Using max model len 257
INFO 01-24 23:36:17 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:21 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:21 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:52519 backend=nccl
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:21 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:21 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:22 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:22 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:22 [default_loader.py:308] Loading weights took 0.24 seconds
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:22 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.440738 seconds
(EngineCore_DP0 pid=842536) WARNING 01-24 23:36:24 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:24 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/425b3d75ef/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:24 [backends.py:703] Dynamo bytecode transform time: 2.03 s
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:26 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:28 [backends.py:278] Compiling a graph for compile range (1, 257) takes 2.28 s
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:28 [monitor.py:34] torch.compile takes 4.31 s in total
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:29 [gpu_worker.py:375] Available KV cache memory: 10.39 GiB
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:29 [kv_cache_utils.py:1291] GPU KV cache size: 340,384 tokens
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:29 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 1251.41x
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:29 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=842536) INFO 01-24 23:36:29 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.15 seconds
INFO 01-24 23:36:30 [llm.py:360] Supported tasks: ['generate']
Throughput: 62.68 requests/s, 8085.51 total tokens/s, 62.68 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-24 23:36:17] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-24 23:36:17] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:36:17] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-24 23:36:17] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:36:17] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:36:17] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:36:17] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:36:17] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:36:17] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:36:17] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-24 23:36:17] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-24 23:36:17] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-24 23:36:17] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-24 23:36:17] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-24 23:36:21] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-24 23:36:21] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:36:21] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-24 23:36:21] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:36:21] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:36:21] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:36:21] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:36:21] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:36:21] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:36:21] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-24 23:36:21] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-24 23:36:21] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-24 23:36:21] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-24 23:36:21] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=842536) [2026-01-24 23:36:21] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=842536) [2026-01-24 23:36:22] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=842536) [2026-01-24 23:36:22] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=842536) [2026-01-24 23:36:22] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=842536) [2026-01-24 23:36:22] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=842536) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=842536) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.28it/s]
(EngineCore_DP0 pid=842536) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.28it/s]
(EngineCore_DP0 pid=842536) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=842536) 2026-01-24 23:36:29,213 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=842536) 2026-01-24 23:36:29,226 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=842536) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 27.70it/s]
(EngineCore_DP0 pid=842536) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3448.22it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:49,  2.54it/s, est. speed input: 325.65 toks/s, output: 2.54 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:05, 22.85it/s, est. speed input: 2310.19 toks/s, output: 18.05 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 38.28it/s, est. speed input: 3625.18 toks/s, output: 28.32 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 49.65it/s, est. speed input: 4556.18 toks/s, output: 35.59 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 58.17it/s, est. speed input: 5260.30 toks/s, output: 41.10 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 64.09it/s, est. speed input: 5798.66 toks/s, output: 45.30 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:01, 68.50it/s, est. speed input: 6233.80 toks/s, output: 48.70 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:00, 71.62it/s, est. speed input: 6588.95 toks/s, output: 51.48 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 73.92it/s, est. speed input: 6887.07 toks/s, output: 53.81 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 75.49it/s, est. speed input: 7137.99 toks/s, output: 55.77 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 76.40it/s, est. speed input: 7348.22 toks/s, output: 57.41 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 77.53it/s, est. speed input: 7561.26 toks/s, output: 59.07 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:01<00:00, 77.89it/s, est. speed input: 7718.70 toks/s, output: 60.30 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 78.21it/s, est. speed input: 7858.93 toks/s, output: 61.40 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:01<00:00, 78.68it/s, est. speed input: 7988.23 toks/s, output: 62.41 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:01<00:00, 78.74it/s, est. speed input: 8099.25 toks/s, output: 63.28 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 78.74it/s, est. speed input: 8175.02 toks/s, output: 63.87 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 63.86it/s, est. speed input: 8175.02 toks/s, output: 63.87 toks/s]
[rank0]:[W124 23:36:32.440345375 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-24 23:36:33
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 385 --max-num-batched-tokens 385 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M256.json


========== M=16 ==========
Time: 2026-01-24 23:38:44
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-24 23:38:47 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [1, 1]
INFO 01-24 23:38:47 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-24 23:38:47 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-24 23:38:47 [model.py:1661] Using max model len 145
INFO 01-24 23:38:48 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:52 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:52 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:55405 backend=nccl
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:52 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:52 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:52 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:53 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:53 [default_loader.py:308] Loading weights took 0.15 seconds
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:53 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.341071 seconds
(EngineCore_DP0 pid=843972) WARNING 01-24 23:38:55 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:55 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/73a128f3ed/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:55 [backends.py:703] Dynamo bytecode transform time: 2.03 s
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:57 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 145) from the cache, took 0.414 s
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:57 [monitor.py:34] torch.compile takes 2.45 s in total
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:57 [gpu_worker.py:375] Available KV cache memory: 10.83 GiB
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:58 [kv_cache_utils.py:1291] GPU KV cache size: 354,768 tokens
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:58 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 2217.30x
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:58 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.01 GiB
(EngineCore_DP0 pid=843972) INFO 01-24 23:38:58 [core.py:259] init engine (profile, create kv cache, warmup model) took 5.08 seconds
INFO 01-24 23:38:59 [llm.py:360] Supported tasks: ['generate']
Throughput: 76.42 requests/s, 1299.19 total tokens/s, 76.42 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-24 23:38:48] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-24 23:38:48] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:38:48] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-24 23:38:48] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:38:48] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:38:48] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:38:48] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:38:48] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:38:48] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:38:48] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-24 23:38:48] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-24 23:38:48] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-24 23:38:48] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-24 23:38:48] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-24 23:38:52] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-24 23:38:52] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:38:52] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-24 23:38:52] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:38:52] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:38:52] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:38:52] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:38:52] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:38:52] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:38:52] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-24 23:38:52] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-24 23:38:52] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-24 23:38:52] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-24 23:38:52] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=843972) [2026-01-24 23:38:52] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=843972) [2026-01-24 23:38:52] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=843972) [2026-01-24 23:38:52] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=843972) [2026-01-24 23:38:52] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=843972) [2026-01-24 23:38:52] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=843972) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=843972) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.42it/s]
(EngineCore_DP0 pid=843972) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.41it/s]
(EngineCore_DP0 pid=843972) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=843972) 2026-01-24 23:38:58,015 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=843972) 2026-01-24 23:38:58,030 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=843972) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 24.93it/s]
(EngineCore_DP0 pid=843972) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.80it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.80it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4789.26it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 54.07it/s, est. speed input: 865.14 toks/s, output: 54.07 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:01, 68.26it/s, est. speed input: 1056.56 toks/s, output: 66.03 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:01, 73.15it/s, est. speed input: 1124.55 toks/s, output: 70.28 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:01, 76.58it/s, est. speed input: 1171.34 toks/s, output: 73.21 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 78.08it/s, est. speed input: 1196.04 toks/s, output: 74.75 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:00, 79.06it/s, est. speed input: 1212.96 toks/s, output: 75.81 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:00<00:00, 79.67it/s, est. speed input: 1224.97 toks/s, output: 76.56 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:00<00:00, 79.36it/s, est. speed input: 1228.90 toks/s, output: 76.81 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:00<00:00, 79.89it/s, est. speed input: 1236.62 toks/s, output: 77.29 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:00, 80.03it/s, est. speed input: 1241.68 toks/s, output: 77.60 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:01<00:00, 80.13it/s, est. speed input: 1245.77 toks/s, output: 77.86 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:01<00:00, 79.69it/s, est. speed input: 1246.96 toks/s, output: 77.93 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:01<00:00, 79.49it/s, est. speed input: 1248.16 toks/s, output: 78.01 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:01<00:00, 78.74it/s, est. speed input: 1246.99 toks/s, output: 77.94 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:01<00:00, 77.54it/s, est. speed input: 1243.63 toks/s, output: 77.73 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.54it/s, est. speed input: 1244.00 toks/s, output: 77.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.74it/s, est. speed input: 1244.00 toks/s, output: 77.75 toks/s]
[rank0]:[W124 23:39:00.684130550 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-24 23:39:02
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-24 23:39:05 [datasets.py:612] Sampling input_len from [127, 127] and output_len from [1, 1]
INFO 01-24 23:39:05 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-24 23:39:05 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-24 23:39:05 [model.py:1661] Using max model len 257
INFO 01-24 23:39:06 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:09 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:09 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:43069 backend=nccl
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:10 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:10 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:10 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:10 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:10 [default_loader.py:308] Loading weights took 0.15 seconds
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:10 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.340906 seconds
(EngineCore_DP0 pid=844328) WARNING 01-24 23:39:13 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:13 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/425b3d75ef/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:13 [backends.py:703] Dynamo bytecode transform time: 2.02 s
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:14 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 257) from the cache, took 0.408 s
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:14 [monitor.py:34] torch.compile takes 2.43 s in total
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:15 [gpu_worker.py:375] Available KV cache memory: 10.81 GiB
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:15 [kv_cache_utils.py:1291] GPU KV cache size: 354,288 tokens
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:15 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 1302.53x
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:16 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.03 GiB
(EngineCore_DP0 pid=844328) INFO 01-24 23:39:16 [core.py:259] init engine (profile, create kv cache, warmup model) took 5.04 seconds
INFO 01-24 23:39:16 [llm.py:360] Supported tasks: ['generate']
Throughput: 76.28 requests/s, 9840.58 total tokens/s, 76.28 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-24 23:39:05] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-24 23:39:05] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:39:05] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-24 23:39:05] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:39:05] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:39:05] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:39:05] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:39:05] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:39:05] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:39:05] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-24 23:39:05] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-24 23:39:05] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-24 23:39:05] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-24 23:39:05] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-24 23:39:09] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-24 23:39:09] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:39:09] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-24 23:39:09] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:39:09] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:39:09] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:39:09] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:39:09] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:39:09] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:39:09] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-24 23:39:09] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-24 23:39:09] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-24 23:39:09] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-24 23:39:09] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=844328) [2026-01-24 23:39:10] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=844328) [2026-01-24 23:39:10] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=844328) [2026-01-24 23:39:10] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=844328) [2026-01-24 23:39:10] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=844328) [2026-01-24 23:39:10] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=844328) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=844328) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.32it/s]
(EngineCore_DP0 pid=844328) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.32it/s]
(EngineCore_DP0 pid=844328) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=844328) 2026-01-24 23:39:15,437 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=844328) 2026-01-24 23:39:15,452 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=844328) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 25.32it/s]
(EngineCore_DP0 pid=844328) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.90it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2552.89it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:01, 71.77it/s, est. speed input: 9188.00 toks/s, output: 71.78 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:01, 76.98it/s, est. speed input: 9753.79 toks/s, output: 76.20 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:01, 78.85it/s, est. speed input: 9961.46 toks/s, output: 77.82 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:01, 77.87it/s, est. speed input: 9914.35 toks/s, output: 77.45 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 78.04it/s, est. speed input: 9935.81 toks/s, output: 77.62 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:00<00:00, 78.77it/s, est. speed input: 9990.74 toks/s, output: 78.05 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:00<00:00, 79.19it/s, est. speed input: 10027.64 toks/s, output: 78.34 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:00<00:00, 79.31it/s, est. speed input: 10046.12 toks/s, output: 78.48 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:00<00:00, 79.66it/s, est. speed input: 10074.16 toks/s, output: 78.70 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:00, 79.99it/s, est. speed input: 10100.36 toks/s, output: 78.91 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 80.13it/s, est. speed input: 10118.54 toks/s, output: 79.05 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:01<00:00, 80.33it/s, est. speed input: 10137.42 toks/s, output: 79.20 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 79.46it/s, est. speed input: 10120.31 toks/s, output: 79.06 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 78.33it/s, est. speed input: 10089.41 toks/s, output: 78.82 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.33it/s, est. speed input: 10076.28 toks/s, output: 78.72 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.72it/s, est. speed input: 10076.28 toks/s, output: 78.72 toks/s]
[rank0]:[W124 23:39:18.104819255 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-24 23:39:19
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 385 --max-num-batched-tokens 385 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M256.json


========== M=16 ==========
Time: 2026-01-24 23:40:38
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-24 23:40:42 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [1, 1]
INFO 01-24 23:40:42 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-24 23:40:42 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-24 23:40:42 [model.py:1661] Using max model len 145
INFO 01-24 23:40:42 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:46 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:46 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:36389 backend=nccl
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:46 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:47 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:47 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:47 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:47 [default_loader.py:308] Loading weights took 0.15 seconds
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:47 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.338188 seconds
(EngineCore_DP0 pid=845784) WARNING 01-24 23:40:49 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:49 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/73a128f3ed/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:49 [backends.py:703] Dynamo bytecode transform time: 2.05 s
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:51 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 145) from the cache, took 0.440 s
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:51 [monitor.py:34] torch.compile takes 2.49 s in total
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:52 [gpu_worker.py:375] Available KV cache memory: 10.83 GiB
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:52 [kv_cache_utils.py:1291] GPU KV cache size: 354,768 tokens
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:52 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 2217.30x
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:52 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.01 GiB
(EngineCore_DP0 pid=845784) INFO 01-24 23:40:52 [core.py:259] init engine (profile, create kv cache, warmup model) took 5.12 seconds
INFO 01-24 23:40:53 [llm.py:360] Supported tasks: ['generate']
Throughput: 76.01 requests/s, 1292.09 total tokens/s, 76.01 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-24 23:40:42] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-24 23:40:42] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:40:42] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-24 23:40:42] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:40:42] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:40:42] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:40:42] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:40:42] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:40:42] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:40:42] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-24 23:40:42] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-24 23:40:42] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-24 23:40:42] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-24 23:40:42] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-24 23:40:46] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-24 23:40:46] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:40:46] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-24 23:40:46] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:40:46] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:40:46] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:40:46] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:40:46] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:40:46] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:40:46] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-24 23:40:46] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-24 23:40:46] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-24 23:40:46] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-24 23:40:46] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=845784) [2026-01-24 23:40:47] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=845784) [2026-01-24 23:40:47] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=845784) [2026-01-24 23:40:47] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=845784) [2026-01-24 23:40:47] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=845784) [2026-01-24 23:40:47] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=845784) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=845784) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.42it/s]
(EngineCore_DP0 pid=845784) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.41it/s]
(EngineCore_DP0 pid=845784) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=845784) 2026-01-24 23:40:52,350 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=845784) 2026-01-24 23:40:52,366 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=845784) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 25.12it/s]
(EngineCore_DP0 pid=845784) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.85it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4685.43it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 54.01it/s, est. speed input: 864.20 toks/s, output: 54.01 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:01, 67.73it/s, est. speed input: 1049.48 toks/s, output: 65.59 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:01, 73.03it/s, est. speed input: 1121.58 toks/s, output: 70.10 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:01, 76.25it/s, est. speed input: 1166.55 toks/s, output: 72.91 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 76.84it/s, est. speed input: 1182.21 toks/s, output: 73.89 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 77.56it/s, est. speed input: 1195.35 toks/s, output: 74.71 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:00<00:00, 78.32it/s, est. speed input: 1206.88 toks/s, output: 75.43 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:00<00:00, 79.02it/s, est. speed input: 1217.46 toks/s, output: 76.09 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:00<00:00, 79.29it/s, est. speed input: 1223.99 toks/s, output: 76.50 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:00, 78.80it/s, est. speed input: 1225.88 toks/s, output: 76.62 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 79.11it/s, est. speed input: 1230.74 toks/s, output: 76.92 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 79.26it/s, est. speed input: 1234.18 toks/s, output: 77.14 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 79.45it/s, est. speed input: 1237.47 toks/s, output: 77.34 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 78.00it/s, est. speed input: 1234.45 toks/s, output: 77.15 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 78.08it/s, est. speed input: 1235.63 toks/s, output: 77.23 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.08it/s, est. speed input: 1237.47 toks/s, output: 77.34 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.34it/s, est. speed input: 1237.47 toks/s, output: 77.34 toks/s]
[rank0]:[W124 23:40:55.033641809 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-24 23:40:56
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-24 23:41:00 [datasets.py:612] Sampling input_len from [127, 127] and output_len from [1, 1]
INFO 01-24 23:41:00 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-24 23:41:00 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-24 23:41:00 [model.py:1661] Using max model len 257
INFO 01-24 23:41:00 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:04 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:04 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:47157 backend=nccl
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:04 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:04 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:05 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:05 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:05 [default_loader.py:308] Loading weights took 0.14 seconds
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:05 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.339505 seconds
(EngineCore_DP0 pid=846124) WARNING 01-24 23:41:07 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:07 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/425b3d75ef/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:07 [backends.py:703] Dynamo bytecode transform time: 2.11 s
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:09 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 257) from the cache, took 0.521 s
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:09 [monitor.py:34] torch.compile takes 2.63 s in total
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:10 [gpu_worker.py:375] Available KV cache memory: 10.81 GiB
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:10 [kv_cache_utils.py:1291] GPU KV cache size: 354,288 tokens
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:10 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 1302.53x
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:11 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.03 GiB
(EngineCore_DP0 pid=846124) INFO 01-24 23:41:11 [core.py:259] init engine (profile, create kv cache, warmup model) took 5.38 seconds
INFO 01-24 23:41:11 [llm.py:360] Supported tasks: ['generate']
Throughput: 77.21 requests/s, 9960.03 total tokens/s, 77.21 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-24 23:41:00] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-24 23:41:00] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:41:00] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-24 23:41:00] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:41:00] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:41:00] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:41:00] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:41:00] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:41:00] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:41:00] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-24 23:41:00] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-24 23:41:00] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-24 23:41:00] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-24 23:41:00] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-24 23:41:04] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-24 23:41:04] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:41:04] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-24 23:41:04] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:41:04] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:41:04] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:41:04] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:41:04] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-24 23:41:04] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-24 23:41:04] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-24 23:41:04] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-24 23:41:04] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-24 23:41:04] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-24 23:41:04] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=846124) [2026-01-24 23:41:05] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=846124) [2026-01-24 23:41:05] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=846124) [2026-01-24 23:41:05] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=846124) [2026-01-24 23:41:05] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=846124) [2026-01-24 23:41:05] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=846124) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=846124) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.52it/s]
(EngineCore_DP0 pid=846124) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.52it/s]
(EngineCore_DP0 pid=846124) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=846124) 2026-01-24 23:41:10,526 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=846124) 2026-01-24 23:41:10,540 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=846124) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 24.96it/s]
(EngineCore_DP0 pid=846124) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.82it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.80it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3179.35it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:01, 68.70it/s, est. speed input: 8795.23 toks/s, output: 68.71 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:01, 74.75it/s, est. speed input: 9451.84 toks/s, output: 73.84 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:01, 76.44it/s, est. speed input: 9648.81 toks/s, output: 75.38 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 77.92it/s, est. speed input: 9806.81 toks/s, output: 76.61 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 78.36it/s, est. speed input: 9870.54 toks/s, output: 77.11 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:00, 79.12it/s, est. speed input: 9947.48 toks/s, output: 77.71 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:00<00:00, 79.34it/s, est. speed input: 9987.76 toks/s, output: 78.03 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:00<00:00, 79.09it/s, est. speed input: 9995.30 toks/s, output: 78.09 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:00<00:00, 79.20it/s, est. speed input: 10013.96 toks/s, output: 78.23 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:00, 80.04it/s, est. speed input: 10061.89 toks/s, output: 78.61 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 80.58it/s, est. speed input: 10099.90 toks/s, output: 78.91 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:01<00:00, 80.71it/s, est. speed input: 10123.35 toks/s, output: 79.09 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:01<00:00, 80.45it/s, est. speed input: 10131.50 toks/s, output: 79.15 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:01<00:00, 80.43it/s, est. speed input: 10143.28 toks/s, output: 79.24 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 79.75it/s, est. speed input: 10133.99 toks/s, output: 79.17 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 79.75it/s, est. speed input: 10133.99 toks/s, output: 79.17 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 79.17it/s, est. speed input: 10133.99 toks/s, output: 79.17 toks/s]
[rank0]:[W124 23:41:13.198356778 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-24 23:41:14
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 385 --max-num-batched-tokens 385 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M256.json


========== M=16 ==========
Time: 2026-01-25 00:17:02
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-25 00:17:06 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [1, 1]
INFO 01-25 00:17:06 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-25 00:17:06 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 00:17:06 [model.py:1661] Using max model len 145
INFO 01-25 00:17:07 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:11 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:11 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:42581 backend=nccl
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:11 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:11 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:11 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:11 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:12 [default_loader.py:308] Loading weights took 0.16 seconds
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:12 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.342843 seconds
(EngineCore_DP0 pid=858810) WARNING 01-25 00:17:14 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:14 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/73a128f3ed/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:14 [backends.py:703] Dynamo bytecode transform time: 2.01 s
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:16 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 145) from the cache, took 0.418 s
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:16 [monitor.py:34] torch.compile takes 2.43 s in total
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:16 [gpu_worker.py:375] Available KV cache memory: 10.83 GiB
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:16 [kv_cache_utils.py:1291] GPU KV cache size: 354,768 tokens
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:16 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 2217.30x
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:17 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.01 GiB
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:17 [core.py:259] init engine (profile, create kv cache, warmup model) took 5.10 seconds
INFO 01-25 00:17:18 [llm.py:360] Supported tasks: ['generate']
Throughput: 77.41 requests/s, 1316.05 total tokens/s, 77.41 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-25 00:17:06] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:17:06] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:06] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:17:06] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:06] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:06] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:06] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:06] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:06] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:06] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:17:06] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:17:06] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:17:06] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:17:06] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 00:17:11] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:17:11] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:11] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:17:11] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:11] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:11] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:11] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:11] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:11] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:11] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:17:11] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:17:11] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:17:11] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:17:11] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=858810) [2026-01-25 00:17:11] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=858810) [2026-01-25 00:17:11] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=858810) [2026-01-25 00:17:11] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=858810) [2026-01-25 00:17:11] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=858810) [2026-01-25 00:17:11] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=858810) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=858810) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.88it/s]
(EngineCore_DP0 pid=858810) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.87it/s]
(EngineCore_DP0 pid=858810) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=858810) 2026-01-25 00:17:16,942 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=858810) 2026-01-25 00:17:16,958 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=858810) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 26.57it/s]
(EngineCore_DP0 pid=858810) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 5344.02it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 53.88it/s, est. speed input: 862.28 toks/s, output: 53.89 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:01, 68.01it/s, est. speed input: 1052.77 toks/s, output: 65.79 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:01, 73.58it/s, est. speed input: 1130.31 toks/s, output: 70.64 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:01, 75.42it/s, est. speed input: 1159.93 toks/s, output: 72.50 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 76.94it/s, est. speed input: 1182.14 toks/s, output: 73.88 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 77.89it/s, est. speed input: 1197.25 toks/s, output: 74.83 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:00<00:00, 78.49it/s, est. speed input: 1208.13 toks/s, output: 75.51 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:00<00:00, 79.62it/s, est. speed input: 1221.48 toks/s, output: 76.34 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:00<00:00, 80.14it/s, est. speed input: 1230.57 toks/s, output: 76.91 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:00, 80.66it/s, est. speed input: 1238.67 toks/s, output: 77.42 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 80.94it/s, est. speed input: 1244.95 toks/s, output: 77.81 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:01<00:00, 80.82it/s, est. speed input: 1248.78 toks/s, output: 78.05 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:01<00:00, 80.74it/s, est. speed input: 1252.01 toks/s, output: 78.25 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:01<00:00, 80.81it/s, est. speed input: 1255.20 toks/s, output: 78.45 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:01<00:00, 80.84it/s, est. speed input: 1257.93 toks/s, output: 78.62 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 80.84it/s, est. speed input: 1258.26 toks/s, output: 78.64 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.63it/s, est. speed input: 1258.26 toks/s, output: 78.64 toks/s]
[rank0]:[W125 00:17:19.574004570 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 00:17:20
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-25 00:17:24 [datasets.py:612] Sampling input_len from [127, 127] and output_len from [1, 1]
INFO 01-25 00:17:24 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-25 00:17:24 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 00:17:24 [model.py:1661] Using max model len 257
INFO 01-25 00:17:24 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:28 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:28 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:59067 backend=nccl
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:28 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:28 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:29 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:29 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:29 [default_loader.py:308] Loading weights took 0.16 seconds
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:29 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.356792 seconds
(EngineCore_DP0 pid=859167) WARNING 01-25 00:17:32 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:32 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/425b3d75ef/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:32 [backends.py:703] Dynamo bytecode transform time: 2.10 s
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:33 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 257) from the cache, took 0.469 s
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:33 [monitor.py:34] torch.compile takes 2.57 s in total
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:34 [gpu_worker.py:375] Available KV cache memory: 10.81 GiB
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:34 [kv_cache_utils.py:1291] GPU KV cache size: 354,288 tokens
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:34 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 1302.53x
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:35 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.03 GiB
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:35 [core.py:259] init engine (profile, create kv cache, warmup model) took 5.27 seconds
INFO 01-25 00:17:35 [llm.py:360] Supported tasks: ['generate']
Throughput: 74.34 requests/s, 9589.98 total tokens/s, 74.34 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-25 00:17:24] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:17:24] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:24] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:17:24] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:24] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:24] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:24] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:24] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:24] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:24] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:17:24] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:17:24] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:17:24] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:17:24] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 00:17:28] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:17:28] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:28] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:17:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:28] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:28] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:28] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:28] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:28] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:28] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:17:28] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:17:28] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:17:28] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:17:28] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=859167) [2026-01-25 00:17:29] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=859167) [2026-01-25 00:17:29] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=859167) [2026-01-25 00:17:29] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=859167) [2026-01-25 00:17:29] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=859167) [2026-01-25 00:17:29] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=859167) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=859167) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.53it/s]
(EngineCore_DP0 pid=859167) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.53it/s]
(EngineCore_DP0 pid=859167) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=859167) 2026-01-25 00:17:34,455 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=859167) 2026-01-25 00:17:34,469 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=859167) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 26.78it/s]
(EngineCore_DP0 pid=859167) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3451.35it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:01, 67.20it/s, est. speed input: 8602.36 toks/s, output: 67.20 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:01, 73.26it/s, est. speed input: 9261.33 toks/s, output: 72.35 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:01, 74.92it/s, est. speed input: 9454.82 toks/s, output: 73.86 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:01, 76.53it/s, est. speed input: 9618.15 toks/s, output: 75.14 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 76.99it/s, est. speed input: 9686.27 toks/s, output: 75.67 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 77.16it/s, est. speed input: 9725.06 toks/s, output: 75.97 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:00<00:00, 76.77it/s, est. speed input: 9725.11 toks/s, output: 75.98 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:00<00:00, 77.25it/s, est. speed input: 9761.38 toks/s, output: 76.26 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:00<00:00, 77.48it/s, est. speed input: 9786.06 toks/s, output: 76.45 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:00, 77.87it/s, est. speed input: 9815.04 toks/s, output: 76.68 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 77.66it/s, est. speed input: 9820.90 toks/s, output: 76.73 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 78.02it/s, est. speed input: 9843.49 toks/s, output: 76.90 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:01<00:00, 78.36it/s, est. speed input: 9865.05 toks/s, output: 77.07 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:01<00:00, 78.59it/s, est. speed input: 9883.69 toks/s, output: 77.22 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:01<00:00, 76.54it/s, est. speed input: 9837.34 toks/s, output: 76.85 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:01<00:00, 73.20it/s, est. speed input: 9741.45 toks/s, output: 76.10 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 73.20it/s, est. speed input: 9730.04 toks/s, output: 76.02 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 76.01it/s, est. speed input: 9730.04 toks/s, output: 76.02 toks/s]
[rank0]:[W125 00:17:37.312702864 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 00:17:38
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 385 --max-num-batched-tokens 385 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-25 00:17:42 [datasets.py:612] Sampling input_len from [255, 255] and output_len from [1, 1]
INFO 01-25 00:17:42 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 385, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 385, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-25 00:17:42 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 00:17:42 [model.py:1661] Using max model len 385
INFO 01-25 00:17:43 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=385.
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:46 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=385, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [385], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:46 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:48131 backend=nccl
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:46 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:47 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:47 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:47 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:47 [default_loader.py:308] Loading weights took 0.15 seconds
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:47 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.328760 seconds
(EngineCore_DP0 pid=859542) WARNING 01-25 00:17:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:50 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/1ab92f29b4/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:50 [backends.py:703] Dynamo bytecode transform time: 2.05 s
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:51 [backends.py:261] Cache the graph of compile range (1, 385) for later use
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:53 [backends.py:278] Compiling a graph for compile range (1, 385) takes 1.83 s
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:53 [monitor.py:34] torch.compile takes 3.88 s in total
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:53 [gpu_worker.py:375] Available KV cache memory: 10.76 GiB
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:53 [kv_cache_utils.py:1291] GPU KV cache size: 352,480 tokens
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:53 [kv_cache_utils.py:1296] Maximum concurrency for 385 tokens per request: 881.20x
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:54 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.03 GiB
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:54 [core.py:259] init engine (profile, create kv cache, warmup model) took 6.65 seconds
INFO 01-25 00:17:55 [llm.py:360] Supported tasks: ['generate']
Throughput: 63.37 requests/s, 16285.23 total tokens/s, 63.37 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-25 00:17:42] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:17:42] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:42] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:17:42] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:42] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:42] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:42] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:42] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:42] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:42] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:17:42] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:17:42] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:17:42] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:17:42] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 00:17:46] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:17:46] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:46] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:17:46] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:46] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:46] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:46] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:46] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:46] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:46] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:17:46] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:17:46] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:17:46] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:17:46] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=859542) [2026-01-25 00:17:47] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=859542) [2026-01-25 00:17:47] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=859542) [2026-01-25 00:17:47] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=859542) [2026-01-25 00:17:47] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=859542) [2026-01-25 00:17:47] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=859542) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=859542) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.48it/s]
(EngineCore_DP0 pid=859542) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.48it/s]
(EngineCore_DP0 pid=859542) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=859542) 2026-01-25 00:17:53,906 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=859542) 2026-01-25 00:17:53,919 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=859542) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 19.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 19.72it/s]
(EngineCore_DP0 pid=859542) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1830.06it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:41,  3.07it/s, est. speed input: 786.99 toks/s, output: 3.07 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 26.27it/s, est. speed input: 5373.59 toks/s, output: 20.99 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 42.15it/s, est. speed input: 8192.58 toks/s, output: 32.00 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 53.04it/s, est. speed input: 10094.49 toks/s, output: 39.43 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 60.75it/s, est. speed input: 11482.04 toks/s, output: 44.85 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 66.24it/s, est. speed input: 12540.94 toks/s, output: 48.99 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 70.19it/s, est. speed input: 13378.40 toks/s, output: 52.26 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:00, 73.10it/s, est. speed input: 14062.01 toks/s, output: 54.93 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 74.42it/s, est. speed input: 14583.80 toks/s, output: 56.97 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 75.15it/s, est. speed input: 15008.70 toks/s, output: 58.63 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 75.65it/s, est. speed input: 15367.90 toks/s, output: 60.03 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 76.38it/s, est. speed input: 15694.03 toks/s, output: 61.30 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 77.14it/s, est. speed input: 15988.47 toks/s, output: 62.45 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 77.96it/s, est. speed input: 16259.01 toks/s, output: 63.51 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 78.34it/s, est. speed input: 16490.83 toks/s, output: 64.42 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 77.55it/s, est. speed input: 16655.89 toks/s, output: 65.06 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.55it/s, est. speed input: 16819.40 toks/s, output: 65.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 65.70it/s, est. speed input: 16819.40 toks/s, output: 65.70 toks/s]
[rank0]:[W125 00:17:57.014699622 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16 ==========
Time: 2026-01-25 00:22:47
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-25 00:22:51 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [1, 1]
INFO 01-25 00:22:51 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-25 00:22:51 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 00:22:51 [model.py:1661] Using max model len 145
INFO 01-25 00:22:51 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=861992) INFO 01-25 00:22:55 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=861992) INFO 01-25 00:22:55 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:57935 backend=nccl
(EngineCore_DP0 pid=861992) INFO 01-25 00:22:55 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=861992) INFO 01-25 00:22:55 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=861992) INFO 01-25 00:22:55 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=861992) INFO 01-25 00:22:56 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=861992) INFO 01-25 00:22:56 [default_loader.py:308] Loading weights took 0.25 seconds
(EngineCore_DP0 pid=861992) INFO 01-25 00:22:56 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.432883 seconds
(EngineCore_DP0 pid=861992) WARNING 01-25 00:22:58 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=861992) INFO 01-25 00:22:58 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/73a128f3ed/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=861992) INFO 01-25 00:22:58 [backends.py:703] Dynamo bytecode transform time: 2.03 s
(EngineCore_DP0 pid=861992) INFO 01-25 00:23:00 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 145) from the cache, took 0.447 s
(EngineCore_DP0 pid=861992) INFO 01-25 00:23:00 [monitor.py:34] torch.compile takes 2.48 s in total
(EngineCore_DP0 pid=861992) INFO 01-25 00:23:00 [gpu_worker.py:375] Available KV cache memory: 10.83 GiB
(EngineCore_DP0 pid=861992) INFO 01-25 00:23:01 [kv_cache_utils.py:1291] GPU KV cache size: 355,008 tokens
(EngineCore_DP0 pid=861992) INFO 01-25 00:23:01 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 2218.80x
(EngineCore_DP0 pid=861992) INFO 01-25 00:23:01 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.01 GiB
(EngineCore_DP0 pid=861992) INFO 01-25 00:23:01 [core.py:259] init engine (profile, create kv cache, warmup model) took 5.03 seconds
INFO 01-25 00:23:02 [llm.py:360] Supported tasks: ['generate']
Throughput: 75.70 requests/s, 1286.91 total tokens/s, 75.70 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-25 00:22:51] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:22:51] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:22:51] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:22:51] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:22:51] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:22:51] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:22:51] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:22:51] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:22:51] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:22:51] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:22:51] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:22:51] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:22:51] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:22:51] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 00:22:55] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:22:55] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:22:55] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:22:55] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:22:55] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:22:55] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:22:55] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:22:55] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:22:55] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:22:55] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:22:55] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:22:55] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:22:55] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:22:55] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=861992) [2026-01-25 00:22:55] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=861992) [2026-01-25 00:22:55] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=861992) [2026-01-25 00:22:55] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=861992) [2026-01-25 00:22:55] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=861992) [2026-01-25 00:22:55] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=861992) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=861992) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.26it/s]
(EngineCore_DP0 pid=861992) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.26it/s]
(EngineCore_DP0 pid=861992) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=861992) 2026-01-25 00:23:01,104 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=861992) 2026-01-25 00:23:01,118 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=861992) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 25.79it/s]
(EngineCore_DP0 pid=861992) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.00it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 8945.02it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 53.38it/s, est. speed input: 854.17 toks/s, output: 53.38 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:01, 65.37it/s, est. speed input: 1016.66 toks/s, output: 63.54 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:01, 68.58it/s, est. speed input: 1063.80 toks/s, output: 66.49 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:01, 73.34it/s, est. speed input: 1120.56 toks/s, output: 70.03 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 75.29it/s, est. speed input: 1147.46 toks/s, output: 71.72 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 76.00it/s, est. speed input: 1162.04 toks/s, output: 72.63 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:00<00:00, 77.11it/s, est. speed input: 1176.86 toks/s, output: 73.55 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:00<00:00, 77.71it/s, est. speed input: 1187.30 toks/s, output: 74.21 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:00<00:00, 76.71it/s, est. speed input: 1188.01 toks/s, output: 74.25 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:00, 76.45it/s, est. speed input: 1190.58 toks/s, output: 74.41 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 77.08it/s, est. speed input: 1196.38 toks/s, output: 74.77 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 78.15it/s, est. speed input: 1204.31 toks/s, output: 75.27 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:01<00:00, 78.65it/s, est. speed input: 1209.65 toks/s, output: 75.60 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 79.15it/s, est. speed input: 1215.23 toks/s, output: 75.95 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:01<00:00, 79.50it/s, est. speed input: 1220.10 toks/s, output: 76.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 79.50it/s, est. speed input: 1222.17 toks/s, output: 76.39 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 76.38it/s, est. speed input: 1222.17 toks/s, output: 76.39 toks/s]
[rank0]:[W125 00:23:04.921276721 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 00:23:05
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-25 00:23:08 [datasets.py:612] Sampling input_len from [127, 127] and output_len from [1, 1]
INFO 01-25 00:23:08 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-25 00:23:08 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 00:23:08 [model.py:1661] Using max model len 257
INFO 01-25 00:23:09 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:12 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:12 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:47415 backend=nccl
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:12 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:13 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:13 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:13 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:13 [default_loader.py:308] Loading weights took 0.24 seconds
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:14 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.436673 seconds
(EngineCore_DP0 pid=862314) WARNING 01-25 00:23:16 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:16 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/425b3d75ef/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:16 [backends.py:703] Dynamo bytecode transform time: 2.01 s
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:17 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 257) from the cache, took 0.406 s
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:17 [monitor.py:34] torch.compile takes 2.41 s in total
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:18 [gpu_worker.py:375] Available KV cache memory: 10.82 GiB
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:18 [kv_cache_utils.py:1291] GPU KV cache size: 354,528 tokens
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:18 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 1303.41x
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:19 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.03 GiB
(EngineCore_DP0 pid=862314) INFO 01-25 00:23:19 [core.py:259] init engine (profile, create kv cache, warmup model) took 5.00 seconds
INFO 01-25 00:23:19 [llm.py:360] Supported tasks: ['generate']
Throughput: 76.39 requests/s, 9854.54 total tokens/s, 76.39 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-25 00:23:09] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:23:09] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:23:09] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:23:09] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:09] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:09] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:09] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:09] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:09] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:23:09] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:23:09] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:23:09] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:23:09] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:23:09] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 00:23:12] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:23:12] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:23:12] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:23:12] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:12] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:12] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:12] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:12] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:12] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:23:12] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:23:12] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:23:12] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:23:12] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:23:12] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=862314) [2026-01-25 00:23:13] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=862314) [2026-01-25 00:23:13] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=862314) [2026-01-25 00:23:13] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=862314) [2026-01-25 00:23:13] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=862314) [2026-01-25 00:23:13] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=862314) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=862314) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.29it/s]
(EngineCore_DP0 pid=862314) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.29it/s]
(EngineCore_DP0 pid=862314) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=862314) 2026-01-25 00:23:18,436 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=862314) 2026-01-25 00:23:18,451 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=862314) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 26.61it/s]
(EngineCore_DP0 pid=862314) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.90it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.90it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 18/128 [00:00<00:00, 119.38it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 703.39it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:00, 169.13it/s, est. speed input: 21650.64 toks/s, output: 169.13 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:00, 102.30it/s, est. speed input: 13919.85 toks/s, output: 108.75 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:00<00:00, 92.86it/s, est. speed input: 12744.74 toks/s, output: 99.57 toks/s]  
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 88.64it/s, est. speed input: 12207.42 toks/s, output: 95.37 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:00<00:00, 85.96it/s, est. speed input: 11874.38 toks/s, output: 92.77 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:00<00:00, 84.08it/s, est. speed input: 11645.09 toks/s, output: 90.98 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:00<00:00, 82.50it/s, est. speed input: 11456.86 toks/s, output: 89.51 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:01<00:00, 81.58it/s, est. speed input: 11318.88 toks/s, output: 88.43 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:01<00:00, 81.22it/s, est. speed input: 11220.31 toks/s, output: 87.66 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:01<00:00, 79.94it/s, est. speed input: 11097.45 toks/s, output: 86.70 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 79.91it/s, est. speed input: 11026.75 toks/s, output: 86.15 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 79.91it/s, est. speed input: 10983.59 toks/s, output: 85.81 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 85.80it/s, est. speed input: 10983.59 toks/s, output: 85.81 toks/s]
[rank0]:[W125 00:23:21.110762871 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 00:23:22
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 385 --max-num-batched-tokens 385 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
INFO 01-25 00:23:26 [datasets.py:612] Sampling input_len from [255, 255] and output_len from [1, 1]
INFO 01-25 00:23:26 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 385, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 385, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-25 00:23:26 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 00:23:26 [model.py:1661] Using max model len 385
INFO 01-25 00:23:26 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=385.
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:30 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=385, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [385], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:30 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:47895 backend=nccl
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:30 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:30 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:31 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:31 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:31 [default_loader.py:308] Loading weights took 0.24 seconds
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:31 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.428217 seconds
(EngineCore_DP0 pid=862666) WARNING 01-25 00:23:33 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:33 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/1ab92f29b4/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:33 [backends.py:703] Dynamo bytecode transform time: 2.02 s
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:35 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 385) from the cache, took 0.240 s
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:35 [monitor.py:34] torch.compile takes 2.26 s in total
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:35 [gpu_worker.py:375] Available KV cache memory: 10.81 GiB
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:35 [kv_cache_utils.py:1291] GPU KV cache size: 354,112 tokens
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:35 [kv_cache_utils.py:1296] Maximum concurrency for 385 tokens per request: 885.28x
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:36 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.03 GiB
(EngineCore_DP0 pid=862666) INFO 01-25 00:23:36 [core.py:259] init engine (profile, create kv cache, warmup model) took 4.91 seconds
INFO 01-25 00:23:37 [llm.py:360] Supported tasks: ['generate']
Throughput: 76.07 requests/s, 19549.85 total tokens/s, 76.07 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-25 00:23:26] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:23:26] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:23:26] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:23:26] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:26] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:26] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:26] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:26] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:26] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:23:26] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:23:26] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:23:26] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:23:26] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:23:26] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 00:23:30] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:23:30] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:23:30] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:23:30] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:30] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:30] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:30] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:30] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:23:30] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:23:30] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:23:30] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:23:30] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:23:30] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:23:30] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=862666) [2026-01-25 00:23:31] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=862666) [2026-01-25 00:23:31] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=862666) [2026-01-25 00:23:31] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=862666) [2026-01-25 00:23:31] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=862666) [2026-01-25 00:23:31] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=862666) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=862666) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.29it/s]
(EngineCore_DP0 pid=862666) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.28it/s]
(EngineCore_DP0 pid=862666) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=862666) 2026-01-25 00:23:35,963 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=862666) 2026-01-25 00:23:35,978 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=862666) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.58it/s]
(EngineCore_DP0 pid=862666) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.85it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1984.92it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:01, 83.99it/s, est. speed input: 21503.27 toks/s, output: 83.99 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:01, 80.97it/s, est. speed input: 20842.84 toks/s, output: 81.41 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:01, 80.51it/s, est. speed input: 20716.64 toks/s, output: 80.92 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 79.75it/s, est. speed input: 20565.95 toks/s, output: 80.33 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:00<00:01, 79.50it/s, est. speed input: 20503.39 toks/s, output: 80.09 toks/s]
Processed prompts:  41%|████      | 52/128 [00:00<00:00, 79.64it/s, est. speed input: 20497.77 toks/s, output: 80.07 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:00<00:00, 79.73it/s, est. speed input: 20492.54 toks/s, output: 80.05 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:00<00:00, 79.29it/s, est. speed input: 20439.38 toks/s, output: 79.84 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:00<00:00, 79.19it/s, est. speed input: 20415.95 toks/s, output: 79.75 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:00, 78.68it/s, est. speed input: 20360.52 toks/s, output: 79.53 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 78.43it/s, est. speed input: 20322.60 toks/s, output: 79.38 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:01<00:00, 78.09it/s, est. speed input: 20279.02 toks/s, output: 79.21 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:01<00:00, 78.94it/s, est. speed input: 20311.13 toks/s, output: 79.34 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:01<00:00, 78.50it/s, est. speed input: 20277.67 toks/s, output: 79.21 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:01<00:00, 78.60it/s, est. speed input: 20271.68 toks/s, output: 79.19 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.60it/s, est. speed input: 20275.89 toks/s, output: 79.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 79.20it/s, est. speed input: 20275.89 toks/s, output: 79.20 toks/s]
[rank0]:[W125 00:23:38.694281391 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16 ==========
Time: 2026-01-25 01:08:56
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 145 --max-num-batched-tokens 145 --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M16.json

