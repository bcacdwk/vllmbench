
========== M=16 ==========
Time: 2026-01-25 22:07:11
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:07:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=493384) WARNING 01-25 22:07:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.53 requests/s, 655.00 total tokens/s, 38.53 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-25 22:07:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:07:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:07:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:07:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:07:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:07:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:07:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:07:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:07:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:07:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:07:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:07:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:07:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:07:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:07:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:07:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:07:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:07:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:07:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:07:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:07:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:07:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:07:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:07:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:07:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:07:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:07:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:07:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=493384) [2026-01-25 22:07:27] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=493384) [2026-01-25 22:07:27] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=493384) [2026-01-25 22:07:27] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=493384) [2026-01-25 22:07:27] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=493384) [2026-01-25 22:07:27] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=493384) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=493384) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.80it/s]
(EngineCore_DP0 pid=493384) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.80it/s]
(EngineCore_DP0 pid=493384) 
(EngineCore_DP0 pid=493384) 2026-01-25 22:07:39,092 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=493384) 2026-01-25 22:07:39,248 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=493384) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.33it/s]
(EngineCore_DP0 pid=493384) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 17.68it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2897.20it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:18,  6.72it/s, est. speed input: 107.56 toks/s, output: 6.72 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:04, 25.20it/s, est. speed input: 354.54 toks/s, output: 22.16 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 31.90it/s, est. speed input: 446.36 toks/s, output: 27.90 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 34.98it/s, est. speed input: 492.45 toks/s, output: 30.78 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 36.95it/s, est. speed input: 522.43 toks/s, output: 32.65 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 38.15it/s, est. speed input: 542.72 toks/s, output: 33.92 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 38.95it/s, est. speed input: 557.55 toks/s, output: 34.85 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:02, 39.47it/s, est. speed input: 568.74 toks/s, output: 35.55 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 39.82it/s, est. speed input: 577.48 toks/s, output: 36.09 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 40.01it/s, est. speed input: 584.31 toks/s, output: 36.52 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 40.17it/s, est. speed input: 590.04 toks/s, output: 36.88 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 40.31it/s, est. speed input: 594.94 toks/s, output: 37.18 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 40.38it/s, est. speed input: 599.00 toks/s, output: 37.44 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 40.37it/s, est. speed input: 602.29 toks/s, output: 37.64 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 40.37it/s, est. speed input: 605.17 toks/s, output: 37.82 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 40.35it/s, est. speed input: 607.65 toks/s, output: 37.98 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 40.31it/s, est. speed input: 609.74 toks/s, output: 38.11 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 40.29it/s, est. speed input: 611.61 toks/s, output: 38.23 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 40.27it/s, est. speed input: 613.30 toks/s, output: 38.33 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 40.39it/s, est. speed input: 615.13 toks/s, output: 38.45 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 40.74it/s, est. speed input: 617.43 toks/s, output: 38.59 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.96it/s, est. speed input: 619.48 toks/s, output: 38.72 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 41.20it/s, est. speed input: 621.51 toks/s, output: 38.84 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 41.04it/s, est. speed input: 622.74 toks/s, output: 38.92 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 40.68it/s, est. speed input: 623.35 toks/s, output: 38.96 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 40.91it/s, est. speed input: 624.84 toks/s, output: 39.05 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.91it/s, est. speed input: 625.15 toks/s, output: 39.07 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.07it/s, est. speed input: 625.15 toks/s, output: 39.07 toks/s]
[rank0]:[W125 22:07:44.722540933 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 22:07:46
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:07:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=494576) WARNING 01-25 22:08:07 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.91 requests/s, 5019.56 total tokens/s, 38.91 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-25 22:07:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:07:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:07:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:07:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:07:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:07:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:07:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:07:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:07:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:07:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:07:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:07:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:07:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:07:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:08:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:08:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:08:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:08:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:08:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:08:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:08:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:08:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:08:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:08:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:08:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:08:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:08:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:08:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=494576) [2026-01-25 22:08:03] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=494576) [2026-01-25 22:08:03] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=494576) [2026-01-25 22:08:03] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=494576) [2026-01-25 22:08:03] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=494576) [2026-01-25 22:08:03] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=494576) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=494576) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.75it/s]
(EngineCore_DP0 pid=494576) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.75it/s]
(EngineCore_DP0 pid=494576) 
(EngineCore_DP0 pid=494576) 2026-01-25 22:08:13,994 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=494576) 2026-01-25 22:08:14,022 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=494576) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.80it/s]
(EngineCore_DP0 pid=494576) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1362.87it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:04, 27.76it/s, est. speed input: 3553.59 toks/s, output: 27.76 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 36.05it/s, est. speed input: 4465.23 toks/s, output: 34.88 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 38.06it/s, est. speed input: 4708.49 toks/s, output: 36.78 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 38.57it/s, est. speed input: 4785.69 toks/s, output: 37.39 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 39.54it/s, est. speed input: 4886.78 toks/s, output: 38.18 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 40.20it/s, est. speed input: 4958.72 toks/s, output: 38.74 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 40.60it/s, est. speed input: 5008.98 toks/s, output: 39.13 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 40.87it/s, est. speed input: 5046.70 toks/s, output: 39.43 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 40.94it/s, est. speed input: 5071.43 toks/s, output: 39.62 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:01, 40.67it/s, est. speed input: 5077.60 toks/s, output: 39.67 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 40.47it/s, est. speed input: 5081.94 toks/s, output: 39.70 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 40.25it/s, est. speed input: 5082.71 toks/s, output: 39.71 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 40.34it/s, est. speed input: 5091.01 toks/s, output: 39.77 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 40.33it/s, est. speed input: 5096.15 toks/s, output: 39.81 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 40.31it/s, est. speed input: 5100.11 toks/s, output: 39.84 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 40.35it/s, est. speed input: 5104.91 toks/s, output: 39.88 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 40.28it/s, est. speed input: 5106.80 toks/s, output: 39.90 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 40.21it/s, est. speed input: 5107.89 toks/s, output: 39.90 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 40.33it/s, est. speed input: 5112.66 toks/s, output: 39.94 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 40.52it/s, est. speed input: 5119.22 toks/s, output: 39.99 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 40.56it/s, est. speed input: 5123.39 toks/s, output: 40.03 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 40.50it/s, est. speed input: 5125.42 toks/s, output: 40.04 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 40.43it/s, est. speed input: 5126.56 toks/s, output: 40.05 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 40.39it/s, est. speed input: 5127.88 toks/s, output: 40.06 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 40.36it/s, est. speed input: 5129.04 toks/s, output: 40.07 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 40.33it/s, est. speed input: 5130.10 toks/s, output: 40.08 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.33it/s, est. speed input: 5130.03 toks/s, output: 40.08 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.08it/s, est. speed input: 5130.03 toks/s, output: 40.08 toks/s]
[rank0]:[W125 22:08:19.831240459 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 22:08:22
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:08:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=495612) WARNING 01-25 22:08:42 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.10 requests/s, 10049.31 total tokens/s, 39.10 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-25 22:08:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:08:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:08:29] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:08:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:08:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:08:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:08:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:08:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:08:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:08:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:08:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:08:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:08:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:08:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:08:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:08:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:08:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:08:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:08:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:08:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:08:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:08:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:08:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:08:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:08:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:08:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:08:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:08:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=495612) [2026-01-25 22:08:38] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=495612) [2026-01-25 22:08:38] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=495612) [2026-01-25 22:08:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=495612) [2026-01-25 22:08:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=495612) [2026-01-25 22:08:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=495612) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=495612) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.77it/s]
(EngineCore_DP0 pid=495612) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.76it/s]
(EngineCore_DP0 pid=495612) 
(EngineCore_DP0 pid=495612) 2026-01-25 22:08:50,280 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=495612) 2026-01-25 22:08:50,307 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=495612) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 16.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 16.05it/s]
(EngineCore_DP0 pid=495612) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.74it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1513.26it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 35.96it/s, est. speed input: 9205.98 toks/s, output: 35.96 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 39.14it/s, est. speed input: 9904.43 toks/s, output: 38.68 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:02, 40.08it/s, est. speed input: 10122.24 toks/s, output: 39.54 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 40.56it/s, est. speed input: 10234.94 toks/s, output: 39.98 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 40.47it/s, est. speed input: 10253.31 toks/s, output: 40.05 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 40.38it/s, est. speed input: 10259.41 toks/s, output: 40.08 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 40.20it/s, est. speed input: 10250.06 toks/s, output: 40.04 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:02, 40.17it/s, est. speed input: 10252.53 toks/s, output: 40.05 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 40.16it/s, est. speed input: 10255.18 toks/s, output: 40.06 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:01, 40.09it/s, est. speed input: 10252.31 toks/s, output: 40.05 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 40.07it/s, est. speed input: 10251.71 toks/s, output: 40.05 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 40.06it/s, est. speed input: 10251.52 toks/s, output: 40.04 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 40.05it/s, est. speed input: 10250.84 toks/s, output: 40.04 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 40.04it/s, est. speed input: 10250.28 toks/s, output: 40.04 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 40.06it/s, est. speed input: 10251.63 toks/s, output: 40.05 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:01, 40.20it/s, est. speed input: 10259.62 toks/s, output: 40.08 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 40.25it/s, est. speed input: 10263.73 toks/s, output: 40.09 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:00, 40.26it/s, est. speed input: 10266.61 toks/s, output: 40.10 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 40.27it/s, est. speed input: 10269.30 toks/s, output: 40.11 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 40.28it/s, est. speed input: 10271.73 toks/s, output: 40.12 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 40.23it/s, est. speed input: 10271.64 toks/s, output: 40.12 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 40.26it/s, est. speed input: 10274.01 toks/s, output: 40.13 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 40.31it/s, est. speed input: 10277.08 toks/s, output: 40.14 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 40.29it/s, est. speed input: 10278.19 toks/s, output: 40.15 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 40.27it/s, est. speed input: 10279.04 toks/s, output: 40.15 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.27it/s, est. speed input: 10280.29 toks/s, output: 40.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.15it/s, est. speed input: 10280.29 toks/s, output: 40.16 toks/s]
[rank0]:[W125 22:08:55.727761897 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

