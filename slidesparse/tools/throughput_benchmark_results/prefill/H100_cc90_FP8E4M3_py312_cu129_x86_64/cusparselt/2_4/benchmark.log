
========== M=16 ==========
Time: 2026-01-25 22:10:48
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:10:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=500346) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=500346) WARNING 01-25 22:11:09 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.61 requests/s, 520.30 total tokens/s, 30.61 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-25 22:10:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:10:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:10:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:10:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:10:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:10:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:10:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:10:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:10:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:10:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:10:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:10:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:10:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:10:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:11:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:11:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:11:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:11:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:11:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:11:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:11:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:11:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:11:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:11:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:11:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:11:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:11:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:11:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=500346) [2026-01-25 22:11:04] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=500346) [2026-01-25 22:11:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=500346) [2026-01-25 22:11:04] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=500346) [2026-01-25 22:11:04] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=500346) [2026-01-25 22:11:04] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=500346) [2026-01-25 22:11:04] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=500346) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=500346) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.76it/s]
(EngineCore_DP0 pid=500346) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.75it/s]
(EngineCore_DP0 pid=500346) 
(EngineCore_DP0 pid=500346) [2026-01-25 22:11:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=500346) [2026-01-25 22:11:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=500346) [2026-01-25 22:11:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=500346) [2026-01-25 22:11:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=500346) [2026-01-25 22:11:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=500346) [2026-01-25 22:11:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=500346) [2026-01-25 22:11:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=500346) [2026-01-25 22:11:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=500346) 2026-01-25 22:11:17,411 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=500346) 2026-01-25 22:11:17,451 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=500346) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.57it/s]
(EngineCore_DP0 pid=500346) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.96it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2544.76it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:36,  3.48it/s, est. speed input: 55.77 toks/s, output: 3.49 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:08, 14.67it/s, est. speed input: 196.89 toks/s, output: 12.30 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:05, 21.16it/s, est. speed input: 273.42 toks/s, output: 17.09 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 25.12it/s, est. speed input: 321.08 toks/s, output: 20.07 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:04, 27.70it/s, est. speed input: 353.85 toks/s, output: 22.11 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 29.37it/s, est. speed input: 377.54 toks/s, output: 23.60 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:03, 30.50it/s, est. speed input: 395.61 toks/s, output: 24.72 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:03, 31.29it/s, est. speed input: 409.84 toks/s, output: 25.61 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 31.80it/s, est. speed input: 421.23 toks/s, output: 26.33 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 32.01it/s, est. speed input: 430.03 toks/s, output: 26.88 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 32.29it/s, est. speed input: 437.89 toks/s, output: 27.37 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 32.50it/s, est. speed input: 444.63 toks/s, output: 27.79 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 32.61it/s, est. speed input: 450.32 toks/s, output: 28.14 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 32.69it/s, est. speed input: 455.25 toks/s, output: 28.45 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 32.73it/s, est. speed input: 459.54 toks/s, output: 28.72 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:02, 32.79it/s, est. speed input: 463.42 toks/s, output: 28.96 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 32.84it/s, est. speed input: 466.89 toks/s, output: 29.18 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 32.85it/s, est. speed input: 469.96 toks/s, output: 29.37 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 32.98it/s, est. speed input: 473.00 toks/s, output: 29.56 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 32.92it/s, est. speed input: 475.44 toks/s, output: 29.71 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 33.02it/s, est. speed input: 477.94 toks/s, output: 29.87 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 33.07it/s, est. speed input: 480.20 toks/s, output: 30.01 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 33.00it/s, est. speed input: 482.07 toks/s, output: 30.13 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:03<00:01, 32.97it/s, est. speed input: 483.82 toks/s, output: 30.24 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:00, 32.97it/s, est. speed input: 485.47 toks/s, output: 30.34 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 33.00it/s, est. speed input: 487.07 toks/s, output: 30.44 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 33.18it/s, est. speed input: 488.82 toks/s, output: 30.55 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 33.38it/s, est. speed input: 490.58 toks/s, output: 30.66 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 33.46it/s, est. speed input: 492.12 toks/s, output: 30.76 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 33.53it/s, est. speed input: 493.60 toks/s, output: 30.85 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 33.12it/s, est. speed input: 494.28 toks/s, output: 30.89 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:04<00:00, 33.04it/s, est. speed input: 495.23 toks/s, output: 30.95 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 33.04it/s, est. speed input: 495.91 toks/s, output: 30.99 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 30.99it/s, est. speed input: 495.91 toks/s, output: 30.99 toks/s]
[rank0]:[W125 22:11:24.105285056 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 22:11:26
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-FP8_M128.json

