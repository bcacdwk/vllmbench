
========== M=512 ==========
Time: 2026-01-25 15:47:07
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:47:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=142988) WARNING 01-25 15:48:08 [backends.py:609] Failed to read file <frozen os>
Throughput: 29.62 requests/s, 15195.10 total tokens/s, 29.62 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-25 15:47:14] INFO font_manager.py:1639: generated new fontManager
[2026-01-25 15:47:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:47:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:47:23] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:23] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:47:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:23] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:23] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:47:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:23] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:23] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:23] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:47:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:23] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:47:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:47:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:47:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:47:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:47:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:47:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:47:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:47:31] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:31] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:47:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:31] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:31] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:47:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:31] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:31] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:31] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:47:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:31] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:47:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:47:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:47:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:47:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:47:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:47:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:47:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=142988) [2026-01-25 15:47:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=142988) [2026-01-25 15:47:33] INFO gemm_wrapper.py:85: Auto-building cublaslt GEMM library from /root/vllmbench/slidesparse/csrc/cublaslt_gemm/build_cublaslt.py...
(EngineCore_DP0 pid=142988) [2026-01-25 15:47:46] INFO gemm_wrapper.py:95: cublaslt GEMM library build completed
(EngineCore_DP0 pid=142988) [2026-01-25 15:47:46] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=142988) [2026-01-25 15:47:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=142988) [2026-01-25 15:47:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=142988) [2026-01-25 15:47:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=142988) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=142988) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.54it/s]
(EngineCore_DP0 pid=142988) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.54it/s]
(EngineCore_DP0 pid=142988) 
(EngineCore_DP0 pid=142988) [2026-01-25 15:48:17] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=142988) 2026-01-25 15:48:24,759 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=142988) 2026-01-25 15:48:24,786 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=142988) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]
(EngineCore_DP0 pid=142988) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 17.46it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  42%|████▏     | 54/128 [00:00<00:00, 539.78it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 714.80it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:49,  2.55it/s, est. speed input: 1306.36 toks/s, output: 2.55 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:10, 11.96it/s, est. speed input: 5015.76 toks/s, output: 9.80 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:06, 18.56it/s, est. speed input: 7336.56 toks/s, output: 14.33 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 23.18it/s, est. speed input: 8922.80 toks/s, output: 17.43 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:04, 26.40it/s, est. speed input: 10075.06 toks/s, output: 19.68 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 28.65it/s, est. speed input: 10950.47 toks/s, output: 21.39 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:03, 30.26it/s, est. speed input: 11642.18 toks/s, output: 22.74 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:03, 31.38it/s, est. speed input: 12198.86 toks/s, output: 23.83 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 32.16it/s, est. speed input: 12657.83 toks/s, output: 24.72 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 32.66it/s, est. speed input: 13036.78 toks/s, output: 25.46 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 33.08it/s, est. speed input: 13365.95 toks/s, output: 26.10 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 33.35it/s, est. speed input: 13646.84 toks/s, output: 26.65 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 33.50it/s, est. speed input: 13887.73 toks/s, output: 27.12 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 33.63it/s, est. speed input: 14101.47 toks/s, output: 27.54 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:02<00:02, 33.74it/s, est. speed input: 14291.61 toks/s, output: 27.91 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:01, 33.79it/s, est. speed input: 14459.64 toks/s, output: 28.24 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 33.87it/s, est. speed input: 14613.25 toks/s, output: 28.54 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 33.92it/s, est. speed input: 14750.78 toks/s, output: 28.81 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 33.99it/s, est. speed input: 14878.54 toks/s, output: 29.06 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 34.00it/s, est. speed input: 14992.37 toks/s, output: 29.28 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 33.96it/s, est. speed input: 15093.35 toks/s, output: 29.48 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 33.94it/s, est. speed input: 15186.41 toks/s, output: 29.66 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 33.83it/s, est. speed input: 15266.64 toks/s, output: 29.82 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:03<00:01, 33.86it/s, est. speed input: 15346.59 toks/s, output: 29.97 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:00, 33.88it/s, est. speed input: 15420.38 toks/s, output: 30.12 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 33.89it/s, est. speed input: 15489.34 toks/s, output: 30.25 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 33.86it/s, est. speed input: 15551.17 toks/s, output: 30.37 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 33.84it/s, est. speed input: 15609.02 toks/s, output: 30.49 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 33.82it/s, est. speed input: 15662.91 toks/s, output: 30.59 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 33.82it/s, est. speed input: 15714.56 toks/s, output: 30.69 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 33.47it/s, est. speed input: 15746.19 toks/s, output: 30.75 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:04<00:00, 33.61it/s, est. speed input: 15793.73 toks/s, output: 30.85 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 33.61it/s, est. speed input: 15827.55 toks/s, output: 30.91 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 30.91it/s, est. speed input: 15827.55 toks/s, output: 30.91 toks/s]
[rank0]:[W125 15:48:32.720247585 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-25 15:48:35
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:48:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=145389) WARNING 01-25 15:48:58 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.33 requests/s, 32114.95 total tokens/s, 31.33 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-25 15:48:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:48:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:48:43] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:43] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:48:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:43] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:43] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:48:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:43] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:43] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:43] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:48:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:43] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:48:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:48:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:48:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:48:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:48:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:48:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:48:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:48:50] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:50] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:48:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:50] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:50] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:48:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:50] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:50] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:50] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:48:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:50] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:48:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:48:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:48:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:48:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:48:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:48:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:48:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=145389) [2026-01-25 15:48:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=145389) [2026-01-25 15:48:51] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=145389) [2026-01-25 15:48:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=145389) [2026-01-25 15:48:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=145389) [2026-01-25 15:48:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=145389) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=145389) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=145389) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=145389) 
(EngineCore_DP0 pid=145389) [2026-01-25 15:49:05] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=145389) 2026-01-25 15:49:11,296 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=145389) 2026-01-25 15:49:11,355 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=145389) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.17it/s]
(EngineCore_DP0 pid=145389) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.74it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|███       | 39/128 [00:00<00:00, 382.44it/s]
Adding requests:  72%|███████▏  | 92/128 [00:00<00:00, 460.35it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 464.63it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:01, 60.54it/s, est. speed input: 61997.78 toks/s, output: 60.54 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:02, 40.28it/s, est. speed input: 43428.63 toks/s, output: 42.41 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 37.04it/s, est. speed input: 40267.80 toks/s, output: 39.32 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 35.69it/s, est. speed input: 38942.09 toks/s, output: 38.03 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 34.79it/s, est. speed input: 38056.76 toks/s, output: 37.16 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 34.23it/s, est. speed input: 37442.24 toks/s, output: 36.56 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:02, 33.85it/s, est. speed input: 36988.58 toks/s, output: 36.12 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:02, 33.56it/s, est. speed input: 36618.01 toks/s, output: 35.76 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 33.34it/s, est. speed input: 36317.35 toks/s, output: 35.47 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 33.15it/s, est. speed input: 36061.47 toks/s, output: 35.22 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 33.03it/s, est. speed input: 35848.49 toks/s, output: 35.01 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:02, 32.92it/s, est. speed input: 35664.26 toks/s, output: 34.83 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:02, 32.85it/s, est. speed input: 35506.95 toks/s, output: 34.67 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 32.83it/s, est. speed input: 35377.21 toks/s, output: 34.55 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 32.86it/s, est. speed input: 35273.79 toks/s, output: 34.45 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:01, 32.85it/s, est. speed input: 35175.63 toks/s, output: 34.35 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:02<00:01, 32.81it/s, est. speed input: 35083.06 toks/s, output: 34.26 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 32.80it/s, est. speed input: 35001.86 toks/s, output: 34.18 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 32.77it/s, est. speed input: 34926.65 toks/s, output: 34.11 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 32.82it/s, est. speed input: 34868.82 toks/s, output: 34.05 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:01, 32.84it/s, est. speed input: 34814.59 toks/s, output: 34.00 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:01, 32.83it/s, est. speed input: 34761.32 toks/s, output: 33.95 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 32.79it/s, est. speed input: 34708.21 toks/s, output: 33.89 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:03<00:00, 32.79it/s, est. speed input: 34663.09 toks/s, output: 33.85 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:03<00:00, 32.81it/s, est. speed input: 34623.15 toks/s, output: 33.81 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:03<00:00, 32.79it/s, est. speed input: 34583.24 toks/s, output: 33.77 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:03<00:00, 32.80it/s, est. speed input: 34548.45 toks/s, output: 33.74 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:03<00:00, 32.85it/s, est. speed input: 34521.40 toks/s, output: 33.71 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:03<00:00, 32.78it/s, est. speed input: 34483.12 toks/s, output: 33.67 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 32.56it/s, est. speed input: 34428.71 toks/s, output: 33.62 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 32.56it/s, est. speed input: 34417.28 toks/s, output: 33.61 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.61it/s, est. speed input: 34417.28 toks/s, output: 33.61 toks/s]
[rank0]:[W125 15:49:17.386827969 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-25 15:49:19
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:49:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=146681) WARNING 01-25 15:49:41 [backends.py:609] Failed to read file <frozen os>
Throughput: 60.12 requests/s, 61621.97 total tokens/s, 60.12 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-25 15:49:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:49:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:49:27] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:27] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:49:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:27] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:27] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:49:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:27] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:27] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:27] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:49:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:27] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:49:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:49:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:49:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:49:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:49:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:49:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:49:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:49:33] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:33] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:49:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:33] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:33] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:49:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:33] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:33] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:33] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:49:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:33] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:49:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:49:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:49:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:49:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:49:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:49:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:49:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=146681) [2026-01-25 15:49:35] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=146681) [2026-01-25 15:49:35] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=146681) [2026-01-25 15:49:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=146681) [2026-01-25 15:49:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=146681) [2026-01-25 15:49:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=146681) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=146681) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=146681) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=146681) 
(EngineCore_DP0 pid=146681) [2026-01-25 15:49:48] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=146681) 2026-01-25 15:49:53,178 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=146681) 2026-01-25 15:49:53,230 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=146681) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 15.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.72it/s]
(EngineCore_DP0 pid=146681) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.35it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.33it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   9%|▉         | 24/256 [00:00<00:00, 237.96it/s]
Adding requests:  28%|██▊       | 72/256 [00:00<00:00, 377.37it/s]
Adding requests:  48%|████▊     | 124/256 [00:00<00:00, 439.75it/s]
Adding requests:  68%|██████▊   | 174/256 [00:00<00:00, 461.84it/s]
Adding requests:  88%|████████▊ | 226/256 [00:00<00:00, 480.66it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 455.03it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▎        | 32/256 [00:00<00:00, 279.42it/s, est. speed input: 286174.35 toks/s, output: 279.43 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:00<00:02, 95.79it/s, est. speed input: 109624.64 toks/s, output: 107.05 toks/s] 
Processed prompts:  30%|██▉       | 76/256 [00:00<00:02, 82.33it/s, est. speed input: 95487.52 toks/s, output: 93.25 toks/s]  
Processed prompts:  34%|███▍      | 88/256 [00:01<00:02, 75.95it/s, est. speed input: 89249.95 toks/s, output: 87.16 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:01<00:02, 72.38it/s, est. speed input: 85801.15 toks/s, output: 83.79 toks/s]
Processed prompts:  42%|████▏     | 107/256 [00:01<00:02, 71.80it/s, est. speed input: 84391.64 toks/s, output: 82.41 toks/s]
Processed prompts:  45%|████▍     | 115/256 [00:01<00:02, 69.55it/s, est. speed input: 82590.46 toks/s, output: 80.65 toks/s]
Processed prompts:  48%|████▊     | 123/256 [00:01<00:01, 67.78it/s, est. speed input: 81090.16 toks/s, output: 79.19 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:01<00:01, 64.29it/s, est. speed input: 79212.99 toks/s, output: 77.35 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:01<00:01, 63.89it/s, est. speed input: 78166.42 toks/s, output: 76.33 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:01<00:01, 63.41it/s, est. speed input: 77208.77 toks/s, output: 75.40 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:02<00:01, 63.03it/s, est. speed input: 76359.60 toks/s, output: 74.57 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:02<00:01, 62.83it/s, est. speed input: 75626.26 toks/s, output: 73.85 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:02<00:01, 62.74it/s, est. speed input: 74988.29 toks/s, output: 73.23 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:02<00:01, 62.76it/s, est. speed input: 74431.70 toks/s, output: 72.69 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:02<00:01, 62.79it/s, est. speed input: 73934.81 toks/s, output: 72.20 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:02<00:00, 62.78it/s, est. speed input: 73478.14 toks/s, output: 71.76 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:02<00:00, 62.78it/s, est. speed input: 73064.68 toks/s, output: 71.35 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:02<00:00, 62.69it/s, est. speed input: 72672.24 toks/s, output: 70.97 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:03<00:00, 62.55it/s, est. speed input: 72299.33 toks/s, output: 70.60 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:03<00:00, 62.45it/s, est. speed input: 71955.45 toks/s, output: 70.27 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:03<00:00, 62.44it/s, est. speed input: 71647.49 toks/s, output: 69.97 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:03<00:00, 62.53it/s, est. speed input: 71375.11 toks/s, output: 69.70 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:03<00:00, 62.58it/s, est. speed input: 71120.32 toks/s, output: 69.45 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 62.58it/s, est. speed input: 70970.43 toks/s, output: 69.31 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 69.30it/s, est. speed input: 70970.43 toks/s, output: 69.31 toks/s]
[rank0]:[W125 15:49:59.617218434 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-25 15:50:02
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:50:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=147878) WARNING 01-25 15:50:27 [backends.py:609] Failed to read file <frozen os>
Throughput: 62.54 requests/s, 64105.01 total tokens/s, 62.54 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-25 15:50:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:50:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:50:11] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:11] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:50:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:11] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:11] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:50:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:11] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:11] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:11] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:50:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:11] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:50:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:50:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:50:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:50:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:50:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:50:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:50:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:50:18] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:18] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:50:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:18] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:18] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:50:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:18] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:18] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:18] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:50:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:18] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:50:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:50:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:50:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:50:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:50:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:50:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:50:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=147878) [2026-01-25 15:50:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=147878) [2026-01-25 15:50:20] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=147878) [2026-01-25 15:50:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=147878) [2026-01-25 15:50:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=147878) [2026-01-25 15:50:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=147878) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=147878) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=147878) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=147878) 
(EngineCore_DP0 pid=147878) [2026-01-25 15:50:33] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=147878) 2026-01-25 15:50:38,393 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=147878) 2026-01-25 15:50:38,421 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=147878) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:01,  2.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  6.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  5.79it/s]
(EngineCore_DP0 pid=147878) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 14.48it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 15.76it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 30/512 [00:00<00:01, 298.66it/s]
Adding requests:  16%|█▌        | 83/512 [00:00<00:00, 431.51it/s]
Adding requests:  26%|██▋       | 135/512 [00:00<00:00, 468.20it/s]
Adding requests:  36%|███▌      | 185/512 [00:00<00:00, 478.37it/s]
Adding requests:  46%|████▋     | 238/512 [00:00<00:00, 493.82it/s]
Adding requests:  56%|█████▋    | 289/512 [00:00<00:00, 496.98it/s]
Adding requests:  66%|██████▋   | 340/512 [00:00<00:00, 498.98it/s]
Adding requests:  77%|███████▋  | 393/512 [00:00<00:00, 505.68it/s]
Adding requests:  87%|████████▋ | 445/512 [00:00<00:00, 506.99it/s]
Adding requests:  97%|█████████▋| 497/512 [00:01<00:00, 508.03it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 490.15it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:00<00:01, 292.38it/s, est. speed input: 299443.45 toks/s, output: 292.40 toks/s]
Processed prompts:  14%|█▍        | 72/512 [00:00<00:03, 114.72it/s, est. speed input: 131456.35 toks/s, output: 128.37 toks/s]
Processed prompts:  17%|█▋        | 89/512 [00:00<00:04, 97.20it/s, est. speed input: 113763.84 toks/s, output: 111.10 toks/s] 
Processed prompts:  20%|█▉        | 102/512 [00:01<00:05, 80.83it/s, est. speed input: 99729.25 toks/s, output: 97.39 toks/s] 
Processed prompts:  22%|██▏       | 112/512 [00:01<00:04, 81.29it/s, est. speed input: 98216.02 toks/s, output: 95.91 toks/s]
Processed prompts:  24%|██▎       | 121/512 [00:01<00:04, 79.98it/s, est. speed input: 96272.30 toks/s, output: 94.02 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:01<00:05, 70.31it/s, est. speed input: 90799.84 toks/s, output: 88.67 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:01<00:05, 69.50it/s, est. speed input: 89115.67 toks/s, output: 87.02 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:01<00:05, 68.92it/s, est. speed input: 87696.73 toks/s, output: 85.64 toks/s]
Processed prompts:  30%|███       | 154/512 [00:01<00:05, 68.20it/s, est. speed input: 86382.96 toks/s, output: 84.36 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:01<00:05, 67.47it/s, est. speed input: 85177.45 toks/s, output: 83.18 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:02<00:05, 66.96it/s, est. speed input: 84122.13 toks/s, output: 82.15 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:02<00:04, 66.82it/s, est. speed input: 83239.04 toks/s, output: 81.29 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:02<00:04, 66.90it/s, est. speed input: 82488.42 toks/s, output: 80.55 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:02<00:04, 66.98it/s, est. speed input: 81816.64 toks/s, output: 79.90 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:02<00:04, 66.98it/s, est. speed input: 81197.31 toks/s, output: 79.29 toks/s]
Processed prompts:  41%|████      | 210/512 [00:02<00:04, 67.06it/s, est. speed input: 80645.74 toks/s, output: 78.75 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:02<00:04, 66.85it/s, est. speed input: 80097.12 toks/s, output: 78.22 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:02<00:04, 66.55it/s, est. speed input: 79568.97 toks/s, output: 77.70 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:03<00:04, 66.33it/s, est. speed input: 79080.30 toks/s, output: 77.23 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:03<00:04, 66.19it/s, est. speed input: 78632.11 toks/s, output: 76.79 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:03<00:03, 66.34it/s, est. speed input: 78253.23 toks/s, output: 76.42 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:03<00:03, 66.56it/s, est. speed input: 77916.85 toks/s, output: 76.09 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:03<00:03, 66.80it/s, est. speed input: 77614.19 toks/s, output: 75.79 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:03<00:03, 66.84it/s, est. speed input: 77316.11 toks/s, output: 75.50 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:03<00:03, 66.74it/s, est. speed input: 77019.89 toks/s, output: 75.21 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:03<00:03, 66.60it/s, est. speed input: 76734.05 toks/s, output: 74.93 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:03<00:03, 66.39it/s, est. speed input: 76453.14 toks/s, output: 74.66 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:04<00:03, 66.25it/s, est. speed input: 76189.55 toks/s, output: 74.40 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:04<00:02, 66.31it/s, est. speed input: 75957.96 toks/s, output: 74.18 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:04<00:02, 66.51it/s, est. speed input: 75755.04 toks/s, output: 73.98 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:04<00:02, 66.66it/s, est. speed input: 75564.66 toks/s, output: 73.79 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:04<00:02, 66.78it/s, est. speed input: 75385.49 toks/s, output: 73.62 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:04<00:02, 68.88it/s, est. speed input: 75402.15 toks/s, output: 73.63 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:04<00:02, 68.15it/s, est. speed input: 75220.69 toks/s, output: 73.46 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:04<00:02, 67.50it/s, est. speed input: 75033.69 toks/s, output: 73.27 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:05<00:02, 66.96it/s, est. speed input: 74848.07 toks/s, output: 73.09 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:05<00:02, 66.64it/s, est. speed input: 74675.50 toks/s, output: 72.92 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:05<00:01, 66.59it/s, est. speed input: 74526.03 toks/s, output: 72.78 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:05<00:01, 66.64it/s, est. speed input: 74389.38 toks/s, output: 72.65 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:05<00:01, 66.61it/s, est. speed input: 74253.65 toks/s, output: 72.51 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:05<00:01, 66.57it/s, est. speed input: 74123.20 toks/s, output: 72.39 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:05<00:01, 66.58it/s, est. speed input: 73999.81 toks/s, output: 72.26 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:05<00:01, 66.54it/s, est. speed input: 73878.26 toks/s, output: 72.15 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:06<00:01, 66.43it/s, est. speed input: 73755.69 toks/s, output: 72.03 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:06<00:01, 66.34it/s, est. speed input: 73636.81 toks/s, output: 71.91 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:06<00:00, 68.46it/s, est. speed input: 73673.16 toks/s, output: 71.95 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:06<00:00, 68.00it/s, est. speed input: 73577.19 toks/s, output: 71.85 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:06<00:00, 67.72it/s, est. speed input: 73487.71 toks/s, output: 71.76 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:06<00:00, 67.53it/s, est. speed input: 73400.99 toks/s, output: 71.68 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:06<00:00, 67.27it/s, est. speed input: 73309.42 toks/s, output: 71.59 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:06<00:00, 67.04it/s, est. speed input: 73218.41 toks/s, output: 71.50 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:06<00:00, 66.91it/s, est. speed input: 73131.80 toks/s, output: 71.42 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:07<00:00, 66.75it/s, est. speed input: 73044.12 toks/s, output: 71.33 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 66.75it/s, est. speed input: 73429.43 toks/s, output: 71.71 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 71.71it/s, est. speed input: 73429.43 toks/s, output: 71.71 toks/s]
[rank0]:[W125 15:50:49.339986401 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-25 15:50:51
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:51:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=149123) WARNING 01-25 15:51:19 [backends.py:609] Failed to read file <frozen os>
Throughput: 69.44 requests/s, 71177.80 total tokens/s, 69.44 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-25 15:51:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:51:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:51:03] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:03] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:51:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:03] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:03] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:51:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:03] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:03] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:03] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:51:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:03] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:51:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:51:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:51:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:51:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:51:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:51:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:51:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:51:10] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:10] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:51:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:10] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:10] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:51:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:10] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:10] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:10] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:51:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:10] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:51:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:51:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:51:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:51:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:51:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:51:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:51:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=149123) [2026-01-25 15:51:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=149123) [2026-01-25 15:51:12] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=149123) [2026-01-25 15:51:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=149123) [2026-01-25 15:51:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=149123) [2026-01-25 15:51:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=149123) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=149123) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=149123) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=149123) 
(EngineCore_DP0 pid=149123) [2026-01-25 15:51:25] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=149123) 2026-01-25 15:51:30,818 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=149123) 2026-01-25 15:51:30,853 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=149123) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:03,  1.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:01<00:01,  1.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:01<00:00,  3.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  3.18it/s]
(EngineCore_DP0 pid=149123) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 13.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 11.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 11.47it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 29/1024 [00:00<00:03, 289.12it/s]
Adding requests:   8%|▊         | 82/1024 [00:00<00:02, 428.49it/s]
Adding requests:  13%|█▎        | 134/1024 [00:00<00:01, 467.37it/s]
Adding requests:  18%|█▊        | 184/1024 [00:00<00:01, 477.98it/s]
Adding requests:  23%|██▎       | 236/1024 [00:00<00:01, 492.35it/s]
Adding requests:  28%|██▊       | 287/1024 [00:00<00:01, 497.29it/s]
Adding requests:  33%|███▎      | 338/1024 [00:00<00:01, 498.66it/s]
Adding requests:  38%|███▊      | 391/1024 [00:00<00:01, 507.52it/s]
Adding requests:  43%|████▎     | 442/1024 [00:00<00:01, 506.80it/s]
Adding requests:  48%|████▊     | 494/1024 [00:01<00:01, 509.55it/s]
Adding requests:  53%|█████▎    | 545/1024 [00:01<00:00, 501.86it/s]
Adding requests:  58%|█████▊    | 598/1024 [00:01<00:00, 509.97it/s]
Adding requests:  63%|██████▎   | 650/1024 [00:01<00:00, 507.18it/s]
Adding requests:  69%|██████▉   | 705/1024 [00:01<00:00, 516.84it/s]
Adding requests:  74%|███████▍  | 757/1024 [00:01<00:00, 516.14it/s]
Adding requests:  79%|███████▉  | 809/1024 [00:01<00:00, 510.60it/s]
Adding requests:  84%|████████▍ | 861/1024 [00:01<00:00, 509.74it/s]
Adding requests:  89%|████████▉ | 914/1024 [00:01<00:00, 514.84it/s]
Adding requests:  94%|█████████▍| 967/1024 [00:01<00:00, 519.21it/s]
Adding requests: 100%|█████████▉| 1020/1024 [00:02<00:00, 519.96it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 502.61it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:00<00:00, 1206.18it/s, est. speed input: 1235474.89 toks/s, output: 1206.27 toks/s]
Processed prompts:  25%|██▌       | 259/1024 [00:01<00:06, 121.03it/s, est. speed input: 144753.71 toks/s, output: 141.36 toks/s]   
Processed prompts:  31%|███       | 313/1024 [00:02<00:06, 105.78it/s, est. speed input: 126981.77 toks/s, output: 124.01 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:03<00:07, 91.95it/s, est. speed input: 114723.67 toks/s, output: 112.03 toks/s] 
Processed prompts:  36%|███▌      | 369/1024 [00:03<00:07, 93.11it/s, est. speed input: 113864.06 toks/s, output: 111.19 toks/s]
Processed prompts:  38%|███▊      | 387/1024 [00:03<00:07, 83.43it/s, est. speed input: 108198.61 toks/s, output: 105.66 toks/s]
Processed prompts:  39%|███▉      | 401/1024 [00:03<00:07, 87.41it/s, est. speed input: 108727.25 toks/s, output: 106.18 toks/s]
Processed prompts:  41%|████      | 415/1024 [00:04<00:07, 81.41it/s, est. speed input: 106058.24 toks/s, output: 103.57 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:04<00:08, 73.20it/s, est. speed input: 102949.68 toks/s, output: 100.54 toks/s]
Processed prompts:  42%|████▏     | 435/1024 [00:04<00:07, 73.93it/s, est. speed input: 102342.92 toks/s, output: 99.94 toks/s] 
Processed prompts:  43%|████▎     | 444/1024 [00:04<00:07, 74.65it/s, est. speed input: 101762.88 toks/s, output: 99.38 toks/s]
Processed prompts:  44%|████▍     | 453/1024 [00:04<00:07, 76.86it/s, est. speed input: 101467.65 toks/s, output: 99.09 toks/s]
Processed prompts:  45%|████▌     | 462/1024 [00:04<00:07, 77.33it/s, est. speed input: 100966.04 toks/s, output: 98.60 toks/s]
Processed prompts:  46%|████▌     | 471/1024 [00:04<00:07, 77.77it/s, est. speed input: 100494.23 toks/s, output: 98.14 toks/s]
Processed prompts:  47%|████▋     | 480/1024 [00:04<00:06, 77.91it/s, est. speed input: 100019.19 toks/s, output: 97.67 toks/s]
Processed prompts:  48%|████▊     | 489/1024 [00:05<00:06, 77.86it/s, est. speed input: 99549.14 toks/s, output: 97.22 toks/s] 
Processed prompts:  49%|████▊     | 497/1024 [00:05<00:06, 75.57it/s, est. speed input: 98917.94 toks/s, output: 96.60 toks/s]
Processed prompts:  49%|████▉     | 505/1024 [00:05<00:07, 73.86it/s, est. speed input: 98312.28 toks/s, output: 96.01 toks/s]
Processed prompts:  50%|█████     | 513/1024 [00:05<00:07, 72.47it/s, est. speed input: 97718.48 toks/s, output: 95.43 toks/s]
Processed prompts:  51%|█████     | 521/1024 [00:05<00:07, 71.49it/s, est. speed input: 97150.61 toks/s, output: 94.87 toks/s]
Processed prompts:  52%|█████▏    | 529/1024 [00:05<00:06, 71.06it/s, est. speed input: 96631.36 toks/s, output: 94.37 toks/s]
Processed prompts:  52%|█████▏    | 537/1024 [00:05<00:06, 70.84it/s, est. speed input: 96141.47 toks/s, output: 93.89 toks/s]
Processed prompts:  53%|█████▎    | 545/1024 [00:05<00:06, 70.54it/s, est. speed input: 95657.40 toks/s, output: 93.42 toks/s]
Processed prompts:  54%|█████▍    | 553/1024 [00:05<00:06, 70.21it/s, est. speed input: 95181.78 toks/s, output: 92.95 toks/s]
Processed prompts:  55%|█████▍    | 561/1024 [00:06<00:06, 70.02it/s, est. speed input: 94727.85 toks/s, output: 92.51 toks/s]
Processed prompts:  56%|█████▌    | 569/1024 [00:06<00:06, 69.76it/s, est. speed input: 94280.21 toks/s, output: 92.07 toks/s]
Processed prompts:  56%|█████▋    | 576/1024 [00:06<00:06, 67.15it/s, est. speed input: 93700.92 toks/s, output: 91.50 toks/s]
Processed prompts:  57%|█████▋    | 583/1024 [00:06<00:06, 65.12it/s, est. speed input: 93124.99 toks/s, output: 90.94 toks/s]
Processed prompts:  58%|█████▊    | 590/1024 [00:06<00:06, 63.87it/s, est. speed input: 92584.40 toks/s, output: 90.41 toks/s]
Processed prompts:  58%|█████▊    | 597/1024 [00:06<00:06, 63.03it/s, est. speed input: 92065.63 toks/s, output: 89.91 toks/s]
Processed prompts:  59%|█████▉    | 604/1024 [00:06<00:06, 62.43it/s, est. speed input: 91563.67 toks/s, output: 89.42 toks/s]
Processed prompts:  60%|█████▉    | 611/1024 [00:06<00:06, 61.94it/s, est. speed input: 91072.50 toks/s, output: 88.94 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:06<00:06, 61.64it/s, est. speed input: 90601.06 toks/s, output: 88.48 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:07<00:06, 64.04it/s, est. speed input: 90288.78 toks/s, output: 88.17 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:07<00:05, 65.62it/s, est. speed input: 89979.44 toks/s, output: 87.87 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:07<00:05, 66.70it/s, est. speed input: 89677.97 toks/s, output: 87.58 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:07<00:05, 67.52it/s, est. speed input: 89390.57 toks/s, output: 87.30 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:07<00:05, 68.19it/s, est. speed input: 89118.52 toks/s, output: 87.03 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:07<00:05, 68.63it/s, est. speed input: 88851.93 toks/s, output: 86.77 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:07<00:05, 68.82it/s, est. speed input: 88586.18 toks/s, output: 86.51 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:07<00:04, 69.10it/s, est. speed input: 88337.61 toks/s, output: 86.27 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:08<00:04, 69.33it/s, est. speed input: 88097.57 toks/s, output: 86.03 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:08<00:04, 69.44it/s, est. speed input: 87861.46 toks/s, output: 85.80 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:08<00:04, 69.26it/s, est. speed input: 87617.20 toks/s, output: 85.56 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:08<00:04, 69.35it/s, est. speed input: 87391.76 toks/s, output: 85.34 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:08<00:04, 69.50it/s, est. speed input: 87177.33 toks/s, output: 85.13 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:08<00:04, 69.59it/s, est. speed input: 86967.87 toks/s, output: 84.93 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:08<00:04, 69.50it/s, est. speed input: 86755.85 toks/s, output: 84.72 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:08<00:03, 69.61it/s, est. speed input: 86558.17 toks/s, output: 84.53 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:08<00:03, 69.60it/s, est. speed input: 86361.38 toks/s, output: 84.34 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:09<00:03, 69.61it/s, est. speed input: 86170.45 toks/s, output: 84.15 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:09<00:03, 69.45it/s, est. speed input: 85975.60 toks/s, output: 83.96 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:09<00:03, 69.44it/s, est. speed input: 85790.56 toks/s, output: 83.78 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:09<00:03, 69.43it/s, est. speed input: 85610.25 toks/s, output: 83.60 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:09<00:03, 69.55it/s, est. speed input: 85440.55 toks/s, output: 83.44 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:09<00:03, 69.35it/s, est. speed input: 85260.62 toks/s, output: 83.26 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:09<00:03, 69.46it/s, est. speed input: 85097.26 toks/s, output: 83.10 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:09<00:02, 69.54it/s, est. speed input: 84938.61 toks/s, output: 82.95 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:09<00:02, 69.58it/s, est. speed input: 84781.81 toks/s, output: 82.79 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:10<00:02, 69.48it/s, est. speed input: 84622.76 toks/s, output: 82.64 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:10<00:02, 69.53it/s, est. speed input: 84473.27 toks/s, output: 82.49 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:10<00:02, 69.38it/s, est. speed input: 84318.23 toks/s, output: 82.34 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:10<00:02, 69.40it/s, est. speed input: 84172.67 toks/s, output: 82.20 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:10<00:02, 69.40it/s, est. speed input: 84029.62 toks/s, output: 82.06 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:10<00:02, 69.41it/s, est. speed input: 83889.99 toks/s, output: 81.92 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:10<00:02, 69.49it/s, est. speed input: 83756.44 toks/s, output: 81.79 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:10<00:01, 69.49it/s, est. speed input: 83623.43 toks/s, output: 81.66 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:11<00:01, 69.36it/s, est. speed input: 83487.61 toks/s, output: 81.53 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:11<00:01, 69.24it/s, est. speed input: 83353.56 toks/s, output: 81.40 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:11<00:01, 69.41it/s, est. speed input: 83232.58 toks/s, output: 81.28 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:11<00:01, 69.46it/s, est. speed input: 83111.10 toks/s, output: 81.16 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:11<00:01, 69.40it/s, est. speed input: 82988.55 toks/s, output: 81.04 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:11<00:01, 71.37it/s, est. speed input: 82945.78 toks/s, output: 81.00 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:11<00:01, 70.82it/s, est. speed input: 82830.62 toks/s, output: 80.89 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:11<00:00, 70.39it/s, est. speed input: 82715.88 toks/s, output: 80.78 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:11<00:00, 70.19it/s, est. speed input: 82607.00 toks/s, output: 80.67 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:12<00:00, 69.96it/s, est. speed input: 82496.66 toks/s, output: 80.56 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:12<00:00, 69.73it/s, est. speed input: 82386.01 toks/s, output: 80.45 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:12<00:00, 71.54it/s, est. speed input: 82348.55 toks/s, output: 80.42 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:12<00:00, 70.89it/s, est. speed input: 82243.58 toks/s, output: 80.32 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:12<00:00, 70.37it/s, est. speed input: 82138.15 toks/s, output: 80.21 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:12<00:00, 70.22it/s, est. speed input: 82041.99 toks/s, output: 80.12 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:12<00:00, 72.87it/s, est. speed input: 82040.32 toks/s, output: 80.12 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 72.87it/s, est. speed input: 82521.30 toks/s, output: 80.59 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 80.59it/s, est. speed input: 82521.30 toks/s, output: 80.59 toks/s]
[rank0]:[W125 15:51:49.402640532 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-25 15:51:51
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:52:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=150656) WARNING 01-25 15:52:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 71.66 requests/s, 73454.51 total tokens/s, 71.66 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-25 15:52:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:52:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:52:06] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:06] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:52:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:06] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:06] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:52:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:06] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:06] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:06] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:52:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:06] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:52:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:52:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:52:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:52:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:52:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:52:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:52:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:52:14] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:14] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:52:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:14] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:14] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:52:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:14] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:14] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:14] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:52:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:14] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:52:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:52:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:52:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:52:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:52:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:52:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:52:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=150656) [2026-01-25 15:52:15] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=150656) [2026-01-25 15:52:15] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=150656) [2026-01-25 15:52:15] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=150656) [2026-01-25 15:52:15] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=150656) [2026-01-25 15:52:15] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=150656) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=150656) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=150656) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=150656) 
(EngineCore_DP0 pid=150656) [2026-01-25 15:52:28] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=150656) 2026-01-25 15:52:33,440 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=150656) 2026-01-25 15:52:33,475 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=150656) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  6.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:01,  3.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:01<00:00,  3.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:01<00:00,  5.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  5.54it/s]
(EngineCore_DP0 pid=150656) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 17.38it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 18.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.20it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 32/2048 [00:00<00:06, 318.51it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 432.08it/s]
Adding requests:   6%|▋         | 133/2048 [00:00<00:04, 457.97it/s]
Adding requests:   9%|▉         | 182/2048 [00:00<00:03, 468.47it/s]
Adding requests:  11%|█▏        | 235/2048 [00:00<00:03, 488.56it/s]
Adding requests:  14%|█▍        | 287/2048 [00:00<00:03, 495.17it/s]
Adding requests:  17%|█▋        | 338/2048 [00:00<00:03, 498.22it/s]
Adding requests:  19%|█▉        | 391/2048 [00:00<00:03, 506.52it/s]
Adding requests:  22%|██▏       | 442/2048 [00:00<00:03, 507.46it/s]
Adding requests:  24%|██▍       | 494/2048 [00:01<00:03, 509.71it/s]
Adding requests:  27%|██▋       | 545/2048 [00:01<00:02, 502.51it/s]
Adding requests:  29%|██▉       | 598/2048 [00:01<00:02, 509.50it/s]
Adding requests:  32%|███▏      | 650/2048 [00:01<00:02, 512.64it/s]
Adding requests:  34%|███▍      | 704/2048 [00:01<00:02, 519.53it/s]
Adding requests:  37%|███▋      | 756/2048 [00:01<00:02, 515.50it/s]
Adding requests:  39%|███▉      | 808/2048 [00:01<00:02, 509.73it/s]
Adding requests:  42%|████▏     | 859/2048 [00:01<00:02, 506.95it/s]
Adding requests:  45%|████▍     | 912/2048 [00:01<00:02, 512.98it/s]
Adding requests:  47%|████▋     | 964/2048 [00:01<00:02, 504.95it/s]
Adding requests:  50%|████▉     | 1017/2048 [00:02<00:02, 510.17it/s]
Adding requests:  52%|█████▏    | 1069/2048 [00:02<00:01, 509.75it/s]
Adding requests:  55%|█████▍    | 1120/2048 [00:02<00:01, 505.97it/s]
Adding requests:  57%|█████▋    | 1174/2048 [00:02<00:01, 513.63it/s]
Adding requests:  60%|█████▉    | 1228/2048 [00:02<00:01, 519.83it/s]
Adding requests:  62%|██████▎   | 1280/2048 [00:02<00:01, 514.86it/s]
Adding requests:  65%|██████▌   | 1333/2048 [00:02<00:01, 518.46it/s]
Adding requests:  68%|██████▊   | 1386/2048 [00:02<00:01, 519.52it/s]
Adding requests:  70%|███████   | 1438/2048 [00:02<00:01, 518.90it/s]
Adding requests:  73%|███████▎  | 1492/2048 [00:02<00:01, 523.44it/s]
Adding requests:  75%|███████▌  | 1545/2048 [00:03<00:00, 524.19it/s]
Adding requests:  78%|███████▊  | 1599/2048 [00:03<00:00, 528.70it/s]
Adding requests:  81%|████████  | 1652/2048 [00:03<00:00, 527.00it/s]
Adding requests:  83%|████████▎ | 1705/2048 [00:03<00:00, 523.60it/s]
Adding requests:  86%|████████▌ | 1758/2048 [00:03<00:00, 522.26it/s]
Adding requests:  88%|████████▊ | 1811/2048 [00:03<00:00, 520.49it/s]
Adding requests:  91%|█████████ | 1864/2048 [00:03<00:00, 516.79it/s]
Adding requests:  94%|█████████▎| 1917/2048 [00:03<00:00, 519.51it/s]
Adding requests:  96%|█████████▌| 1969/2048 [00:03<00:00, 517.66it/s]
Adding requests:  99%|█████████▉| 2023/2048 [00:03<00:00, 521.86it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 510.12it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:00<00:00, 2421.15it/s, est. speed input: 2479773.27 toks/s, output: 2421.30 toks/s]
Processed prompts:  25%|██▌       | 517/2048 [00:03<00:11, 127.88it/s, est. speed input: 154170.97 toks/s, output: 150.56 toks/s]   
Processed prompts:  30%|███       | 621/2048 [00:04<00:12, 109.81it/s, est. speed input: 133236.82 toks/s, output: 130.11 toks/s]
Processed prompts:  33%|███▎      | 682/2048 [00:05<00:13, 99.20it/s, est. speed input: 123257.01 toks/s, output: 120.37 toks/s] 
Processed prompts:  35%|███▌      | 722/2048 [00:06<00:14, 90.75it/s, est. speed input: 116714.16 toks/s, output: 113.98 toks/s]
Processed prompts:  37%|███▋      | 750/2048 [00:06<00:13, 93.94it/s, est. speed input: 117097.34 toks/s, output: 114.35 toks/s]
Processed prompts:  38%|███▊      | 773/2048 [00:07<00:14, 85.06it/s, est. speed input: 113002.44 toks/s, output: 110.35 toks/s]
Processed prompts:  39%|███▊      | 790/2048 [00:07<00:14, 83.89it/s, est. speed input: 111916.74 toks/s, output: 109.29 toks/s]
Processed prompts:  39%|███▉      | 804/2048 [00:07<00:15, 80.56it/s, est. speed input: 110486.67 toks/s, output: 107.90 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:07<00:15, 77.40it/s, est. speed input: 109164.91 toks/s, output: 106.61 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:07<00:15, 76.19it/s, est. speed input: 108149.10 toks/s, output: 105.61 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:08<00:15, 75.10it/s, est. speed input: 107184.15 toks/s, output: 104.67 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:08<00:15, 74.23it/s, est. speed input: 106274.95 toks/s, output: 103.78 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:08<00:15, 73.54it/s, est. speed input: 105415.41 toks/s, output: 102.94 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:08<00:15, 73.09it/s, est. speed input: 104609.69 toks/s, output: 102.16 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:09<00:15, 72.77it/s, est. speed input: 103845.34 toks/s, output: 101.41 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:09<00:15, 73.61it/s, est. speed input: 103246.98 toks/s, output: 100.83 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:09<00:15, 72.96it/s, est. speed input: 102532.03 toks/s, output: 100.13 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:09<00:14, 72.66it/s, est. speed input: 101868.16 toks/s, output: 99.48 toks/s] 
Processed prompts:  48%|████▊     | 978/2048 [00:09<00:14, 73.67it/s, est. speed input: 101361.72 toks/s, output: 98.99 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:10<00:14, 73.03it/s, est. speed input: 100740.34 toks/s, output: 98.38 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:10<00:14, 72.58it/s, est. speed input: 100144.68 toks/s, output: 97.80 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:10<00:14, 72.32it/s, est. speed input: 99579.77 toks/s, output: 97.25 toks/s] 
Processed prompts:  51%|█████     | 1042/2048 [00:10<00:13, 72.24it/s, est. speed input: 99047.90 toks/s, output: 96.73 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:10<00:13, 72.10it/s, est. speed input: 98529.86 toks/s, output: 96.22 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:11<00:13, 71.88it/s, est. speed input: 98021.77 toks/s, output: 95.72 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:11<00:13, 71.70it/s, est. speed input: 97530.56 toks/s, output: 95.24 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:11<00:13, 71.61it/s, est. speed input: 97062.07 toks/s, output: 94.79 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:11<00:12, 71.69it/s, est. speed input: 96623.20 toks/s, output: 94.36 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:12<00:12, 71.75it/s, est. speed input: 96200.36 toks/s, output: 93.95 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:12<00:12, 72.86it/s, est. speed input: 95877.40 toks/s, output: 93.63 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:12<00:12, 72.40it/s, est. speed input: 95469.97 toks/s, output: 93.23 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:12<00:11, 72.12it/s, est. speed input: 95079.43 toks/s, output: 92.85 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:12<00:11, 72.06it/s, est. speed input: 94712.48 toks/s, output: 92.49 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:13<00:11, 71.88it/s, est. speed input: 94347.55 toks/s, output: 92.14 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:13<00:11, 71.69it/s, est. speed input: 93990.55 toks/s, output: 91.79 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:13<00:11, 71.57it/s, est. speed input: 93646.06 toks/s, output: 91.45 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:13<00:10, 71.70it/s, est. speed input: 93327.28 toks/s, output: 91.14 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:14<00:10, 71.76it/s, est. speed input: 93016.91 toks/s, output: 90.84 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:14<00:10, 71.80it/s, est. speed input: 92715.64 toks/s, output: 90.54 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:14<00:10, 71.73it/s, est. speed input: 92417.43 toks/s, output: 90.25 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:14<00:10, 71.70it/s, est. speed input: 92129.44 toks/s, output: 89.97 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:15<00:09, 71.76it/s, est. speed input: 91855.13 toks/s, output: 89.70 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:15<00:09, 71.78it/s, est. speed input: 91587.42 toks/s, output: 89.44 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:15<00:09, 71.81it/s, est. speed input: 91327.99 toks/s, output: 89.19 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:15<00:09, 71.79it/s, est. speed input: 91073.79 toks/s, output: 88.94 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:15<00:08, 71.78it/s, est. speed input: 90826.96 toks/s, output: 88.70 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:16<00:08, 71.76it/s, est. speed input: 90586.55 toks/s, output: 88.46 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:16<00:08, 71.68it/s, est. speed input: 90348.64 toks/s, output: 88.23 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:16<00:08, 71.67it/s, est. speed input: 90119.68 toks/s, output: 88.01 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:16<00:08, 71.72it/s, est. speed input: 89899.89 toks/s, output: 87.79 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:17<00:07, 71.80it/s, est. speed input: 89688.54 toks/s, output: 87.59 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:17<00:07, 71.82it/s, est. speed input: 89480.34 toks/s, output: 87.38 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:17<00:07, 71.71it/s, est. speed input: 89271.45 toks/s, output: 87.18 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:17<00:07, 71.72it/s, est. speed input: 89072.09 toks/s, output: 86.98 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:17<00:06, 71.74it/s, est. speed input: 88878.01 toks/s, output: 86.79 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:18<00:06, 71.79it/s, est. speed input: 88691.21 toks/s, output: 86.61 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:18<00:06, 71.76it/s, est. speed input: 88505.60 toks/s, output: 86.43 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:18<00:06, 71.74it/s, est. speed input: 88323.78 toks/s, output: 86.25 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:18<00:05, 71.85it/s, est. speed input: 88152.86 toks/s, output: 86.09 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:19<00:05, 71.75it/s, est. speed input: 87977.26 toks/s, output: 85.92 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:19<00:05, 71.69it/s, est. speed input: 87806.36 toks/s, output: 85.75 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:19<00:05, 71.67it/s, est. speed input: 87640.37 toks/s, output: 85.59 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:19<00:05, 71.67it/s, est. speed input: 87478.60 toks/s, output: 85.43 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:19<00:04, 71.78it/s, est. speed input: 87325.82 toks/s, output: 85.28 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:20<00:04, 71.76it/s, est. speed input: 87171.70 toks/s, output: 85.13 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:20<00:04, 71.71it/s, est. speed input: 87019.59 toks/s, output: 84.98 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:20<00:04, 71.70it/s, est. speed input: 86871.67 toks/s, output: 84.84 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:20<00:03, 71.69it/s, est. speed input: 86727.24 toks/s, output: 84.69 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:21<00:03, 71.73it/s, est. speed input: 86587.36 toks/s, output: 84.56 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:21<00:03, 71.68it/s, est. speed input: 86447.60 toks/s, output: 84.42 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:21<00:03, 71.72it/s, est. speed input: 86313.34 toks/s, output: 84.29 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:21<00:03, 71.72it/s, est. speed input: 86181.04 toks/s, output: 84.16 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:21<00:02, 71.70it/s, est. speed input: 86050.53 toks/s, output: 84.03 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:22<00:02, 71.65it/s, est. speed input: 85921.48 toks/s, output: 83.91 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:22<00:02, 72.79it/s, est. speed input: 85840.82 toks/s, output: 83.83 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:22<00:02, 72.47it/s, est. speed input: 85718.41 toks/s, output: 83.71 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:22<00:01, 72.22it/s, est. speed input: 85597.39 toks/s, output: 83.59 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:23<00:01, 72.03it/s, est. speed input: 85478.02 toks/s, output: 83.47 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:23<00:01, 71.88it/s, est. speed input: 85360.25 toks/s, output: 83.36 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:23<00:01, 73.08it/s, est. speed input: 85293.12 toks/s, output: 83.29 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:23<00:01, 72.61it/s, est. speed input: 85179.59 toks/s, output: 83.18 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:23<00:00, 72.32it/s, est. speed input: 85069.19 toks/s, output: 83.08 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:24<00:00, 72.07it/s, est. speed input: 84959.24 toks/s, output: 82.97 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:24<00:00, 71.93it/s, est. speed input: 84852.38 toks/s, output: 82.86 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:24<00:00, 73.36it/s, est. speed input: 84800.92 toks/s, output: 82.81 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:24<00:00, 73.36it/s, est. speed input: 85383.67 toks/s, output: 83.38 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:24<00:00, 83.38it/s, est. speed input: 85383.67 toks/s, output: 83.38 toks/s]
[rank0]:[W125 15:53:05.681615448 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-25 15:53:08
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:53:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=152507) WARNING 01-25 15:53:48 [backends.py:609] Failed to read file <frozen os>
Throughput: 73.08 requests/s, 74907.21 total tokens/s, 73.08 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-25 15:53:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:53:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:53:32] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:32] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:53:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:32] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:32] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:53:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:32] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:32] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:32] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:53:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:32] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:53:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:53:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:53:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:53:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:53:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:53:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:53:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:53:39] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:39] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:53:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:39] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:39] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:53:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:39] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:39] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:39] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:53:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:39] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:53:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:53:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:53:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:53:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:53:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:53:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:53:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=152507) [2026-01-25 15:53:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=152507) [2026-01-25 15:53:41] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=152507) [2026-01-25 15:53:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=152507) [2026-01-25 15:53:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=152507) [2026-01-25 15:53:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=152507) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=152507) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.58it/s]
(EngineCore_DP0 pid=152507) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.57it/s]
(EngineCore_DP0 pid=152507) 
(EngineCore_DP0 pid=152507) [rank0]:W0125 15:53:53.740000 152507 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=152507) [rank0]:W0125 15:53:54.958000 152507 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=152507) [2026-01-25 15:53:55] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=152507) [rank0]:W0125 15:53:56.519000 152507 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=152507) [rank0]:W0125 15:53:56.645000 152507 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=152507) 2026-01-25 15:54:02,038 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=152507) 2026-01-25 15:54:02,155 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=152507) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  5.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00, 10.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:01,  4.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:01<00:01,  4.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:01<00:00,  5.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  7.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  7.21it/s]
(EngineCore_DP0 pid=152507) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.64it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.49it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 18.76it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 18.65it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 34/4096 [00:00<00:12, 336.08it/s]
Adding requests:   2%|▏         | 86/4096 [00:00<00:09, 440.06it/s]
Adding requests:   3%|▎         | 137/4096 [00:00<00:08, 469.94it/s]
Adding requests:   5%|▍         | 187/4096 [00:00<00:08, 481.00it/s]
Adding requests:   6%|▌         | 239/4096 [00:00<00:07, 491.78it/s]
Adding requests:   7%|▋         | 290/4096 [00:00<00:07, 496.18it/s]
Adding requests:   8%|▊         | 340/4096 [00:00<00:07, 497.36it/s]
Adding requests:  10%|▉         | 393/4096 [00:00<00:07, 505.14it/s]
Adding requests:  11%|█         | 444/4096 [00:00<00:07, 505.01it/s]
Adding requests:  12%|█▏        | 495/4096 [00:01<00:07, 506.45it/s]
Adding requests:  13%|█▎        | 546/4096 [00:01<00:07, 497.81it/s]
Adding requests:  15%|█▍        | 599/4096 [00:01<00:06, 505.20it/s]
Adding requests:  16%|█▌        | 651/4096 [00:01<00:06, 509.52it/s]
Adding requests:  17%|█▋        | 705/4096 [00:01<00:06, 516.15it/s]
Adding requests:  18%|█▊        | 757/4096 [00:01<00:06, 515.53it/s]
Adding requests:  20%|█▉        | 809/4096 [00:01<00:06, 508.14it/s]
Adding requests:  21%|██        | 860/4096 [00:01<00:06, 506.98it/s]
Adding requests:  22%|██▏       | 913/4096 [00:01<00:06, 512.38it/s]
Adding requests:  24%|██▎       | 966/4096 [00:01<00:06, 515.66it/s]
Adding requests:  25%|██▍       | 1019/4096 [00:02<00:05, 517.64it/s]
Adding requests:  26%|██▌       | 1071/4096 [00:02<00:05, 514.20it/s]
Adding requests:  27%|██▋       | 1123/4096 [00:02<00:05, 513.51it/s]
Adding requests:  29%|██▊       | 1177/4096 [00:02<00:05, 519.45it/s]
Adding requests:  30%|███       | 1229/4096 [00:02<00:05, 510.01it/s]
Adding requests:  31%|███▏      | 1281/4096 [00:02<00:05, 507.70it/s]
Adding requests:  33%|███▎      | 1335/4096 [00:02<00:05, 515.74it/s]
Adding requests:  34%|███▍      | 1387/4096 [00:02<00:05, 516.35it/s]
Adding requests:  35%|███▌      | 1440/4096 [00:02<00:05, 517.46it/s]
Adding requests:  36%|███▋      | 1494/4096 [00:02<00:04, 521.29it/s]
Adding requests:  38%|███▊      | 1547/4096 [00:03<00:04, 523.01it/s]
Adding requests:  39%|███▉      | 1601/4096 [00:03<00:04, 527.78it/s]
Adding requests:  40%|████      | 1654/4096 [00:03<00:04, 524.42it/s]
Adding requests:  42%|████▏     | 1707/4096 [00:03<00:04, 520.91it/s]
Adding requests:  43%|████▎     | 1760/4096 [00:03<00:04, 520.86it/s]
Adding requests:  44%|████▍     | 1813/4096 [00:03<00:04, 521.22it/s]
Adding requests:  46%|████▌     | 1866/4096 [00:03<00:04, 517.34it/s]
Adding requests:  47%|████▋     | 1919/4096 [00:03<00:04, 519.02it/s]
Adding requests:  48%|████▊     | 1971/4096 [00:03<00:04, 516.66it/s]
Adding requests:  49%|████▉     | 2024/4096 [00:03<00:03, 520.13it/s]
Adding requests:  51%|█████     | 2077/4096 [00:04<00:03, 521.88it/s]
Adding requests:  52%|█████▏    | 2130/4096 [00:04<00:03, 516.68it/s]
Adding requests:  53%|█████▎    | 2182/4096 [00:04<00:03, 509.46it/s]
Adding requests:  55%|█████▍    | 2236/4096 [00:04<00:03, 515.95it/s]
Adding requests:  56%|█████▌    | 2288/4096 [00:04<00:03, 515.08it/s]
Adding requests:  57%|█████▋    | 2340/4096 [00:04<00:03, 515.60it/s]
Adding requests:  58%|█████▊    | 2393/4096 [00:04<00:03, 517.88it/s]
Adding requests:  60%|█████▉    | 2445/4096 [00:04<00:03, 504.07it/s]
Adding requests:  61%|██████    | 2498/4096 [00:04<00:03, 509.49it/s]
Adding requests:  62%|██████▏   | 2550/4096 [00:04<00:03, 509.25it/s]
Adding requests:  64%|██████▎   | 2602/4096 [00:05<00:02, 510.92it/s]
Adding requests:  65%|██████▍   | 2655/4096 [00:05<00:02, 514.99it/s]
Adding requests:  66%|██████▌   | 2707/4096 [00:05<00:02, 511.12it/s]
Adding requests:  67%|██████▋   | 2759/4096 [00:05<00:02, 511.23it/s]
Adding requests:  69%|██████▊   | 2811/4096 [00:05<00:02, 509.39it/s]
Adding requests:  70%|██████▉   | 2863/4096 [00:05<00:02, 509.78it/s]
Adding requests:  71%|███████   | 2915/4096 [00:05<00:02, 512.29it/s]
Adding requests:  72%|███████▏  | 2967/4096 [00:05<00:02, 507.91it/s]
Adding requests:  74%|███████▎  | 3019/4096 [00:05<00:02, 510.31it/s]
Adding requests:  75%|███████▍  | 3071/4096 [00:06<00:02, 511.08it/s]
Adding requests:  76%|███████▌  | 3123/4096 [00:06<00:01, 512.88it/s]
Adding requests:  78%|███████▊  | 3175/4096 [00:06<00:01, 512.49it/s]
Adding requests:  79%|███████▉  | 3227/4096 [00:06<00:01, 511.54it/s]
Adding requests:  80%|████████  | 3280/4096 [00:06<00:01, 515.01it/s]
Adding requests:  81%|████████▏ | 3332/4096 [00:06<00:01, 512.72it/s]
Adding requests:  83%|████████▎ | 3385/4096 [00:06<00:01, 515.19it/s]
Adding requests:  84%|████████▍ | 3437/4096 [00:06<00:01, 515.88it/s]
Adding requests:  85%|████████▌ | 3489/4096 [00:06<00:01, 505.95it/s]
Adding requests:  86%|████████▋ | 3540/4096 [00:06<00:01, 505.28it/s]
Adding requests:  88%|████████▊ | 3591/4096 [00:07<00:00, 506.42it/s]
Adding requests:  89%|████████▉ | 3642/4096 [00:07<00:00, 502.36it/s]
Adding requests:  90%|█████████ | 3694/4096 [00:07<00:00, 507.01it/s]
Adding requests:  91%|█████████▏| 3745/4096 [00:07<00:00, 505.72it/s]
Adding requests:  93%|█████████▎| 3796/4096 [00:07<00:00, 499.78it/s]
Adding requests:  94%|█████████▍| 3848/4096 [00:07<00:00, 505.21it/s]
Adding requests:  95%|█████████▌| 3900/4096 [00:07<00:00, 508.23it/s]
Adding requests:  96%|█████████▋| 3952/4096 [00:07<00:00, 510.06it/s]
Adding requests:  98%|█████████▊| 4004/4096 [00:07<00:00, 509.77it/s]
Adding requests:  99%|█████████▉| 4055/4096 [00:07<00:00, 507.18it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 509.94it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 566/4096 [00:00<00:01, 3036.90it/s, est. speed input: 3110164.84 toks/s, output: 3037.01 toks/s]
Processed prompts:  21%|██        | 870/4096 [00:04<00:18, 172.01it/s, est. speed input: 215885.29 toks/s, output: 210.82 toks/s]   
Processed prompts:  24%|██▍       | 1000/4096 [00:05<00:22, 135.40it/s, est. speed input: 174824.12 toks/s, output: 170.73 toks/s]
Processed prompts:  26%|██▌       | 1074/4096 [00:06<00:24, 124.54it/s, est. speed input: 163386.39 toks/s, output: 159.56 toks/s]
Processed prompts:  27%|██▋       | 1123/4096 [00:07<00:27, 108.53it/s, est. speed input: 151233.89 toks/s, output: 147.69 toks/s]
Processed prompts:  28%|██▊       | 1156/4096 [00:08<00:28, 104.18it/s, est. speed input: 147426.05 toks/s, output: 143.97 toks/s]
Processed prompts:  29%|██▉       | 1181/4096 [00:08<00:30, 96.09it/s, est. speed input: 142803.29 toks/s, output: 139.46 toks/s] 
Processed prompts:  29%|██▉       | 1206/4096 [00:08<00:32, 88.48it/s, est. speed input: 138673.91 toks/s, output: 135.42 toks/s]
Processed prompts:  30%|███       | 1238/4096 [00:09<00:33, 85.05it/s, est. speed input: 135661.11 toks/s, output: 132.48 toks/s]
Processed prompts:  31%|███       | 1270/4096 [00:09<00:34, 82.21it/s, est. speed input: 132942.16 toks/s, output: 129.83 toks/s]
Processed prompts:  32%|███▏      | 1302/4096 [00:10<00:34, 79.97it/s, est. speed input: 130474.34 toks/s, output: 127.42 toks/s]
Processed prompts:  33%|███▎      | 1334/4096 [00:10<00:35, 78.23it/s, est. speed input: 128210.79 toks/s, output: 125.21 toks/s]
Processed prompts:  33%|███▎      | 1366/4096 [00:11<00:35, 76.91it/s, est. speed input: 126127.14 toks/s, output: 123.17 toks/s]
Processed prompts:  34%|███▍      | 1398/4096 [00:11<00:35, 75.83it/s, est. speed input: 124180.61 toks/s, output: 121.27 toks/s]
Processed prompts:  35%|███▍      | 1430/4096 [00:11<00:35, 75.08it/s, est. speed input: 122383.20 toks/s, output: 119.51 toks/s]
Processed prompts:  36%|███▌      | 1462/4096 [00:12<00:35, 74.49it/s, est. speed input: 120703.92 toks/s, output: 117.87 toks/s]
Processed prompts:  36%|███▋      | 1494/4096 [00:12<00:35, 74.13it/s, est. speed input: 119148.84 toks/s, output: 116.36 toks/s]
Processed prompts:  37%|███▋      | 1526/4096 [00:13<00:34, 73.80it/s, est. speed input: 117683.38 toks/s, output: 114.92 toks/s]
Processed prompts:  38%|███▊      | 1558/4096 [00:13<00:34, 73.66it/s, est. speed input: 116329.28 toks/s, output: 113.60 toks/s]
Processed prompts:  39%|███▉      | 1590/4096 [00:14<00:34, 73.51it/s, est. speed input: 115049.83 toks/s, output: 112.35 toks/s]
Processed prompts:  40%|███▉      | 1622/4096 [00:14<00:33, 73.41it/s, est. speed input: 113845.72 toks/s, output: 111.18 toks/s]
Processed prompts:  40%|████      | 1654/4096 [00:15<00:33, 73.36it/s, est. speed input: 112717.37 toks/s, output: 110.08 toks/s]
Processed prompts:  41%|████      | 1686/4096 [00:15<00:32, 73.26it/s, est. speed input: 111641.54 toks/s, output: 109.02 toks/s]
Processed prompts:  42%|████▏     | 1718/4096 [00:15<00:32, 73.17it/s, est. speed input: 110623.98 toks/s, output: 108.03 toks/s]
Processed prompts:  43%|████▎     | 1750/4096 [00:16<00:32, 73.14it/s, est. speed input: 109664.48 toks/s, output: 107.09 toks/s]
Processed prompts:  44%|████▎     | 1782/4096 [00:16<00:31, 73.04it/s, est. speed input: 108744.00 toks/s, output: 106.20 toks/s]
Processed prompts:  44%|████▍     | 1814/4096 [00:17<00:31, 73.08it/s, est. speed input: 107885.74 toks/s, output: 105.36 toks/s]
Processed prompts:  45%|████▌     | 1846/4096 [00:17<00:30, 73.06it/s, est. speed input: 107063.31 toks/s, output: 104.55 toks/s]
Processed prompts:  46%|████▌     | 1878/4096 [00:18<00:30, 73.70it/s, est. speed input: 106357.13 toks/s, output: 103.86 toks/s]
Processed prompts:  47%|████▋     | 1910/4096 [00:18<00:29, 73.50it/s, est. speed input: 105610.12 toks/s, output: 103.13 toks/s]
Processed prompts:  47%|████▋     | 1942/4096 [00:18<00:29, 73.95it/s, est. speed input: 104962.57 toks/s, output: 102.50 toks/s]
Processed prompts:  48%|████▊     | 1974/4096 [00:19<00:28, 73.64it/s, est. speed input: 104277.16 toks/s, output: 101.83 toks/s]
Processed prompts:  49%|████▉     | 2006/4096 [00:19<00:28, 73.35it/s, est. speed input: 103614.11 toks/s, output: 101.19 toks/s]
Processed prompts:  50%|████▉     | 2038/4096 [00:20<00:28, 73.18it/s, est. speed input: 102982.68 toks/s, output: 100.57 toks/s]
Processed prompts:  51%|█████     | 2070/4096 [00:20<00:27, 73.00it/s, est. speed input: 102372.64 toks/s, output: 99.97 toks/s] 
Processed prompts:  51%|█████▏    | 2102/4096 [00:21<00:27, 72.96it/s, est. speed input: 101796.41 toks/s, output: 99.41 toks/s]
Processed prompts:  52%|█████▏    | 2134/4096 [00:21<00:26, 72.88it/s, est. speed input: 101238.22 toks/s, output: 98.87 toks/s]
Processed prompts:  53%|█████▎    | 2166/4096 [00:22<00:26, 72.90it/s, est. speed input: 100709.89 toks/s, output: 98.35 toks/s]
Processed prompts:  54%|█████▎    | 2198/4096 [00:22<00:26, 72.88it/s, est. speed input: 100199.05 toks/s, output: 97.85 toks/s]
Processed prompts:  54%|█████▍    | 2230/4096 [00:22<00:25, 74.19it/s, est. speed input: 99821.18 toks/s, output: 97.48 toks/s] 
Processed prompts:  55%|█████▌    | 2262/4096 [00:23<00:24, 73.84it/s, est. speed input: 99350.96 toks/s, output: 97.02 toks/s]
Processed prompts:  56%|█████▌    | 2294/4096 [00:23<00:24, 74.20it/s, est. speed input: 98946.93 toks/s, output: 96.63 toks/s]
Processed prompts:  57%|█████▋    | 2326/4096 [00:24<00:23, 74.47it/s, est. speed input: 98558.45 toks/s, output: 96.25 toks/s]
Processed prompts:  58%|█████▊    | 2358/4096 [00:24<00:23, 73.92it/s, est. speed input: 98125.86 toks/s, output: 95.83 toks/s]
Processed prompts:  58%|█████▊    | 2390/4096 [00:25<00:23, 73.61it/s, est. speed input: 97714.29 toks/s, output: 95.42 toks/s]
Processed prompts:  59%|█████▉    | 2422/4096 [00:25<00:22, 73.36it/s, est. speed input: 97314.77 toks/s, output: 95.03 toks/s]
Processed prompts:  60%|█████▉    | 2454/4096 [00:25<00:22, 73.25it/s, est. speed input: 96932.91 toks/s, output: 94.66 toks/s]
Processed prompts:  61%|██████    | 2486/4096 [00:26<00:21, 73.70it/s, est. speed input: 96602.34 toks/s, output: 94.34 toks/s]
Processed prompts:  61%|██████▏   | 2518/4096 [00:26<00:21, 73.52it/s, est. speed input: 96246.95 toks/s, output: 93.99 toks/s]
Processed prompts:  62%|██████▏   | 2550/4096 [00:27<00:21, 73.29it/s, est. speed input: 95895.42 toks/s, output: 93.65 toks/s]
Processed prompts:  63%|██████▎   | 2582/4096 [00:27<00:20, 73.72it/s, est. speed input: 95595.93 toks/s, output: 93.36 toks/s]
Processed prompts:  64%|██████▍   | 2614/4096 [00:28<00:20, 73.53it/s, est. speed input: 95272.80 toks/s, output: 93.04 toks/s]
Processed prompts:  65%|██████▍   | 2646/4096 [00:28<00:19, 73.38it/s, est. speed input: 94957.86 toks/s, output: 92.73 toks/s]
Processed prompts:  65%|██████▌   | 2678/4096 [00:28<00:19, 73.29it/s, est. speed input: 94653.76 toks/s, output: 92.44 toks/s]
Processed prompts:  66%|██████▌   | 2710/4096 [00:29<00:18, 73.15it/s, est. speed input: 94353.77 toks/s, output: 92.14 toks/s]
Processed prompts:  67%|██████▋   | 2742/4096 [00:29<00:18, 73.11it/s, est. speed input: 94066.50 toks/s, output: 91.86 toks/s]
Processed prompts:  68%|██████▊   | 2774/4096 [00:30<00:18, 73.02it/s, est. speed input: 93783.50 toks/s, output: 91.59 toks/s]
Processed prompts:  69%|██████▊   | 2806/4096 [00:30<00:17, 72.96it/s, est. speed input: 93508.32 toks/s, output: 91.32 toks/s]
Processed prompts:  69%|██████▉   | 2838/4096 [00:31<00:17, 72.94it/s, est. speed input: 93242.97 toks/s, output: 91.06 toks/s]
Processed prompts:  70%|███████   | 2870/4096 [00:31<00:16, 72.83it/s, est. speed input: 92978.63 toks/s, output: 90.80 toks/s]
Processed prompts:  71%|███████   | 2902/4096 [00:32<00:16, 72.90it/s, est. speed input: 92730.81 toks/s, output: 90.56 toks/s]
Processed prompts:  72%|███████▏  | 2934/4096 [00:32<00:15, 72.81it/s, est. speed input: 92481.36 toks/s, output: 90.31 toks/s]
Processed prompts:  72%|███████▏  | 2966/4096 [00:32<00:15, 72.89it/s, est. speed input: 92246.34 toks/s, output: 90.08 toks/s]
Processed prompts:  73%|███████▎  | 2998/4096 [00:33<00:15, 72.82it/s, est. speed input: 92010.81 toks/s, output: 89.85 toks/s]
Processed prompts:  74%|███████▍  | 3030/4096 [00:33<00:14, 72.80it/s, est. speed input: 91783.27 toks/s, output: 89.63 toks/s]
Processed prompts:  75%|███████▍  | 3062/4096 [00:34<00:14, 72.71it/s, est. speed input: 91556.85 toks/s, output: 89.41 toks/s]
Processed prompts:  76%|███████▌  | 3094/4096 [00:34<00:13, 72.73it/s, est. speed input: 91341.23 toks/s, output: 89.20 toks/s]
Processed prompts:  76%|███████▋  | 3126/4096 [00:35<00:13, 73.35it/s, est. speed input: 91161.77 toks/s, output: 89.03 toks/s]
Processed prompts:  77%|███████▋  | 3158/4096 [00:35<00:12, 73.18it/s, est. speed input: 90956.33 toks/s, output: 88.82 toks/s]
Processed prompts:  78%|███████▊  | 3190/4096 [00:35<00:12, 73.10it/s, est. speed input: 90757.74 toks/s, output: 88.63 toks/s]
Processed prompts:  79%|███████▊  | 3222/4096 [00:36<00:11, 72.94it/s, est. speed input: 90558.60 toks/s, output: 88.44 toks/s]
Processed prompts:  79%|███████▉  | 3254/4096 [00:36<00:11, 72.92it/s, est. speed input: 90368.82 toks/s, output: 88.25 toks/s]
Processed prompts:  80%|████████  | 3286/4096 [00:37<00:11, 72.87it/s, est. speed input: 90181.82 toks/s, output: 88.07 toks/s]
Processed prompts:  81%|████████  | 3318/4096 [00:37<00:10, 72.82it/s, est. speed input: 89998.53 toks/s, output: 87.89 toks/s]
Processed prompts:  82%|████████▏ | 3350/4096 [00:38<00:10, 72.78it/s, est. speed input: 89819.01 toks/s, output: 87.71 toks/s]
Processed prompts:  83%|████████▎ | 3382/4096 [00:38<00:09, 72.81it/s, est. speed input: 89646.16 toks/s, output: 87.54 toks/s]
Processed prompts:  83%|████████▎ | 3414/4096 [00:39<00:09, 72.72it/s, est. speed input: 89472.34 toks/s, output: 87.38 toks/s]
Processed prompts:  84%|████████▍ | 3446/4096 [00:39<00:08, 72.75it/s, est. speed input: 89306.68 toks/s, output: 87.21 toks/s]
Processed prompts:  85%|████████▍ | 3478/4096 [00:39<00:08, 72.71it/s, est. speed input: 89142.01 toks/s, output: 87.05 toks/s]
Processed prompts:  86%|████████▌ | 3510/4096 [00:40<00:08, 72.76it/s, est. speed input: 88984.06 toks/s, output: 86.90 toks/s]
Processed prompts:  86%|████████▋ | 3542/4096 [00:40<00:07, 72.75it/s, est. speed input: 88827.43 toks/s, output: 86.75 toks/s]
Processed prompts:  87%|████████▋ | 3574/4096 [00:41<00:07, 72.69it/s, est. speed input: 88672.21 toks/s, output: 86.59 toks/s]
Processed prompts:  88%|████████▊ | 3606/4096 [00:41<00:06, 72.68it/s, est. speed input: 88521.61 toks/s, output: 86.45 toks/s]
Processed prompts:  89%|████████▉ | 3638/4096 [00:42<00:06, 72.72it/s, est. speed input: 88376.16 toks/s, output: 86.30 toks/s]
Processed prompts:  90%|████████▉ | 3670/4096 [00:42<00:05, 72.74it/s, est. speed input: 88233.08 toks/s, output: 86.17 toks/s]
Processed prompts:  90%|█████████ | 3702/4096 [00:43<00:05, 72.72it/s, est. speed input: 88091.60 toks/s, output: 86.03 toks/s]
Processed prompts:  91%|█████████ | 3734/4096 [00:43<00:04, 73.29it/s, est. speed input: 87976.81 toks/s, output: 85.91 toks/s]
Processed prompts:  92%|█████████▏| 3766/4096 [00:43<00:04, 73.08it/s, est. speed input: 87839.73 toks/s, output: 85.78 toks/s]
Processed prompts:  93%|█████████▎| 3798/4096 [00:44<00:04, 72.92it/s, est. speed input: 87705.28 toks/s, output: 85.65 toks/s]
Processed prompts:  94%|█████████▎| 3830/4096 [00:44<00:03, 72.83it/s, est. speed input: 87573.70 toks/s, output: 85.52 toks/s]
Processed prompts:  94%|█████████▍| 3862/4096 [00:45<00:03, 72.76it/s, est. speed input: 87444.64 toks/s, output: 85.40 toks/s]
Processed prompts:  95%|█████████▌| 3894/4096 [00:45<00:02, 72.64it/s, est. speed input: 87315.63 toks/s, output: 85.27 toks/s]
Processed prompts:  96%|█████████▌| 3926/4096 [00:46<00:02, 72.62it/s, est. speed input: 87191.07 toks/s, output: 85.15 toks/s]
Processed prompts:  97%|█████████▋| 3958/4096 [00:46<00:01, 72.63it/s, est. speed input: 87070.05 toks/s, output: 85.03 toks/s]
Processed prompts:  97%|█████████▋| 3990/4096 [00:46<00:01, 72.64it/s, est. speed input: 86951.36 toks/s, output: 84.91 toks/s]
Processed prompts:  98%|█████████▊| 4022/4096 [00:47<00:01, 73.25it/s, est. speed input: 86856.98 toks/s, output: 84.82 toks/s]
Processed prompts:  99%|█████████▉| 4054/4096 [00:47<00:00, 73.17it/s, est. speed input: 86745.81 toks/s, output: 84.71 toks/s]
Processed prompts: 100%|█████████▉| 4086/4096 [00:48<00:00, 90.70it/s, est. speed input: 87147.26 toks/s, output: 85.10 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:48<00:00, 90.70it/s, est. speed input: 87359.54 toks/s, output: 85.31 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:48<00:00, 85.31it/s, est. speed input: 87359.54 toks/s, output: 85.31 toks/s]
[rank0]:[W125 15:55:02.049774221 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-25 15:55:04
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:55:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=155178) WARNING 01-25 15:56:00 [backends.py:609] Failed to read file <frozen os>
Throughput: 73.09 requests/s, 74915.41 total tokens/s, 73.09 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-25 15:55:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:55:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:55:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:44] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:55:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:44] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:55:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:44] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:55:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:55:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:55:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:55:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:55:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:55:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:55:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:55:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:55:51] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:51] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:55:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:51] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:51] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:55:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:51] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:51] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:51] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:55:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:51] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:55:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:55:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:55:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:55:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:55:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:55:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:55:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=155178) [2026-01-25 15:55:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=155178) [2026-01-25 15:55:53] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=155178) [2026-01-25 15:55:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=155178) [2026-01-25 15:55:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=155178) [2026-01-25 15:55:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=155178) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=155178) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.57it/s]
(EngineCore_DP0 pid=155178) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.57it/s]
(EngineCore_DP0 pid=155178) 
(EngineCore_DP0 pid=155178) [rank0]:W0125 15:56:06.097000 155178 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=155178) [rank0]:W0125 15:56:06.400000 155178 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=155178) [2026-01-25 15:56:06] WARNING gemm_wrapper.py:335: No cuBLASLt config for model 'Llama3.2-3B-INT8', using default algorithm
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=155178) [rank0]:W0125 15:56:07.539000 155178 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=155178) [rank0]:W0125 15:56:07.665000 155178 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=155178) 2026-01-25 15:56:12,038 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=155178) 2026-01-25 15:56:12,074 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=155178) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:15,  1.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:04,  3.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:02,  6.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:00, 10.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  9.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  9.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  5.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  5.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  7.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  6.74it/s]
(EngineCore_DP0 pid=155178) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 17.19it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 17.92it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 18.28it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 18.63it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 18.68it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 16.07it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   1%|          | 41/8192 [00:00<00:20, 404.67it/s]
Adding requests:   1%|          | 93/8192 [00:00<00:17, 468.58it/s]
Adding requests:   2%|▏         | 144/8192 [00:00<00:16, 483.12it/s]
Adding requests:   2%|▏         | 194/8192 [00:00<00:16, 486.69it/s]
Adding requests:   3%|▎         | 246/8192 [00:00<00:16, 495.67it/s]
Adding requests:   4%|▎         | 296/8192 [00:00<00:15, 496.82it/s]
Adding requests:   4%|▍         | 346/8192 [00:00<00:15, 494.08it/s]
Adding requests:   5%|▍         | 398/8192 [00:00<00:15, 499.60it/s]
Adding requests:   5%|▌         | 448/8192 [00:00<00:15, 498.89it/s]
Adding requests:   6%|▌         | 498/8192 [00:01<00:15, 497.88it/s]
Adding requests:   7%|▋         | 548/8192 [00:01<00:15, 490.26it/s]
Adding requests:   7%|▋         | 600/8192 [00:01<00:15, 494.77it/s]
Adding requests:   8%|▊         | 652/8192 [00:01<00:15, 501.47it/s]
Adding requests:   9%|▊         | 703/8192 [00:01<00:14, 502.22it/s]
Adding requests:   9%|▉         | 754/8192 [00:01<00:14, 500.72it/s]
Adding requests:  10%|▉         | 805/8192 [00:01<00:14, 494.12it/s]
Adding requests:  10%|█         | 855/8192 [00:01<00:14, 493.92it/s]
Adding requests:  11%|█         | 907/8192 [00:01<00:14, 501.02it/s]
Adding requests:  12%|█▏        | 958/8192 [00:01<00:14, 503.45it/s]
Adding requests:  12%|█▏        | 1009/8192 [00:02<00:14, 504.97it/s]
Adding requests:  13%|█▎        | 1060/8192 [00:02<00:14, 506.24it/s]
Adding requests:  14%|█▎        | 1111/8192 [00:02<00:14, 502.16it/s]
Adding requests:  14%|█▍        | 1163/8192 [00:02<00:13, 505.15it/s]
Adding requests:  15%|█▍        | 1216/8192 [00:02<00:13, 511.54it/s]
Adding requests:  15%|█▌        | 1268/8192 [00:02<00:13, 507.80it/s]
Adding requests:  16%|█▌        | 1319/8192 [00:02<00:13, 508.43it/s]
Adding requests:  17%|█▋        | 1371/8192 [00:02<00:13, 510.14it/s]
Adding requests:  17%|█▋        | 1423/8192 [00:02<00:13, 512.84it/s]
Adding requests:  18%|█▊        | 1475/8192 [00:02<00:13, 513.68it/s]
Adding requests:  19%|█▊        | 1528/8192 [00:03<00:12, 516.12it/s]
Adding requests:  19%|█▉        | 1581/8192 [00:03<00:12, 517.63it/s]
Adding requests:  20%|█▉        | 1635/8192 [00:03<00:12, 522.26it/s]
Adding requests:  21%|██        | 1688/8192 [00:03<00:12, 515.40it/s]
Adding requests:  21%|██▏       | 1741/8192 [00:03<00:12, 517.63it/s]
Adding requests:  22%|██▏       | 1793/8192 [00:03<00:12, 515.58it/s]
Adding requests:  23%|██▎       | 1845/8192 [00:03<00:12, 516.84it/s]
Adding requests:  23%|██▎       | 1897/8192 [00:03<00:12, 501.62it/s]
Adding requests:  24%|██▍       | 1949/8192 [00:03<00:12, 504.51it/s]
Adding requests:  24%|██▍       | 2001/8192 [00:03<00:12, 507.02it/s]
Adding requests:  25%|██▌       | 2053/8192 [00:04<00:12, 509.94it/s]
Adding requests:  26%|██▌       | 2105/8192 [00:04<00:11, 511.93it/s]
Adding requests:  26%|██▋       | 2157/8192 [00:04<00:11, 503.48it/s]
Adding requests:  27%|██▋       | 2208/8192 [00:04<00:11, 500.48it/s]
Adding requests:  28%|██▊       | 2260/8192 [00:04<00:11, 505.62it/s]
Adding requests:  28%|██▊       | 2312/8192 [00:04<00:11, 509.05it/s]
Adding requests:  29%|██▉       | 2363/8192 [00:04<00:11, 506.96it/s]
Adding requests:  29%|██▉       | 2414/8192 [00:04<00:11, 506.99it/s]
Adding requests:  30%|███       | 2466/8192 [00:04<00:11, 508.50it/s]
Adding requests:  31%|███       | 2517/8192 [00:04<00:11, 507.96it/s]
Adding requests:  31%|███▏      | 2570/8192 [00:05<00:10, 514.01it/s]
Adding requests:  32%|███▏      | 2622/8192 [00:05<00:10, 513.66it/s]
Adding requests:  33%|███▎      | 2674/8192 [00:05<00:10, 514.78it/s]
Adding requests:  33%|███▎      | 2726/8192 [00:05<00:10, 512.80it/s]
Adding requests:  34%|███▍      | 2778/8192 [00:05<00:10, 510.05it/s]
Adding requests:  35%|███▍      | 2830/8192 [00:05<00:10, 505.36it/s]
Adding requests:  35%|███▌      | 2882/8192 [00:05<00:10, 507.61it/s]
Adding requests:  36%|███▌      | 2933/8192 [00:05<00:10, 505.16it/s]
Adding requests:  36%|███▋      | 2984/8192 [00:05<00:10, 505.80it/s]
Adding requests:  37%|███▋      | 3035/8192 [00:06<00:10, 504.57it/s]
Adding requests:  38%|███▊      | 3086/8192 [00:06<00:10, 503.07it/s]
Adding requests:  38%|███▊      | 3137/8192 [00:06<00:10, 504.39it/s]
Adding requests:  39%|███▉      | 3188/8192 [00:06<00:10, 494.59it/s]
Adding requests:  40%|███▉      | 3240/8192 [00:06<00:09, 500.62it/s]
Adding requests:  40%|████      | 3292/8192 [00:06<00:09, 504.62it/s]
Adding requests:  41%|████      | 3344/8192 [00:06<00:09, 506.69it/s]
Adding requests:  41%|████▏     | 3395/8192 [00:06<00:09, 507.56it/s]
Adding requests:  42%|████▏     | 3446/8192 [00:06<00:09, 506.79it/s]
Adding requests:  43%|████▎     | 3497/8192 [00:06<00:09, 503.95it/s]
Adding requests:  43%|████▎     | 3548/8192 [00:07<00:09, 505.01it/s]
Adding requests:  44%|████▍     | 3599/8192 [00:07<00:09, 504.37it/s]
Adding requests:  45%|████▍     | 3650/8192 [00:07<00:09, 501.07it/s]
Adding requests:  45%|████▌     | 3702/8192 [00:07<00:08, 504.26it/s]
Adding requests:  46%|████▌     | 3753/8192 [00:07<00:08, 502.23it/s]
Adding requests:  46%|████▋     | 3806/8192 [00:07<00:08, 509.16it/s]
Adding requests:  47%|████▋     | 3859/8192 [00:07<00:08, 515.28it/s]
Adding requests:  48%|████▊     | 3911/8192 [00:07<00:08, 509.85it/s]
Adding requests:  48%|████▊     | 3963/8192 [00:07<00:08, 512.42it/s]
Adding requests:  49%|████▉     | 4015/8192 [00:07<00:08, 509.65it/s]
Adding requests:  50%|████▉     | 4066/8192 [00:08<00:08, 506.46it/s]
Adding requests:  50%|█████     | 4118/8192 [00:08<00:08, 509.09it/s]
Adding requests:  51%|█████     | 4171/8192 [00:08<00:07, 513.10it/s]
Adding requests:  52%|█████▏    | 4223/8192 [00:08<00:07, 514.91it/s]
Adding requests:  52%|█████▏    | 4275/8192 [00:08<00:07, 514.97it/s]
Adding requests:  53%|█████▎    | 4327/8192 [00:08<00:07, 515.23it/s]
Adding requests:  53%|█████▎    | 4381/8192 [00:08<00:07, 520.46it/s]
Adding requests:  54%|█████▍    | 4434/8192 [00:08<00:07, 517.09it/s]
Adding requests:  55%|█████▍    | 4486/8192 [00:08<00:07, 514.29it/s]
Adding requests:  55%|█████▌    | 4538/8192 [00:08<00:07, 500.97it/s]
Adding requests:  56%|█████▌    | 4589/8192 [00:09<00:07, 501.71it/s]
Adding requests:  57%|█████▋    | 4643/8192 [00:09<00:06, 511.16it/s]
Adding requests:  57%|█████▋    | 4695/8192 [00:09<00:06, 506.88it/s]
Adding requests:  58%|█████▊    | 4748/8192 [00:09<00:06, 512.02it/s]
Adding requests:  59%|█████▊    | 4800/8192 [00:09<00:06, 509.57it/s]
Adding requests:  59%|█████▉    | 4852/8192 [00:09<00:06, 511.61it/s]
Adding requests:  60%|█████▉    | 4904/8192 [00:09<00:06, 507.49it/s]
Adding requests:  61%|██████    | 4957/8192 [00:09<00:06, 512.17it/s]
Adding requests:  61%|██████    | 5009/8192 [00:09<00:06, 512.56it/s]
Adding requests:  62%|██████▏   | 5061/8192 [00:09<00:06, 513.99it/s]
Adding requests:  62%|██████▏   | 5114/8192 [00:10<00:05, 516.81it/s]
Adding requests:  63%|██████▎   | 5166/8192 [00:10<00:05, 514.84it/s]
Adding requests:  64%|██████▎   | 5218/8192 [00:10<00:05, 512.95it/s]
Adding requests:  64%|██████▍   | 5270/8192 [00:10<00:05, 507.53it/s]
Adding requests:  65%|██████▍   | 5322/8192 [00:10<00:05, 509.14it/s]
Adding requests:  66%|██████▌   | 5373/8192 [00:10<00:05, 508.64it/s]
Adding requests:  66%|██████▌   | 5425/8192 [00:10<00:05, 509.55it/s]
Adding requests:  67%|██████▋   | 5476/8192 [00:10<00:05, 504.44it/s]
Adding requests:  67%|██████▋   | 5527/8192 [00:10<00:05, 501.02it/s]
Adding requests:  68%|██████▊   | 5578/8192 [00:11<00:05, 501.27it/s]
Adding requests:  69%|██████▊   | 5629/8192 [00:11<00:05, 498.66it/s]
Adding requests:  69%|██████▉   | 5679/8192 [00:11<00:05, 498.14it/s]
Adding requests:  70%|██████▉   | 5731/8192 [00:11<00:04, 503.95it/s]
Adding requests:  71%|███████   | 5783/8192 [00:11<00:04, 507.72it/s]
Adding requests:  71%|███████   | 5834/8192 [00:11<00:04, 505.16it/s]
Adding requests:  72%|███████▏  | 5885/8192 [00:11<00:04, 500.73it/s]
Adding requests:  72%|███████▏  | 5937/8192 [00:11<00:04, 504.87it/s]
Adding requests:  73%|███████▎  | 5990/8192 [00:11<00:04, 510.89it/s]
Adding requests:  74%|███████▍  | 6043/8192 [00:11<00:04, 514.82it/s]
Adding requests:  74%|███████▍  | 6095/8192 [00:12<00:04, 514.32it/s]
Adding requests:  75%|███████▌  | 6148/8192 [00:12<00:03, 516.66it/s]
Adding requests:  76%|███████▌  | 6201/8192 [00:12<00:03, 520.05it/s]
Adding requests:  76%|███████▋  | 6255/8192 [00:12<00:03, 525.76it/s]
Adding requests:  77%|███████▋  | 6309/8192 [00:12<00:03, 527.78it/s]
Adding requests:  78%|███████▊  | 6363/8192 [00:12<00:03, 530.03it/s]
Adding requests:  78%|███████▊  | 6417/8192 [00:12<00:03, 529.41it/s]
Adding requests:  79%|███████▉  | 6471/8192 [00:12<00:03, 530.88it/s]
Adding requests:  80%|███████▉  | 6526/8192 [00:12<00:03, 534.86it/s]
Adding requests:  80%|████████  | 6580/8192 [00:12<00:03, 530.24it/s]
Adding requests:  81%|████████  | 6634/8192 [00:13<00:02, 526.06it/s]
Adding requests:  82%|████████▏ | 6687/8192 [00:13<00:02, 525.04it/s]
Adding requests:  82%|████████▏ | 6740/8192 [00:13<00:02, 525.41it/s]
Adding requests:  83%|████████▎ | 6794/8192 [00:13<00:02, 528.17it/s]
Adding requests:  84%|████████▎ | 6847/8192 [00:13<00:02, 525.75it/s]
Adding requests:  84%|████████▍ | 6901/8192 [00:13<00:02, 529.11it/s]
Adding requests:  85%|████████▍ | 6956/8192 [00:13<00:02, 532.85it/s]
Adding requests:  86%|████████▌ | 7010/8192 [00:13<00:02, 525.73it/s]
Adding requests:  86%|████████▌ | 7063/8192 [00:13<00:02, 523.91it/s]
Adding requests:  87%|████████▋ | 7116/8192 [00:13<00:02, 524.76it/s]
Adding requests:  88%|████████▊ | 7169/8192 [00:14<00:01, 520.31it/s]
Adding requests:  88%|████████▊ | 7222/8192 [00:14<00:01, 520.59it/s]
Adding requests:  89%|████████▉ | 7275/8192 [00:14<00:01, 508.14it/s]
Adding requests:  89%|████████▉ | 7329/8192 [00:14<00:01, 515.99it/s]
Adding requests:  90%|█████████ | 7381/8192 [00:14<00:01, 515.26it/s]
Adding requests:  91%|█████████ | 7437/8192 [00:14<00:01, 526.34it/s]
Adding requests:  91%|█████████▏| 7490/8192 [00:14<00:01, 525.33it/s]
Adding requests:  92%|█████████▏| 7543/8192 [00:14<00:01, 525.77it/s]
Adding requests:  93%|█████████▎| 7596/8192 [00:14<00:01, 521.68it/s]
Adding requests:  93%|█████████▎| 7650/8192 [00:14<00:01, 524.98it/s]
Adding requests:  94%|█████████▍| 7703/8192 [00:15<00:00, 526.17it/s]
Adding requests:  95%|█████████▍| 7756/8192 [00:15<00:00, 522.16it/s]
Adding requests:  95%|█████████▌| 7809/8192 [00:15<00:00, 516.91it/s]
Adding requests:  96%|█████████▌| 7863/8192 [00:15<00:00, 522.65it/s]
Adding requests:  97%|█████████▋| 7916/8192 [00:15<00:00, 517.64it/s]
Adding requests:  97%|█████████▋| 7968/8192 [00:15<00:00, 516.75it/s]
Adding requests:  98%|█████████▊| 8020/8192 [00:15<00:00, 512.57it/s]
Adding requests:  99%|█████████▊| 8073/8192 [00:15<00:00, 516.14it/s]
Adding requests:  99%|█████████▉| 8126/8192 [00:15<00:00, 520.04it/s]
Adding requests: 100%|█████████▉| 8179/8192 [00:16<00:00, 519.20it/s]
Adding requests: 100%|██████████| 8192/8192 [00:16<00:00, 510.92it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▎        | 1118/8192 [00:00<00:01, 3831.69it/s, est. speed input: 3923972.86 toks/s, output: 3831.79 toks/s]
Processed prompts:  18%|█▊        | 1502/8192 [00:05<00:31, 215.27it/s, est. speed input: 279308.55 toks/s, output: 272.76 toks/s]   
Processed prompts:  20%|██        | 1665/8192 [00:07<00:36, 176.97it/s, est. speed input: 235203.03 toks/s, output: 229.69 toks/s]
Processed prompts:  21%|██▏       | 1758/8192 [00:08<00:46, 138.67it/s, est. speed input: 200250.03 toks/s, output: 195.56 toks/s]
Processed prompts:  22%|██▏       | 1822/8192 [00:09<00:50, 127.19it/s, est. speed input: 189461.51 toks/s, output: 185.02 toks/s]
Processed prompts:  23%|██▎       | 1886/8192 [00:10<00:54, 116.11it/s, est. speed input: 180174.15 toks/s, output: 175.95 toks/s]
Processed prompts:  24%|██▍       | 1950/8192 [00:11<00:58, 106.72it/s, est. speed input: 172457.77 toks/s, output: 168.42 toks/s]
Processed prompts:  25%|██▍       | 2014/8192 [00:12<01:02, 98.56it/s, est. speed input: 165662.33 toks/s, output: 161.78 toks/s] 
Processed prompts:  25%|██▌       | 2078/8192 [00:13<01:06, 92.03it/s, est. speed input: 159747.37 toks/s, output: 156.00 toks/s]
Processed prompts:  26%|██▌       | 2142/8192 [00:14<01:09, 86.99it/s, est. speed input: 154553.51 toks/s, output: 150.93 toks/s]
Processed prompts:  27%|██▋       | 2206/8192 [00:15<01:11, 83.78it/s, est. speed input: 150175.21 toks/s, output: 146.66 toks/s]
Processed prompts:  28%|██▊       | 2270/8192 [00:15<01:12, 81.17it/s, est. speed input: 146188.69 toks/s, output: 142.76 toks/s]
Processed prompts:  28%|██▊       | 2334/8192 [00:16<01:13, 79.28it/s, est. speed input: 142615.63 toks/s, output: 139.27 toks/s]
Processed prompts:  29%|██▉       | 2398/8192 [00:17<01:14, 77.56it/s, est. speed input: 139285.35 toks/s, output: 136.02 toks/s]
Processed prompts:  30%|███       | 2462/8192 [00:18<01:14, 76.67it/s, est. speed input: 136360.26 toks/s, output: 133.16 toks/s]
Processed prompts:  31%|███       | 2526/8192 [00:19<01:14, 75.67it/s, est. speed input: 133596.51 toks/s, output: 130.47 toks/s]
Processed prompts:  32%|███▏      | 2590/8192 [00:20<01:14, 75.33it/s, est. speed input: 131163.00 toks/s, output: 128.09 toks/s]
Processed prompts:  32%|███▏      | 2654/8192 [00:21<01:14, 74.71it/s, est. speed input: 128839.45 toks/s, output: 125.82 toks/s]
Processed prompts:  33%|███▎      | 2718/8192 [00:21<01:13, 74.32it/s, est. speed input: 126708.23 toks/s, output: 123.74 toks/s]
Processed prompts:  34%|███▍      | 2782/8192 [00:22<01:13, 74.02it/s, est. speed input: 124735.15 toks/s, output: 121.81 toks/s]
Processed prompts:  35%|███▍      | 2846/8192 [00:23<01:12, 73.77it/s, est. speed input: 122899.22 toks/s, output: 120.02 toks/s]
Processed prompts:  36%|███▌      | 2910/8192 [00:24<01:11, 73.64it/s, est. speed input: 121201.62 toks/s, output: 118.36 toks/s]
Processed prompts:  36%|███▋      | 2974/8192 [00:25<01:10, 73.55it/s, est. speed input: 119621.16 toks/s, output: 116.82 toks/s]
Processed prompts:  37%|███▋      | 3038/8192 [00:26<01:10, 73.44it/s, est. speed input: 118138.62 toks/s, output: 115.37 toks/s]
Processed prompts:  38%|███▊      | 3102/8192 [00:27<01:09, 73.70it/s, est. speed input: 116806.85 toks/s, output: 114.07 toks/s]
Processed prompts:  39%|███▊      | 3166/8192 [00:28<01:08, 73.56it/s, est. speed input: 115505.65 toks/s, output: 112.80 toks/s]
Processed prompts:  39%|███▉      | 3230/8192 [00:28<01:07, 73.49it/s, est. speed input: 114286.35 toks/s, output: 111.61 toks/s]
Processed prompts:  40%|████      | 3294/8192 [00:29<01:06, 73.33it/s, est. speed input: 113121.57 toks/s, output: 110.47 toks/s]
Processed prompts:  41%|████      | 3358/8192 [00:30<01:05, 73.25it/s, est. speed input: 112028.62 toks/s, output: 109.40 toks/s]
Processed prompts:  42%|████▏     | 3422/8192 [00:31<01:05, 73.24it/s, est. speed input: 111002.02 toks/s, output: 108.40 toks/s]
Processed prompts:  43%|████▎     | 3486/8192 [00:32<01:04, 73.21it/s, est. speed input: 110028.60 toks/s, output: 107.45 toks/s]
Processed prompts:  43%|████▎     | 3550/8192 [00:33<01:03, 73.22it/s, est. speed input: 109109.70 toks/s, output: 106.55 toks/s]
Processed prompts:  44%|████▍     | 3614/8192 [00:34<01:02, 73.15it/s, est. speed input: 108229.00 toks/s, output: 105.69 toks/s]
Processed prompts:  45%|████▍     | 3678/8192 [00:35<01:01, 73.17it/s, est. speed input: 107400.15 toks/s, output: 104.88 toks/s]
Processed prompts:  46%|████▌     | 3742/8192 [00:35<01:00, 73.41it/s, est. speed input: 106638.19 toks/s, output: 104.14 toks/s]
Processed prompts:  46%|████▋     | 3806/8192 [00:36<00:59, 73.33it/s, est. speed input: 105883.35 toks/s, output: 103.40 toks/s]
Processed prompts:  47%|████▋     | 3870/8192 [00:37<00:59, 73.23it/s, est. speed input: 105159.00 toks/s, output: 102.69 toks/s]
Processed prompts:  48%|████▊     | 3934/8192 [00:38<00:58, 73.21it/s, est. speed input: 104473.58 toks/s, output: 102.02 toks/s]
Processed prompts:  49%|████▉     | 3998/8192 [00:39<00:57, 73.46it/s, est. speed input: 103845.25 toks/s, output: 101.41 toks/s]
Processed prompts:  50%|████▉     | 4062/8192 [00:40<00:56, 73.36it/s, est. speed input: 103215.74 toks/s, output: 100.80 toks/s]
Processed prompts:  50%|█████     | 4126/8192 [00:41<00:55, 73.26it/s, est. speed input: 102610.66 toks/s, output: 100.21 toks/s]
Processed prompts:  51%|█████     | 4190/8192 [00:42<00:54, 73.53it/s, est. speed input: 102063.23 toks/s, output: 99.67 toks/s] 
Processed prompts:  52%|█████▏    | 4254/8192 [00:42<00:53, 73.65it/s, est. speed input: 101531.98 toks/s, output: 99.15 toks/s]
Processed prompts:  53%|█████▎    | 4318/8192 [00:43<00:52, 73.79it/s, est. speed input: 101025.66 toks/s, output: 98.66 toks/s]
Processed prompts:  53%|█████▎    | 4382/8192 [00:44<00:51, 73.54it/s, est. speed input: 100508.82 toks/s, output: 98.15 toks/s]
Processed prompts:  54%|█████▍    | 4446/8192 [00:45<00:51, 73.36it/s, est. speed input: 100011.49 toks/s, output: 97.67 toks/s]
Processed prompts:  55%|█████▌    | 4510/8192 [00:46<00:50, 73.24it/s, est. speed input: 99533.52 toks/s, output: 97.20 toks/s] 
Processed prompts:  56%|█████▌    | 4574/8192 [00:47<00:49, 73.12it/s, est. speed input: 99069.33 toks/s, output: 96.75 toks/s]
Processed prompts:  57%|█████▋    | 4638/8192 [00:48<00:48, 73.04it/s, est. speed input: 98623.41 toks/s, output: 96.31 toks/s]
Processed prompts:  57%|█████▋    | 4702/8192 [00:49<00:47, 73.05it/s, est. speed input: 98198.35 toks/s, output: 95.90 toks/s]
Processed prompts:  58%|█████▊    | 4766/8192 [00:49<00:46, 73.31it/s, est. speed input: 97807.71 toks/s, output: 95.52 toks/s]
Processed prompts:  59%|█████▉    | 4830/8192 [00:50<00:45, 73.47it/s, est. speed input: 97428.94 toks/s, output: 95.15 toks/s]
Processed prompts:  60%|█████▉    | 4894/8192 [00:51<00:45, 73.26it/s, est. speed input: 97038.98 toks/s, output: 94.76 toks/s]
Processed prompts:  61%|██████    | 4958/8192 [00:52<00:43, 73.56it/s, est. speed input: 96694.33 toks/s, output: 94.43 toks/s]
Processed prompts:  61%|██████▏   | 5022/8192 [00:53<00:43, 73.29it/s, est. speed input: 96326.58 toks/s, output: 94.07 toks/s]
Processed prompts:  62%|██████▏   | 5086/8192 [00:54<00:42, 73.13it/s, est. speed input: 95973.20 toks/s, output: 93.72 toks/s]
Processed prompts:  63%|██████▎   | 5150/8192 [00:55<00:41, 73.11it/s, est. speed input: 95636.88 toks/s, output: 93.40 toks/s]
Processed prompts:  64%|██████▎   | 5214/8192 [00:56<00:40, 72.99it/s, est. speed input: 95304.01 toks/s, output: 93.07 toks/s]
Processed prompts:  64%|██████▍   | 5278/8192 [00:56<00:39, 72.93it/s, est. speed input: 94983.16 toks/s, output: 92.76 toks/s]
Processed prompts:  65%|██████▌   | 5342/8192 [00:57<00:39, 72.90it/s, est. speed input: 94672.70 toks/s, output: 92.45 toks/s]
Processed prompts:  66%|██████▌   | 5406/8192 [00:58<00:38, 72.87it/s, est. speed input: 94371.14 toks/s, output: 92.16 toks/s]
Processed prompts:  67%|██████▋   | 5470/8192 [00:59<00:37, 72.83it/s, est. speed input: 94077.09 toks/s, output: 91.87 toks/s]
Processed prompts:  68%|██████▊   | 5534/8192 [01:00<00:36, 73.10it/s, est. speed input: 93810.43 toks/s, output: 91.61 toks/s]
Processed prompts:  68%|██████▊   | 5598/8192 [01:01<00:35, 72.99it/s, est. speed input: 93533.04 toks/s, output: 91.34 toks/s]
Processed prompts:  69%|██████▉   | 5662/8192 [01:02<00:34, 72.92it/s, est. speed input: 93263.87 toks/s, output: 91.08 toks/s]
Processed prompts:  70%|██████▉   | 5726/8192 [01:03<00:33, 72.88it/s, est. speed input: 93002.73 toks/s, output: 90.82 toks/s]
Processed prompts:  71%|███████   | 5790/8192 [01:03<00:32, 72.84it/s, est. speed input: 92747.96 toks/s, output: 90.57 toks/s]
Processed prompts:  71%|███████▏  | 5854/8192 [01:04<00:32, 72.80it/s, est. speed input: 92499.37 toks/s, output: 90.33 toks/s]
Processed prompts:  72%|███████▏  | 5918/8192 [01:05<00:31, 72.76it/s, est. speed input: 92257.08 toks/s, output: 90.09 toks/s]
Processed prompts:  73%|███████▎  | 5982/8192 [01:06<00:30, 72.75it/s, est. speed input: 92021.79 toks/s, output: 89.86 toks/s]
Processed prompts:  74%|███████▍  | 6046/8192 [01:07<00:29, 72.74it/s, est. speed input: 91792.42 toks/s, output: 89.64 toks/s]
Processed prompts:  75%|███████▍  | 6110/8192 [01:08<00:28, 72.71it/s, est. speed input: 91568.18 toks/s, output: 89.42 toks/s]
Processed prompts:  75%|███████▌  | 6174/8192 [01:09<00:27, 72.72it/s, est. speed input: 91350.79 toks/s, output: 89.21 toks/s]
Processed prompts:  76%|███████▌  | 6238/8192 [01:10<00:26, 72.72it/s, est. speed input: 91138.83 toks/s, output: 89.00 toks/s]
Processed prompts:  77%|███████▋  | 6302/8192 [01:10<00:25, 72.69it/s, est. speed input: 90930.76 toks/s, output: 88.80 toks/s]
Processed prompts:  78%|███████▊  | 6366/8192 [01:11<00:25, 72.69it/s, est. speed input: 90728.51 toks/s, output: 88.60 toks/s]
Processed prompts:  78%|███████▊  | 6430/8192 [01:12<00:24, 72.67it/s, est. speed input: 90530.07 toks/s, output: 88.41 toks/s]
Processed prompts:  79%|███████▉  | 6494/8192 [01:13<00:23, 72.64it/s, est. speed input: 90336.04 toks/s, output: 88.22 toks/s]
Processed prompts:  80%|████████  | 6558/8192 [01:14<00:22, 72.98it/s, est. speed input: 90163.82 toks/s, output: 88.05 toks/s]
Processed prompts:  81%|████████  | 6622/8192 [01:15<00:21, 73.19it/s, est. speed input: 89994.43 toks/s, output: 87.89 toks/s]
Processed prompts:  82%|████████▏ | 6686/8192 [01:16<00:20, 72.99it/s, est. speed input: 89812.48 toks/s, output: 87.71 toks/s]
Processed prompts:  82%|████████▏ | 6750/8192 [01:17<00:19, 72.90it/s, est. speed input: 89636.83 toks/s, output: 87.54 toks/s]
Processed prompts:  83%|████████▎ | 6814/8192 [01:17<00:18, 72.82it/s, est. speed input: 89464.54 toks/s, output: 87.37 toks/s]
Processed prompts:  84%|████████▍ | 6878/8192 [01:18<00:18, 72.75it/s, est. speed input: 89295.38 toks/s, output: 87.20 toks/s]
Processed prompts:  85%|████████▍ | 6942/8192 [01:19<00:17, 72.70it/s, est. speed input: 89129.88 toks/s, output: 87.04 toks/s]
Processed prompts:  86%|████████▌ | 7006/8192 [01:20<00:16, 72.68it/s, est. speed input: 88968.51 toks/s, output: 86.88 toks/s]
Processed prompts:  86%|████████▋ | 7070/8192 [01:21<00:15, 72.65it/s, est. speed input: 88810.27 toks/s, output: 86.73 toks/s]
Processed prompts:  87%|████████▋ | 7134/8192 [01:22<00:14, 72.95it/s, est. speed input: 88668.98 toks/s, output: 86.59 toks/s]
Processed prompts:  88%|████████▊ | 7198/8192 [01:23<00:13, 72.81it/s, est. speed input: 88515.92 toks/s, output: 86.44 toks/s]
Processed prompts:  89%|████████▊ | 7262/8192 [01:24<00:12, 73.06it/s, est. speed input: 88380.62 toks/s, output: 86.31 toks/s]
Processed prompts:  89%|████████▉ | 7326/8192 [01:25<00:11, 72.95it/s, est. speed input: 88236.28 toks/s, output: 86.17 toks/s]
Processed prompts:  90%|█████████ | 7390/8192 [01:25<00:10, 73.04it/s, est. speed input: 88101.58 toks/s, output: 86.04 toks/s]
Processed prompts:  91%|█████████ | 7454/8192 [01:26<00:10, 72.92it/s, est. speed input: 87962.45 toks/s, output: 85.90 toks/s]
Processed prompts:  92%|█████████▏| 7518/8192 [01:27<00:09, 72.84it/s, est. speed input: 87825.90 toks/s, output: 85.77 toks/s]
Processed prompts:  93%|█████████▎| 7582/8192 [01:28<00:08, 72.84it/s, est. speed input: 87694.52 toks/s, output: 85.64 toks/s]
Processed prompts:  93%|█████████▎| 7646/8192 [01:29<00:07, 72.73it/s, est. speed input: 87561.51 toks/s, output: 85.51 toks/s]
Processed prompts:  94%|█████████▍| 7710/8192 [01:30<00:06, 72.65it/s, est. speed input: 87430.97 toks/s, output: 85.38 toks/s]
Processed prompts:  95%|█████████▍| 7774/8192 [01:31<00:05, 72.67it/s, est. speed input: 87305.70 toks/s, output: 85.26 toks/s]
Processed prompts:  96%|█████████▌| 7838/8192 [01:32<00:04, 72.58it/s, est. speed input: 87178.98 toks/s, output: 85.14 toks/s]
Processed prompts:  96%|█████████▋| 7902/8192 [01:32<00:03, 72.56it/s, est. speed input: 87056.39 toks/s, output: 85.02 toks/s]
Processed prompts:  97%|█████████▋| 7966/8192 [01:33<00:03, 72.58it/s, est. speed input: 86937.10 toks/s, output: 84.90 toks/s]
Processed prompts:  98%|█████████▊| 8030/8192 [01:34<00:02, 72.57it/s, est. speed input: 86819.28 toks/s, output: 84.78 toks/s]
Processed prompts:  99%|█████████▉| 8094/8192 [01:35<00:01, 72.99it/s, est. speed input: 86719.26 toks/s, output: 84.69 toks/s]
Processed prompts: 100%|█████████▉| 8158/8192 [01:36<00:00, 84.80it/s, est. speed input: 86977.49 toks/s, output: 84.94 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:36<00:00, 84.80it/s, est. speed input: 87339.45 toks/s, output: 85.29 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:36<00:00, 85.29it/s, est. speed input: 87339.45 toks/s, output: 85.29 toks/s]
[rank0]:[W125 15:58:10.111310054 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16 ==========
Time: 2026-01-25 21:49:06
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:49:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=461454) WARNING 01-25 21:49:26 [backends.py:609] Failed to read file <frozen os>
Throughput: 33.21 requests/s, 564.51 total tokens/s, 33.21 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-25 21:49:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:49:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:49:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:49:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:49:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:49:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:49:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:49:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:49:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:49:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:49:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:49:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:49:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:49:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:49:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:49:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:49:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:49:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=461454) [2026-01-25 21:49:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=461454) [2026-01-25 21:49:22] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=461454) [2026-01-25 21:49:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=461454) [2026-01-25 21:49:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=461454) [2026-01-25 21:49:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=461454) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=461454) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.55it/s]
(EngineCore_DP0 pid=461454) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.55it/s]
(EngineCore_DP0 pid=461454) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=461454) 2026-01-25 21:49:34,931 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=461454) 2026-01-25 21:49:34,959 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=461454) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  6.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  1.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.09it/s]
(EngineCore_DP0 pid=461454) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.27it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 6585.60it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<01:11,  1.77it/s, est. speed input: 28.37 toks/s, output: 1.77 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:13,  9.42it/s, est. speed input: 119.72 toks/s, output: 7.48 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:07, 15.93it/s, est. speed input: 185.45 toks/s, output: 11.59 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:05, 21.45it/s, est. speed input: 236.24 toks/s, output: 14.76 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:04, 25.85it/s, est. speed input: 276.22 toks/s, output: 17.26 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:03, 29.27it/s, est. speed input: 308.66 toks/s, output: 19.29 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:03, 31.85it/s, est. speed input: 335.47 toks/s, output: 20.97 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:02, 33.74it/s, est. speed input: 357.94 toks/s, output: 22.37 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 35.13it/s, est. speed input: 377.13 toks/s, output: 23.57 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 36.09it/s, est. speed input: 393.57 toks/s, output: 24.60 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 36.84it/s, est. speed input: 408.05 toks/s, output: 25.50 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 37.38it/s, est. speed input: 420.78 toks/s, output: 26.30 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 37.74it/s, est. speed input: 432.01 toks/s, output: 27.00 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 37.99it/s, est. speed input: 442.03 toks/s, output: 27.63 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:02<00:01, 38.12it/s, est. speed input: 450.91 toks/s, output: 28.18 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:01, 38.51it/s, est. speed input: 459.51 toks/s, output: 28.72 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 38.57it/s, est. speed input: 466.94 toks/s, output: 29.18 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 38.54it/s, est. speed input: 473.56 toks/s, output: 29.60 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 38.53it/s, est. speed input: 479.64 toks/s, output: 29.98 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 38.52it/s, est. speed input: 485.21 toks/s, output: 30.33 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 38.72it/s, est. speed input: 490.70 toks/s, output: 30.67 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 38.95it/s, est. speed input: 495.92 toks/s, output: 30.99 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:00, 39.11it/s, est. speed input: 500.77 toks/s, output: 31.30 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 39.19it/s, est. speed input: 505.22 toks/s, output: 31.58 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:00, 39.30it/s, est. speed input: 509.45 toks/s, output: 31.84 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 39.36it/s, est. speed input: 513.40 toks/s, output: 32.09 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 39.38it/s, est. speed input: 517.07 toks/s, output: 32.32 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 39.39it/s, est. speed input: 520.51 toks/s, output: 32.53 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 39.35it/s, est. speed input: 523.68 toks/s, output: 32.73 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 39.34it/s, est. speed input: 526.69 toks/s, output: 32.92 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 39.38it/s, est. speed input: 529.61 toks/s, output: 33.10 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 39.33it/s, est. speed input: 532.26 toks/s, output: 33.27 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.33it/s, est. speed input: 534.19 toks/s, output: 33.39 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.38it/s, est. speed input: 534.19 toks/s, output: 33.39 toks/s]
[rank0]:[W125 21:49:41.543638672 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 21:49:43
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:49:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=462618) WARNING 01-25 21:50:04 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.90 requests/s, 4759.74 total tokens/s, 36.90 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-25 21:49:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:49:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:49:50] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:49:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:49:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:49:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:49:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:49:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:49:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:49:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:49:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:49:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:49:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:49:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:49:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:49:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:49:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:49:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:49:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=462618) [2026-01-25 21:49:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=462618) [2026-01-25 21:49:59] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=462618) [2026-01-25 21:49:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=462618) [2026-01-25 21:49:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=462618) [2026-01-25 21:49:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=462618) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=462618) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.13it/s]
(EngineCore_DP0 pid=462618) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.12it/s]
(EngineCore_DP0 pid=462618) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=462618) 2026-01-25 21:50:12,274 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=462618) 2026-01-25 21:50:12,346 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=462618) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.63it/s]
(EngineCore_DP0 pid=462618) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.77it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1399.38it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:24,  5.16it/s, est. speed input: 660.23 toks/s, output: 5.16 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:07, 15.58it/s, est. speed input: 1732.22 toks/s, output: 13.53 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 26.20it/s, est. speed input: 2749.11 toks/s, output: 21.48 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:03, 31.62it/s, est. speed input: 3303.52 toks/s, output: 25.81 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:03, 34.76it/s, est. speed input: 3653.45 toks/s, output: 28.54 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 36.69it/s, est. speed input: 3893.56 toks/s, output: 30.42 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 37.59it/s, est. speed input: 4030.63 toks/s, output: 31.49 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 38.57it/s, est. speed input: 4172.21 toks/s, output: 32.59 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:02, 39.01it/s, est. speed input: 4275.26 toks/s, output: 33.40 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 39.45it/s, est. speed input: 4363.31 toks/s, output: 34.09 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 39.78it/s, est. speed input: 4436.84 toks/s, output: 34.66 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 39.94it/s, est. speed input: 4496.13 toks/s, output: 35.13 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 40.07it/s, est. speed input: 4546.99 toks/s, output: 35.52 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 40.20it/s, est. speed input: 4591.85 toks/s, output: 35.87 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 40.34it/s, est. speed input: 4632.05 toks/s, output: 36.19 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 40.48it/s, est. speed input: 4668.08 toks/s, output: 36.47 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 40.43it/s, est. speed input: 4696.86 toks/s, output: 36.69 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 40.27it/s, est. speed input: 4719.73 toks/s, output: 36.87 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:00, 40.20it/s, est. speed input: 4741.17 toks/s, output: 37.04 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 40.11it/s, est. speed input: 4759.39 toks/s, output: 37.18 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 40.04it/s, est. speed input: 4775.82 toks/s, output: 37.31 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 40.00it/s, est. speed input: 4791.01 toks/s, output: 37.43 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 39.99it/s, est. speed input: 4805.15 toks/s, output: 37.54 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 39.98it/s, est. speed input: 4815.56 toks/s, output: 37.62 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 40.03it/s, est. speed input: 4828.48 toks/s, output: 37.72 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 40.11it/s, est. speed input: 4841.10 toks/s, output: 37.82 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 40.11it/s, est. speed input: 4852.06 toks/s, output: 37.91 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.11it/s, est. speed input: 4853.23 toks/s, output: 37.92 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.91it/s, est. speed input: 4853.23 toks/s, output: 37.92 toks/s]
[rank0]:[W125 21:50:17.745532190 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 21:50:19
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:50:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=463640) WARNING 01-25 21:50:39 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.44 requests/s, 9364.34 total tokens/s, 36.44 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-25 21:50:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:50:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:50:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:50:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:50:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:50:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:50:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:50:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:50:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:50:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:50:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:50:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:50:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:50:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:50:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:50:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:50:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:50:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:50:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:50:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:50:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:50:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:50:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:50:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:50:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:50:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:50:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:50:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=463640) [2026-01-25 21:50:35] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=463640) [2026-01-25 21:50:35] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=463640) [2026-01-25 21:50:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=463640) [2026-01-25 21:50:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=463640) [2026-01-25 21:50:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=463640) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=463640) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.55it/s]
(EngineCore_DP0 pid=463640) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.54it/s]
(EngineCore_DP0 pid=463640) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=463640) 2026-01-25 21:50:47,769 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=463640) 2026-01-25 21:50:47,797 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=463640) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.44it/s]
(EngineCore_DP0 pid=463640) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.28it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  95%|█████████▍| 121/128 [00:00<00:00, 1206.79it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1223.85it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:17,  7.29it/s, est. speed input: 1867.51 toks/s, output: 7.29 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:05, 22.65it/s, est. speed input: 5148.22 toks/s, output: 20.11 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 29.53it/s, est. speed input: 6578.36 toks/s, output: 25.69 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 33.15it/s, est. speed input: 7363.04 toks/s, output: 28.76 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 35.28it/s, est. speed input: 7858.36 toks/s, output: 30.69 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 36.60it/s, est. speed input: 8199.03 toks/s, output: 32.03 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 37.46it/s, est. speed input: 8448.89 toks/s, output: 33.00 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 37.52it/s, est. speed input: 8595.36 toks/s, output: 33.57 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 37.76it/s, est. speed input: 8725.28 toks/s, output: 34.08 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 38.14it/s, est. speed input: 8845.72 toks/s, output: 34.55 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 38.46it/s, est. speed input: 8949.43 toks/s, output: 34.96 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 38.73it/s, est. speed input: 9039.16 toks/s, output: 35.31 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 38.89it/s, est. speed input: 9114.13 toks/s, output: 35.60 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 39.01it/s, est. speed input: 9179.32 toks/s, output: 35.86 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 39.11it/s, est. speed input: 9236.86 toks/s, output: 36.08 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 39.17it/s, est. speed input: 9286.91 toks/s, output: 36.28 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 39.15it/s, est. speed input: 9328.45 toks/s, output: 36.44 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 39.19it/s, est. speed input: 9367.82 toks/s, output: 36.59 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 39.00it/s, est. speed input: 9394.14 toks/s, output: 36.70 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 38.89it/s, est. speed input: 9418.50 toks/s, output: 36.79 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 38.74it/s, est. speed input: 9437.92 toks/s, output: 36.87 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 38.90it/s, est. speed input: 9465.41 toks/s, output: 36.97 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 38.96it/s, est. speed input: 9488.49 toks/s, output: 37.06 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 39.07it/s, est. speed input: 9512.02 toks/s, output: 37.16 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 39.16it/s, est. speed input: 9534.06 toks/s, output: 37.24 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 39.16it/s, est. speed input: 9552.53 toks/s, output: 37.31 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 39.12it/s, est. speed input: 9568.57 toks/s, output: 37.38 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 38.92it/s, est. speed input: 9578.52 toks/s, output: 37.42 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 38.76it/s, est. speed input: 9586.99 toks/s, output: 37.45 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 38.67it/s, est. speed input: 9595.82 toks/s, output: 37.48 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 38.67it/s, est. speed input: 9605.37 toks/s, output: 37.52 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 38.61it/s, est. speed input: 9613.03 toks/s, output: 37.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.61it/s, est. speed input: 9618.35 toks/s, output: 37.57 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.57it/s, est. speed input: 9618.35 toks/s, output: 37.57 toks/s]
[rank0]:[W125 21:50:53.493027990 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16 ==========
Time: 2026-01-26 05:00:23
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 05:00:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=839358) WARNING 01-26 05:00:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 33.83 requests/s, 575.11 total tokens/s, 33.83 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-26 05:00:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 05:00:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 05:00:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 05:00:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:00:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:00:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:00:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:00:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:00:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 05:00:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 05:00:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 05:00:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 05:00:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 05:00:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 05:00:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 05:00:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 05:00:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 05:00:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:00:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:00:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:00:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:00:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:00:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 05:00:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 05:00:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 05:00:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 05:00:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 05:00:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=839358) [2026-01-26 05:00:39] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=839358) [2026-01-26 05:00:39] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=839358) [2026-01-26 05:00:39] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=839358) [2026-01-26 05:00:39] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=839358) [2026-01-26 05:00:39] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=839358) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=839358) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.28it/s]
(EngineCore_DP0 pid=839358) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.28it/s]
(EngineCore_DP0 pid=839358) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=839358) 2026-01-26 05:00:48,364 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=839358) 2026-01-26 05:00:48,398 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=839358) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.12it/s]
(EngineCore_DP0 pid=839358) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.92it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2838.90it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:59,  2.15it/s, est. speed input: 34.42 toks/s, output: 2.15 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:11, 11.02it/s, est. speed input: 141.35 toks/s, output: 8.83 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:06, 18.04it/s, est. speed input: 214.42 toks/s, output: 13.40 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 23.59it/s, est. speed input: 268.57 toks/s, output: 16.79 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:04, 27.74it/s, est. speed input: 309.70 toks/s, output: 19.36 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 30.83it/s, est. speed input: 342.25 toks/s, output: 21.39 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:03, 33.13it/s, est. speed input: 368.74 toks/s, output: 23.05 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:02, 34.72it/s, est. speed input: 390.41 toks/s, output: 24.40 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 35.87it/s, est. speed input: 408.66 toks/s, output: 25.54 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 36.66it/s, est. speed input: 424.13 toks/s, output: 26.51 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 37.26it/s, est. speed input: 437.55 toks/s, output: 27.35 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 37.69it/s, est. speed input: 449.28 toks/s, output: 28.08 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 37.96it/s, est. speed input: 459.49 toks/s, output: 28.72 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 38.15it/s, est. speed input: 468.55 toks/s, output: 29.28 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 38.30it/s, est. speed input: 476.64 toks/s, output: 29.79 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:01, 38.41it/s, est. speed input: 483.94 toks/s, output: 30.25 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 38.49it/s, est. speed input: 490.52 toks/s, output: 30.66 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 38.54it/s, est. speed input: 496.48 toks/s, output: 31.03 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 38.56it/s, est. speed input: 501.88 toks/s, output: 31.37 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 38.61it/s, est. speed input: 506.87 toks/s, output: 31.68 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 38.61it/s, est. speed input: 511.42 toks/s, output: 31.96 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 38.84it/s, est. speed input: 515.99 toks/s, output: 32.25 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:00, 39.05it/s, est. speed input: 520.31 toks/s, output: 32.52 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 39.08it/s, est. speed input: 524.13 toks/s, output: 32.76 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 39.03it/s, est. speed input: 527.58 toks/s, output: 32.97 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 39.16it/s, est. speed input: 531.02 toks/s, output: 33.19 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 39.10it/s, est. speed input: 534.04 toks/s, output: 33.38 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 38.95it/s, est. speed input: 536.71 toks/s, output: 33.54 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 38.88it/s, est. speed input: 539.25 toks/s, output: 33.70 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 38.80it/s, est. speed input: 541.62 toks/s, output: 33.85 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 38.71it/s, est. speed input: 543.79 toks/s, output: 33.99 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 39.02it/s, est. speed input: 546.32 toks/s, output: 34.14 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.02it/s, est. speed input: 548.05 toks/s, output: 34.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.25it/s, est. speed input: 548.05 toks/s, output: 34.25 toks/s]
[rank0]:[W126 05:00:54.803734894 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 05:00:56
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 05:01:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=840480) WARNING 01-26 05:01:16 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.28 requests/s, 4550.73 total tokens/s, 35.28 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-26 05:01:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 05:01:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 05:01:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 05:01:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:01:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:01:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:01:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:01:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:01:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 05:01:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 05:01:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 05:01:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 05:01:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 05:01:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 05:01:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 05:01:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 05:01:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 05:01:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:01:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:01:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:01:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:01:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 05:01:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 05:01:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 05:01:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 05:01:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 05:01:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 05:01:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=840480) [2026-01-26 05:01:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=840480) [2026-01-26 05:01:12] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=840480) [2026-01-26 05:01:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=840480) [2026-01-26 05:01:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=840480) [2026-01-26 05:01:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=840480) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=840480) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.58it/s]
(EngineCore_DP0 pid=840480) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.57it/s]
(EngineCore_DP0 pid=840480) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=840480) 2026-01-26 05:01:21,418 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=840480) 2026-01-26 05:01:21,462 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=840480) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  6.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.93it/s]
(EngineCore_DP0 pid=840480) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.74it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1392.09it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:32,  3.96it/s, est. speed input: 506.65 toks/s, output: 3.96 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:07, 16.18it/s, est. speed input: 1747.61 toks/s, output: 13.65 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 23.89it/s, est. speed input: 2461.35 toks/s, output: 19.23 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 28.92it/s, est. speed input: 2923.59 toks/s, output: 22.84 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 32.30it/s, est. speed input: 3248.21 toks/s, output: 25.37 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 34.58it/s, est. speed input: 3487.40 toks/s, output: 27.24 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 35.92it/s, est. speed input: 3662.77 toks/s, output: 28.61 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 36.83it/s, est. speed input: 3801.03 toks/s, output: 29.69 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 37.44it/s, est. speed input: 3912.54 toks/s, output: 30.57 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 37.81it/s, est. speed input: 4003.04 toks/s, output: 31.27 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 38.11it/s, est. speed input: 4080.29 toks/s, output: 31.88 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 38.25it/s, est. speed input: 4144.25 toks/s, output: 32.38 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 38.37it/s, est. speed input: 4199.95 toks/s, output: 32.81 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 38.45it/s, est. speed input: 4248.19 toks/s, output: 33.19 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 38.51it/s, est. speed input: 4290.86 toks/s, output: 33.52 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 38.51it/s, est. speed input: 4327.54 toks/s, output: 33.81 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 38.50it/s, est. speed input: 4360.12 toks/s, output: 34.06 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 38.51it/s, est. speed input: 4389.64 toks/s, output: 34.29 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 38.55it/s, est. speed input: 4416.90 toks/s, output: 34.51 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 38.54it/s, est. speed input: 4440.86 toks/s, output: 34.69 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 38.53it/s, est. speed input: 4462.78 toks/s, output: 34.86 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 38.51it/s, est. speed input: 4482.50 toks/s, output: 35.02 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 38.53it/s, est. speed input: 4501.16 toks/s, output: 35.16 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 38.58it/s, est. speed input: 4518.81 toks/s, output: 35.30 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 38.60it/s, est. speed input: 4534.97 toks/s, output: 35.43 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 38.59it/s, est. speed input: 4549.64 toks/s, output: 35.54 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 38.57it/s, est. speed input: 4563.11 toks/s, output: 35.65 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 38.79it/s, est. speed input: 4578.78 toks/s, output: 35.77 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 39.03it/s, est. speed input: 4594.54 toks/s, output: 35.89 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 39.17it/s, est. speed input: 4608.91 toks/s, output: 36.01 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 39.06it/s, est. speed input: 4619.91 toks/s, output: 36.09 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 38.87it/s, est. speed input: 4628.92 toks/s, output: 36.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.87it/s, est. speed input: 4635.40 toks/s, output: 36.21 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.21it/s, est. speed input: 4635.40 toks/s, output: 36.21 toks/s]
[rank0]:[W126 05:01:26.943670824 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 05:01:28
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M256.json

