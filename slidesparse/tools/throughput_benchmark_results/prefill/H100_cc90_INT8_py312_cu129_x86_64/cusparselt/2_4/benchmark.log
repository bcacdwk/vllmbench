
========== M=512 ==========
Time: 2026-01-25 15:58:12
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:58:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=157791) [INFO] Compress extension not found, building...
(EngineCore_DP0 pid=157791) ============================================================
(EngineCore_DP0 pid=157791) cuSPARSELt Compress Extension Builder
(EngineCore_DP0 pid=157791) ============================================================
(EngineCore_DP0 pid=157791) Extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64
(EngineCore_DP0 pid=157791) Source: cusparselt_compress.cu
(EngineCore_DP0 pid=157791) Build dir: /root/vllmbench/slidesparse/weight_convert/build
(EngineCore_DP0 pid=157791) ------------------------------------------------------------
(EngineCore_DP0 pid=157791) GPU: H100 (NVIDIA H100 PCIe)
(EngineCore_DP0 pid=157791) CC: cc90 (sm_90)
(EngineCore_DP0 pid=157791) Python: py312
(EngineCore_DP0 pid=157791) CUDA: cu129
(EngineCore_DP0 pid=157791) Arch: x86_64
(EngineCore_DP0 pid=157791) ============================================================
(EngineCore_DP0 pid=157791) ðŸ”¨ Building cusparselt_compress_H100_cc90_py312_cu129_x86_64...
(EngineCore_DP0 pid=157791) Command: /usr/local/cuda/bin/nvcc -std=c++17 -O3 -Xcompiler -fPIC --shared -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_100,code=sm_100 -gencode=arch=compute_120,code=sm_120 -gencode=arch=compute_121,code=sm_121 -I /usr/local/cuda/include /root/vllmbench/slidesparse/weight_convert/cusparselt_compress.cu -L/usr/lib/x86_64-linux-gnu -lcusparseLt -lcusparse -lcuda -o /root/vllmbench/slidesparse/weight_convert/build/cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=157791) âœ“ Built: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=157791) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=157791) WARNING 01-25 15:58:54 [backends.py:609] Failed to read file <frozen os>
Throughput: 33.86 requests/s, 17369.61 total tokens/s, 33.86 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-25 15:58:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:58:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:58:19] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:19] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:58:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:19] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:19] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:58:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:19] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:19] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:19] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:58:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:19] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:58:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:58:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:58:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:58:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:58:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:58:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:58:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:58:26] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:26] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:58:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:26] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:26] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:58:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:26] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:26] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:26] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:58:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:26] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:58:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:58:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:58:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:58:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:58:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:58:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:58:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:28] INFO gemm_wrapper.py:85: Auto-building cusparselt GEMM library from /root/vllmbench/slidesparse/csrc/cusparselt_gemm/build_cusparselt.py...
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:40] INFO gemm_wrapper.py:95: cusparselt GEMM library build completed
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:40] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=157791) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=157791) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.60it/s]
(EngineCore_DP0 pid=157791) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.60it/s]
(EngineCore_DP0 pid=157791) 
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=157791) [2026-01-25 15:58:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=157791) [2026-01-25 15:59:00] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=157791) 2026-01-25 15:59:06,369 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=157791) 2026-01-25 15:59:06,418 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=157791) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  2.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=157791) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14.84it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 54/128 [00:00<00:00, 534.55it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<00:00, 708.59it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:28,  4.48it/s, est. speed input: 2295.89 toks/s, output: 4.48 toks/s]
Processed prompts:   4%|â–         | 5/128 [00:00<00:07, 17.24it/s, est. speed input: 7538.72 toks/s, output: 14.72 toks/s]
Processed prompts:   7%|â–‹         | 9/128 [00:00<00:04, 24.52it/s, est. speed input: 10331.32 toks/s, output: 20.18 toks/s]
Processed prompts:  10%|â–ˆ         | 13/128 [00:00<00:03, 28.93it/s, est. speed input: 12047.12 toks/s, output: 23.53 toks/s]
Processed prompts:  13%|â–ˆâ–Ž        | 17/128 [00:00<00:03, 31.75it/s, est. speed input: 13207.99 toks/s, output: 25.80 toks/s]
Processed prompts:  16%|â–ˆâ–‹        | 21/128 [00:00<00:03, 33.56it/s, est. speed input: 14038.98 toks/s, output: 27.42 toks/s]
Processed prompts:  20%|â–ˆâ–‰        | 25/128 [00:00<00:02, 34.82it/s, est. speed input: 14674.73 toks/s, output: 28.66 toks/s]
Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 29/128 [00:00<00:02, 35.70it/s, est. speed input: 15176.52 toks/s, output: 29.64 toks/s]
Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 33/128 [00:01<00:02, 36.29it/s, est. speed input: 15577.21 toks/s, output: 30.42 toks/s]
Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 37/128 [00:01<00:02, 36.70it/s, est. speed input: 15906.63 toks/s, output: 31.07 toks/s]
Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 41/128 [00:01<00:02, 37.01it/s, est. speed input: 16184.97 toks/s, output: 31.61 toks/s]
Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 45/128 [00:01<00:02, 37.23it/s, est. speed input: 16422.84 toks/s, output: 32.07 toks/s]
Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 49/128 [00:01<00:02, 37.33it/s, est. speed input: 16620.99 toks/s, output: 32.46 toks/s]
Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 53/128 [00:01<00:02, 37.47it/s, est. speed input: 16799.81 toks/s, output: 32.81 toks/s]
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 57/128 [00:01<00:01, 37.58it/s, est. speed input: 16957.80 toks/s, output: 33.12 toks/s]
Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 61/128 [00:01<00:01, 37.60it/s, est. speed input: 17092.30 toks/s, output: 33.38 toks/s]
Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 65/128 [00:01<00:01, 37.63it/s, est. speed input: 17214.23 toks/s, output: 33.62 toks/s]
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 69/128 [00:02<00:01, 37.69it/s, est. speed input: 17325.31 toks/s, output: 33.84 toks/s]
Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 73/128 [00:02<00:01, 37.67it/s, est. speed input: 17421.63 toks/s, output: 34.03 toks/s]
Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 77/128 [00:02<00:01, 37.68it/s, est. speed input: 17509.97 toks/s, output: 34.20 toks/s]
Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 81/128 [00:02<00:01, 37.74it/s, est. speed input: 17594.80 toks/s, output: 34.36 toks/s]
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 85/128 [00:02<00:01, 37.78it/s, est. speed input: 17671.30 toks/s, output: 34.51 toks/s]
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 89/128 [00:02<00:01, 37.84it/s, est. speed input: 17744.47 toks/s, output: 34.66 toks/s]
Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 93/128 [00:02<00:00, 37.90it/s, est. speed input: 17812.33 toks/s, output: 34.79 toks/s]
Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 97/128 [00:02<00:00, 37.91it/s, est. speed input: 17873.38 toks/s, output: 34.91 toks/s]
Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 101/128 [00:02<00:00, 37.89it/s, est. speed input: 17928.64 toks/s, output: 35.02 toks/s]
Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 105/128 [00:02<00:00, 37.87it/s, est. speed input: 17979.52 toks/s, output: 35.12 toks/s]
Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 109/128 [00:03<00:00, 37.86it/s, est. speed input: 18026.93 toks/s, output: 35.21 toks/s]
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 113/128 [00:03<00:00, 37.84it/s, est. speed input: 18070.89 toks/s, output: 35.29 toks/s]
Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 117/128 [00:03<00:00, 37.85it/s, est. speed input: 18113.14 toks/s, output: 35.38 toks/s]
Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 121/128 [00:03<00:00, 37.83it/s, est. speed input: 18151.25 toks/s, output: 35.45 toks/s]
Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 125/128 [00:03<00:00, 37.82it/s, est. speed input: 18187.46 toks/s, output: 35.52 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:03<00:00, 37.82it/s, est. speed input: 18214.03 toks/s, output: 35.57 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:03<00:00, 35.57it/s, est. speed input: 18214.03 toks/s, output: 35.57 toks/s]
[rank0]:[W125 15:59:13.519654195 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-25 15:59:15
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 15:59:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=159648) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=159648) WARNING 01-25 15:59:38 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.21 requests/s, 36092.39 total tokens/s, 35.21 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-25 15:59:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:59:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:59:22] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:22] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:59:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:22] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:22] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:59:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:22] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:22] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:22] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:59:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:22] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:59:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:59:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:59:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:59:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:59:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 15:59:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 15:59:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 15:59:29] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:29] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 15:59:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:29] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:29] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 15:59:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:29] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:29] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:29] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 15:59:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:29] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 15:59:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 15:59:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 15:59:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 15:59:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 15:59:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 15:59:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 15:59:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=159648) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=159648) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.58it/s]
(EngineCore_DP0 pid=159648) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.58it/s]
(EngineCore_DP0 pid=159648) 
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=159648) [2026-01-25 15:59:44] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=159648) 2026-01-25 15:59:50,133 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=159648) 2026-01-25 15:59:50,182 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=159648) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.62it/s]
(EngineCore_DP0 pid=159648) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 17.44it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  21%|â–ˆâ–ˆ        | 27/128 [00:00<00:00, 265.02it/s]
Adding requests:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 80/128 [00:00<00:00, 418.81it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<00:00, 431.98it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|â–‹         | 9/128 [00:00<00:01, 72.46it/s, est. speed input: 74210.81 toks/s, output: 72.46 toks/s]
Processed prompts:  13%|â–ˆâ–Ž        | 17/128 [00:00<00:02, 47.23it/s, est. speed input: 51203.93 toks/s, output: 50.00 toks/s]
Processed prompts:  18%|â–ˆâ–Š        | 23/128 [00:00<00:02, 42.85it/s, est. speed input: 46949.10 toks/s, output: 45.85 toks/s]
Processed prompts:  22%|â–ˆâ–ˆâ–       | 28/128 [00:00<00:02, 40.92it/s, est. speed input: 45066.56 toks/s, output: 44.01 toks/s]
Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 33/128 [00:00<00:02, 39.70it/s, est. speed input: 43848.99 toks/s, output: 42.82 toks/s]
Processed prompts:  30%|â–ˆâ–ˆâ–‰       | 38/128 [00:00<00:02, 38.92it/s, est. speed input: 42999.67 toks/s, output: 41.99 toks/s]
Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 42/128 [00:01<00:02, 38.48it/s, est. speed input: 42488.37 toks/s, output: 41.49 toks/s]
Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 46/128 [00:01<00:02, 38.05it/s, est. speed input: 42034.71 toks/s, output: 41.05 toks/s]
Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 50/128 [00:01<00:02, 37.70it/s, est. speed input: 41652.61 toks/s, output: 40.68 toks/s]
Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 54/128 [00:01<00:01, 37.53it/s, est. speed input: 41356.93 toks/s, output: 40.39 toks/s]
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 58/128 [00:01<00:01, 37.40it/s, est. speed input: 41106.04 toks/s, output: 40.14 toks/s]
Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 62/128 [00:01<00:01, 37.32it/s, est. speed input: 40892.28 toks/s, output: 39.93 toks/s]
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 66/128 [00:01<00:01, 37.23it/s, est. speed input: 40697.89 toks/s, output: 39.74 toks/s]
Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 70/128 [00:01<00:01, 37.19it/s, est. speed input: 40532.78 toks/s, output: 39.58 toks/s]
Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 74/128 [00:01<00:01, 37.17it/s, est. speed input: 40387.93 toks/s, output: 39.44 toks/s]
Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 78/128 [00:01<00:01, 37.17it/s, est. speed input: 40262.13 toks/s, output: 39.32 toks/s]
Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 82/128 [00:02<00:01, 37.13it/s, est. speed input: 40142.33 toks/s, output: 39.20 toks/s]
Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 86/128 [00:02<00:01, 37.10it/s, est. speed input: 40033.42 toks/s, output: 39.09 toks/s]
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 90/128 [00:02<00:01, 37.11it/s, est. speed input: 39937.82 toks/s, output: 39.00 toks/s]
Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 94/128 [00:02<00:00, 37.16it/s, est. speed input: 39860.09 toks/s, output: 38.93 toks/s]
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 98/128 [00:02<00:00, 37.22it/s, est. speed input: 39791.52 toks/s, output: 38.86 toks/s]
Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 102/128 [00:02<00:00, 37.18it/s, est. speed input: 39717.93 toks/s, output: 38.79 toks/s]
Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 106/128 [00:02<00:00, 37.20it/s, est. speed input: 39656.13 toks/s, output: 38.73 toks/s]
Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 110/128 [00:02<00:00, 36.74it/s, est. speed input: 39533.87 toks/s, output: 38.61 toks/s]
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 114/128 [00:02<00:00, 36.75it/s, est. speed input: 39465.69 toks/s, output: 38.54 toks/s]
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 118/128 [00:03<00:00, 36.76it/s, est. speed input: 39401.19 toks/s, output: 38.48 toks/s]
Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 122/128 [00:03<00:00, 36.81it/s, est. speed input: 39346.70 toks/s, output: 38.42 toks/s]
Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 126/128 [00:03<00:00, 36.87it/s, est. speed input: 39299.47 toks/s, output: 38.38 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:03<00:00, 36.87it/s, est. speed input: 39277.76 toks/s, output: 38.36 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:03<00:00, 38.35it/s, est. speed input: 39277.76 toks/s, output: 38.36 toks/s]
[rank0]:[W125 15:59:55.977038661 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-25 15:59:57
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:00:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=160820) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=160820) WARNING 01-25 16:00:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 65.90 requests/s, 67545.63 total tokens/s, 65.90 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-25 16:00:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:00:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:05] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:05] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:00:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:05] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:05] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:00:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:05] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:05] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:05] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:00:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:05] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:00:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:00:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:00:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:00:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:00:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:00:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:12] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:12] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:00:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:12] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:12] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:00:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:12] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:12] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:12] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:00:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:12] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:00:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:00:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:00:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:00:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=160820) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=160820) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=160820) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=160820) 
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=160820) [2026-01-25 16:00:26] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=160820) 2026-01-25 16:00:31,347 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=160820) 2026-01-25 16:00:31,379 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=160820) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 13.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 12.78it/s]
(EngineCore_DP0 pid=160820) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  8.51it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.04it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  10%|â–‰         | 25/256 [00:00<00:00, 244.63it/s]
Adding requests:  29%|â–ˆâ–ˆâ–Š       | 73/256 [00:00<00:00, 379.51it/s]
Adding requests:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 124/256 [00:00<00:00, 437.64it/s]
Adding requests:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 174/256 [00:00<00:00, 459.91it/s]
Adding requests:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 222/256 [00:00<00:00, 466.58it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [00:00<00:00, 449.02it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|â–‰         | 24/256 [00:00<00:01, 221.24it/s, est. speed input: 226589.56 toks/s, output: 221.25 toks/s]
Processed prompts:  18%|â–ˆâ–Š        | 47/256 [00:00<00:01, 106.34it/s, est. speed input: 118310.20 toks/s, output: 115.53 toks/s]
Processed prompts:  24%|â–ˆâ–ˆâ–       | 61/256 [00:00<00:02, 92.60it/s, est. speed input: 104659.97 toks/s, output: 102.20 toks/s] 
Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 72/256 [00:00<00:02, 84.16it/s, est. speed input: 97102.28 toks/s, output: 94.82 toks/s]  
Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 82/256 [00:00<00:02, 81.14it/s, est. speed input: 93819.97 toks/s, output: 91.62 toks/s]
Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 91/256 [00:01<00:02, 81.58it/s, est. speed input: 92852.18 toks/s, output: 90.68 toks/s]
Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 100/256 [00:01<00:02, 76.94it/s, est. speed input: 89901.10 toks/s, output: 87.79 toks/s]
Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 108/256 [00:01<00:01, 75.82it/s, est. speed input: 88536.94 toks/s, output: 86.46 toks/s]
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 116/256 [00:01<00:01, 74.65it/s, est. speed input: 87288.37 toks/s, output: 85.24 toks/s]
Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 124/256 [00:01<00:01, 73.78it/s, est. speed input: 86224.09 toks/s, output: 84.20 toks/s]
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 132/256 [00:01<00:01, 73.19it/s, est. speed input: 85323.21 toks/s, output: 83.32 toks/s]
Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 140/256 [00:01<00:01, 72.78it/s, est. speed input: 84547.94 toks/s, output: 82.56 toks/s]
Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 148/256 [00:01<00:01, 72.46it/s, est. speed input: 83859.49 toks/s, output: 81.89 toks/s]
Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 156/256 [00:01<00:01, 72.21it/s, est. speed input: 83245.67 toks/s, output: 81.29 toks/s]
Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 164/256 [00:02<00:01, 72.06it/s, est. speed input: 82706.24 toks/s, output: 80.77 toks/s]
Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 172/256 [00:02<00:01, 72.02it/s, est. speed input: 82235.32 toks/s, output: 80.31 toks/s]
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 180/256 [00:02<00:01, 71.94it/s, est. speed input: 81802.93 toks/s, output: 79.88 toks/s]
Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 188/256 [00:02<00:00, 71.89it/s, est. speed input: 81410.66 toks/s, output: 79.50 toks/s]
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 196/256 [00:02<00:00, 71.82it/s, est. speed input: 81047.96 toks/s, output: 79.15 toks/s]
Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 204/256 [00:02<00:00, 71.77it/s, est. speed input: 80717.28 toks/s, output: 78.82 toks/s]
Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 212/256 [00:02<00:00, 71.74it/s, est. speed input: 80414.15 toks/s, output: 78.53 toks/s]
Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 220/256 [00:02<00:00, 71.68it/s, est. speed input: 80129.83 toks/s, output: 78.25 toks/s]
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 228/256 [00:02<00:00, 71.74it/s, est. speed input: 79880.45 toks/s, output: 78.01 toks/s]
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 236/256 [00:03<00:00, 71.72it/s, est. speed input: 79642.40 toks/s, output: 77.77 toks/s]
Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 244/256 [00:03<00:00, 71.75it/s, est. speed input: 79425.05 toks/s, output: 77.56 toks/s]
Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 252/256 [00:03<00:00, 71.65it/s, est. speed input: 79209.66 toks/s, output: 77.35 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [00:03<00:00, 71.65it/s, est. speed input: 79127.52 toks/s, output: 77.27 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [00:03<00:00, 77.27it/s, est. speed input: 79127.52 toks/s, output: 77.27 toks/s]
[rank0]:[W125 16:00:37.430297344 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-25 16:00:39
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:00:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=161973) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=161973) WARNING 01-25 16:01:04 [backends.py:609] Failed to read file <frozen os>
Throughput: 80.62 requests/s, 82633.65 total tokens/s, 80.62 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-25 16:00:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:00:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:48] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:48] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:00:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:48] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:48] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:00:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:48] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:48] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:48] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:00:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:48] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:00:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:00:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:00:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:00:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:00:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:00:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:54] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:54] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:00:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:54] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:54] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:00:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:54] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:54] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:54] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:00:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:54] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:00:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:00:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:00:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:00:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:00:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:00:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:00:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=161973) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=161973) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.51it/s]
(EngineCore_DP0 pid=161973) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.51it/s]
(EngineCore_DP0 pid=161973) 
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=161973) [2026-01-25 16:00:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=161973) [2026-01-25 16:01:09] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=161973) 2026-01-25 16:01:15,326 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=161973) 2026-01-25 16:01:15,364 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=161973) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  6.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00, 12.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 12.00it/s]
(EngineCore_DP0 pid=161973) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00, 18.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 19.37it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|â–Œ         | 30/512 [00:00<00:01, 296.94it/s]
Adding requests:  16%|â–ˆâ–Œ        | 82/512 [00:00<00:01, 426.32it/s]
Adding requests:  26%|â–ˆâ–ˆâ–Œ       | 134/512 [00:00<00:00, 465.57it/s]
Adding requests:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 184/512 [00:00<00:00, 477.22it/s]
Adding requests:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 236/512 [00:00<00:00, 492.13it/s]
Adding requests:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 287/512 [00:00<00:00, 496.63it/s]
Adding requests:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 338/512 [00:00<00:00, 498.64it/s]
Adding requests:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 391/512 [00:00<00:00, 506.65it/s]
Adding requests:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 442/512 [00:00<00:00, 507.47it/s]
Adding requests:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 494/512 [00:01<00:00, 509.32it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 512/512 [00:01<00:00, 489.66it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|â–ˆâ–        | 74/512 [00:00<00:00, 561.50it/s, est. speed input: 575066.19 toks/s, output: 561.53 toks/s]
Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 131/512 [00:00<00:02, 144.11it/s, est. speed input: 168838.58 toks/s, output: 164.88 toks/s]
Processed prompts:  31%|â–ˆâ–ˆâ–ˆâ–      | 160/512 [00:01<00:02, 121.53it/s, est. speed input: 144825.85 toks/s, output: 141.43 toks/s]
Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 180/512 [00:01<00:02, 111.31it/s, est. speed input: 134874.04 toks/s, output: 131.71 toks/s]
Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 196/512 [00:01<00:03, 104.86it/s, est. speed input: 129034.86 toks/s, output: 126.01 toks/s]
Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 209/512 [00:01<00:02, 101.78it/s, est. speed input: 125922.77 toks/s, output: 122.97 toks/s]
Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 221/512 [00:01<00:02, 97.48it/s, est. speed input: 122728.58 toks/s, output: 119.85 toks/s] 
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 232/512 [00:01<00:03, 92.17it/s, est. speed input: 119468.77 toks/s, output: 116.67 toks/s]
Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 242/512 [00:02<00:03, 86.50it/s, est. speed input: 116294.04 toks/s, output: 113.57 toks/s]
Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 254/512 [00:02<00:03, 85.99it/s, est. speed input: 114444.94 toks/s, output: 111.76 toks/s]
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 266/512 [00:02<00:02, 85.69it/s, est. speed input: 112833.10 toks/s, output: 110.19 toks/s]
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 278/512 [00:02<00:02, 85.39it/s, est. speed input: 111382.35 toks/s, output: 108.77 toks/s]
Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 290/512 [00:02<00:02, 84.93it/s, est. speed input: 110026.07 toks/s, output: 107.45 toks/s]
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 302/512 [00:02<00:02, 84.34it/s, est. speed input: 108750.70 toks/s, output: 106.20 toks/s]
Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 314/512 [00:02<00:02, 83.72it/s, est. speed input: 107553.24 toks/s, output: 105.03 toks/s]
Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 326/512 [00:03<00:02, 83.90it/s, est. speed input: 106590.16 toks/s, output: 104.09 toks/s]
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 338/512 [00:03<00:02, 84.10it/s, est. speed input: 105724.02 toks/s, output: 103.25 toks/s]
Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 350/512 [00:03<00:01, 85.57it/s, est. speed input: 105158.74 toks/s, output: 102.69 toks/s]
Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 362/512 [00:03<00:01, 85.38it/s, est. speed input: 104435.37 toks/s, output: 101.99 toks/s]
Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 374/512 [00:03<00:01, 84.98it/s, est. speed input: 103725.41 toks/s, output: 101.29 toks/s]
Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 386/512 [00:03<00:01, 84.44it/s, est. speed input: 103029.22 toks/s, output: 100.61 toks/s]
Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 398/512 [00:03<00:01, 84.30it/s, est. speed input: 102416.52 toks/s, output: 100.02 toks/s]
Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 410/512 [00:04<00:01, 84.36it/s, est. speed input: 101868.94 toks/s, output: 99.48 toks/s] 
Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 422/512 [00:04<00:01, 83.99it/s, est. speed input: 101303.97 toks/s, output: 98.93 toks/s]
Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 434/512 [00:04<00:00, 84.22it/s, est. speed input: 100837.07 toks/s, output: 98.47 toks/s]
Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 446/512 [00:04<00:00, 84.34it/s, est. speed input: 100395.02 toks/s, output: 98.04 toks/s]
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 458/512 [00:04<00:00, 85.80it/s, est. speed input: 100141.74 toks/s, output: 97.79 toks/s]
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 470/512 [00:04<00:00, 84.76it/s, est. speed input: 99667.69 toks/s, output: 97.33 toks/s] 
Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 482/512 [00:04<00:00, 84.25it/s, est. speed input: 99243.66 toks/s, output: 96.92 toks/s]
Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 494/512 [00:05<00:00, 83.85it/s, est. speed input: 98838.48 toks/s, output: 96.52 toks/s]
Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 506/512 [00:05<00:00, 83.34it/s, est. speed input: 98430.54 toks/s, output: 96.12 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 512/512 [00:05<00:00, 83.34it/s, est. speed input: 98860.40 toks/s, output: 96.54 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 512/512 [00:05<00:00, 96.54it/s, est. speed input: 98860.40 toks/s, output: 96.54 toks/s]
[rank0]:[W125 16:01:24.022361961 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-25 16:01:25
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:01:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=163192) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=163192) WARNING 01-25 16:01:52 [backends.py:609] Failed to read file <frozen os>
Throughput: 87.04 requests/s, 89217.44 total tokens/s, 87.04 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-25 16:01:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:01:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:01:36] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:36] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:01:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:36] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:36] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:01:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:36] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:36] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:36] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:01:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:36] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:01:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:01:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:01:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:01:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:01:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:01:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:01:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:01:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:44] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:01:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:44] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:01:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:44] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:01:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:44] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:01:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:01:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:01:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:01:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:01:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:01:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:01:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=163192) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=163192) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=163192) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=163192) 
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=163192) [2026-01-25 16:01:58] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=163192) 2026-01-25 16:02:03,574 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=163192) 2026-01-25 16:02:03,598 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=163192) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:01,  2.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:00,  3.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  5.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.80it/s]
(EngineCore_DP0 pid=163192) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00, 18.68it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 19.53it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|â–         | 25/1024 [00:00<00:04, 246.32it/s]
Adding requests:   7%|â–‹         | 72/1024 [00:00<00:02, 374.68it/s]
Adding requests:  12%|â–ˆâ–        | 120/1024 [00:00<00:02, 420.02it/s]
Adding requests:  16%|â–ˆâ–‹        | 167/1024 [00:00<00:01, 437.09it/s]
Adding requests:  21%|â–ˆâ–ˆ        | 214/1024 [00:00<00:01, 447.35it/s]
Adding requests:  26%|â–ˆâ–ˆâ–Œ       | 263/1024 [00:00<00:01, 459.95it/s]
Adding requests:  30%|â–ˆâ–ˆâ–ˆ       | 311/1024 [00:00<00:01, 463.43it/s]
Adding requests:  35%|â–ˆâ–ˆâ–ˆâ–      | 358/1024 [00:00<00:01, 464.95it/s]
Adding requests:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 405/1024 [00:00<00:01, 466.03it/s]
Adding requests:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 453/1024 [00:01<00:01, 467.87it/s]
Adding requests:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 500/1024 [00:01<00:01, 468.32it/s]
Adding requests:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 547/1024 [00:01<00:01, 459.97it/s]
Adding requests:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 596/1024 [00:01<00:00, 467.72it/s]
Adding requests:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 645/1024 [00:01<00:00, 473.07it/s]
Adding requests:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 695/1024 [00:01<00:00, 479.00it/s]
Adding requests:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 743/1024 [00:01<00:00, 478.43it/s]
Adding requests:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 791/1024 [00:01<00:00, 478.50it/s]
Adding requests:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 839/1024 [00:01<00:00, 466.44it/s]
Adding requests:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 889/1024 [00:01<00:00, 474.79it/s]
Adding requests:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1024 [00:02<00:00, 477.48it/s]
Adding requests:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 987/1024 [00:02<00:00, 480.51it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:02<00:00, 463.34it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|â–ˆâ–‰        | 194/1024 [00:00<00:00, 1263.75it/s, est. speed input: 1294285.79 toks/s, output: 1263.81 toks/s]
Processed prompts:  31%|â–ˆâ–ˆâ–ˆâ–      | 321/1024 [00:01<00:03, 177.85it/s, est. speed input: 215725.18 toks/s, output: 210.67 toks/s]   
Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 379/1024 [00:02<00:04, 137.57it/s, est. speed input: 172633.05 toks/s, output: 168.59 toks/s]
Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 414/1024 [00:02<00:04, 128.02it/s, est. speed input: 162216.26 toks/s, output: 158.41 toks/s]
Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 439/1024 [00:02<00:04, 120.60it/s, est. speed input: 155582.15 toks/s, output: 151.94 toks/s]
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 459/1024 [00:03<00:05, 110.87it/s, est. speed input: 148954.89 toks/s, output: 145.46 toks/s]
Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 475/1024 [00:03<00:05, 106.77it/s, est. speed input: 145688.84 toks/s, output: 142.27 toks/s]
Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 490/1024 [00:03<00:05, 101.87it/s, est. speed input: 142494.78 toks/s, output: 139.15 toks/s]
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 506/1024 [00:03<00:05, 98.62it/s, est. speed input: 139860.04 toks/s, output: 136.58 toks/s] 
Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 522/1024 [00:03<00:05, 95.81it/s, est. speed input: 137456.75 toks/s, output: 134.23 toks/s]
Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 538/1024 [00:04<00:05, 93.54it/s, est. speed input: 135270.05 toks/s, output: 132.10 toks/s]
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 554/1024 [00:04<00:05, 91.85it/s, est. speed input: 133287.18 toks/s, output: 130.16 toks/s]
Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 570/1024 [00:04<00:05, 90.65it/s, est. speed input: 131487.81 toks/s, output: 128.40 toks/s]
Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 586/1024 [00:04<00:04, 89.58it/s, est. speed input: 129794.39 toks/s, output: 126.75 toks/s]
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 602/1024 [00:04<00:04, 88.85it/s, est. speed input: 128239.95 toks/s, output: 125.23 toks/s]
Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 618/1024 [00:04<00:04, 88.35it/s, est. speed input: 126804.13 toks/s, output: 123.83 toks/s]
Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 634/1024 [00:05<00:04, 88.05it/s, est. speed input: 125480.19 toks/s, output: 122.54 toks/s]
Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 650/1024 [00:05<00:04, 87.89it/s, est. speed input: 124255.84 toks/s, output: 121.34 toks/s]
Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 666/1024 [00:05<00:04, 87.69it/s, est. speed input: 123098.11 toks/s, output: 120.21 toks/s]
Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 682/1024 [00:05<00:03, 87.53it/s, est. speed input: 122013.04 toks/s, output: 119.15 toks/s]
Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 698/1024 [00:05<00:03, 87.28it/s, est. speed input: 120975.16 toks/s, output: 118.14 toks/s]
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 714/1024 [00:06<00:03, 87.06it/s, est. speed input: 119993.20 toks/s, output: 117.18 toks/s]
Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 730/1024 [00:06<00:03, 87.22it/s, est. speed input: 119110.91 toks/s, output: 116.32 toks/s]
Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 746/1024 [00:06<00:03, 87.27it/s, est. speed input: 118271.06 toks/s, output: 115.50 toks/s]
Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 762/1024 [00:06<00:03, 87.16it/s, est. speed input: 117459.85 toks/s, output: 114.71 toks/s]
Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 778/1024 [00:06<00:02, 87.11it/s, est. speed input: 116695.60 toks/s, output: 113.96 toks/s]
Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 794/1024 [00:07<00:02, 87.04it/s, est. speed input: 115967.42 toks/s, output: 113.25 toks/s]
Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 810/1024 [00:07<00:02, 86.97it/s, est. speed input: 115273.85 toks/s, output: 112.57 toks/s]
Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 826/1024 [00:07<00:02, 87.14it/s, est. speed input: 114638.21 toks/s, output: 111.95 toks/s]
Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 842/1024 [00:07<00:02, 87.01it/s, est. speed input: 114007.71 toks/s, output: 111.34 toks/s]
Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 858/1024 [00:07<00:01, 87.20it/s, est. speed input: 113436.27 toks/s, output: 110.78 toks/s]
Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 874/1024 [00:07<00:01, 87.11it/s, est. speed input: 112869.26 toks/s, output: 110.22 toks/s]
Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 890/1024 [00:08<00:01, 87.11it/s, est. speed input: 112332.53 toks/s, output: 109.70 toks/s]
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 906/1024 [00:08<00:01, 87.12it/s, est. speed input: 111821.75 toks/s, output: 109.20 toks/s]
Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 922/1024 [00:08<00:01, 87.05it/s, est. speed input: 111325.34 toks/s, output: 108.72 toks/s]
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1024 [00:08<00:00, 88.51it/s, est. speed input: 110984.52 toks/s, output: 108.38 toks/s]
Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 954/1024 [00:08<00:00, 88.15it/s, est. speed input: 110537.16 toks/s, output: 107.95 toks/s]
Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 970/1024 [00:09<00:00, 87.74it/s, est. speed input: 110094.58 toks/s, output: 107.51 toks/s]
Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 986/1024 [00:09<00:00, 89.06it/s, est. speed input: 109801.29 toks/s, output: 107.23 toks/s]
Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1002/1024 [00:09<00:00, 88.50it/s, est. speed input: 109400.36 toks/s, output: 106.84 toks/s]
Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1018/1024 [00:09<00:00, 89.65it/s, est. speed input: 109133.28 toks/s, output: 106.58 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:09<00:00, 89.65it/s, est. speed input: 109772.13 toks/s, output: 107.20 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:09<00:00, 107.20it/s, est. speed input: 109772.13 toks/s, output: 107.20 toks/s]
[rank0]:[W125 16:02:18.688068259 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-25 16:02:20
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:02:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=164555) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=164555) WARNING 01-25 16:02:51 [backends.py:609] Failed to read file <frozen os>
Throughput: 89.46 requests/s, 91697.40 total tokens/s, 89.46 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-25 16:02:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:02:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:02:35] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:35] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:02:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:35] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:35] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:02:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:35] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:35] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:35] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:02:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:35] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:02:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:02:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:02:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:02:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:02:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:02:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:02:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:02:42] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:42] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:02:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:42] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:42] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:02:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:42] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:42] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:42] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:02:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:42] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:02:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:02:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:02:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:02:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:02:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:02:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:02:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:44] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=164555) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=164555) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=164555) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=164555) 
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=164555) [2026-01-25 16:02:57] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=164555) 2026-01-25 16:03:01,892 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=164555) 2026-01-25 16:03:01,921 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=164555) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:00<00:00,  9.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:00<00:00,  6.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:01<00:00,  3.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:01<00:00,  5.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  5.71it/s]
(EngineCore_DP0 pid=164555) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 17.70it/s]
Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00, 11.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12.77it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|â–         | 33/2048 [00:00<00:06, 329.75it/s]
Adding requests:   4%|â–         | 85/2048 [00:00<00:04, 441.12it/s]
Adding requests:   7%|â–‹         | 136/2048 [00:00<00:04, 469.41it/s]
Adding requests:   9%|â–‰         | 185/2048 [00:00<00:03, 477.10it/s]
Adding requests:  12%|â–ˆâ–        | 237/2048 [00:00<00:03, 490.37it/s]
Adding requests:  14%|â–ˆâ–        | 288/2048 [00:00<00:03, 494.17it/s]
Adding requests:  17%|â–ˆâ–‹        | 338/2048 [00:00<00:03, 495.96it/s]
Adding requests:  19%|â–ˆâ–‰        | 390/2048 [00:00<00:03, 501.39it/s]
Adding requests:  22%|â–ˆâ–ˆâ–       | 441/2048 [00:00<00:03, 502.24it/s]
Adding requests:  24%|â–ˆâ–ˆâ–       | 492/2048 [00:01<00:03, 502.86it/s]
Adding requests:  27%|â–ˆâ–ˆâ–‹       | 543/2048 [00:01<00:03, 495.18it/s]
Adding requests:  29%|â–ˆâ–ˆâ–‰       | 596/2048 [00:01<00:02, 501.64it/s]
Adding requests:  32%|â–ˆâ–ˆâ–ˆâ–      | 648/2048 [00:01<00:02, 506.79it/s]
Adding requests:  34%|â–ˆâ–ˆâ–ˆâ–      | 702/2048 [00:01<00:02, 513.55it/s]
Adding requests:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 754/2048 [00:01<00:02, 504.61it/s]
Adding requests:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 805/2048 [00:01<00:02, 498.50it/s]
Adding requests:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 855/2048 [00:01<00:02, 495.18it/s]
Adding requests:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 908/2048 [00:01<00:02, 504.10it/s]
Adding requests:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 960/2048 [00:01<00:02, 507.31it/s]
Adding requests:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1013/2048 [00:02<00:02, 512.21it/s]
Adding requests:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1065/2048 [00:02<00:01, 513.97it/s]
Adding requests:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1117/2048 [00:02<00:01, 506.86it/s]
Adding requests:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1170/2048 [00:02<00:01, 512.78it/s]
Adding requests:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1224/2048 [00:02<00:01, 519.03it/s]
Adding requests:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1276/2048 [00:02<00:01, 509.56it/s]
Adding requests:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1329/2048 [00:02<00:01, 513.81it/s]
Adding requests:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1382/2048 [00:02<00:01, 515.77it/s]
Adding requests:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1434/2048 [00:02<00:01, 516.66it/s]
Adding requests:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1487/2048 [00:02<00:01, 519.29it/s]
Adding requests:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1540/2048 [00:03<00:00, 520.87it/s]
Adding requests:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1594/2048 [00:03<00:00, 525.64it/s]
Adding requests:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1647/2048 [00:03<00:00, 525.95it/s]
Adding requests:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1700/2048 [00:03<00:00, 516.99it/s]
Adding requests:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1753/2048 [00:03<00:00, 517.99it/s]
Adding requests:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1805/2048 [00:03<00:00, 514.59it/s]
Adding requests:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1858/2048 [00:03<00:00, 516.89it/s]
Adding requests:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1910/2048 [00:03<00:00, 502.06it/s]
Adding requests:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1962/2048 [00:03<00:00, 506.97it/s]
Adding requests:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2015/2048 [00:03<00:00, 511.21it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 506.04it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|â–ˆâ–‹        | 354/2048 [00:00<00:00, 2524.61it/s, est. speed input: 2585644.48 toks/s, output: 2524.75 toks/s]
Processed prompts:  30%|â–ˆâ–ˆâ–‰       | 607/2048 [00:02<00:07, 181.03it/s, est. speed input: 221320.85 toks/s, output: 216.13 toks/s]   
Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 717/2048 [00:04<00:09, 144.59it/s, est. speed input: 180964.19 toks/s, output: 176.72 toks/s]
Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 781/2048 [00:04<00:09, 131.26it/s, est. speed input: 167613.83 toks/s, output: 163.68 toks/s]
Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 823/2048 [00:05<00:10, 120.22it/s, est. speed input: 158711.22 toks/s, output: 154.99 toks/s]
Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 853/2048 [00:05<00:10, 114.31it/s, est. speed input: 154149.40 toks/s, output: 150.54 toks/s]
Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 876/2048 [00:05<00:10, 115.87it/s, est. speed input: 153486.13 toks/s, output: 149.89 toks/s]
Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 896/2048 [00:06<00:09, 115.42it/s, est. speed input: 152349.72 toks/s, output: 148.78 toks/s]
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 913/2048 [00:06<00:10, 112.18it/s, est. speed input: 150754.19 toks/s, output: 147.22 toks/s]
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 928/2048 [00:06<00:10, 106.97it/s, est. speed input: 148932.58 toks/s, output: 145.44 toks/s]
Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 941/2048 [00:06<00:10, 101.14it/s, est. speed input: 147147.98 toks/s, output: 143.70 toks/s]
Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 953/2048 [00:06<00:11, 93.27it/s, est. speed input: 145061.77 toks/s, output: 141.66 toks/s] 
Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 963/2048 [00:06<00:12, 84.02it/s, est. speed input: 142791.37 toks/s, output: 139.44 toks/s]
Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 978/2048 [00:07<00:12, 85.30it/s, est. speed input: 141566.58 toks/s, output: 138.25 toks/s]
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 994/2048 [00:07<00:12, 86.30it/s, est. speed input: 140313.52 toks/s, output: 137.02 toks/s]
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1010/2048 [00:07<00:11, 87.15it/s, est. speed input: 139135.30 toks/s, output: 135.87 toks/s]
Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1026/2048 [00:07<00:11, 87.79it/s, est. speed input: 138015.63 toks/s, output: 134.78 toks/s]
Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1042/2048 [00:07<00:11, 88.40it/s, est. speed input: 136965.86 toks/s, output: 133.76 toks/s]
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1058/2048 [00:07<00:11, 88.72it/s, est. speed input: 135949.14 toks/s, output: 132.76 toks/s]
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1074/2048 [00:08<00:10, 88.70it/s, est. speed input: 134949.02 toks/s, output: 131.79 toks/s]
Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1090/2048 [00:08<00:10, 88.74it/s, est. speed input: 133997.92 toks/s, output: 130.86 toks/s]
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1106/2048 [00:08<00:10, 88.91it/s, est. speed input: 133102.01 toks/s, output: 129.98 toks/s]
Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1122/2048 [00:08<00:10, 89.06it/s, est. speed input: 132246.14 toks/s, output: 129.15 toks/s]
Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1138/2048 [00:08<00:10, 89.19it/s, est. speed input: 131427.91 toks/s, output: 128.35 toks/s]
Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1154/2048 [00:09<00:09, 90.64it/s, est. speed input: 130770.79 toks/s, output: 127.71 toks/s]
Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1170/2048 [00:09<00:09, 90.08it/s, est. speed input: 129992.38 toks/s, output: 126.95 toks/s]
Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1186/2048 [00:09<00:09, 89.95it/s, est. speed input: 129266.84 toks/s, output: 126.24 toks/s]
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1202/2048 [00:09<00:09, 89.80it/s, est. speed input: 128562.78 toks/s, output: 125.55 toks/s]
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1218/2048 [00:09<00:09, 89.85it/s, est. speed input: 127898.64 toks/s, output: 124.90 toks/s]
Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1234/2048 [00:09<00:09, 89.64it/s, est. speed input: 127237.41 toks/s, output: 124.25 toks/s]
Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1250/2048 [00:10<00:08, 89.40it/s, est. speed input: 126591.02 toks/s, output: 123.62 toks/s]
Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1266/2048 [00:10<00:08, 89.27it/s, est. speed input: 125971.19 toks/s, output: 123.02 toks/s]
Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1282/2048 [00:10<00:08, 89.32it/s, est. speed input: 125383.37 toks/s, output: 122.44 toks/s]
Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1298/2048 [00:10<00:08, 89.36it/s, est. speed input: 124815.85 toks/s, output: 121.89 toks/s]
Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1314/2048 [00:10<00:08, 89.23it/s, est. speed input: 124255.49 toks/s, output: 121.34 toks/s]
Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1330/2048 [00:11<00:08, 89.07it/s, est. speed input: 123707.60 toks/s, output: 120.81 toks/s]
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1346/2048 [00:11<00:07, 89.02it/s, est. speed input: 123181.87 toks/s, output: 120.29 toks/s]
Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1362/2048 [00:11<00:07, 89.02it/s, est. speed input: 122675.89 toks/s, output: 119.80 toks/s]
Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1378/2048 [00:11<00:07, 89.16it/s, est. speed input: 122195.83 toks/s, output: 119.33 toks/s]
Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1394/2048 [00:11<00:07, 89.21it/s, est. speed input: 121726.11 toks/s, output: 118.87 toks/s]
Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1410/2048 [00:11<00:07, 89.11it/s, est. speed input: 121261.92 toks/s, output: 118.42 toks/s]
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1426/2048 [00:12<00:06, 89.20it/s, est. speed input: 120821.95 toks/s, output: 117.99 toks/s]
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1442/2048 [00:12<00:06, 89.25it/s, est. speed input: 120393.71 toks/s, output: 117.57 toks/s]
Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1458/2048 [00:12<00:06, 89.36it/s, est. speed input: 119983.35 toks/s, output: 117.17 toks/s]
Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1474/2048 [00:12<00:06, 89.47it/s, est. speed input: 119586.90 toks/s, output: 116.78 toks/s]
Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1490/2048 [00:12<00:06, 89.54it/s, est. speed input: 119199.77 toks/s, output: 116.41 toks/s]
Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1506/2048 [00:12<00:06, 89.50it/s, est. speed input: 118818.82 toks/s, output: 116.03 toks/s]
Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1522/2048 [00:13<00:05, 89.55it/s, est. speed input: 118452.77 toks/s, output: 115.68 toks/s]
Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1538/2048 [00:13<00:05, 89.34it/s, est. speed input: 118081.95 toks/s, output: 115.31 toks/s]
Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1554/2048 [00:13<00:05, 89.37it/s, est. speed input: 117731.13 toks/s, output: 114.97 toks/s]
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1570/2048 [00:13<00:05, 89.36it/s, est. speed input: 117388.11 toks/s, output: 114.64 toks/s]
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1586/2048 [00:13<00:05, 90.75it/s, est. speed input: 117131.37 toks/s, output: 114.39 toks/s]
Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1602/2048 [00:14<00:04, 90.39it/s, est. speed input: 116807.66 toks/s, output: 114.07 toks/s]
Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1618/2048 [00:14<00:04, 90.04it/s, est. speed input: 116487.33 toks/s, output: 113.76 toks/s]
Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1634/2048 [00:14<00:04, 89.90it/s, est. speed input: 116179.88 toks/s, output: 113.46 toks/s]
Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1650/2048 [00:14<00:04, 89.90it/s, est. speed input: 115885.61 toks/s, output: 113.17 toks/s]
Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1666/2048 [00:14<00:04, 89.71it/s, est. speed input: 115588.46 toks/s, output: 112.88 toks/s]
Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1682/2048 [00:14<00:04, 89.59it/s, est. speed input: 115298.72 toks/s, output: 112.60 toks/s]
Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1698/2048 [00:15<00:03, 89.45it/s, est. speed input: 115013.64 toks/s, output: 112.32 toks/s]
Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1714/2048 [00:15<00:03, 89.39it/s, est. speed input: 114736.91 toks/s, output: 112.05 toks/s]
Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1730/2048 [00:15<00:03, 89.34it/s, est. speed input: 114466.00 toks/s, output: 111.78 toks/s]
Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1746/2048 [00:15<00:03, 89.29it/s, est. speed input: 114200.66 toks/s, output: 111.52 toks/s]
Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1762/2048 [00:15<00:03, 89.37it/s, est. speed input: 113946.83 toks/s, output: 111.28 toks/s]
Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1778/2048 [00:16<00:03, 89.27it/s, est. speed input: 113691.02 toks/s, output: 111.03 toks/s]
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1794/2048 [00:16<00:02, 89.20it/s, est. speed input: 113441.23 toks/s, output: 110.78 toks/s]
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1810/2048 [00:16<00:02, 89.21it/s, est. speed input: 113199.54 toks/s, output: 110.55 toks/s]
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1826/2048 [00:16<00:02, 89.17it/s, est. speed input: 112960.96 toks/s, output: 110.31 toks/s]
Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1842/2048 [00:16<00:02, 89.30it/s, est. speed input: 112734.82 toks/s, output: 110.09 toks/s]
Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1858/2048 [00:16<00:02, 89.19it/s, est. speed input: 112504.13 toks/s, output: 109.87 toks/s]
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1874/2048 [00:17<00:01, 90.77it/s, est. speed input: 112350.37 toks/s, output: 109.72 toks/s]
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1890/2048 [00:17<00:01, 90.43it/s, est. speed input: 112138.03 toks/s, output: 109.51 toks/s]
Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1906/2048 [00:17<00:01, 90.25it/s, est. speed input: 111932.30 toks/s, output: 109.31 toks/s]
Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1922/2048 [00:17<00:01, 89.95it/s, est. speed input: 111723.13 toks/s, output: 109.10 toks/s]
Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1938/2048 [00:17<00:01, 89.81it/s, est. speed input: 111521.46 toks/s, output: 108.91 toks/s]
Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1954/2048 [00:17<00:01, 91.15it/s, est. speed input: 111381.70 toks/s, output: 108.77 toks/s]
Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1970/2048 [00:18<00:00, 90.77it/s, est. speed input: 111192.19 toks/s, output: 108.59 toks/s]
Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1986/2048 [00:18<00:00, 90.55it/s, est. speed input: 111007.93 toks/s, output: 108.41 toks/s]
Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2002/2048 [00:18<00:00, 90.25it/s, est. speed input: 110821.89 toks/s, output: 108.22 toks/s]
Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2018/2048 [00:18<00:00, 90.04it/s, est. speed input: 110638.60 toks/s, output: 108.05 toks/s]
Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2034/2048 [00:18<00:00, 92.02it/s, est. speed input: 110539.49 toks/s, output: 107.95 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:18<00:00, 92.02it/s, est. speed input: 111296.28 toks/s, output: 108.69 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:18<00:00, 108.69it/s, est. speed input: 111296.28 toks/s, output: 108.69 toks/s]
[rank0]:[W125 16:03:28.277395840 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-25 16:03:30
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:03:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=166168) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=166168) WARNING 01-25 16:04:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 90.41 requests/s, 92670.95 total tokens/s, 90.41 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-25 16:03:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:03:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:03:53] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:03:53] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:03:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:03:53] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:03:53] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:03:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:03:53] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:03:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:03:53] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:03:53] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:03:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:03:53] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:03:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:03:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:03:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:03:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:03:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:03:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:03:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:04:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:04:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:04:00] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:04:00] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:04:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:04:00] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:04:00] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:04:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:04:00] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:04:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:04:00] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:04:00] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:04:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:04:00] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:04:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:04:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:04:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:04:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:04:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:04:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:04:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=166168) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=166168) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=166168) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=166168) 
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=166168) [rank0]:W0125 16:04:15.357000 166168 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=166168) [rank0]:W0125 16:04:15.441000 166168 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=166168) [2026-01-25 16:04:15] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=166168) [rank0]:W0125 16:04:16.372000 166168 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=166168) [rank0]:W0125 16:04:16.493000 166168 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=166168) 2026-01-25 16:04:20,439 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=166168) 2026-01-25 16:04:20,466 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=166168) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 2/11 [00:00<00:00, 16.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 4/11 [00:00<00:00, 11.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 6/11 [00:00<00:00, 11.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 8/11 [00:00<00:00,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 10/11 [00:01<00:00,  5.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:01<00:00,  7.79it/s]
(EngineCore_DP0 pid=166168) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|â–ˆâ–        | 1/7 [00:00<00:00,  6.49it/s]
Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:00<00:00,  8.10it/s]
Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:00<00:00, 11.91it/s]
Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:00<00:00, 10.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 10.62it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 34/4096 [00:00<00:12, 336.49it/s]
Adding requests:   2%|â–         | 86/4096 [00:00<00:09, 441.27it/s]
Adding requests:   3%|â–Ž         | 137/4096 [00:00<00:08, 470.86it/s]
Adding requests:   5%|â–         | 188/4096 [00:00<00:08, 475.97it/s]
Adding requests:   6%|â–Œ         | 240/4096 [00:00<00:07, 488.86it/s]
Adding requests:   7%|â–‹         | 292/4096 [00:00<00:07, 496.70it/s]
Adding requests:   8%|â–Š         | 343/4096 [00:00<00:07, 498.72it/s]
Adding requests:  10%|â–‰         | 396/4096 [00:00<00:07, 508.39it/s]
Adding requests:  11%|â–ˆ         | 448/4096 [00:00<00:07, 509.97it/s]
Adding requests:  12%|â–ˆâ–        | 500/4096 [00:01<00:07, 508.34it/s]
Adding requests:  13%|â–ˆâ–Ž        | 551/4096 [00:01<00:07, 504.79it/s]
Adding requests:  15%|â–ˆâ–        | 603/4096 [00:01<00:06, 507.98it/s]
Adding requests:  16%|â–ˆâ–Œ        | 657/4096 [00:01<00:06, 516.38it/s]
Adding requests:  17%|â–ˆâ–‹        | 711/4096 [00:01<00:06, 522.57it/s]
Adding requests:  19%|â–ˆâ–Š        | 764/4096 [00:01<00:06, 520.45it/s]
Adding requests:  20%|â–ˆâ–‰        | 817/4096 [00:01<00:06, 511.09it/s]
Adding requests:  21%|â–ˆâ–ˆ        | 869/4096 [00:01<00:06, 511.89it/s]
Adding requests:  23%|â–ˆâ–ˆâ–Ž       | 922/4096 [00:01<00:06, 517.00it/s]
Adding requests:  24%|â–ˆâ–ˆâ–       | 975/4096 [00:01<00:06, 519.34it/s]
Adding requests:  25%|â–ˆâ–ˆâ–Œ       | 1028/4096 [00:02<00:05, 521.87it/s]
Adding requests:  26%|â–ˆâ–ˆâ–‹       | 1081/4096 [00:02<00:05, 519.61it/s]
Adding requests:  28%|â–ˆâ–ˆâ–Š       | 1133/4096 [00:02<00:05, 517.20it/s]
Adding requests:  29%|â–ˆâ–ˆâ–‰       | 1185/4096 [00:02<00:05, 511.95it/s]
Adding requests:  30%|â–ˆâ–ˆâ–ˆ       | 1238/4096 [00:02<00:05, 515.87it/s]
Adding requests:  31%|â–ˆâ–ˆâ–ˆâ–      | 1290/4096 [00:02<00:05, 514.26it/s]
Adding requests:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1344/4096 [00:02<00:05, 521.56it/s]
Adding requests:  34%|â–ˆâ–ˆâ–ˆâ–      | 1398/4096 [00:02<00:05, 526.28it/s]
Adding requests:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1451/4096 [00:02<00:05, 525.29it/s]
Adding requests:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1506/4096 [00:02<00:04, 528.73it/s]
Adding requests:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1560/4096 [00:03<00:04, 530.69it/s]
Adding requests:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1615/4096 [00:03<00:04, 533.62it/s]
Adding requests:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1669/4096 [00:03<00:04, 529.30it/s]
Adding requests:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1723/4096 [00:03<00:04, 531.87it/s]
Adding requests:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1777/4096 [00:03<00:04, 528.17it/s]
Adding requests:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1830/4096 [00:03<00:04, 527.63it/s]
Adding requests:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1883/4096 [00:03<00:04, 527.33it/s]
Adding requests:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1936/4096 [00:03<00:04, 526.52it/s]
Adding requests:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1989/4096 [00:03<00:04, 524.73it/s]
Adding requests:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2043/4096 [00:03<00:03, 527.45it/s]
Adding requests:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2097/4096 [00:04<00:03, 529.25it/s]
Adding requests:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2150/4096 [00:04<00:03, 523.54it/s]
Adding requests:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2203/4096 [00:04<00:03, 518.60it/s]
Adding requests:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2258/4096 [00:04<00:03, 525.89it/s]
Adding requests:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2311/4096 [00:04<00:03, 523.08it/s]
Adding requests:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2364/4096 [00:04<00:03, 519.77it/s]
Adding requests:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2416/4096 [00:04<00:03, 505.78it/s]
Adding requests:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2468/4096 [00:04<00:03, 509.51it/s]
Adding requests:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2520/4096 [00:04<00:03, 512.54it/s]
Adding requests:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2575/4096 [00:04<00:02, 520.75it/s]
Adding requests:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2628/4096 [00:05<00:02, 522.13it/s]
Adding requests:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2682/4096 [00:05<00:02, 525.61it/s]
Adding requests:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2735/4096 [00:05<00:02, 521.37it/s]
Adding requests:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2788/4096 [00:05<00:02, 522.32it/s]
Adding requests:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2841/4096 [00:05<00:02, 521.10it/s]
Adding requests:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2895/4096 [00:05<00:02, 525.41it/s]
Adding requests:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2948/4096 [00:05<00:02, 519.49it/s]
Adding requests:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3001/4096 [00:05<00:02, 520.94it/s]
Adding requests:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3054/4096 [00:05<00:01, 522.21it/s]
Adding requests:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3107/4096 [00:06<00:01, 519.27it/s]
Adding requests:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3160/4096 [00:06<00:01, 521.28it/s]
Adding requests:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3213/4096 [00:06<00:01, 521.73it/s]
Adding requests:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3267/4096 [00:06<00:01, 525.68it/s]
Adding requests:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3320/4096 [00:06<00:01, 525.93it/s]
Adding requests:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3373/4096 [00:06<00:01, 526.41it/s]
Adding requests:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3427/4096 [00:06<00:01, 528.46it/s]
Adding requests:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3480/4096 [00:06<00:01, 518.52it/s]
Adding requests:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3532/4096 [00:06<00:01, 518.45it/s]
Adding requests:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3585/4096 [00:06<00:00, 519.47it/s]
Adding requests:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3637/4096 [00:07<00:00, 519.53it/s]
Adding requests:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3690/4096 [00:07<00:00, 521.87it/s]
Adding requests:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3743/4096 [00:07<00:00, 506.05it/s]
Adding requests:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3799/4096 [00:07<00:00, 518.77it/s]
Adding requests:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3853/4096 [00:07<00:00, 523.10it/s]
Adding requests:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3906/4096 [00:07<00:00, 523.85it/s]
Adding requests:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3960/4096 [00:07<00:00, 526.52it/s]
Adding requests:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4013/4096 [00:07<00:00, 526.92it/s]
Adding requests:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4066/4096 [00:07<00:00, 520.73it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4096/4096 [00:07<00:00, 517.84it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|â–ˆâ–‹        | 691/4096 [00:00<00:00, 5921.57it/s, est. speed input: 6064937.13 toks/s, output: 5921.92 toks/s]
Processed prompts:  31%|â–ˆâ–ˆâ–ˆâ–      | 1284/4096 [00:06<00:16, 167.60it/s, est. speed input: 203561.43 toks/s, output: 198.79 toks/s]  
Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1534/4096 [00:09<00:18, 136.78it/s, est. speed input: 168903.18 toks/s, output: 164.94 toks/s]
Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1674/4096 [00:10<00:18, 128.51it/s, est. speed input: 159898.84 toks/s, output: 156.15 toks/s]
Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1763/4096 [00:11<00:19, 120.02it/s, est. speed input: 153167.18 toks/s, output: 149.58 toks/s]
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1823/4096 [00:12<00:19, 114.63it/s, est. speed input: 149385.72 toks/s, output: 145.88 toks/s]
Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1866/4096 [00:12<00:19, 115.27it/s, est. speed input: 148687.11 toks/s, output: 145.20 toks/s]
Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1899/4096 [00:13<00:19, 112.97it/s, est. speed input: 147375.79 toks/s, output: 143.92 toks/s]
Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1925/4096 [00:13<00:20, 107.10it/s, est. speed input: 145497.04 toks/s, output: 142.09 toks/s]
Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1945/4096 [00:13<00:21, 98.70it/s, est. speed input: 143370.39 toks/s, output: 140.01 toks/s] 
Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1971/4096 [00:14<00:22, 93.65it/s, est. speed input: 141686.38 toks/s, output: 138.37 toks/s]
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2003/4096 [00:14<00:22, 93.02it/s, est. speed input: 140510.25 toks/s, output: 137.22 toks/s]
Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2035/4096 [00:14<00:22, 92.29it/s, est. speed input: 139360.78 toks/s, output: 136.09 toks/s]
Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2067/4096 [00:15<00:22, 91.79it/s, est. speed input: 138277.80 toks/s, output: 135.04 toks/s]
Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2099/4096 [00:15<00:21, 91.52it/s, est. speed input: 137257.64 toks/s, output: 134.04 toks/s]
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2131/4096 [00:16<00:21, 91.10it/s, est. speed input: 136257.02 toks/s, output: 133.06 toks/s]
Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2163/4096 [00:16<00:21, 90.84it/s, est. speed input: 135304.63 toks/s, output: 132.13 toks/s]
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2195/4096 [00:16<00:20, 90.78it/s, est. speed input: 134408.07 toks/s, output: 131.26 toks/s]
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2227/4096 [00:17<00:20, 92.24it/s, est. speed input: 133701.82 toks/s, output: 130.57 toks/s]
Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2259/4096 [00:17<00:20, 91.63it/s, est. speed input: 132859.33 toks/s, output: 129.75 toks/s]
Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2291/4096 [00:17<00:19, 92.04it/s, est. speed input: 132131.56 toks/s, output: 129.03 toks/s]
Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2323/4096 [00:18<00:19, 92.20it/s, est. speed input: 131418.59 toks/s, output: 128.34 toks/s]
Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2355/4096 [00:18<00:18, 91.65it/s, est. speed input: 130672.34 toks/s, output: 127.61 toks/s]
Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2387/4096 [00:18<00:18, 91.15it/s, est. speed input: 129944.23 toks/s, output: 126.90 toks/s]
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2419/4096 [00:19<00:18, 90.93it/s, est. speed input: 129254.22 toks/s, output: 126.22 toks/s]
Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2451/4096 [00:19<00:18, 90.79it/s, est. speed input: 128590.95 toks/s, output: 125.58 toks/s]
Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2483/4096 [00:19<00:17, 91.39it/s, est. speed input: 128008.88 toks/s, output: 125.01 toks/s]
Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2515/4096 [00:20<00:17, 91.04it/s, est. speed input: 127383.83 toks/s, output: 124.40 toks/s]
Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2547/4096 [00:20<00:17, 90.88it/s, est. speed input: 126787.43 toks/s, output: 123.82 toks/s]
Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2579/4096 [00:20<00:16, 91.37it/s, est. speed input: 126257.78 toks/s, output: 123.30 toks/s]
Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2611/4096 [00:21<00:16, 91.09it/s, est. speed input: 125698.27 toks/s, output: 122.75 toks/s]
Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2643/4096 [00:21<00:15, 90.91it/s, est. speed input: 125158.23 toks/s, output: 122.22 toks/s]
Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2675/4096 [00:21<00:15, 90.74it/s, est. speed input: 124632.24 toks/s, output: 121.71 toks/s]
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2707/4096 [00:22<00:15, 90.51it/s, est. speed input: 124114.47 toks/s, output: 121.21 toks/s]
Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2739/4096 [00:22<00:15, 90.40it/s, est. speed input: 123617.16 toks/s, output: 120.72 toks/s]
Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2771/4096 [00:23<00:14, 90.42it/s, est. speed input: 123141.10 toks/s, output: 120.25 toks/s]
Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2803/4096 [00:23<00:14, 90.37it/s, est. speed input: 122676.09 toks/s, output: 119.80 toks/s]
Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2835/4096 [00:23<00:13, 90.26it/s, est. speed input: 122218.87 toks/s, output: 119.35 toks/s]
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2867/4096 [00:24<00:13, 90.20it/s, est. speed input: 121776.65 toks/s, output: 118.92 toks/s]
Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2899/4096 [00:24<00:13, 90.18it/s, est. speed input: 121348.95 toks/s, output: 118.50 toks/s]
Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2931/4096 [00:24<00:12, 90.20it/s, est. speed input: 120935.37 toks/s, output: 118.10 toks/s]
Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2963/4096 [00:25<00:12, 90.17it/s, est. speed input: 120531.26 toks/s, output: 117.71 toks/s]
Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2995/4096 [00:25<00:12, 90.11it/s, est. speed input: 120135.67 toks/s, output: 117.32 toks/s]
Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3027/4096 [00:25<00:11, 90.21it/s, est. speed input: 119759.07 toks/s, output: 116.95 toks/s]
Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3059/4096 [00:26<00:11, 90.20it/s, est. speed input: 119388.12 toks/s, output: 116.59 toks/s]
Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3091/4096 [00:26<00:11, 90.08it/s, est. speed input: 119020.49 toks/s, output: 116.23 toks/s]
Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3123/4096 [00:26<00:10, 90.84it/s, est. speed input: 118711.03 toks/s, output: 115.93 toks/s]
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3155/4096 [00:27<00:10, 90.60it/s, est. speed input: 118365.94 toks/s, output: 115.59 toks/s]
Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3187/4096 [00:27<00:10, 90.29it/s, est. speed input: 118021.92 toks/s, output: 115.26 toks/s]
Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3219/4096 [00:28<00:09, 90.20it/s, est. speed input: 117693.20 toks/s, output: 114.93 toks/s]
Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3251/4096 [00:28<00:09, 90.32it/s, est. speed input: 117382.87 toks/s, output: 114.63 toks/s]
Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3283/4096 [00:28<00:09, 90.09it/s, est. speed input: 117063.50 toks/s, output: 114.32 toks/s]
Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3315/4096 [00:29<00:08, 90.10it/s, est. speed input: 116760.73 toks/s, output: 114.02 toks/s]
Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3347/4096 [00:29<00:08, 90.14it/s, est. speed input: 116467.45 toks/s, output: 113.74 toks/s]
Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3379/4096 [00:29<00:07, 90.03it/s, est. speed input: 116173.86 toks/s, output: 113.45 toks/s]
Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3411/4096 [00:30<00:07, 90.13it/s, est. speed input: 115895.96 toks/s, output: 113.18 toks/s]
Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3443/4096 [00:30<00:07, 90.09it/s, est. speed input: 115618.96 toks/s, output: 112.91 toks/s]
Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3475/4096 [00:30<00:06, 89.90it/s, est. speed input: 115340.89 toks/s, output: 112.64 toks/s]
Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3507/4096 [00:31<00:06, 90.04it/s, est. speed input: 115082.02 toks/s, output: 112.38 toks/s]
Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3539/4096 [00:31<00:06, 90.04it/s, est. speed input: 114824.23 toks/s, output: 112.13 toks/s]
Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3571/4096 [00:31<00:05, 89.93it/s, est. speed input: 114566.96 toks/s, output: 111.88 toks/s]
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3603/4096 [00:32<00:05, 90.09it/s, est. speed input: 114326.80 toks/s, output: 111.65 toks/s]
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3635/4096 [00:32<00:05, 89.97it/s, est. speed input: 114080.76 toks/s, output: 111.41 toks/s]
Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3667/4096 [00:32<00:04, 90.01it/s, est. speed input: 113845.81 toks/s, output: 111.18 toks/s]
Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3699/4096 [00:33<00:04, 90.12it/s, est. speed input: 113619.81 toks/s, output: 110.96 toks/s]
Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3731/4096 [00:33<00:04, 90.70it/s, est. speed input: 113420.30 toks/s, output: 110.76 toks/s]
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3763/4096 [00:34<00:03, 90.55it/s, est. speed input: 113201.03 toks/s, output: 110.55 toks/s]
Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3795/4096 [00:34<00:03, 90.35it/s, est. speed input: 112981.97 toks/s, output: 110.33 toks/s]
Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3827/4096 [00:34<00:02, 89.97it/s, est. speed input: 112757.40 toks/s, output: 110.11 toks/s]
Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3859/4096 [00:35<00:02, 89.98it/s, est. speed input: 112548.80 toks/s, output: 109.91 toks/s]
Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3891/4096 [00:35<00:02, 90.06it/s, est. speed input: 112347.35 toks/s, output: 109.71 toks/s]
Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3923/4096 [00:35<00:01, 89.93it/s, est. speed input: 112142.47 toks/s, output: 109.51 toks/s]
Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3955/4096 [00:36<00:01, 90.09it/s, est. speed input: 111951.48 toks/s, output: 109.33 toks/s]
Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3987/4096 [00:36<00:01, 89.92it/s, est. speed input: 111753.45 toks/s, output: 109.13 toks/s]
Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4019/4096 [00:36<00:00, 90.59it/s, est. speed input: 111589.99 toks/s, output: 108.97 toks/s]
Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4051/4096 [00:37<00:00, 90.60it/s, est. speed input: 111411.93 toks/s, output: 108.80 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4083/4096 [00:37<00:00, 108.67it/s, est. speed input: 111819.04 toks/s, output: 109.20 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4096/4096 [00:37<00:00, 108.67it/s, est. speed input: 112173.46 toks/s, output: 109.54 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4096/4096 [00:37<00:00, 109.54it/s, est. speed input: 112173.46 toks/s, output: 109.54 toks/s]
[rank0]:[W125 16:05:09.758097722 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-25 16:05:11
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 16:05:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=168312) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=168312) WARNING 01-25 16:06:08 [backends.py:609] Failed to read file <frozen os>
Throughput: 90.46 requests/s, 92717.30 total tokens/s, 90.46 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-25 16:05:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:05:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:05:52] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:52] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:05:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:52] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:52] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:05:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:52] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:52] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:52] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:05:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:52] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:05:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:05:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:05:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:05:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:05:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 16:05:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 16:05:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 16:05:59] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:59] INFO kernels.py:78: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 16:05:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:59] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:59] INFO kernels.py:78: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 16:05:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:59] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:59] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:59] INFO kernels.py:78: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 16:05:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:59] WARNING kernels.py:115: Tuned kernel not found for Llama3.2-3B-INT8, using basic kernel
[2026-01-25 16:05:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 16:05:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 16:05:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 16:05:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 16:05:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 16:05:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 16:05:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=168312) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=168312) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.57it/s]
(EngineCore_DP0 pid=168312) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.57it/s]
(EngineCore_DP0 pid=168312) 
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=168312) [rank0]:W0125 16:06:14.372000 168312 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=168312) [rank0]:W0125 16:06:14.454000 168312 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=168312) [2026-01-25 16:06:14] WARNING gemm_wrapper.py:391: No cuSPARSELt config for model 'Llama3.2-3B-INT8', using default algorithm
(EngineCore_DP0 pid=168312) [rank0]:W0125 16:06:15.694000 168312 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=168312) [rank0]:W0125 16:06:15.815000 168312 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=168312) 2026-01-25 16:06:20,530 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=168312) 2026-01-25 16:06:20,595 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=168312) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|â–Œ         | 1/19 [00:00<00:03,  4.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 3/19 [00:00<00:02,  5.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|â–ˆâ–ˆ        | 4/19 [00:00<00:03,  4.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|â–ˆâ–ˆâ–‹       | 5/19 [00:01<00:03,  3.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [00:01<00:02,  5.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [00:01<00:01,  5.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [00:01<00:01,  7.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:01<00:00, 10.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [00:01<00:00, 12.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [00:01<00:00, 13.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:02<00:00, 15.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:02<00:00,  8.81it/s]
(EngineCore_DP0 pid=168312) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|â–‰         | 1/11 [00:00<00:02,  4.04it/s]
Capturing CUDA graphs (decode, FULL):  18%|â–ˆâ–Š        | 2/11 [00:00<00:02,  4.24it/s]
Capturing CUDA graphs (decode, FULL):  27%|â–ˆâ–ˆâ–‹       | 3/11 [00:00<00:02,  3.12it/s]
Capturing CUDA graphs (decode, FULL):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 5/11 [00:01<00:01,  5.53it/s]
Capturing CUDA graphs (decode, FULL):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 7/11 [00:01<00:00,  8.17it/s]
Capturing CUDA graphs (decode, FULL):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 9/11 [00:01<00:00,  8.68it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:01<00:00, 10.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:01<00:00,  7.44it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   1%|          | 50/8192 [00:00<00:16, 494.04it/s]
Adding requests:   1%|          | 102/8192 [00:00<00:16, 503.90it/s]
Adding requests:   2%|â–         | 153/8192 [00:00<00:16, 498.88it/s]
Adding requests:   2%|â–         | 204/8192 [00:00<00:15, 499.75it/s]
Adding requests:   3%|â–Ž         | 257/8192 [00:00<00:15, 507.36it/s]
Adding requests:   4%|â–         | 308/8192 [00:00<00:15, 502.70it/s]
Adding requests:   4%|â–         | 359/8192 [00:00<00:15, 504.11it/s]
Adding requests:   5%|â–Œ         | 411/8192 [00:00<00:15, 506.90it/s]
Adding requests:   6%|â–Œ         | 462/8192 [00:00<00:15, 507.40it/s]
Adding requests:   6%|â–‹         | 513/8192 [00:01<00:15, 501.84it/s]
Adding requests:   7%|â–‹         | 564/8192 [00:01<00:15, 501.07it/s]
Adding requests:   8%|â–Š         | 615/8192 [00:01<00:15, 503.42it/s]
Adding requests:   8%|â–Š         | 668/8192 [00:01<00:14, 511.34it/s]
Adding requests:   9%|â–‰         | 722/8192 [00:01<00:14, 517.22it/s]
Adding requests:   9%|â–‰         | 774/8192 [00:01<00:14, 513.60it/s]
Adding requests:  10%|â–ˆ         | 826/8192 [00:01<00:14, 496.64it/s]
Adding requests:  11%|â–ˆ         | 878/8192 [00:01<00:14, 502.70it/s]
Adding requests:  11%|â–ˆâ–        | 931/8192 [00:01<00:14, 510.20it/s]
Adding requests:  12%|â–ˆâ–        | 983/8192 [00:01<00:14, 512.85it/s]
Adding requests:  13%|â–ˆâ–Ž        | 1036/8192 [00:02<00:13, 516.95it/s]
Adding requests:  13%|â–ˆâ–Ž        | 1088/8192 [00:02<00:13, 513.79it/s]
Adding requests:  14%|â–ˆâ–        | 1140/8192 [00:02<00:13, 511.62it/s]
Adding requests:  15%|â–ˆâ–        | 1196/8192 [00:02<00:13, 523.19it/s]
Adding requests:  15%|â–ˆâ–Œ        | 1249/8192 [00:02<00:13, 522.26it/s]
Adding requests:  16%|â–ˆâ–Œ        | 1302/8192 [00:02<00:13, 518.62it/s]
Adding requests:  17%|â–ˆâ–‹        | 1356/8192 [00:02<00:13, 521.96it/s]
Adding requests:  17%|â–ˆâ–‹        | 1410/8192 [00:02<00:12, 526.38it/s]
Adding requests:  18%|â–ˆâ–Š        | 1463/8192 [00:02<00:12, 524.84it/s]
Adding requests:  19%|â–ˆâ–Š        | 1516/8192 [00:02<00:12, 524.54it/s]
Adding requests:  19%|â–ˆâ–‰        | 1569/8192 [00:03<00:12, 525.05it/s]
Adding requests:  20%|â–ˆâ–‰        | 1623/8192 [00:03<00:12, 528.99it/s]
Adding requests:  20%|â–ˆâ–ˆ        | 1676/8192 [00:03<00:12, 521.78it/s]
Adding requests:  21%|â–ˆâ–ˆ        | 1730/8192 [00:03<00:12, 524.30it/s]
Adding requests:  22%|â–ˆâ–ˆâ–       | 1783/8192 [00:03<00:12, 519.16it/s]
Adding requests:  22%|â–ˆâ–ˆâ–       | 1836/8192 [00:03<00:12, 520.73it/s]
Adding requests:  23%|â–ˆâ–ˆâ–Ž       | 1889/8192 [00:03<00:12, 520.65it/s]
Adding requests:  24%|â–ˆâ–ˆâ–Ž       | 1942/8192 [00:03<00:12, 519.26it/s]
Adding requests:  24%|â–ˆâ–ˆâ–       | 1994/8192 [00:03<00:12, 503.09it/s]
Adding requests:  25%|â–ˆâ–ˆâ–Œ       | 2048/8192 [00:03<00:11, 512.28it/s]
Adding requests:  26%|â–ˆâ–ˆâ–Œ       | 2101/8192 [00:04<00:11, 517.40it/s]
Adding requests:  26%|â–ˆâ–ˆâ–‹       | 2153/8192 [00:04<00:11, 513.79it/s]
Adding requests:  27%|â–ˆâ–ˆâ–‹       | 2205/8192 [00:04<00:11, 510.61it/s]
Adding requests:  28%|â–ˆâ–ˆâ–Š       | 2260/8192 [00:04<00:11, 517.41it/s]
Adding requests:  28%|â–ˆâ–ˆâ–Š       | 2314/8192 [00:04<00:11, 521.65it/s]
Adding requests:  29%|â–ˆâ–ˆâ–‰       | 2367/8192 [00:04<00:11, 518.24it/s]
Adding requests:  30%|â–ˆâ–ˆâ–‰       | 2420/8192 [00:04<00:11, 519.86it/s]
Adding requests:  30%|â–ˆâ–ˆâ–ˆ       | 2473/8192 [00:04<00:11, 519.62it/s]
Adding requests:  31%|â–ˆâ–ˆâ–ˆ       | 2525/8192 [00:04<00:10, 517.83it/s]
Adding requests:  31%|â–ˆâ–ˆâ–ˆâ–      | 2580/8192 [00:05<00:10, 525.28it/s]
Adding requests:  32%|â–ˆâ–ˆâ–ˆâ–      | 2633/8192 [00:05<00:10, 521.01it/s]
Adding requests:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2686/8192 [00:05<00:10, 521.99it/s]
Adding requests:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2739/8192 [00:05<00:10, 517.31it/s]
Adding requests:  34%|â–ˆâ–ˆâ–ˆâ–      | 2791/8192 [00:05<00:10, 515.35it/s]
Adding requests:  35%|â–ˆâ–ˆâ–ˆâ–      | 2844/8192 [00:05<00:10, 517.27it/s]
Adding requests:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 2897/8192 [00:05<00:10, 520.97it/s]
Adding requests:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 2950/8192 [00:05<00:10, 515.82it/s]
Adding requests:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 3003/8192 [00:05<00:10, 517.41it/s]
Adding requests:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 3056/8192 [00:05<00:09, 518.74it/s]
Adding requests:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3108/8192 [00:06<00:09, 514.49it/s]
Adding requests:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 3160/8192 [00:06<00:09, 515.76it/s]
Adding requests:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 3212/8192 [00:06<00:09, 516.30it/s]
Adding requests:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 3265/8192 [00:06<00:09, 520.21it/s]
Adding requests:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 3318/8192 [00:06<00:09, 508.76it/s]
Adding requests:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 3371/8192 [00:06<00:09, 514.25it/s]
Adding requests:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3425/8192 [00:06<00:09, 518.87it/s]
Adding requests:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3477/8192 [00:06<00:09, 506.81it/s]
Adding requests:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3528/8192 [00:06<00:09, 507.06it/s]
Adding requests:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3580/8192 [00:06<00:09, 509.84it/s]
Adding requests:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3632/8192 [00:07<00:08, 510.43it/s]
Adding requests:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3685/8192 [00:07<00:08, 513.83it/s]
Adding requests:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 3737/8192 [00:07<00:08, 512.60it/s]
Adding requests:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 3792/8192 [00:07<00:08, 522.53it/s]
Adding requests:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 3846/8192 [00:07<00:08, 525.23it/s]
Adding requests:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 3899/8192 [00:07<00:08, 523.06it/s]
Adding requests:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 3952/8192 [00:07<00:08, 522.65it/s]
Adding requests:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4005/8192 [00:07<00:08, 521.76it/s]
Adding requests:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4058/8192 [00:07<00:07, 518.55it/s]
Adding requests:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4111/8192 [00:07<00:07, 519.27it/s]
Adding requests:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4164/8192 [00:08<00:07, 521.17it/s]
Adding requests:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 4217/8192 [00:08<00:07, 522.27it/s]
Adding requests:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 4270/8192 [00:08<00:07, 521.95it/s]
Adding requests:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 4323/8192 [00:08<00:07, 524.08it/s]
Adding requests:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 4377/8192 [00:08<00:07, 528.66it/s]
Adding requests:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 4430/8192 [00:08<00:07, 525.49it/s]
Adding requests:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 4483/8192 [00:08<00:07, 522.49it/s]
Adding requests:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 4536/8192 [00:08<00:07, 518.70it/s]
Adding requests:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 4589/8192 [00:08<00:06, 519.91it/s]
Adding requests:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4642/8192 [00:08<00:06, 513.45it/s]
Adding requests:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4694/8192 [00:09<00:06, 511.07it/s]
Adding requests:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 4747/8192 [00:09<00:06, 516.11it/s]
Adding requests:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 4799/8192 [00:09<00:06, 515.20it/s]
Adding requests:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 4851/8192 [00:09<00:06, 516.16it/s]
Adding requests:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 4903/8192 [00:09<00:06, 512.52it/s]
Adding requests:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 4956/8192 [00:09<00:06, 516.99it/s]
Adding requests:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 5008/8192 [00:09<00:06, 517.68it/s]
Adding requests:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5062/8192 [00:09<00:05, 521.69it/s]
Adding requests:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5116/8192 [00:09<00:05, 526.50it/s]
Adding requests:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5169/8192 [00:10<00:05, 525.83it/s]
Adding requests:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5222/8192 [00:10<00:05, 523.06it/s]
Adding requests:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5275/8192 [00:10<00:05, 518.25it/s]
Adding requests:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 5329/8192 [00:10<00:05, 523.76it/s]
Adding requests:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 5382/8192 [00:10<00:05, 523.24it/s]
Adding requests:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 5435/8192 [00:10<00:05, 524.00it/s]
Adding requests:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 5488/8192 [00:10<00:05, 517.13it/s]
Adding requests:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 5540/8192 [00:10<00:05, 516.29it/s]
Adding requests:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 5592/8192 [00:10<00:05, 516.59it/s]
Adding requests:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 5644/8192 [00:10<00:04, 517.05it/s]
Adding requests:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 5696/8192 [00:11<00:04, 515.20it/s]
Adding requests:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 5749/8192 [00:11<00:04, 517.12it/s]
Adding requests:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 5802/8192 [00:11<00:04, 518.19it/s]
Adding requests:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5854/8192 [00:11<00:04, 516.47it/s]
Adding requests:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5908/8192 [00:11<00:04, 522.59it/s]
Adding requests:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 5961/8192 [00:11<00:04, 521.71it/s]
Adding requests:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 6014/8192 [00:11<00:04, 513.21it/s]
Adding requests:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6067/8192 [00:11<00:04, 517.04it/s]
Adding requests:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6119/8192 [00:11<00:04, 513.63it/s]
Adding requests:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6171/8192 [00:11<00:03, 512.79it/s]
Adding requests:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6225/8192 [00:12<00:03, 519.12it/s]
Adding requests:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 6279/8192 [00:12<00:03, 522.94it/s]
Adding requests:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 6333/8192 [00:12<00:03, 526.39it/s]
Adding requests:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 6386/8192 [00:12<00:03, 527.44it/s]
Adding requests:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 6441/8192 [00:12<00:03, 532.23it/s]
Adding requests:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 6496/8192 [00:12<00:03, 535.10it/s]
Adding requests:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 6550/8192 [00:12<00:03, 534.16it/s]
Adding requests:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 6604/8192 [00:12<00:02, 531.62it/s]
Adding requests:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 6658/8192 [00:12<00:02, 532.23it/s]
Adding requests:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 6712/8192 [00:12<00:02, 529.25it/s]
Adding requests:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 6765/8192 [00:13<00:02, 527.42it/s]
Adding requests:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 6819/8192 [00:13<00:02, 531.07it/s]
Adding requests:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 6874/8192 [00:13<00:02, 534.05it/s]
Adding requests:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 6928/8192 [00:13<00:02, 535.61it/s]
Adding requests:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6982/8192 [00:13<00:02, 532.21it/s]
Adding requests:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 7036/8192 [00:13<00:02, 529.94it/s]
Adding requests:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 7090/8192 [00:13<00:02, 528.05it/s]
Adding requests:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 7143/8192 [00:13<00:01, 526.96it/s]
Adding requests:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7196/8192 [00:13<00:01, 525.27it/s]
Adding requests:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7250/8192 [00:13<00:01, 528.04it/s]
Adding requests:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 7304/8192 [00:14<00:01, 529.57it/s]
Adding requests:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 7357/8192 [00:14<00:01, 512.70it/s]
Adding requests:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 7412/8192 [00:14<00:01, 522.31it/s]
Adding requests:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 7467/8192 [00:14<00:01, 527.75it/s]
Adding requests:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 7521/8192 [00:14<00:01, 529.12it/s]
Adding requests:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 7574/8192 [00:14<00:01, 527.48it/s]
Adding requests:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 7627/8192 [00:14<00:01, 524.95it/s]
Adding requests:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 7682/8192 [00:14<00:00, 531.26it/s]
Adding requests:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 7736/8192 [00:14<00:00, 529.68it/s]
Adding requests:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 7789/8192 [00:15<00:00, 524.04it/s]
Adding requests:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 7843/8192 [00:15<00:00, 527.61it/s]
Adding requests:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 7896/8192 [00:15<00:00, 526.94it/s]
Adding requests:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 7949/8192 [00:15<00:00, 520.78it/s]
Adding requests:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 8002/8192 [00:15<00:00, 519.49it/s]
Adding requests:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 8054/8192 [00:15<00:00, 517.91it/s]
Adding requests:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 8108/8192 [00:15<00:00, 521.90it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 8161/8192 [00:15<00:00, 523.64it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8192/8192 [00:15<00:00, 519.33it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|â–ˆâ–‹        | 1386/8192 [00:00<00:01, 6567.46it/s, est. speed input: 6725831.18 toks/s, output: 6567.68 toks/s]
Processed prompts:  25%|â–ˆâ–ˆâ–       | 2043/8192 [00:07<00:27, 227.10it/s, est. speed input: 289420.23 toks/s, output: 282.64 toks/s]   
Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 2319/8192 [00:10<00:32, 181.60it/s, est. speed input: 237350.83 toks/s, output: 231.79 toks/s]
Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 2474/8192 [00:12<00:37, 151.61it/s, est. speed input: 209267.73 toks/s, output: 204.36 toks/s]
Processed prompts:  31%|â–ˆâ–ˆâ–ˆâ–      | 2571/8192 [00:12<00:37, 150.23it/s, est. speed input: 205676.33 toks/s, output: 200.86 toks/s]
Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 2638/8192 [00:13<00:39, 142.25it/s, est. speed input: 200002.07 toks/s, output: 195.31 toks/s]
Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2686/8192 [00:14<00:42, 129.53it/s, est. speed input: 193511.97 toks/s, output: 188.98 toks/s]
Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2730/8192 [00:14<00:46, 116.35it/s, est. speed input: 187377.09 toks/s, output: 182.98 toks/s]
Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 2794/8192 [00:15<00:48, 110.68it/s, est. speed input: 183090.99 toks/s, output: 178.80 toks/s]
Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–      | 2858/8192 [00:16<00:50, 105.90it/s, est. speed input: 179185.31 toks/s, output: 174.99 toks/s]
Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 2922/8192 [00:17<00:51, 102.01it/s, est. speed input: 175600.68 toks/s, output: 171.48 toks/s]
Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 2986/8192 [00:17<00:52, 98.93it/s, est. speed input: 172293.02 toks/s, output: 168.25 toks/s] 
Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 3050/8192 [00:18<00:53, 96.62it/s, est. speed input: 169245.97 toks/s, output: 165.28 toks/s]
Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3114/8192 [00:19<00:53, 95.34it/s, est. speed input: 166521.66 toks/s, output: 162.62 toks/s]
Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 3178/8192 [00:19<00:53, 93.93it/s, est. speed input: 163888.02 toks/s, output: 160.05 toks/s]
Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 3242/8192 [00:20<00:53, 92.91it/s, est. speed input: 161435.50 toks/s, output: 157.65 toks/s]
Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 3306/8192 [00:21<00:53, 92.16it/s, est. speed input: 159140.48 toks/s, output: 155.41 toks/s]
Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 3370/8192 [00:21<00:52, 91.66it/s, est. speed input: 157000.14 toks/s, output: 153.32 toks/s]
Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3434/8192 [00:22<00:52, 91.29it/s, est. speed input: 154990.32 toks/s, output: 151.36 toks/s]
Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3498/8192 [00:23<00:51, 91.04it/s, est. speed input: 153105.08 toks/s, output: 149.52 toks/s]
Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3562/8192 [00:24<00:50, 90.82it/s, est. speed input: 151321.13 toks/s, output: 147.77 toks/s]
Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3626/8192 [00:24<00:50, 90.61it/s, est. speed input: 149631.96 toks/s, output: 146.12 toks/s]
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 3690/8192 [00:25<00:49, 91.02it/s, est. speed input: 148119.98 toks/s, output: 144.65 toks/s]
Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 3754/8192 [00:26<00:48, 90.76it/s, est. speed input: 146609.35 toks/s, output: 143.17 toks/s]
Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 3818/8192 [00:26<00:48, 90.58it/s, est. speed input: 145178.80 toks/s, output: 141.78 toks/s]
Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 3882/8192 [00:27<00:47, 90.47it/s, est. speed input: 143823.94 toks/s, output: 140.45 toks/s]
Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 3946/8192 [00:28<00:46, 90.37it/s, est. speed input: 142532.68 toks/s, output: 139.19 toks/s]
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4010/8192 [00:29<00:46, 90.81it/s, est. speed input: 141370.09 toks/s, output: 138.06 toks/s]
Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4074/8192 [00:29<00:45, 90.63it/s, est. speed input: 140201.61 toks/s, output: 136.92 toks/s]
Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4138/8192 [00:30<00:44, 90.52it/s, est. speed input: 139089.79 toks/s, output: 135.83 toks/s]
Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 4202/8192 [00:31<00:43, 90.81it/s, est. speed input: 138071.30 toks/s, output: 134.84 toks/s]
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 4266/8192 [00:31<00:43, 91.06it/s, est. speed input: 137102.38 toks/s, output: 133.89 toks/s]
Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 4330/8192 [00:32<00:42, 91.22it/s, est. speed input: 136172.96 toks/s, output: 132.98 toks/s]
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 4394/8192 [00:33<00:41, 90.92it/s, est. speed input: 135239.18 toks/s, output: 132.07 toks/s]
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 4458/8192 [00:33<00:41, 90.65it/s, est. speed input: 134339.05 toks/s, output: 131.19 toks/s]
Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 4522/8192 [00:34<00:40, 90.51it/s, est. speed input: 133480.12 toks/s, output: 130.35 toks/s]
Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 4586/8192 [00:35<00:39, 90.43it/s, est. speed input: 132657.16 toks/s, output: 129.55 toks/s]
Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4650/8192 [00:36<00:39, 90.27it/s, est. speed input: 131856.58 toks/s, output: 128.77 toks/s]
Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 4714/8192 [00:36<00:38, 90.23it/s, est. speed input: 131093.74 toks/s, output: 128.02 toks/s]
Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 4778/8192 [00:37<00:37, 90.65it/s, est. speed input: 130400.71 toks/s, output: 127.34 toks/s]
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 4842/8192 [00:38<00:36, 90.82it/s, est. speed input: 129721.78 toks/s, output: 126.68 toks/s]
Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 4906/8192 [00:38<00:36, 90.58it/s, est. speed input: 129036.13 toks/s, output: 126.01 toks/s]
Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 4970/8192 [00:39<00:35, 90.94it/s, est. speed input: 128419.56 toks/s, output: 125.41 toks/s]
Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5034/8192 [00:40<00:34, 90.63it/s, est. speed input: 127778.71 toks/s, output: 124.78 toks/s]
Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5098/8192 [00:41<00:34, 90.52it/s, est. speed input: 127167.91 toks/s, output: 124.19 toks/s]
Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5162/8192 [00:41<00:33, 90.37it/s, est. speed input: 126571.96 toks/s, output: 123.61 toks/s]
Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5226/8192 [00:42<00:32, 90.21it/s, est. speed input: 125992.13 toks/s, output: 123.04 toks/s]
Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5290/8192 [00:43<00:32, 90.09it/s, est. speed input: 125430.70 toks/s, output: 122.49 toks/s]
Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 5354/8192 [00:43<00:31, 90.07it/s, est. speed input: 124892.15 toks/s, output: 121.96 toks/s]
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 5418/8192 [00:44<00:30, 90.01it/s, est. speed input: 124367.72 toks/s, output: 121.45 toks/s]
Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 5482/8192 [00:45<00:30, 89.83it/s, est. speed input: 123849.88 toks/s, output: 120.95 toks/s]
Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 5546/8192 [00:46<00:29, 90.31it/s, est. speed input: 123390.40 toks/s, output: 120.50 toks/s]
Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 5610/8192 [00:46<00:28, 90.23it/s, est. speed input: 122916.12 toks/s, output: 120.04 toks/s]
Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 5674/8192 [00:47<00:27, 90.07it/s, est. speed input: 122449.27 toks/s, output: 119.58 toks/s]
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 5738/8192 [00:48<00:27, 90.02it/s, est. speed input: 121999.97 toks/s, output: 119.14 toks/s]
Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 5802/8192 [00:48<00:26, 89.86it/s, est. speed input: 121555.33 toks/s, output: 118.71 toks/s]
Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5866/8192 [00:49<00:25, 89.93it/s, est. speed input: 121135.55 toks/s, output: 118.30 toks/s]
Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5930/8192 [00:50<00:25, 89.70it/s, est. speed input: 120710.24 toks/s, output: 117.88 toks/s]
Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 5994/8192 [00:51<00:24, 89.70it/s, est. speed input: 120306.82 toks/s, output: 117.49 toks/s]
Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6058/8192 [00:51<00:23, 89.73it/s, est. speed input: 119916.29 toks/s, output: 117.11 toks/s]
Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6122/8192 [00:52<00:23, 89.70it/s, est. speed input: 119532.78 toks/s, output: 116.73 toks/s]
Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6186/8192 [00:53<00:22, 89.66it/s, est. speed input: 119159.22 toks/s, output: 116.37 toks/s]
Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 6250/8192 [00:53<00:21, 89.66it/s, est. speed input: 118796.87 toks/s, output: 116.01 toks/s]
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 6314/8192 [00:54<00:20, 89.71it/s, est. speed input: 118446.56 toks/s, output: 115.67 toks/s]
Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 6378/8192 [00:55<00:20, 89.67it/s, est. speed input: 118101.49 toks/s, output: 115.33 toks/s]
Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 6442/8192 [00:56<00:19, 89.59it/s, est. speed input: 117762.35 toks/s, output: 115.00 toks/s]
Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 6506/8192 [00:56<00:18, 89.59it/s, est. speed input: 117434.70 toks/s, output: 114.68 toks/s]
Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 6570/8192 [00:57<00:18, 90.04it/s, est. speed input: 117139.24 toks/s, output: 114.39 toks/s]
Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 6634/8192 [00:58<00:17, 90.23it/s, est. speed input: 116844.45 toks/s, output: 114.11 toks/s]
Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 6698/8192 [00:58<00:16, 90.00it/s, est. speed input: 116538.02 toks/s, output: 113.81 toks/s]
Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 6762/8192 [00:59<00:15, 89.94it/s, est. speed input: 116243.54 toks/s, output: 113.52 toks/s]
Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 6826/8192 [01:00<00:15, 89.89it/s, est. speed input: 115956.04 toks/s, output: 113.24 toks/s]
Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 6890/8192 [01:00<00:14, 89.69it/s, est. speed input: 115666.90 toks/s, output: 112.96 toks/s]
Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 6954/8192 [01:01<00:13, 89.69it/s, est. speed input: 115391.55 toks/s, output: 112.69 toks/s]
Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 7018/8192 [01:02<00:13, 89.65it/s, est. speed input: 115120.14 toks/s, output: 112.42 toks/s]
Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 7082/8192 [01:03<00:12, 89.62it/s, est. speed input: 114855.17 toks/s, output: 112.16 toks/s]
Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 7146/8192 [01:03<00:11, 90.02it/s, est. speed input: 114615.75 toks/s, output: 111.93 toks/s]
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7210/8192 [01:04<00:10, 89.87it/s, est. speed input: 114361.97 toks/s, output: 111.68 toks/s]
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 7274/8192 [01:05<00:10, 90.18it/s, est. speed input: 114132.31 toks/s, output: 111.46 toks/s]
Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 7338/8192 [01:05<00:09, 89.93it/s, est. speed input: 113886.45 toks/s, output: 111.22 toks/s]
Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 7402/8192 [01:06<00:08, 90.29it/s, est. speed input: 113670.27 toks/s, output: 111.01 toks/s]
Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 7466/8192 [01:07<00:08, 90.06it/s, est. speed input: 113436.87 toks/s, output: 110.78 toks/s]
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 7530/8192 [01:08<00:07, 89.79it/s, est. speed input: 113203.68 toks/s, output: 110.55 toks/s]
Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 7594/8192 [01:08<00:06, 89.76it/s, est. speed input: 112982.04 toks/s, output: 110.33 toks/s]
Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 7658/8192 [01:09<00:05, 89.69it/s, est. speed input: 112763.07 toks/s, output: 110.12 toks/s]
Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 7722/8192 [01:10<00:05, 89.65it/s, est. speed input: 112549.01 toks/s, output: 109.91 toks/s]
Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 7786/8192 [01:10<00:04, 89.54it/s, est. speed input: 112335.72 toks/s, output: 109.70 toks/s]
Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 7850/8192 [01:11<00:03, 89.60it/s, est. speed input: 112132.31 toks/s, output: 109.50 toks/s]
Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 7914/8192 [01:12<00:03, 89.55it/s, est. speed input: 111929.34 toks/s, output: 109.31 toks/s]
Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 7978/8192 [01:13<00:02, 89.51it/s, est. speed input: 111729.80 toks/s, output: 109.11 toks/s]
Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 8042/8192 [01:13<00:01, 89.39it/s, est. speed input: 111530.66 toks/s, output: 108.92 toks/s]
Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 8106/8192 [01:14<00:00, 90.03it/s, est. speed input: 111363.89 toks/s, output: 108.75 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 8170/8192 [01:14<00:00, 111.83it/s, est. speed input: 111869.55 toks/s, output: 109.25 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8192/8192 [01:14<00:00, 111.83it/s, est. speed input: 112169.69 toks/s, output: 109.54 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8192/8192 [01:14<00:00, 109.54it/s, est. speed input: 112169.69 toks/s, output: 109.54 toks/s]
[rank0]:[W125 16:07:56.840251659 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16 ==========
Time: 2026-01-25 21:52:46
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:52:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=467806) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=467806) WARNING 01-25 21:53:07 [backends.py:609] Failed to read file <frozen os>
Throughput: 27.96 requests/s, 475.31 total tokens/s, 27.96 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-25 21:52:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:52:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:52:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:52:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:52:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:52:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:52:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:52:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:52:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:52:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:52:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:52:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:52:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:52:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:53:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:53:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:53:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:53:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:53:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:53:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:53:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:53:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:53:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:53:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:53:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:53:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:53:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:53:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=467806) [2026-01-25 21:53:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=467806) [2026-01-25 21:53:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=467806) [2026-01-25 21:53:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=467806) [2026-01-25 21:53:03] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=467806) [2026-01-25 21:53:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=467806) [2026-01-25 21:53:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=467806) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=467806) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.66it/s]
(EngineCore_DP0 pid=467806) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.65it/s]
(EngineCore_DP0 pid=467806) 
(EngineCore_DP0 pid=467806) [2026-01-25 21:53:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=467806) [2026-01-25 21:53:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=467806) [2026-01-25 21:53:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=467806) [2026-01-25 21:53:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=467806) [2026-01-25 21:53:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=467806) [2026-01-25 21:53:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=467806) [2026-01-25 21:53:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=467806) [2026-01-25 21:53:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=467806) 2026-01-25 21:53:16,070 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=467806) 2026-01-25 21:53:16,109 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=467806) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  3.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.94it/s]
(EngineCore_DP0 pid=467806) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.85it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.83it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<00:00, 2544.69it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<01:23,  1.53it/s, est. speed input: 24.48 toks/s, output: 1.53 toks/s]
Processed prompts:   4%|â–         | 5/128 [00:00<00:15,  8.12it/s, est. speed input: 103.26 toks/s, output: 6.45 toks/s]
Processed prompts:   7%|â–‹         | 9/128 [00:00<00:08, 13.85it/s, est. speed input: 160.69 toks/s, output: 10.04 toks/s]
Processed prompts:  10%|â–ˆ         | 13/128 [00:01<00:06, 18.56it/s, est. speed input: 204.29 toks/s, output: 12.77 toks/s]
Processed prompts:  13%|â–ˆâ–Ž        | 17/128 [00:01<00:04, 22.27it/s, est. speed input: 238.48 toks/s, output: 14.90 toks/s]
Processed prompts:  16%|â–ˆâ–‹        | 21/128 [00:01<00:04, 25.28it/s, est. speed input: 266.68 toks/s, output: 16.67 toks/s]
Processed prompts:  20%|â–ˆâ–‰        | 25/128 [00:01<00:03, 27.50it/s, est. speed input: 289.82 toks/s, output: 18.11 toks/s]
Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 29/128 [00:01<00:03, 29.06it/s, est. speed input: 309.01 toks/s, output: 19.31 toks/s]
Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 33/128 [00:01<00:03, 30.14it/s, est. speed input: 325.18 toks/s, output: 20.32 toks/s]
Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 37/128 [00:01<00:02, 31.03it/s, est. speed input: 339.41 toks/s, output: 21.21 toks/s]
Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 41/128 [00:01<00:02, 31.56it/s, est. speed input: 351.54 toks/s, output: 21.97 toks/s]
Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 45/128 [00:01<00:02, 31.94it/s, est. speed input: 362.20 toks/s, output: 22.64 toks/s]
Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 49/128 [00:02<00:02, 32.21it/s, est. speed input: 371.62 toks/s, output: 23.23 toks/s]
Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 53/128 [00:02<00:02, 32.40it/s, est. speed input: 380.02 toks/s, output: 23.75 toks/s]
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 57/128 [00:02<00:02, 32.69it/s, est. speed input: 387.87 toks/s, output: 24.24 toks/s]
Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 61/128 [00:02<00:02, 31.97it/s, est. speed input: 393.11 toks/s, output: 24.57 toks/s]
Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 65/128 [00:02<00:01, 32.32it/s, est. speed input: 399.48 toks/s, output: 24.97 toks/s]
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 69/128 [00:02<00:01, 32.52it/s, est. speed input: 405.20 toks/s, output: 25.32 toks/s]
Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 73/128 [00:02<00:01, 32.44it/s, est. speed input: 410.02 toks/s, output: 25.63 toks/s]
Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 77/128 [00:02<00:01, 32.46it/s, est. speed input: 414.59 toks/s, output: 25.91 toks/s]
Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 81/128 [00:03<00:01, 32.53it/s, est. speed input: 418.88 toks/s, output: 26.18 toks/s]
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 85/128 [00:03<00:01, 32.57it/s, est. speed input: 422.83 toks/s, output: 26.43 toks/s]
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 89/128 [00:03<00:01, 32.62it/s, est. speed input: 426.52 toks/s, output: 26.66 toks/s]
Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 93/128 [00:03<00:01, 32.56it/s, est. speed input: 429.81 toks/s, output: 26.86 toks/s]
Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 97/128 [00:03<00:00, 32.71it/s, est. speed input: 433.15 toks/s, output: 27.07 toks/s]
Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 101/128 [00:03<00:00, 32.69it/s, est. speed input: 436.10 toks/s, output: 27.26 toks/s]
Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 105/128 [00:03<00:00, 32.70it/s, est. speed input: 438.90 toks/s, output: 27.43 toks/s]
Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 109/128 [00:03<00:00, 32.77it/s, est. speed input: 441.60 toks/s, output: 27.60 toks/s]
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 113/128 [00:04<00:00, 32.77it/s, est. speed input: 444.08 toks/s, output: 27.75 toks/s]
Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 117/128 [00:04<00:00, 32.79it/s, est. speed input: 446.45 toks/s, output: 27.90 toks/s]
Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 121/128 [00:04<00:00, 32.79it/s, est. speed input: 448.65 toks/s, output: 28.04 toks/s]
Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 125/128 [00:04<00:00, 32.86it/s, est. speed input: 450.83 toks/s, output: 28.18 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:04<00:00, 32.86it/s, est. speed input: 452.53 toks/s, output: 28.28 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:04<00:00, 28.28it/s, est. speed input: 452.53 toks/s, output: 28.28 toks/s]
[rank0]:[W125 21:53:23.692566969 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 21:53:25
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:53:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=468974) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=468974) WARNING 01-25 21:53:44 [backends.py:609] Failed to read file <frozen os>
Throughput: 28.21 requests/s, 3639.23 total tokens/s, 28.21 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-25 21:53:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:53:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:53:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:53:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:53:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:53:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:53:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:53:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:53:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:53:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:53:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:53:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:53:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:53:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:53:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:53:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:53:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:53:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:53:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:53:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:53:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:53:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:53:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:53:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:53:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:53:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:53:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:53:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=468974) [2026-01-25 21:53:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=468974) [2026-01-25 21:53:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=468974) [2026-01-25 21:53:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=468974) [2026-01-25 21:53:40] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=468974) [2026-01-25 21:53:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=468974) [2026-01-25 21:53:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=468974) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=468974) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.61it/s]
(EngineCore_DP0 pid=468974) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.60it/s]
(EngineCore_DP0 pid=468974) 
(EngineCore_DP0 pid=468974) [2026-01-25 21:53:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=468974) [2026-01-25 21:53:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=468974) [2026-01-25 21:53:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=468974) [2026-01-25 21:53:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=468974) [2026-01-25 21:53:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=468974) [2026-01-25 21:53:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=468974) [2026-01-25 21:53:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=468974) [2026-01-25 21:53:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=468974) 2026-01-25 21:53:53,076 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=468974) 2026-01-25 21:53:53,123 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=468974) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.01it/s]
(EngineCore_DP0 pid=468974) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 19.15it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<00:00, 1392.97it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<01:00,  2.11it/s, est. speed input: 269.63 toks/s, output: 2.11 toks/s]
Processed prompts:   4%|â–         | 5/128 [00:00<00:11, 10.31it/s, est. speed input: 1070.15 toks/s, output: 8.36 toks/s]
Processed prompts:   7%|â–‹         | 9/128 [00:00<00:07, 16.50it/s, est. speed input: 1597.86 toks/s, output: 12.48 toks/s]
Processed prompts:  10%|â–ˆ         | 13/128 [00:00<00:05, 21.08it/s, est. speed input: 1972.77 toks/s, output: 15.41 toks/s]
Processed prompts:  13%|â–ˆâ–Ž        | 17/128 [00:00<00:04, 24.40it/s, est. speed input: 2251.94 toks/s, output: 17.59 toks/s]
Processed prompts:  16%|â–ˆâ–‹        | 21/128 [00:01<00:04, 26.74it/s, est. speed input: 2466.59 toks/s, output: 19.27 toks/s]
Processed prompts:  20%|â–ˆâ–‰        | 25/128 [00:01<00:03, 28.31it/s, est. speed input: 2634.12 toks/s, output: 20.58 toks/s]
Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 29/128 [00:01<00:03, 29.23it/s, est. speed input: 2765.13 toks/s, output: 21.60 toks/s]
Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 33/128 [00:01<00:03, 30.20it/s, est. speed input: 2882.45 toks/s, output: 22.52 toks/s]
Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 37/128 [00:01<00:02, 30.91it/s, est. speed input: 2982.33 toks/s, output: 23.30 toks/s]
Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 41/128 [00:01<00:02, 31.43it/s, est. speed input: 3068.13 toks/s, output: 23.97 toks/s]
Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 45/128 [00:01<00:02, 31.52it/s, est. speed input: 3136.31 toks/s, output: 24.50 toks/s]
Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 49/128 [00:01<00:02, 31.55it/s, est. speed input: 3195.02 toks/s, output: 24.96 toks/s]
Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 53/128 [00:02<00:02, 31.61it/s, est. speed input: 3247.46 toks/s, output: 25.37 toks/s]
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 57/128 [00:02<00:02, 31.62it/s, est. speed input: 3293.30 toks/s, output: 25.73 toks/s]
Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 61/128 [00:02<00:02, 31.67it/s, est. speed input: 3334.83 toks/s, output: 26.05 toks/s]
Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 65/128 [00:02<00:01, 31.68it/s, est. speed input: 3371.79 toks/s, output: 26.34 toks/s]
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 69/128 [00:02<00:01, 31.69it/s, est. speed input: 3405.35 toks/s, output: 26.60 toks/s]
Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 73/128 [00:02<00:01, 31.72it/s, est. speed input: 3436.08 toks/s, output: 26.84 toks/s]
Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 77/128 [00:02<00:01, 31.76it/s, est. speed input: 3464.34 toks/s, output: 27.06 toks/s]
Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 81/128 [00:02<00:01, 31.88it/s, est. speed input: 3491.63 toks/s, output: 27.28 toks/s]
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 85/128 [00:03<00:01, 31.92it/s, est. speed input: 3516.16 toks/s, output: 27.47 toks/s]
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 89/128 [00:03<00:01, 31.92it/s, est. speed input: 3538.30 toks/s, output: 27.64 toks/s]
Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 93/128 [00:03<00:01, 31.94it/s, est. speed input: 3559.02 toks/s, output: 27.80 toks/s]
Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 97/128 [00:03<00:00, 31.95it/s, est. speed input: 3578.30 toks/s, output: 27.96 toks/s]
Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 101/128 [00:03<00:00, 31.95it/s, est. speed input: 3596.05 toks/s, output: 28.09 toks/s]
Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 105/128 [00:03<00:00, 32.00it/s, est. speed input: 3613.30 toks/s, output: 28.23 toks/s]
Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 109/128 [00:03<00:00, 31.95it/s, est. speed input: 3628.43 toks/s, output: 28.35 toks/s]
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 113/128 [00:03<00:00, 31.93it/s, est. speed input: 3642.74 toks/s, output: 28.46 toks/s]
Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 117/128 [00:04<00:00, 31.93it/s, est. speed input: 3656.31 toks/s, output: 28.56 toks/s]
Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 121/128 [00:04<00:00, 31.69it/s, est. speed input: 3666.31 toks/s, output: 28.64 toks/s]
Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 125/128 [00:04<00:00, 31.77it/s, est. speed input: 3678.49 toks/s, output: 28.74 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:04<00:00, 31.77it/s, est. speed input: 3687.20 toks/s, output: 28.81 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:04<00:00, 28.80it/s, est. speed input: 3687.20 toks/s, output: 28.81 toks/s]
[rank0]:[W125 21:53:59.573343086 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 21:54:01
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:54:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=470020) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=470020) WARNING 01-25 21:54:21 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.63 requests/s, 8129.45 total tokens/s, 31.63 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-25 21:54:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:54:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:54:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:54:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:54:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:54:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:54:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:54:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:54:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:54:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:54:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:54:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:54:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:54:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:54:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:54:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:54:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:54:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:54:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:54:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:54:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:54:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:54:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:54:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:54:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:54:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:54:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:54:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=470020) [2026-01-25 21:54:17] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=470020) [2026-01-25 21:54:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=470020) [2026-01-25 21:54:17] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=470020) [2026-01-25 21:54:17] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=470020) [2026-01-25 21:54:17] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=470020) [2026-01-25 21:54:17] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=470020) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=470020) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.66it/s]
(EngineCore_DP0 pid=470020) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.65it/s]
(EngineCore_DP0 pid=470020) 
(EngineCore_DP0 pid=470020) [2026-01-25 21:54:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=470020) [2026-01-25 21:54:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=470020) [2026-01-25 21:54:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=470020) [2026-01-25 21:54:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=470020) [2026-01-25 21:54:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=470020) [2026-01-25 21:54:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=470020) [2026-01-25 21:54:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=470020) [2026-01-25 21:54:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=470020) 2026-01-25 21:54:29,638 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=470020) 2026-01-25 21:54:29,668 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=470020) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.75it/s]
(EngineCore_DP0 pid=470020) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 18.41it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 81/128 [00:00<00:00, 807.42it/s]
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<00:00, 993.25it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|â–         | 2/128 [00:00<00:07, 16.70it/s, est. speed input: 4275.62 toks/s, output: 16.70 toks/s]
Processed prompts:   5%|â–         | 6/128 [00:00<00:04, 26.60it/s, est. speed input: 6430.36 toks/s, output: 25.11 toks/s]
Processed prompts:   8%|â–Š         | 10/128 [00:00<00:03, 29.86it/s, est. speed input: 7164.97 toks/s, output: 27.99 toks/s]
Processed prompts:  11%|â–ˆ         | 14/128 [00:00<00:03, 31.42it/s, est. speed input: 7536.58 toks/s, output: 29.44 toks/s]
Processed prompts:  14%|â–ˆâ–        | 18/128 [00:00<00:03, 32.25it/s, est. speed input: 7754.97 toks/s, output: 30.29 toks/s]
Processed prompts:  17%|â–ˆâ–‹        | 22/128 [00:00<00:03, 32.43it/s, est. speed input: 7863.51 toks/s, output: 30.72 toks/s]
Processed prompts:  20%|â–ˆâ–ˆ        | 26/128 [00:00<00:03, 32.64it/s, est. speed input: 7949.93 toks/s, output: 31.05 toks/s]
Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 30/128 [00:00<00:02, 32.75it/s, est. speed input: 8012.16 toks/s, output: 31.30 toks/s]
Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 34/128 [00:01<00:02, 32.79it/s, est. speed input: 8057.49 toks/s, output: 31.47 toks/s]
Processed prompts:  30%|â–ˆâ–ˆâ–‰       | 38/128 [00:01<00:02, 32.88it/s, est. speed input: 8099.55 toks/s, output: 31.64 toks/s]
Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 42/128 [00:01<00:02, 32.91it/s, est. speed input: 8130.39 toks/s, output: 31.76 toks/s]
Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 46/128 [00:01<00:02, 32.91it/s, est. speed input: 8155.15 toks/s, output: 31.86 toks/s]
Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 50/128 [00:01<00:02, 33.03it/s, est. speed input: 8183.91 toks/s, output: 31.97 toks/s]
Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 54/128 [00:01<00:02, 33.21it/s, est. speed input: 8213.99 toks/s, output: 32.09 toks/s]
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 58/128 [00:01<00:02, 33.37it/s, est. speed input: 8241.91 toks/s, output: 32.19 toks/s]
Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 62/128 [00:01<00:01, 33.31it/s, est. speed input: 8257.76 toks/s, output: 32.26 toks/s]
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 66/128 [00:02<00:01, 33.24it/s, est. speed input: 8270.01 toks/s, output: 32.30 toks/s]
Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 70/128 [00:02<00:01, 33.27it/s, est. speed input: 8284.64 toks/s, output: 32.36 toks/s]
Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 74/128 [00:02<00:01, 33.23it/s, est. speed input: 8295.36 toks/s, output: 32.40 toks/s]
Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 78/128 [00:02<00:01, 33.11it/s, est. speed input: 8300.98 toks/s, output: 32.43 toks/s]
Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 82/128 [00:02<00:01, 33.16it/s, est. speed input: 8311.14 toks/s, output: 32.47 toks/s]
Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 86/128 [00:02<00:01, 33.26it/s, est. speed input: 8323.19 toks/s, output: 32.51 toks/s]
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 90/128 [00:02<00:01, 33.19it/s, est. speed input: 8328.98 toks/s, output: 32.53 toks/s]
Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 94/128 [00:02<00:01, 33.15it/s, est. speed input: 8334.56 toks/s, output: 32.56 toks/s]
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 98/128 [00:03<00:00, 33.18it/s, est. speed input: 8341.80 toks/s, output: 32.58 toks/s]
Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 102/128 [00:03<00:00, 33.08it/s, est. speed input: 8344.20 toks/s, output: 32.59 toks/s]
Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 106/128 [00:03<00:00, 33.26it/s, est. speed input: 8354.51 toks/s, output: 32.63 toks/s]
Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 110/128 [00:03<00:00, 33.40it/s, est. speed input: 8364.43 toks/s, output: 32.67 toks/s]
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 114/128 [00:03<00:00, 33.21it/s, est. speed input: 8365.30 toks/s, output: 32.68 toks/s]
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 118/128 [00:03<00:00, 33.06it/s, est. speed input: 8365.76 toks/s, output: 32.68 toks/s]
Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 122/128 [00:03<00:00, 32.93it/s, est. speed input: 8365.23 toks/s, output: 32.68 toks/s]
Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 126/128 [00:03<00:00, 32.83it/s, est. speed input: 8364.58 toks/s, output: 32.67 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:03<00:00, 32.83it/s, est. speed input: 8368.34 toks/s, output: 32.69 toks/s]
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:03<00:00, 32.69it/s, est. speed input: 8368.34 toks/s, output: 32.69 toks/s]
[rank0]:[W125 21:54:35.746670211 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

