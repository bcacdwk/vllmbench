
========== M=512 ==========
Time: 2026-01-25 16:07:59
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M512.json


========== M=16 ==========
Time: 2026-01-25 21:56:27
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:56:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=474114) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=474114) WARNING 01-25 21:56:47 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.19 requests/s, 530.19 total tokens/s, 31.19 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-25 21:56:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:56:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:56:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:56:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:56:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:56:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:56:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:56:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:56:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:56:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:56:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:56:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:56:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:56:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:56:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:56:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:56:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:56:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:56:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:56:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:56:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:56:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:56:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:56:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:56:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:56:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:56:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:56:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=474114) [2026-01-25 21:56:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=474114) [2026-01-25 21:56:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=474114) [2026-01-25 21:56:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=474114) [2026-01-25 21:56:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=474114) [2026-01-25 21:56:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=474114) [2026-01-25 21:56:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=474114) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=474114) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.18it/s]
(EngineCore_DP0 pid=474114) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.18it/s]
(EngineCore_DP0 pid=474114) 
(EngineCore_DP0 pid=474114) [2026-01-25 21:56:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=474114) [2026-01-25 21:56:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5283840 bytes
(EngineCore_DP0 pid=474114) [2026-01-25 21:56:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=474114) [2026-01-25 21:56:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3522560 bytes
(EngineCore_DP0 pid=474114) [2026-01-25 21:56:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=474114) [2026-01-25 21:56:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28180480 bytes
(EngineCore_DP0 pid=474114) [2026-01-25 21:56:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=474114) [2026-01-25 21:56:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14008320 bytes
(EngineCore_DP0 pid=474114) 2026-01-25 21:56:54,755 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=474114) 2026-01-25 21:56:54,786 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=474114) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.73it/s]
(EngineCore_DP0 pid=474114) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.57it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2555.60it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:32,  3.95it/s, est. speed input: 63.22 toks/s, output: 3.95 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:07, 15.92it/s, est. speed input: 215.55 toks/s, output: 13.47 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:05, 22.35it/s, est. speed input: 293.52 toks/s, output: 18.34 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 26.08it/s, est. speed input: 340.26 toks/s, output: 21.27 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 28.39it/s, est. speed input: 371.41 toks/s, output: 23.21 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 30.07it/s, est. speed input: 394.77 toks/s, output: 24.67 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 31.26it/s, est. speed input: 412.78 toks/s, output: 25.80 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:03, 32.09it/s, est. speed input: 426.93 toks/s, output: 26.68 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 32.63it/s, est. speed input: 438.19 toks/s, output: 27.39 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 33.04it/s, est. speed input: 447.59 toks/s, output: 27.97 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 33.31it/s, est. speed input: 455.37 toks/s, output: 28.46 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 33.50it/s, est. speed input: 462.01 toks/s, output: 28.87 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 33.35it/s, est. speed input: 466.77 toks/s, output: 29.17 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 33.20it/s, est. speed input: 470.76 toks/s, output: 29.42 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 33.15it/s, est. speed input: 474.41 toks/s, output: 29.65 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:02, 33.13it/s, est. speed input: 477.66 toks/s, output: 29.85 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 33.11it/s, est. speed input: 480.52 toks/s, output: 30.03 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 33.10it/s, est. speed input: 483.09 toks/s, output: 30.19 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 33.08it/s, est. speed input: 485.39 toks/s, output: 30.34 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 33.08it/s, est. speed input: 487.49 toks/s, output: 30.47 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 33.16it/s, est. speed input: 489.58 toks/s, output: 30.60 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 33.37it/s, est. speed input: 491.82 toks/s, output: 30.74 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 33.52it/s, est. speed input: 493.88 toks/s, output: 30.87 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:03<00:01, 33.66it/s, est. speed input: 495.84 toks/s, output: 30.99 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:00, 33.57it/s, est. speed input: 497.29 toks/s, output: 31.08 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 33.58it/s, est. speed input: 498.77 toks/s, output: 31.17 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 33.58it/s, est. speed input: 500.13 toks/s, output: 31.26 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 33.50it/s, est. speed input: 501.27 toks/s, output: 31.33 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 33.37it/s, est. speed input: 502.21 toks/s, output: 31.39 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 33.34it/s, est. speed input: 503.19 toks/s, output: 31.45 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 33.10it/s, est. speed input: 503.75 toks/s, output: 31.48 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 33.15it/s, est. speed input: 504.61 toks/s, output: 31.54 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 33.15it/s, est. speed input: 505.44 toks/s, output: 31.59 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 31.59it/s, est. speed input: 505.44 toks/s, output: 31.59 toks/s]
[rank0]:[W125 21:57:01.103731339 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 21:57:03
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:57:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=475169) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=475169) WARNING 01-25 21:57:23 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.55 requests/s, 3940.90 total tokens/s, 30.55 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-25 21:57:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:57:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:57:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:57:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:57:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:57:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:57:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:57:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:57:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:57:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:57:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:57:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:57:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:57:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:57:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:57:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:57:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:57:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=475169) [2026-01-25 21:57:19] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=475169) [2026-01-25 21:57:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=475169) [2026-01-25 21:57:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=475169) [2026-01-25 21:57:19] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=475169) [2026-01-25 21:57:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=475169) [2026-01-25 21:57:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=475169) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=475169) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.91it/s]
(EngineCore_DP0 pid=475169) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.90it/s]
(EngineCore_DP0 pid=475169) 
(EngineCore_DP0 pid=475169) [2026-01-25 21:57:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=475169) [2026-01-25 21:57:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5283840 bytes
(EngineCore_DP0 pid=475169) [2026-01-25 21:57:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=475169) [2026-01-25 21:57:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3522560 bytes
(EngineCore_DP0 pid=475169) [2026-01-25 21:57:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=475169) [2026-01-25 21:57:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28180480 bytes
(EngineCore_DP0 pid=475169) [2026-01-25 21:57:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=475169) [2026-01-25 21:57:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14008320 bytes
(EngineCore_DP0 pid=475169) 2026-01-25 21:57:31,151 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=475169) 2026-01-25 21:57:31,205 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=475169) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 13.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 13.28it/s]
(EngineCore_DP0 pid=475169) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.55it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.54it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1379.80it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:31,  3.99it/s, est. speed input: 511.27 toks/s, output: 3.99 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:07, 15.94it/s, est. speed input: 1730.32 toks/s, output: 13.52 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:05, 22.41it/s, est. speed input: 2356.54 toks/s, output: 18.41 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 26.26it/s, est. speed input: 2737.80 toks/s, output: 21.39 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 28.62it/s, est. speed input: 2990.96 toks/s, output: 23.37 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 29.94it/s, est. speed input: 3162.35 toks/s, output: 24.70 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 30.78it/s, est. speed input: 3289.58 toks/s, output: 25.70 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:03, 31.29it/s, est. speed input: 3386.30 toks/s, output: 26.45 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 31.95it/s, est. speed input: 3474.55 toks/s, output: 27.14 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 32.42it/s, est. speed input: 3547.23 toks/s, output: 27.71 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 32.77it/s, est. speed input: 3608.97 toks/s, output: 28.19 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 33.00it/s, est. speed input: 3660.71 toks/s, output: 28.60 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 33.10it/s, est. speed input: 3703.53 toks/s, output: 28.93 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 33.25it/s, est. speed input: 3742.77 toks/s, output: 29.24 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 33.37it/s, est. speed input: 3777.66 toks/s, output: 29.51 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:02, 33.43it/s, est. speed input: 3807.81 toks/s, output: 29.75 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 33.48it/s, est. speed input: 3834.90 toks/s, output: 29.96 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 33.38it/s, est. speed input: 3856.43 toks/s, output: 30.13 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 33.21it/s, est. speed input: 3873.86 toks/s, output: 30.26 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 33.08it/s, est. speed input: 3889.41 toks/s, output: 30.39 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 33.00it/s, est. speed input: 3903.63 toks/s, output: 30.50 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 32.93it/s, est. speed input: 3916.45 toks/s, output: 30.60 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 32.92it/s, est. speed input: 3928.69 toks/s, output: 30.69 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:03<00:01, 32.82it/s, est. speed input: 3938.63 toks/s, output: 30.77 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:00, 32.77it/s, est. speed input: 3948.03 toks/s, output: 30.84 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 32.74it/s, est. speed input: 3956.80 toks/s, output: 30.91 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 32.71it/s, est. speed input: 3964.80 toks/s, output: 30.97 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 32.67it/s, est. speed input: 3971.89 toks/s, output: 31.03 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 32.68it/s, est. speed input: 3979.07 toks/s, output: 31.09 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 32.70it/s, est. speed input: 3986.07 toks/s, output: 31.14 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 32.53it/s, est. speed input: 3990.13 toks/s, output: 31.17 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:04<00:00, 32.60it/s, est. speed input: 3996.33 toks/s, output: 31.22 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 32.60it/s, est. speed input: 4000.55 toks/s, output: 31.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 31.25it/s, est. speed input: 4000.55 toks/s, output: 31.25 toks/s]
[rank0]:[W125 21:57:37.520439814 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 21:57:39
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:57:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=476213) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=476213) WARNING 01-25 21:57:59 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.42 requests/s, 8332.19 total tokens/s, 32.42 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-25 21:57:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:57:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:57:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:57:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:57:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:57:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:57:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:57:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:57:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:57:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:57:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 21:57:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 21:57:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 21:57:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 21:57:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 21:57:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:57:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:57:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:57:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=476213) [2026-01-25 21:57:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=476213) [2026-01-25 21:57:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=476213) [2026-01-25 21:57:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=476213) [2026-01-25 21:57:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=476213) [2026-01-25 21:57:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=476213) [2026-01-25 21:57:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=476213) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=476213) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.85it/s]
(EngineCore_DP0 pid=476213) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.85it/s]
(EngineCore_DP0 pid=476213) 
(EngineCore_DP0 pid=476213) [2026-01-25 21:57:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=476213) [2026-01-25 21:57:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5283840 bytes
(EngineCore_DP0 pid=476213) [2026-01-25 21:57:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=476213) [2026-01-25 21:57:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3522560 bytes
(EngineCore_DP0 pid=476213) [2026-01-25 21:57:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=476213) [2026-01-25 21:57:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28180480 bytes
(EngineCore_DP0 pid=476213) [2026-01-25 21:57:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=476213) [2026-01-25 21:57:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14008320 bytes
(EngineCore_DP0 pid=476213) 2026-01-25 21:58:06,980 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=476213) 2026-01-25 21:58:07,047 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=476213) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.71it/s]
(EngineCore_DP0 pid=476213) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 18.75it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  72%|███████▏  | 92/128 [00:00<00:00, 916.35it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1045.03it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:02, 42.31it/s, est. speed input: 10834.39 toks/s, output: 42.31 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:03, 36.61it/s, est. speed input: 9567.39 toks/s, output: 37.37 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:03, 35.15it/s, est. speed input: 9234.08 toks/s, output: 36.07 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:03, 34.22it/s, est. speed input: 9028.42 toks/s, output: 35.26 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:03, 33.72it/s, est. speed input: 8905.92 toks/s, output: 34.79 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:03, 33.47it/s, est. speed input: 8831.32 toks/s, output: 34.50 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 33.43it/s, est. speed input: 8790.29 toks/s, output: 34.34 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 33.46it/s, est. speed input: 8765.50 toks/s, output: 34.24 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:02, 33.34it/s, est. speed input: 8733.06 toks/s, output: 34.11 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 33.26it/s, est. speed input: 8707.16 toks/s, output: 34.01 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 33.20it/s, est. speed input: 8685.16 toks/s, output: 33.93 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 33.15it/s, est. speed input: 8666.59 toks/s, output: 33.85 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:02, 33.13it/s, est. speed input: 8651.75 toks/s, output: 33.80 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:02, 33.07it/s, est. speed input: 8636.34 toks/s, output: 33.74 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 33.05it/s, est. speed input: 8624.00 toks/s, output: 33.69 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 33.13it/s, est. speed input: 8618.17 toks/s, output: 33.66 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:02<00:01, 33.15it/s, est. speed input: 8611.05 toks/s, output: 33.64 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:01, 33.06it/s, est. speed input: 8600.15 toks/s, output: 33.59 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 32.99it/s, est. speed input: 8589.69 toks/s, output: 33.55 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 32.93it/s, est. speed input: 8580.07 toks/s, output: 33.52 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 32.91it/s, est. speed input: 8572.20 toks/s, output: 33.48 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:01, 32.91it/s, est. speed input: 8565.33 toks/s, output: 33.46 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:01, 33.16it/s, est. speed input: 8568.47 toks/s, output: 33.47 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 33.28it/s, est. speed input: 8569.61 toks/s, output: 33.47 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:03<00:00, 33.39it/s, est. speed input: 8571.21 toks/s, output: 33.48 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:03<00:00, 33.46it/s, est. speed input: 8572.91 toks/s, output: 33.49 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:03<00:00, 33.52it/s, est. speed input: 8574.43 toks/s, output: 33.49 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:03<00:00, 33.36it/s, est. speed input: 8569.75 toks/s, output: 33.48 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 33.50it/s, est. speed input: 8572.84 toks/s, output: 33.49 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 33.28it/s, est. speed input: 8566.60 toks/s, output: 33.46 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 33.38it/s, est. speed input: 8568.02 toks/s, output: 33.47 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.38it/s, est. speed input: 8569.47 toks/s, output: 33.47 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.47it/s, est. speed input: 8569.47 toks/s, output: 33.47 toks/s]
[rank0]:[W125 21:58:12.971907014 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

