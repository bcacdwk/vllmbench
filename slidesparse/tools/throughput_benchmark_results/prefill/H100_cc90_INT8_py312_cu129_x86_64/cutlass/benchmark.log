
========== M=16 ==========
Time: 2026-01-25 21:45:44
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:45:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 188.34 requests/s, 3201.78 total tokens/s, 188.34 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-25 21:45:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:45:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 21:45:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:45:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 21:45:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 21:45:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 21:45:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 21:45:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 21:45:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:45:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:45:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:45:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:45:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:45:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 21:45:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 21:45:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 21:45:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 21:45:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:45:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:45:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:45:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:45:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 21:45:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 21:45:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 21:45:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 21:45:51] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-25 21:45:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=7, cuSPARSELt=5 models
[2026-01-25 21:45:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:45:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:45:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:45:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:45:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:45:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 21:45:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:45:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 21:45:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 21:45:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 21:45:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 21:45:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 21:45:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:45:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:45:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:45:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:45:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:45:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 21:45:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 21:45:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 21:45:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 21:45:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:45:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:45:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:45:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:45:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 21:45:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 21:45:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 21:45:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 21:45:58] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-25 21:45:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=7, cuSPARSELt=5 models
[2026-01-25 21:45:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:45:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:45:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:45:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=455231) [2026-01-25 21:45:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=455231) [2026-01-25 21:45:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=455231) [2026-01-25 21:45:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=CUTLASS, symmetric=True
(EngineCore_DP0 pid=455231) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=455231) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.53it/s]
(EngineCore_DP0 pid=455231) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.52it/s]
(EngineCore_DP0 pid=455231) 
(EngineCore_DP0 pid=455231) 2026-01-25 21:46:15,888 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=455231) 2026-01-25 21:46:15,904 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=455231) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 66.89it/s]
(EngineCore_DP0 pid=455231) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 70.66it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2534.08it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:00, 206.54it/s, est. speed input: 3305.96 toks/s, output: 206.56 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:00, 205.57it/s, est. speed input: 3292.17 toks/s, output: 205.73 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:00<00:00, 204.59it/s, est. speed input: 3279.61 toks/s, output: 204.96 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:00<00:00, 204.23it/s, est. speed input: 3274.25 toks/s, output: 204.63 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:00<00:00, 203.68it/s, est. speed input: 3268.03 toks/s, output: 204.24 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:00<00:00, 203.71it/s, est. speed input: 3266.69 toks/s, output: 204.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 203.71it/s, est. speed input: 3265.74 toks/s, output: 204.10 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 204.05it/s, est. speed input: 3265.74 toks/s, output: 204.10 toks/s]
[rank0]:[W125 21:46:18.338027412 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 21:46:20
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:46:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 173.20 requests/s, 22343.35 total tokens/s, 173.20 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-25 21:46:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:46:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 21:46:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:46:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 21:46:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 21:46:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 21:46:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 21:46:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 21:46:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:46:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:46:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:46:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:46:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:46:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 21:46:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 21:46:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 21:46:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 21:46:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:46:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:46:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:46:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:46:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 21:46:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 21:46:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 21:46:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 21:46:27] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-25 21:46:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=7, cuSPARSELt=5 models
[2026-01-25 21:46:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:46:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:46:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:46:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:46:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:46:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 21:46:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:46:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 21:46:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 21:46:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 21:46:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 21:46:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 21:46:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:46:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:46:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:46:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:46:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:46:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 21:46:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 21:46:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 21:46:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 21:46:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:46:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:46:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:46:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:46:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 21:46:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 21:46:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 21:46:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 21:46:34] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-25 21:46:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=7, cuSPARSELt=5 models
[2026-01-25 21:46:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:46:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:46:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:46:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=456439) [2026-01-25 21:46:36] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=456439) [2026-01-25 21:46:36] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=456439) [2026-01-25 21:46:36] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=CUTLASS, symmetric=True
(EngineCore_DP0 pid=456439) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=456439) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.50it/s]
(EngineCore_DP0 pid=456439) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.50it/s]
(EngineCore_DP0 pid=456439) 
(EngineCore_DP0 pid=456439) 2026-01-25 21:46:51,027 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=456439) 2026-01-25 21:46:51,142 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=456439) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 26.38it/s]
(EngineCore_DP0 pid=456439) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 37.35it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1290.82it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:00, 189.98it/s, est. speed input: 24326.26 toks/s, output: 189.99 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:00, 197.40it/s, est. speed input: 25128.09 toks/s, output: 196.29 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:00<00:00, 200.10it/s, est. speed input: 25426.14 toks/s, output: 198.62 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:00<00:00, 201.18it/s, est. speed input: 25559.45 toks/s, output: 199.67 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:00<00:00, 201.69it/s, est. speed input: 25634.17 toks/s, output: 200.26 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:00<00:00, 201.88it/s, est. speed input: 25676.02 toks/s, output: 200.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 201.88it/s, est. speed input: 25679.30 toks/s, output: 200.61 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 200.54it/s, est. speed input: 25679.30 toks/s, output: 200.61 toks/s]
[rank0]:[W125 21:46:53.685982195 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 21:46:55
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 21:47:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 170.59 requests/s, 43840.74 total tokens/s, 170.59 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-25 21:47:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:47:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 21:47:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:47:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 21:47:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 21:47:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 21:47:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 21:47:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 21:47:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:47:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:47:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:47:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:47:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:47:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 21:47:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 21:47:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 21:47:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 21:47:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:47:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:47:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:47:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:47:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 21:47:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 21:47:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 21:47:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 21:47:02] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-25 21:47:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=7, cuSPARSELt=5 models
[2026-01-25 21:47:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:47:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:47:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:47:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 21:47:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 21:47:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 21:47:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:47:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 21:47:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 21:47:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 21:47:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 21:47:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 21:47:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:47:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:47:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:47:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:47:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:47:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 21:47:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 21:47:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 21:47:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 21:47:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:47:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 21:47:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:47:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 21:47:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 21:47:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 21:47:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 21:47:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 21:47:09] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-25 21:47:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=7, cuSPARSELt=5 models
[2026-01-25 21:47:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 21:47:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 21:47:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 21:47:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=457589) [2026-01-25 21:47:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=457589) [2026-01-25 21:47:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=457589) [2026-01-25 21:47:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=CUTLASS, symmetric=True
(EngineCore_DP0 pid=457589) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=457589) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.52it/s]
(EngineCore_DP0 pid=457589) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.52it/s]
(EngineCore_DP0 pid=457589) 
(EngineCore_DP0 pid=457589) 2026-01-25 21:47:23,234 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=457589) 2026-01-25 21:47:23,239 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=457589) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 19.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 19.18it/s]
(EngineCore_DP0 pid=457589) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 46.42it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  55%|█████▍    | 70/128 [00:00<00:00, 698.18it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 914.80it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:00, 232.33it/s, est. speed input: 59495.46 toks/s, output: 232.34 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:00, 215.60it/s, est. speed input: 55805.73 toks/s, output: 217.96 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:00<00:00, 211.54it/s, est. speed input: 54845.27 toks/s, output: 214.22 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:00<00:00, 209.45it/s, est. speed input: 54333.30 toks/s, output: 212.23 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:00<00:00, 207.87it/s, est. speed input: 53977.71 toks/s, output: 210.84 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 207.87it/s, est. speed input: 53835.62 toks/s, output: 210.29 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 210.20it/s, est. speed input: 53835.62 toks/s, output: 210.29 toks/s]
[rank0]:[W125 21:47:26.107414912 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16 ==========
Time: 2026-01-26 04:57:18
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:57:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 190.61 requests/s, 3240.34 total tokens/s, 190.61 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-26 04:57:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:57:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 04:57:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:57:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 04:57:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 04:57:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 04:57:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 04:57:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 04:57:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:57:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:57:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:57:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:57:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:57:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 04:57:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 04:57:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 04:57:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 04:57:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:57:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:57:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:57:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:57:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 04:57:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 04:57:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 04:57:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 04:57:25] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-26 04:57:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=5 models
[2026-01-26 04:57:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:57:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:57:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:57:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:57:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:57:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 04:57:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:57:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 04:57:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 04:57:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 04:57:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 04:57:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 04:57:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:57:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:57:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:57:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:57:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:57:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 04:57:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 04:57:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 04:57:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 04:57:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:57:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:57:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:57:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:57:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 04:57:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 04:57:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 04:57:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 04:57:32] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-26 04:57:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=5 models
[2026-01-26 04:57:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:57:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:57:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:57:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=833407) [2026-01-26 04:57:33] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=833407) [2026-01-26 04:57:33] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=833407) [2026-01-26 04:57:33] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=CUTLASS, symmetric=True
(EngineCore_DP0 pid=833407) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=833407) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.62it/s]
(EngineCore_DP0 pid=833407) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.61it/s]
(EngineCore_DP0 pid=833407) 
(EngineCore_DP0 pid=833407) 2026-01-26 04:57:43,849 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=833407) 2026-01-26 04:57:43,870 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=833407) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.03it/s]
(EngineCore_DP0 pid=833407) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 35.07it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2447.35it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:00, 208.94it/s, est. speed input: 3344.18 toks/s, output: 208.96 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:00, 208.16it/s, est. speed input: 3333.08 toks/s, output: 208.29 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:00<00:00, 207.83it/s, est. speed input: 3328.21 toks/s, output: 208.00 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:00<00:00, 207.55it/s, est. speed input: 3324.75 toks/s, output: 207.78 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:00<00:00, 207.21it/s, est. speed input: 3320.96 toks/s, output: 207.55 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:00<00:00, 207.01it/s, est. speed input: 3318.42 toks/s, output: 207.39 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 207.01it/s, est. speed input: 3318.12 toks/s, output: 207.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 207.30it/s, est. speed input: 3318.12 toks/s, output: 207.37 toks/s]
[rank0]:[W126 04:57:46.448247965 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 04:57:48
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:57:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 178.25 requests/s, 22994.45 total tokens/s, 178.25 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-26 04:57:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:57:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 04:57:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:57:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 04:57:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 04:57:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 04:57:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 04:57:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 04:57:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:57:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:57:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:57:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:57:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:57:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 04:57:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 04:57:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 04:57:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 04:57:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:57:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:57:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:57:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:57:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 04:57:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 04:57:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 04:57:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 04:57:55] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-26 04:57:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=5 models
[2026-01-26 04:57:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:57:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:57:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:57:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:58:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:58:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 04:58:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:58:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 04:58:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 04:58:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 04:58:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 04:58:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 04:58:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:58:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:58:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:58:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:58:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:58:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 04:58:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 04:58:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 04:58:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 04:58:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:58:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:58:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:58:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:58:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 04:58:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 04:58:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 04:58:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 04:58:02] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-26 04:58:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=5 models
[2026-01-26 04:58:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:58:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:58:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:58:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=834367) [2026-01-26 04:58:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=834367) [2026-01-26 04:58:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=834367) [2026-01-26 04:58:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=CUTLASS, symmetric=True
(EngineCore_DP0 pid=834367) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=834367) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.55it/s]
(EngineCore_DP0 pid=834367) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.55it/s]
(EngineCore_DP0 pid=834367) 
(EngineCore_DP0 pid=834367) 2026-01-26 04:58:14,011 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=834367) 2026-01-26 04:58:14,016 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=834367) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 97.87it/s]
(EngineCore_DP0 pid=834367) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 67.14it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1684.92it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:00, 169.24it/s, est. speed input: 21671.37 toks/s, output: 169.26 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:00, 188.19it/s, est. speed input: 23725.18 toks/s, output: 185.32 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:00<00:00, 196.69it/s, est. speed input: 24632.36 toks/s, output: 192.42 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:00<00:00, 200.67it/s, est. speed input: 25086.35 toks/s, output: 195.97 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:00<00:00, 202.77it/s, est. speed input: 25353.12 toks/s, output: 198.06 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:00<00:00, 204.07it/s, est. speed input: 25532.91 toks/s, output: 199.47 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 204.07it/s, est. speed input: 25574.84 toks/s, output: 199.79 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 199.72it/s, est. speed input: 25574.84 toks/s, output: 199.79 toks/s]
[rank0]:[W126 04:58:16.312067336 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 04:58:18
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 04:58:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 170.75 requests/s, 43881.85 total tokens/s, 170.75 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-26 04:58:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:58:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 04:58:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:58:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 04:58:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 04:58:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 04:58:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 04:58:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 04:58:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:58:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:58:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:58:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:58:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:58:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 04:58:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 04:58:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 04:58:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 04:58:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:58:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:58:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:58:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:58:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 04:58:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 04:58:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 04:58:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 04:58:25] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-26 04:58:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=5 models
[2026-01-26 04:58:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:58:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:58:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:58:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 04:58:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 04:58:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 04:58:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:58:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 04:58:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 04:58:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 04:58:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 04:58:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 04:58:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:58:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:58:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:58:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:58:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:58:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 04:58:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 04:58:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 04:58:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 04:58:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:58:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 04:58:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:58:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 04:58:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 04:58:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 04:58:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 04:58:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 04:58:32] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-26 04:58:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=5 models
[2026-01-26 04:58:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 04:58:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 04:58:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 04:58:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=835281) [2026-01-26 04:58:33] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=835281) [2026-01-26 04:58:33] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=835281) [2026-01-26 04:58:33] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=CUTLASS, symmetric=True
(EngineCore_DP0 pid=835281) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=835281) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.29it/s]
(EngineCore_DP0 pid=835281) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.29it/s]
(EngineCore_DP0 pid=835281) 
(EngineCore_DP0 pid=835281) 2026-01-26 04:58:43,816 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=835281) 2026-01-26 04:58:43,821 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=835281) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 96.53it/s]
(EngineCore_DP0 pid=835281) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 69.25it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  90%|████████▉ | 115/128 [00:00<00:00, 1145.21it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1180.85it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:00, 183.00it/s, est. speed input: 46863.41 toks/s, output: 183.01 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:00, 195.70it/s, est. speed input: 49618.21 toks/s, output: 193.79 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:00<00:00, 199.97it/s, est. speed input: 50573.39 toks/s, output: 197.53 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:00<00:00, 201.89it/s, est. speed input: 51038.18 toks/s, output: 199.35 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:00<00:00, 201.84it/s, est. speed input: 51162.35 toks/s, output: 199.84 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:00<00:00, 201.56it/s, est. speed input: 51212.75 toks/s, output: 200.04 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 201.56it/s, est. speed input: 51223.98 toks/s, output: 200.08 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 200.02it/s, est. speed input: 51223.98 toks/s, output: 200.08 toks/s]
[rank0]:[W126 04:58:46.236721205 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16 ==========
Time: 2026-01-26 07:28:55
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 07:29:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 197.62 requests/s, 3359.61 total tokens/s, 197.62 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-26 07:29:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 07:29:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 07:29:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 07:29:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 07:29:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 07:29:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:02] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-26 07:29:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-26 07:29:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 07:29:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 07:29:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 07:29:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 07:29:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 07:29:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 07:29:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 07:29:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 07:29:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 07:29:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:09] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-26 07:29:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-26 07:29:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 07:29:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 07:29:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 07:29:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=962021) [2026-01-26 07:29:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=962021) [2026-01-26 07:29:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=962021) [2026-01-26 07:29:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=CUTLASS, symmetric=True
(EngineCore_DP0 pid=962021) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=962021) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.55it/s]
(EngineCore_DP0 pid=962021) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.55it/s]
(EngineCore_DP0 pid=962021) 
(EngineCore_DP0 pid=962021) 2026-01-26 07:29:20,094 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=962021) 2026-01-26 07:29:20,164 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=962021) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.81it/s]
(EngineCore_DP0 pid=962021) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2793.34it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:00, 230.57it/s, est. speed input: 3690.34 toks/s, output: 230.59 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:00, 217.91it/s, est. speed input: 3516.08 toks/s, output: 219.73 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:00<00:00, 214.55it/s, est. speed input: 3467.25 toks/s, output: 216.68 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:00<00:00, 212.39it/s, est. speed input: 3437.00 toks/s, output: 214.80 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:00<00:00, 211.46it/s, est. speed input: 3421.12 toks/s, output: 213.81 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 211.46it/s, est. speed input: 3414.16 toks/s, output: 213.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 213.28it/s, est. speed input: 3414.16 toks/s, output: 213.37 toks/s]
[rank0]:[W126 07:29:22.732255090 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 07:29:24
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 07:29:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 189.98 requests/s, 24506.92 total tokens/s, 189.98 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-26 07:29:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 07:29:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 07:29:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 07:29:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 07:29:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 07:29:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:31] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-26 07:29:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-26 07:29:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 07:29:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 07:29:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 07:29:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 07:29:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 07:29:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 07:29:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 07:29:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 07:29:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 07:29:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:29:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:29:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 07:29:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 07:29:38] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-26 07:29:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-26 07:29:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 07:29:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 07:29:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 07:29:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=963047) [2026-01-26 07:29:39] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=963047) [2026-01-26 07:29:39] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=963047) [2026-01-26 07:29:39] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=CUTLASS, symmetric=True
(EngineCore_DP0 pid=963047) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=963047) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.57it/s]
(EngineCore_DP0 pid=963047) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.57it/s]
(EngineCore_DP0 pid=963047) 
(EngineCore_DP0 pid=963047) 2026-01-26 07:29:48,879 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=963047) 2026-01-26 07:29:48,889 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=963047) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 32.24it/s]
(EngineCore_DP0 pid=963047) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 66.12it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1301.56it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:00, 286.04it/s, est. speed input: 36627.86 toks/s, output: 286.07 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:00<00:00, 235.66it/s, est. speed input: 30987.57 toks/s, output: 242.06 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:00<00:00, 224.27it/s, est. speed input: 29635.34 toks/s, output: 231.51 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:00<00:00, 219.02it/s, est. speed input: 28994.17 toks/s, output: 226.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 219.02it/s, est. speed input: 28562.80 toks/s, output: 223.14 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 223.03it/s, est. speed input: 28562.80 toks/s, output: 223.14 toks/s]
[rank0]:[W126 07:29:51.196762852 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 07:29:53
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 07:30:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 158.34 requests/s, 40693.73 total tokens/s, 158.34 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-26 07:30:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 07:30:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 07:30:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:30:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 07:30:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 07:30:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 07:30:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 07:30:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 07:30:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:30:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:30:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:30:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:30:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:30:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 07:30:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 07:30:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 07:30:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 07:30:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:30:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:30:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:30:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:30:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 07:30:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 07:30:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 07:30:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 07:30:00] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-26 07:30:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-26 07:30:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 07:30:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 07:30:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 07:30:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 07:30:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 07:30:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 07:30:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:30:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 07:30:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 07:30:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 07:30:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 07:30:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 07:30:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:30:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:30:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:30:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:30:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:30:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 07:30:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 07:30:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 07:30:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 07:30:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:30:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 07:30:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:30:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 07:30:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 07:30:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 07:30:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 07:30:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 07:30:07] INFO kernels.py:719: Preloaded 20 Triton kernels from H100_cc90_py312_cu129_x86_64
[2026-01-26 07:30:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-26 07:30:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 07:30:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 07:30:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 07:30:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=963994) [2026-01-26 07:30:09] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=963994) [2026-01-26 07:30:09] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=963994) [2026-01-26 07:30:09] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=CUTLASS, symmetric=True
(EngineCore_DP0 pid=963994) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=963994) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.53it/s]
(EngineCore_DP0 pid=963994) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.52it/s]
(EngineCore_DP0 pid=963994) 
(EngineCore_DP0 pid=963994) 2026-01-26 07:30:19,436 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=963994) 2026-01-26 07:30:19,441 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=963994) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 87.33it/s]
(EngineCore_DP0 pid=963994) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 67.50it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  63%|██████▎   | 81/128 [00:00<00:00, 808.49it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 918.17it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:00, 159.90it/s, est. speed input: 40949.40 toks/s, output: 159.92 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:00, 176.36it/s, est. speed input: 44529.05 toks/s, output: 173.92 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:00<00:00, 186.64it/s, est. speed input: 46656.77 toks/s, output: 182.23 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:00<00:00, 191.49it/s, est. speed input: 47723.56 toks/s, output: 186.41 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:00<00:00, 194.84it/s, est. speed input: 48470.66 toks/s, output: 189.33 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:00<00:00, 196.48it/s, est. speed input: 48909.92 toks/s, output: 191.04 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 196.48it/s, est. speed input: 49142.91 toks/s, output: 191.96 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 191.88it/s, est. speed input: 49142.91 toks/s, output: 191.96 toks/s]
[rank0]:[W126 07:30:22.061914882 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

