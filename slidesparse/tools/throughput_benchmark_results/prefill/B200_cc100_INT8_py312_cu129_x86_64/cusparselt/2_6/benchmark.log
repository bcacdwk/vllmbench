
========== M=16 ==========
Time: 2026-01-26 13:03:22
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:03:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=448572) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=448572) WARNING 01-26 13:03:41 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=448572) WARNING 01-26 13:03:48 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 44.67 requests/s, 759.46 total tokens/s, 44.67 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-26 13:03:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:03:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:03:29] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:03:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:03:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:03:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:03:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:03:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:03:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:03:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:03:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:03:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:03:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:03:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:03:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:03:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:03:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:03:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:03:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:03:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:03:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:03:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:03:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:03:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:03:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:03:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:03:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:03:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=448572) [2026-01-26 13:03:36] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=448572) [2026-01-26 13:03:36] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=448572) [2026-01-26 13:03:36] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=448572) [2026-01-26 13:03:36] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=448572) [2026-01-26 13:03:36] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=448572) [2026-01-26 13:03:36] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=448572) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=448572) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.05it/s]
(EngineCore_DP0 pid=448572) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.05it/s]
(EngineCore_DP0 pid=448572) 
(EngineCore_DP0 pid=448572) [2026-01-26 13:03:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=448572) [2026-01-26 13:03:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=448572) [2026-01-26 13:03:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=448572) [2026-01-26 13:03:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=448572) [2026-01-26 13:03:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=448572) [2026-01-26 13:03:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=448572) [2026-01-26 13:03:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=448572) [2026-01-26 13:03:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=448572) 2026-01-26 13:03:48,502 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=448572) 2026-01-26 13:03:48,525 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=448572) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.80it/s]
(EngineCore_DP0 pid=448572) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 19.40it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4920.14it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 50.09it/s, est. speed input: 801.66 toks/s, output: 50.10 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:02, 46.93it/s, est. speed input: 758.06 toks/s, output: 47.38 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 45.85it/s, est. speed input: 743.39 toks/s, output: 46.46 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 45.43it/s, est. speed input: 736.94 toks/s, output: 46.06 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 45.11it/s, est. speed input: 732.20 toks/s, output: 45.76 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 44.82it/s, est. speed input: 728.34 toks/s, output: 45.52 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 44.86it/s, est. speed input: 727.04 toks/s, output: 45.44 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 44.78it/s, est. speed input: 725.42 toks/s, output: 45.34 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:01, 44.80it/s, est. speed input: 724.60 toks/s, output: 45.29 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 44.92it/s, est. speed input: 724.44 toks/s, output: 45.28 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 44.96it/s, est. speed input: 724.14 toks/s, output: 45.26 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 45.01it/s, est. speed input: 723.94 toks/s, output: 45.25 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 45.07it/s, est. speed input: 723.91 toks/s, output: 45.24 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 45.21it/s, est. speed input: 724.24 toks/s, output: 45.26 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 45.24it/s, est. speed input: 724.27 toks/s, output: 45.27 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 45.19it/s, est. speed input: 724.08 toks/s, output: 45.25 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 44.81it/s, est. speed input: 722.84 toks/s, output: 45.18 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 44.94it/s, est. speed input: 722.90 toks/s, output: 45.18 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 44.63it/s, est. speed input: 721.83 toks/s, output: 45.11 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 44.75it/s, est. speed input: 721.77 toks/s, output: 45.11 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 44.83it/s, est. speed input: 721.70 toks/s, output: 45.11 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 44.88it/s, est. speed input: 721.62 toks/s, output: 45.10 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 44.94it/s, est. speed input: 721.61 toks/s, output: 45.10 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 45.03it/s, est. speed input: 721.71 toks/s, output: 45.11 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 45.05it/s, est. speed input: 721.70 toks/s, output: 45.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.05it/s, est. speed input: 721.62 toks/s, output: 45.10 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.10it/s, est. speed input: 721.62 toks/s, output: 45.10 toks/s]
[rank0]:[W126 13:03:53.988320314 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 13:03:55
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:04:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=449636) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=449636) WARNING 01-26 13:04:14 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=449636) WARNING 01-26 13:04:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 41.02 requests/s, 5291.75 total tokens/s, 41.02 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-26 13:04:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:04:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:04:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:04:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:04:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:04:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:04:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:04:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:04:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:04:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:04:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:04:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:04:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:04:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:04:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:04:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:04:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:04:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=449636) [2026-01-26 13:04:09] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=449636) [2026-01-26 13:04:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=449636) [2026-01-26 13:04:09] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=449636) [2026-01-26 13:04:09] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=449636) [2026-01-26 13:04:09] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=449636) [2026-01-26 13:04:09] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=449636) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=449636) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.26it/s]
(EngineCore_DP0 pid=449636) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.26it/s]
(EngineCore_DP0 pid=449636) 
(EngineCore_DP0 pid=449636) [2026-01-26 13:04:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=449636) [2026-01-26 13:04:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=449636) [2026-01-26 13:04:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=449636) [2026-01-26 13:04:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=449636) [2026-01-26 13:04:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=449636) [2026-01-26 13:04:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=449636) [2026-01-26 13:04:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=449636) [2026-01-26 13:04:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=449636) 2026-01-26 13:04:21,670 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=449636) 2026-01-26 13:04:21,693 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=449636) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.73it/s]
(EngineCore_DP0 pid=449636) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.77it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2020.65it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:18,  6.78it/s, est. speed input: 868.27 toks/s, output: 6.78 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:04, 26.07it/s, est. speed input: 2921.74 toks/s, output: 22.83 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 33.60it/s, est. speed input: 3731.02 toks/s, output: 29.15 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:02, 37.48it/s, est. speed input: 4167.47 toks/s, output: 32.56 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 39.71it/s, est. speed input: 4439.40 toks/s, output: 34.68 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 41.04it/s, est. speed input: 4622.88 toks/s, output: 36.12 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 42.00it/s, est. speed input: 4760.43 toks/s, output: 37.19 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 42.58it/s, est. speed input: 4862.84 toks/s, output: 37.99 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 43.09it/s, est. speed input: 4948.05 toks/s, output: 38.66 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:01, 43.34it/s, est. speed input: 5013.08 toks/s, output: 39.16 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 43.51it/s, est. speed input: 5066.77 toks/s, output: 39.58 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 43.62it/s, est. speed input: 5111.45 toks/s, output: 39.93 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 43.69it/s, est. speed input: 5149.19 toks/s, output: 40.23 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 43.73it/s, est. speed input: 5181.40 toks/s, output: 40.48 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 43.64it/s, est. speed input: 5206.31 toks/s, output: 40.67 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 43.35it/s, est. speed input: 5222.45 toks/s, output: 40.80 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 43.38it/s, est. speed input: 5242.20 toks/s, output: 40.95 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:00, 43.44it/s, est. speed input: 5260.65 toks/s, output: 41.10 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 43.44it/s, est. speed input: 5276.33 toks/s, output: 41.22 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 43.18it/s, est. speed input: 5285.12 toks/s, output: 41.29 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 43.34it/s, est. speed input: 5299.68 toks/s, output: 41.40 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 43.45it/s, est. speed input: 5312.81 toks/s, output: 41.51 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 43.53it/s, est. speed input: 5325.00 toks/s, output: 41.60 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 43.63it/s, est. speed input: 5336.90 toks/s, output: 41.69 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 43.67it/s, est. speed input: 5347.27 toks/s, output: 41.78 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 43.78it/s, est. speed input: 5358.16 toks/s, output: 41.86 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 43.78it/s, est. speed input: 5361.38 toks/s, output: 41.89 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.88it/s, est. speed input: 5361.38 toks/s, output: 41.89 toks/s]
[rank0]:[W126 13:04:26.852489819 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 13:04:28
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:04:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=450668) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=450668) WARNING 01-26 13:04:47 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=450668) WARNING 01-26 13:04:54 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 39.69 requests/s, 10201.41 total tokens/s, 39.69 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-26 13:04:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:04:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:04:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:04:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:04:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:04:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:04:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:04:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:04:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:04:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:04:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:04:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:04:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:04:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:04:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:04:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:04:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:04:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:04:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=450668) [2026-01-26 13:04:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=450668) [2026-01-26 13:04:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=450668) [2026-01-26 13:04:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=450668) [2026-01-26 13:04:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=450668) [2026-01-26 13:04:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=450668) [2026-01-26 13:04:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=450668) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=450668) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=450668) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=450668) 
(EngineCore_DP0 pid=450668) [2026-01-26 13:04:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=450668) [2026-01-26 13:04:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=450668) [2026-01-26 13:04:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=450668) [2026-01-26 13:04:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=450668) [2026-01-26 13:04:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=450668) [2026-01-26 13:04:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=450668) [2026-01-26 13:04:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=450668) [2026-01-26 13:04:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=450668) 2026-01-26 13:04:54,465 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=450668) 2026-01-26 13:04:54,485 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=450668) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.95it/s]
(EngineCore_DP0 pid=450668) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.69it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 372.75it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 421.52it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 38.94it/s, est. speed input: 9968.41 toks/s, output: 38.94 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:02, 41.92it/s, est. speed input: 10623.91 toks/s, output: 41.50 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:02, 42.93it/s, est. speed input: 10851.49 toks/s, output: 42.39 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 43.40it/s, est. speed input: 10964.46 toks/s, output: 42.83 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 43.68it/s, est. speed input: 11033.26 toks/s, output: 43.10 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 43.77it/s, est. speed input: 11070.22 toks/s, output: 43.24 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 43.89it/s, est. speed input: 11103.26 toks/s, output: 43.37 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:02, 43.95it/s, est. speed input: 11125.73 toks/s, output: 43.46 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:01, 43.93it/s, est. speed input: 11138.34 toks/s, output: 43.51 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:01, 43.97it/s, est. speed input: 11152.21 toks/s, output: 43.56 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 44.02it/s, est. speed input: 11166.09 toks/s, output: 43.62 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 44.00it/s, est. speed input: 11173.07 toks/s, output: 43.64 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 44.03it/s, est. speed input: 11182.43 toks/s, output: 43.68 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 43.84it/s, est. speed input: 11177.19 toks/s, output: 43.66 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 43.58it/s, est. speed input: 11165.37 toks/s, output: 43.61 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:01, 43.78it/s, est. speed input: 11175.30 toks/s, output: 43.65 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:01, 43.53it/s, est. speed input: 11164.80 toks/s, output: 43.61 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:00, 43.78it/s, est. speed input: 11175.47 toks/s, output: 43.65 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 43.94it/s, est. speed input: 11184.59 toks/s, output: 43.69 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 44.03it/s, est. speed input: 11191.50 toks/s, output: 43.72 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 44.02it/s, est. speed input: 11194.96 toks/s, output: 43.73 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 44.09it/s, est. speed input: 11201.17 toks/s, output: 43.75 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 44.18it/s, est. speed input: 11208.15 toks/s, output: 43.78 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 44.18it/s, est. speed input: 11212.41 toks/s, output: 43.80 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:02<00:00, 44.19it/s, est. speed input: 11216.65 toks/s, output: 43.81 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.19it/s, est. speed input: 11222.34 toks/s, output: 43.84 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.84it/s, est. speed input: 11222.34 toks/s, output: 43.84 toks/s]
[rank0]:[W126 13:04:59.720204219 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 13:38:23
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:38:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=511227) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=511227) WARNING 01-26 13:38:42 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=511227) WARNING 01-26 13:38:49 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 43.40 requests/s, 22262.90 total tokens/s, 43.40 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 13:38:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:38:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:38:29] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:38:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:38:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:38:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:38:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:38:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:38:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:38:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:38:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:38:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:38:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:38:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:38:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:38:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:38:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:38:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=511227) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=511227) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.70it/s]
(EngineCore_DP0 pid=511227) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.70it/s]
(EngineCore_DP0 pid=511227) 
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=511227) 2026-01-26 13:38:49,451 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=511227) 2026-01-26 13:38:49,473 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=511227) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.48it/s]
(EngineCore_DP0 pid=511227) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.41it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  58%|█████▊    | 74/128 [00:00<00:00, 731.99it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 748.89it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:01, 84.47it/s, est. speed input: 43254.44 toks/s, output: 84.48 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:01, 55.81it/s, est. speed input: 30193.58 toks/s, output: 58.97 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 50.72it/s, est. speed input: 27713.72 toks/s, output: 54.13 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 48.48it/s, est. speed input: 26608.68 toks/s, output: 51.97 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 47.23it/s, est. speed input: 25994.63 toks/s, output: 50.77 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 46.34it/s, est. speed input: 25544.68 toks/s, output: 49.89 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 45.72it/s, est. speed input: 25202.49 toks/s, output: 49.22 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 45.21it/s, est. speed input: 24919.49 toks/s, output: 48.67 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 44.85it/s, est. speed input: 24689.15 toks/s, output: 48.22 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 44.78it/s, est. speed input: 24529.16 toks/s, output: 47.91 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 44.62it/s, est. speed input: 24378.51 toks/s, output: 47.61 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 44.56it/s, est. speed input: 24258.21 toks/s, output: 47.38 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 44.59it/s, est. speed input: 24162.48 toks/s, output: 47.19 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 44.55it/s, est. speed input: 24071.85 toks/s, output: 47.01 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 44.59it/s, est. speed input: 24000.08 toks/s, output: 46.88 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 44.52it/s, est. speed input: 23926.49 toks/s, output: 46.73 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 44.48it/s, est. speed input: 23860.94 toks/s, output: 46.60 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 44.43it/s, est. speed input: 23801.03 toks/s, output: 46.49 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 44.48it/s, est. speed input: 23753.67 toks/s, output: 46.39 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 44.58it/s, est. speed input: 23716.86 toks/s, output: 46.32 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 44.54it/s, est. speed input: 23674.11 toks/s, output: 46.24 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 44.56it/s, est. speed input: 23638.53 toks/s, output: 46.17 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 44.56it/s, est. speed input: 23605.27 toks/s, output: 46.10 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.56it/s, est. speed input: 23597.13 toks/s, output: 46.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 46.09it/s, est. speed input: 23597.13 toks/s, output: 46.09 toks/s]
[rank0]:[W126 13:38:54.572888589 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 13:38:56
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:39:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=512249) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=512249) WARNING 01-26 13:39:16 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=512249) WARNING 01-26 13:39:23 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 40.77 requests/s, 41787.13 total tokens/s, 40.77 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 13:39:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:39:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:39:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:39:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:39:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:39:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:39:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:39:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:39:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:39:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:39:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:39:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:39:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:39:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:11] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:11] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:11] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:11] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:11] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=512249) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=512249) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.79it/s]
(EngineCore_DP0 pid=512249) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.79it/s]
(EngineCore_DP0 pid=512249) 
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=512249) 2026-01-26 13:39:23,177 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=512249) 2026-01-26 13:39:23,200 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=512249) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.46it/s]
(EngineCore_DP0 pid=512249) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.11it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  28%|██▊       | 36/128 [00:00<00:00, 353.86it/s]
Adding requests:  59%|█████▉    | 76/128 [00:00<00:00, 376.40it/s]
Adding requests:  92%|█████████▏| 118/128 [00:00<00:00, 391.79it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 389.33it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:01, 92.73it/s, est. speed input: 94961.24 toks/s, output: 92.73 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:01, 56.48it/s, est. speed input: 61624.97 toks/s, output: 60.18 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:01, 51.12it/s, est. speed input: 56338.60 toks/s, output: 55.02 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:01, 48.74it/s, est. speed input: 53982.47 toks/s, output: 52.72 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 47.13it/s, est. speed input: 52387.17 toks/s, output: 51.16 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 46.08it/s, est. speed input: 51369.87 toks/s, output: 50.17 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:01, 45.33it/s, est. speed input: 50589.02 toks/s, output: 49.40 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 44.79it/s, est. speed input: 49968.02 toks/s, output: 48.80 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 44.36it/s, est. speed input: 49450.27 toks/s, output: 48.29 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 44.13it/s, est. speed input: 49042.81 toks/s, output: 47.89 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 43.93it/s, est. speed input: 48686.08 toks/s, output: 47.54 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 43.73it/s, est. speed input: 48366.07 toks/s, output: 47.23 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 43.71it/s, est. speed input: 48119.88 toks/s, output: 46.99 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 43.64it/s, est. speed input: 47891.83 toks/s, output: 46.77 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 43.48it/s, est. speed input: 47667.09 toks/s, output: 46.55 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 43.37it/s, est. speed input: 47467.35 toks/s, output: 46.35 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 43.39it/s, est. speed input: 47308.36 toks/s, output: 46.20 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 43.42it/s, est. speed input: 47168.09 toks/s, output: 46.06 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 43.39it/s, est. speed input: 47032.72 toks/s, output: 45.93 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 43.35it/s, est. speed input: 46906.93 toks/s, output: 45.81 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 43.33it/s, est. speed input: 46793.37 toks/s, output: 45.70 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 43.37it/s, est. speed input: 46696.81 toks/s, output: 45.60 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.37it/s, est. speed input: 46647.69 toks/s, output: 45.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.55it/s, est. speed input: 46647.69 toks/s, output: 45.55 toks/s]
[rank0]:[W126 13:39:28.439558000 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 13:39:30
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:39:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=513290) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=513290) WARNING 01-26 13:39:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=513290) WARNING 01-26 13:39:57 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 82.95 requests/s, 85026.45 total tokens/s, 82.95 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 13:39:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:39:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:39:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:39:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:39:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:39:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:39:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:39:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:39:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:39:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:39:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:39:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:39:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:39:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=513290) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=513290) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.78it/s]
(EngineCore_DP0 pid=513290) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.78it/s]
(EngineCore_DP0 pid=513290) 
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=513290) 2026-01-26 13:39:57,496 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=513290) 2026-01-26 13:39:57,517 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=513290) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.24it/s]
(EngineCore_DP0 pid=513290) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.46it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  16%|█▌        | 40/256 [00:00<00:00, 390.47it/s]
Adding requests:  33%|███▎      | 84/256 [00:00<00:00, 415.55it/s]
Adding requests:  50%|█████     | 128/256 [00:00<00:00, 426.53it/s]
Adding requests:  67%|██████▋   | 172/256 [00:00<00:00, 429.50it/s]
Adding requests:  84%|████████▍ | 216/256 [00:00<00:00, 432.95it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 433.96it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:00<00:00, 472.35it/s, est. speed input: 483719.78 toks/s, output: 472.36 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:00<00:01, 131.79it/s, est. speed input: 151695.07 toks/s, output: 148.14 toks/s]
Processed prompts:  48%|████▊     | 123/256 [00:00<00:01, 115.67it/s, est. speed input: 134117.48 toks/s, output: 130.97 toks/s]
Processed prompts:  55%|█████▌    | 141/256 [00:01<00:01, 107.18it/s, est. speed input: 125894.41 toks/s, output: 122.94 toks/s]
Processed prompts:  61%|██████    | 155/256 [00:01<00:00, 101.44it/s, est. speed input: 120879.59 toks/s, output: 118.05 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:01<00:00, 98.03it/s, est. speed input: 117805.93 toks/s, output: 115.04 toks/s] 
Processed prompts:  70%|██████▉   | 178/256 [00:01<00:00, 93.39it/s, est. speed input: 114610.38 toks/s, output: 111.92 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:01<00:00, 91.86it/s, est. speed input: 112860.76 toks/s, output: 110.22 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:01<00:00, 90.02it/s, est. speed input: 111151.41 toks/s, output: 108.55 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:01<00:00, 89.11it/s, est. speed input: 109803.88 toks/s, output: 107.23 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:02<00:00, 88.36it/s, est. speed input: 108600.07 toks/s, output: 106.05 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 87.91it/s, est. speed input: 107547.67 toks/s, output: 105.03 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:02<00:00, 87.50it/s, est. speed input: 106587.44 toks/s, output: 104.09 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:02<00:00, 86.99it/s, est. speed input: 105675.46 toks/s, output: 103.20 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 86.99it/s, est. speed input: 105070.99 toks/s, output: 102.61 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 102.60it/s, est. speed input: 105070.99 toks/s, output: 102.61 toks/s]
[rank0]:[W126 13:40:02.959055421 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 13:40:04
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:40:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=514341) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=514341) WARNING 01-26 13:40:25 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=514341) WARNING 01-26 13:40:32 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 160.80 requests/s, 164817.36 total tokens/s, 160.80 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 13:40:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:40:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:40:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:40:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:40:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:40:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:40:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:40:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:40:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:40:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:40:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:40:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:40:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:40:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=514341) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=514341) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.07it/s]
(EngineCore_DP0 pid=514341) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.07it/s]
(EngineCore_DP0 pid=514341) 
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=514341) 2026-01-26 13:40:32,703 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=514341) 2026-01-26 13:40:32,725 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=514341) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 23.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 20.00it/s]
(EngineCore_DP0 pid=514341) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.10it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.08it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 40/512 [00:00<00:01, 399.40it/s]
Adding requests:  17%|█▋        | 85/512 [00:00<00:01, 424.67it/s]
Adding requests:  25%|██▌       | 128/512 [00:00<00:00, 423.11it/s]
Adding requests:  34%|███▎      | 172/512 [00:00<00:00, 427.29it/s]
Adding requests:  43%|████▎     | 218/512 [00:00<00:00, 436.44it/s]
Adding requests:  52%|█████▏    | 266/512 [00:00<00:00, 449.42it/s]
Adding requests:  61%|██████    | 312/512 [00:00<00:00, 449.86it/s]
Adding requests:  70%|██████▉   | 358/512 [00:00<00:00, 452.98it/s]
Adding requests:  79%|███████▉  | 406/512 [00:00<00:00, 459.26it/s]
Adding requests:  88%|████████▊ | 453/512 [00:01<00:00, 460.71it/s]
Adding requests:  98%|█████████▊| 500/512 [00:01<00:00, 461.75it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 449.43it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:00<00:00, 1585.81it/s, est. speed input: 1623968.09 toks/s, output: 1585.83 toks/s]
Processed prompts:  67%|██████▋   | 341/512 [00:01<00:00, 291.55it/s, est. speed input: 343432.13 toks/s, output: 335.38 toks/s]   
Processed prompts:  81%|████████▏ | 417/512 [00:01<00:00, 242.55it/s, est. speed input: 290676.38 toks/s, output: 283.86 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:01<00:00, 218.71it/s, est. speed input: 268075.37 toks/s, output: 261.79 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:01<00:00, 208.88it/s, est. speed input: 258456.10 toks/s, output: 252.40 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 208.88it/s, est. speed input: 256534.44 toks/s, output: 250.52 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 250.51it/s, est. speed input: 256534.44 toks/s, output: 250.52 toks/s]
[rank0]:[W126 13:40:38.464750881 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 13:40:39
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:40:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=515427) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=515427) WARNING 01-26 13:41:03 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=515427) WARNING 01-26 13:41:10 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 300.17 requests/s, 307675.96 total tokens/s, 300.17 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 13:40:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:40:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:40:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:40:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:40:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:40:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:40:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:40:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:40:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:40:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:40:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:40:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:40:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:40:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=515427) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=515427) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.39it/s]
(EngineCore_DP0 pid=515427) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.39it/s]
(EngineCore_DP0 pid=515427) 
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=515427) 2026-01-26 13:41:10,494 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=515427) 2026-01-26 13:41:10,516 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=515427) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 15.58it/s]
(EngineCore_DP0 pid=515427) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.24it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 23.13it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 41/1024 [00:00<00:02, 402.90it/s]
Adding requests:   8%|▊         | 86/1024 [00:00<00:02, 430.17it/s]
Adding requests:  13%|█▎        | 131/1024 [00:00<00:02, 438.97it/s]
Adding requests:  17%|█▋        | 176/1024 [00:00<00:01, 440.46it/s]
Adding requests:  22%|██▏       | 223/1024 [00:00<00:01, 449.47it/s]
Adding requests:  26%|██▋       | 271/1024 [00:00<00:01, 459.59it/s]
Adding requests:  31%|███       | 317/1024 [00:00<00:01, 452.77it/s]
Adding requests:  36%|███▌      | 366/1024 [00:00<00:01, 461.52it/s]
Adding requests:  40%|████      | 413/1024 [00:00<00:01, 463.34it/s]
Adding requests:  45%|████▌     | 461/1024 [00:01<00:01, 467.31it/s]
Adding requests:  50%|████▉     | 508/1024 [00:01<00:01, 465.06it/s]
Adding requests:  54%|█████▍    | 555/1024 [00:01<00:01, 464.36it/s]
Adding requests:  59%|█████▉    | 602/1024 [00:01<00:00, 465.98it/s]
Adding requests:  64%|██████▎   | 652/1024 [00:01<00:00, 474.80it/s]
Adding requests:  69%|██████▊   | 702/1024 [00:01<00:00, 481.65it/s]
Adding requests:  73%|███████▎  | 751/1024 [00:01<00:00, 479.32it/s]
Adding requests:  78%|███████▊  | 799/1024 [00:01<00:00, 476.04it/s]
Adding requests:  83%|████████▎ | 847/1024 [00:01<00:00, 466.62it/s]
Adding requests:  88%|████████▊ | 897/1024 [00:01<00:00, 475.22it/s]
Adding requests:  92%|█████████▏| 945/1024 [00:02<00:00, 475.53it/s]
Adding requests:  97%|█████████▋| 994/1024 [00:02<00:00, 478.13it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 466.40it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:00<00:00, 5497.04it/s, est. speed input: 5629302.52 toks/s, output: 5497.13 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 5497.04it/s, est. speed input: 863290.52 toks/s, output: 843.05 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 842.98it/s, est. speed input: 863290.52 toks/s, output: 843.05 toks/s] 
[rank0]:[W126 13:41:16.789073693 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 13:41:18
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:41:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=516585) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=516585) WARNING 01-26 13:41:46 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=516585) WARNING 01-26 13:41:53 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 461.30 requests/s, 472834.42 total tokens/s, 461.30 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 13:41:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:41:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:41:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:41:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:41:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:41:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:41:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:41:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:41:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:41:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:41:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:41:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:41:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:41:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:41:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:41:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:41:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:41:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=516585) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=516585) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.43it/s]
(EngineCore_DP0 pid=516585) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.42it/s]
(EngineCore_DP0 pid=516585) 
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=516585) 2026-01-26 13:41:53,430 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=516585) 2026-01-26 13:41:53,466 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=516585) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 21.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 23.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 21.62it/s]
(EngineCore_DP0 pid=516585) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 23.49it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 23.68it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 40/2048 [00:00<00:05, 399.46it/s]
Adding requests:   4%|▍         | 85/2048 [00:00<00:04, 424.90it/s]
Adding requests:   6%|▋         | 130/2048 [00:00<00:04, 433.49it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:04, 434.54it/s]
Adding requests:  11%|█         | 220/2048 [00:00<00:04, 442.23it/s]
Adding requests:  13%|█▎        | 268/2048 [00:00<00:03, 452.43it/s]
Adding requests:  15%|█▌        | 314/2048 [00:00<00:03, 451.87it/s]
Adding requests:  18%|█▊        | 360/2048 [00:00<00:03, 453.62it/s]
Adding requests:  20%|█▉        | 408/2048 [00:00<00:03, 459.39it/s]
Adding requests:  22%|██▏       | 455/2048 [00:01<00:03, 462.60it/s]
Adding requests:  25%|██▍       | 502/2048 [00:01<00:03, 461.22it/s]
Adding requests:  27%|██▋       | 549/2048 [00:01<00:03, 456.58it/s]
Adding requests:  29%|██▉       | 596/2048 [00:01<00:03, 458.66it/s]
Adding requests:  31%|███▏      | 644/2048 [00:01<00:03, 464.88it/s]
Adding requests:  34%|███▍      | 694/2048 [00:01<00:02, 472.63it/s]
Adding requests:  36%|███▌      | 742/2048 [00:01<00:02, 471.75it/s]
Adding requests:  39%|███▊      | 790/2048 [00:01<00:02, 469.83it/s]
Adding requests:  41%|████      | 837/2048 [00:01<00:02, 460.01it/s]
Adding requests:  43%|████▎     | 886/2048 [00:01<00:02, 467.51it/s]
Adding requests:  46%|████▌     | 935/2048 [00:02<00:02, 470.71it/s]
Adding requests:  48%|████▊     | 983/2048 [00:02<00:02, 473.38it/s]
Adding requests:  50%|█████     | 1032/2048 [00:02<00:02, 475.95it/s]
Adding requests:  53%|█████▎    | 1080/2048 [00:02<00:02, 473.42it/s]
Adding requests:  55%|█████▌    | 1128/2048 [00:02<00:01, 469.59it/s]
Adding requests:  58%|█████▊    | 1178/2048 [00:02<00:01, 475.83it/s]
Adding requests:  60%|█████▉    | 1228/2048 [00:02<00:01, 481.06it/s]
Adding requests:  62%|██████▏   | 1277/2048 [00:02<00:01, 472.85it/s]
Adding requests:  65%|██████▍   | 1326/2048 [00:02<00:01, 476.93it/s]
Adding requests:  67%|██████▋   | 1375/2048 [00:02<00:01, 479.58it/s]
Adding requests:  70%|██████▉   | 1424/2048 [00:03<00:01, 480.24it/s]
Adding requests:  72%|███████▏  | 1473/2048 [00:03<00:01, 480.28it/s]
Adding requests:  74%|███████▍  | 1522/2048 [00:03<00:01, 482.08it/s]
Adding requests:  77%|███████▋  | 1571/2048 [00:03<00:00, 482.71it/s]
Adding requests:  79%|███████▉  | 1620/2048 [00:03<00:00, 475.78it/s]
Adding requests:  81%|████████▏ | 1668/2048 [00:03<00:00, 474.25it/s]
Adding requests:  84%|████████▍ | 1717/2048 [00:03<00:00, 477.03it/s]
Adding requests:  86%|████████▌ | 1765/2048 [00:03<00:00, 474.25it/s]
Adding requests:  89%|████████▊ | 1813/2048 [00:03<00:00, 474.15it/s]
Adding requests:  91%|█████████ | 1861/2048 [00:03<00:00, 472.51it/s]
Adding requests:  93%|█████████▎| 1909/2048 [00:04<00:00, 471.09it/s]
Adding requests:  96%|█████████▌| 1957/2048 [00:04<00:00, 473.61it/s]
Adding requests:  98%|█████████▊| 2006/2048 [00:04<00:00, 477.71it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 468.30it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 31567.41it/s, est. speed input: 32334535.47 toks/s, output: 31574.61 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 31518.76it/s, est. speed input: 32334535.47 toks/s, output: 31574.61 toks/s]
[rank0]:[W126 13:42:00.383973370 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 13:42:01
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:42:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=517848) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=517848) WARNING 01-26 13:42:39 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=517848) WARNING 01-26 13:42:46 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 466.73 requests/s, 478402.83 total tokens/s, 466.73 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 13:42:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:42:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:42:26] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:42:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:42:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:42:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:42:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:42:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:42:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:42:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:42:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:42:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:42:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:42:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:42:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:42:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:42:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:42:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:34] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=517848) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=517848) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.42it/s]
(EngineCore_DP0 pid=517848) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.42it/s]
(EngineCore_DP0 pid=517848) 
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=517848) [rank0]:W0126 13:42:43.017000 517848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=517848) [rank0]:W0126 13:42:43.087000 517848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=517848) [rank0]:W0126 13:42:43.887000 517848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=517848) [rank0]:W0126 13:42:43.985000 517848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=517848) 2026-01-26 13:42:46,323 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=517848) 2026-01-26 13:42:46,348 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=517848) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 19.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 20.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 22.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 19.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 19.88it/s]
(EngineCore_DP0 pid=517848) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 21.87it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 22.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 22.95it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 40/4096 [00:00<00:10, 397.79it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 420.52it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:09, 431.23it/s]
Adding requests:   4%|▍         | 173/4096 [00:00<00:09, 434.42it/s]
Adding requests:   5%|▌         | 219/4096 [00:00<00:08, 440.92it/s]
Adding requests:   7%|▋         | 267/4096 [00:00<00:08, 452.80it/s]
Adding requests:   8%|▊         | 313/4096 [00:00<00:08, 451.05it/s]
Adding requests:   9%|▉         | 359/4096 [00:00<00:08, 453.34it/s]
Adding requests:  10%|▉         | 406/4096 [00:00<00:08, 457.92it/s]
Adding requests:  11%|█         | 453/4096 [00:01<00:07, 459.18it/s]
Adding requests:  12%|█▏        | 500/4096 [00:01<00:07, 459.81it/s]
Adding requests:  13%|█▎        | 546/4096 [00:01<00:07, 455.09it/s]
Adding requests:  15%|█▍        | 596/4096 [00:01<00:07, 463.94it/s]
Adding requests:  16%|█▌        | 644/4096 [00:01<00:07, 468.11it/s]
Adding requests:  17%|█▋        | 694/4096 [00:01<00:07, 475.90it/s]
Adding requests:  18%|█▊        | 742/4096 [00:01<00:07, 473.23it/s]
Adding requests:  19%|█▉        | 790/4096 [00:01<00:06, 472.55it/s]
Adding requests:  20%|██        | 838/4096 [00:01<00:07, 463.97it/s]
Adding requests:  22%|██▏       | 885/4096 [00:01<00:06, 462.46it/s]
Adding requests:  23%|██▎       | 934/4096 [00:02<00:06, 469.19it/s]
Adding requests:  24%|██▍       | 981/4096 [00:02<00:06, 459.54it/s]
Adding requests:  25%|██▌       | 1030/4096 [00:02<00:06, 466.09it/s]
Adding requests:  26%|██▋       | 1077/4096 [00:02<00:06, 462.86it/s]
Adding requests:  27%|██▋       | 1124/4096 [00:02<00:06, 462.80it/s]
Adding requests:  29%|██▊       | 1173/4096 [00:02<00:06, 470.01it/s]
Adding requests:  30%|██▉       | 1223/4096 [00:02<00:06, 476.65it/s]
Adding requests:  31%|███       | 1271/4096 [00:02<00:05, 472.81it/s]
Adding requests:  32%|███▏      | 1320/4096 [00:02<00:05, 475.39it/s]
Adding requests:  33%|███▎      | 1369/4096 [00:02<00:05, 478.58it/s]
Adding requests:  35%|███▍      | 1417/4096 [00:03<00:05, 478.68it/s]
Adding requests:  36%|███▌      | 1466/4096 [00:03<00:05, 481.21it/s]
Adding requests:  37%|███▋      | 1515/4096 [00:03<00:05, 482.85it/s]
Adding requests:  38%|███▊      | 1564/4096 [00:03<00:05, 482.25it/s]
Adding requests:  39%|███▉      | 1614/4096 [00:03<00:05, 485.51it/s]
Adding requests:  41%|████      | 1663/4096 [00:03<00:05, 481.70it/s]
Adding requests:  42%|████▏     | 1712/4096 [00:03<00:04, 481.08it/s]
Adding requests:  43%|████▎     | 1761/4096 [00:03<00:04, 479.01it/s]
Adding requests:  44%|████▍     | 1810/4096 [00:03<00:04, 479.85it/s]
Adding requests:  45%|████▌     | 1859/4096 [00:03<00:04, 480.54it/s]
Adding requests:  47%|████▋     | 1908/4096 [00:04<00:04, 467.96it/s]
Adding requests:  48%|████▊     | 1956/4096 [00:04<00:04, 470.69it/s]
Adding requests:  49%|████▉     | 2005/4096 [00:04<00:04, 474.91it/s]
Adding requests:  50%|█████     | 2054/4096 [00:04<00:04, 476.91it/s]
Adding requests:  51%|█████▏    | 2102/4096 [00:04<00:04, 459.94it/s]
Adding requests:  52%|█████▏    | 2149/4096 [00:04<00:04, 459.35it/s]
Adding requests:  54%|█████▎    | 2196/4096 [00:04<00:04, 459.17it/s]
Adding requests:  55%|█████▍    | 2245/4096 [00:04<00:03, 465.89it/s]
Adding requests:  56%|█████▌    | 2292/4096 [00:04<00:03, 466.84it/s]
Adding requests:  57%|█████▋    | 2340/4096 [00:05<00:03, 469.18it/s]
Adding requests:  58%|█████▊    | 2388/4096 [00:05<00:03, 471.92it/s]
Adding requests:  59%|█████▉    | 2437/4096 [00:05<00:03, 474.26it/s]
Adding requests:  61%|██████    | 2485/4096 [00:05<00:03, 475.07it/s]
Adding requests:  62%|██████▏   | 2533/4096 [00:05<00:03, 476.01it/s]
Adding requests:  63%|██████▎   | 2582/4096 [00:05<00:03, 479.05it/s]
Adding requests:  64%|██████▍   | 2630/4096 [00:05<00:03, 478.13it/s]
Adding requests:  65%|██████▌   | 2678/4096 [00:05<00:03, 472.05it/s]
Adding requests:  67%|██████▋   | 2726/4096 [00:05<00:02, 473.44it/s]
Adding requests:  68%|██████▊   | 2774/4096 [00:05<00:02, 474.20it/s]
Adding requests:  69%|██████▉   | 2822/4096 [00:06<00:02, 469.23it/s]
Adding requests:  70%|███████   | 2871/4096 [00:06<00:02, 473.10it/s]
Adding requests:  71%|███████▏  | 2919/4096 [00:06<00:02, 474.09it/s]
Adding requests:  72%|███████▏  | 2967/4096 [00:06<00:02, 472.29it/s]
Adding requests:  74%|███████▎  | 3015/4096 [00:06<00:02, 473.87it/s]
Adding requests:  75%|███████▍  | 3063/4096 [00:06<00:02, 473.17it/s]
Adding requests:  76%|███████▌  | 3111/4096 [00:06<00:02, 462.24it/s]
Adding requests:  77%|███████▋  | 3159/4096 [00:06<00:02, 466.04it/s]
Adding requests:  78%|███████▊  | 3206/4096 [00:06<00:01, 466.86it/s]
Adding requests:  79%|███████▉  | 3255/4096 [00:06<00:01, 473.14it/s]
Adding requests:  81%|████████  | 3303/4096 [00:07<00:01, 473.72it/s]
Adding requests:  82%|████████▏ | 3352/4096 [00:07<00:01, 476.86it/s]
Adding requests:  83%|████████▎ | 3400/4096 [00:07<00:01, 475.33it/s]
Adding requests:  84%|████████▍ | 3448/4096 [00:07<00:01, 476.04it/s]
Adding requests:  85%|████████▌ | 3496/4096 [00:07<00:01, 472.11it/s]
Adding requests:  87%|████████▋ | 3544/4096 [00:07<00:01, 474.17it/s]
Adding requests:  88%|████████▊ | 3592/4096 [00:07<00:01, 473.41it/s]
Adding requests:  89%|████████▉ | 3640/4096 [00:07<00:00, 470.03it/s]
Adding requests:  90%|█████████ | 3689/4096 [00:07<00:00, 473.51it/s]
Adding requests:  91%|█████████ | 3737/4096 [00:07<00:00, 472.95it/s]
Adding requests:  92%|█████████▏| 3787/4096 [00:08<00:00, 480.28it/s]
Adding requests:  94%|█████████▎| 3836/4096 [00:08<00:00, 480.97it/s]
Adding requests:  95%|█████████▍| 3885/4096 [00:08<00:00, 483.30it/s]
Adding requests:  96%|█████████▌| 3934/4096 [00:08<00:00, 482.93it/s]
Adding requests:  97%|█████████▋| 3983/4096 [00:08<00:00, 481.62it/s]
Adding requests:  98%|█████████▊| 4032/4096 [00:08<00:00, 479.58it/s]
Adding requests: 100%|█████████▉| 4080/4096 [00:08<00:00, 475.66it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 470.40it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 61662.12it/s, est. speed input: 63156068.53 toks/s, output: 61671.20 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 61564.24it/s, est. speed input: 63156068.53 toks/s, output: 61671.20 toks/s]
[rank0]:[W126 13:42:57.065522874 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 13:42:59
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:43:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=519450) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=519450) WARNING 01-26 13:43:55 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=519450) WARNING 01-26 13:44:02 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 470.45 requests/s, 482206.74 total tokens/s, 470.45 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 13:43:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:43:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:43:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:43:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:43:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:43:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:43:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:43:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:43:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:43:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:43:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:43:50] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:43:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:43:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:43:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:43:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:43:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:43:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=519450) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=519450) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=519450) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=519450) 
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=519450) [rank0]:W0126 13:43:59.218000 519450 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=519450) [rank0]:W0126 13:43:59.288000 519450 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=519450) [rank0]:W0126 13:44:00.096000 519450 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=519450) [rank0]:W0126 13:44:00.195000 519450 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=519450) 2026-01-26 13:44:02,518 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=519450) 2026-01-26 13:44:02,586 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=519450) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:00, 17.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:00, 19.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 20.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 20.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 21.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:00<00:00, 22.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 20.00it/s]
(EngineCore_DP0 pid=519450) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00, 21.48it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 22.58it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 22.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 22.87it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 36/8192 [00:00<00:22, 355.57it/s]
Adding requests:   1%|          | 80/8192 [00:00<00:20, 404.02it/s]
Adding requests:   2%|▏         | 124/8192 [00:00<00:19, 418.93it/s]
Adding requests:   2%|▏         | 166/8192 [00:00<00:19, 418.63it/s]
Adding requests:   3%|▎         | 211/8192 [00:00<00:18, 427.81it/s]
Adding requests:   3%|▎         | 259/8192 [00:00<00:17, 444.74it/s]
Adding requests:   4%|▎         | 304/8192 [00:00<00:17, 442.83it/s]
Adding requests:   4%|▍         | 351/8192 [00:00<00:17, 450.80it/s]
Adding requests:   5%|▍         | 399/8192 [00:00<00:17, 457.30it/s]
Adding requests:   5%|▌         | 446/8192 [00:01<00:16, 459.33it/s]
Adding requests:   6%|▌         | 492/8192 [00:01<00:17, 451.55it/s]
Adding requests:   7%|▋         | 538/8192 [00:01<00:17, 440.00it/s]
Adding requests:   7%|▋         | 588/8192 [00:01<00:16, 454.92it/s]
Adding requests:   8%|▊         | 635/8192 [00:01<00:16, 458.54it/s]
Adding requests:   8%|▊         | 684/8192 [00:01<00:16, 466.65it/s]
Adding requests:   9%|▉         | 733/8192 [00:01<00:15, 472.07it/s]
Adding requests:  10%|▉         | 781/8192 [00:01<00:15, 467.19it/s]
Adding requests:  10%|█         | 828/8192 [00:01<00:16, 458.42it/s]
Adding requests:  11%|█         | 876/8192 [00:01<00:15, 462.18it/s]
Adding requests:  11%|█▏        | 925/8192 [00:02<00:15, 467.77it/s]
Adding requests:  12%|█▏        | 973/8192 [00:02<00:15, 469.55it/s]
Adding requests:  12%|█▏        | 1021/8192 [00:02<00:15, 472.46it/s]
Adding requests:  13%|█▎        | 1069/8192 [00:02<00:15, 469.90it/s]
Adding requests:  14%|█▎        | 1117/8192 [00:02<00:15, 465.12it/s]
Adding requests:  14%|█▍        | 1167/8192 [00:02<00:14, 473.20it/s]
Adding requests:  15%|█▍        | 1217/8192 [00:02<00:14, 480.28it/s]
Adding requests:  15%|█▌        | 1266/8192 [00:02<00:14, 474.88it/s]
Adding requests:  16%|█▌        | 1314/8192 [00:02<00:14, 474.41it/s]
Adding requests:  17%|█▋        | 1363/8192 [00:02<00:14, 478.32it/s]
Adding requests:  17%|█▋        | 1413/8192 [00:03<00:14, 483.19it/s]
Adding requests:  18%|█▊        | 1462/8192 [00:03<00:14, 468.16it/s]
Adding requests:  18%|█▊        | 1510/8192 [00:03<00:14, 470.91it/s]
Adding requests:  19%|█▉        | 1558/8192 [00:03<00:14, 472.18it/s]
Adding requests:  20%|█▉        | 1606/8192 [00:03<00:14, 465.27it/s]
Adding requests:  20%|██        | 1653/8192 [00:03<00:14, 466.51it/s]
Adding requests:  21%|██        | 1700/8192 [00:03<00:13, 466.87it/s]
Adding requests:  21%|██▏       | 1748/8192 [00:03<00:13, 470.54it/s]
Adding requests:  22%|██▏       | 1797/8192 [00:03<00:13, 474.78it/s]
Adding requests:  23%|██▎       | 1846/8192 [00:03<00:13, 476.77it/s]
Adding requests:  23%|██▎       | 1894/8192 [00:04<00:13, 475.37it/s]
Adding requests:  24%|██▎       | 1942/8192 [00:04<00:13, 476.20it/s]
Adding requests:  24%|██▍       | 1990/8192 [00:04<00:13, 475.97it/s]
Adding requests:  25%|██▍       | 2039/8192 [00:04<00:12, 479.66it/s]
Adding requests:  26%|██▌       | 2089/8192 [00:04<00:12, 482.70it/s]
Adding requests:  26%|██▌       | 2138/8192 [00:04<00:12, 478.14it/s]
Adding requests:  27%|██▋       | 2186/8192 [00:04<00:12, 472.31it/s]
Adding requests:  27%|██▋       | 2236/8192 [00:04<00:12, 478.35it/s]
Adding requests:  28%|██▊       | 2284/8192 [00:04<00:12, 477.01it/s]
Adding requests:  28%|██▊       | 2333/8192 [00:05<00:12, 478.05it/s]
Adding requests:  29%|██▉       | 2381/8192 [00:05<00:12, 477.73it/s]
Adding requests:  30%|██▉       | 2430/8192 [00:05<00:11, 480.80it/s]
Adding requests:  30%|███       | 2479/8192 [00:05<00:11, 477.65it/s]
Adding requests:  31%|███       | 2527/8192 [00:05<00:12, 471.01it/s]
Adding requests:  31%|███▏      | 2575/8192 [00:05<00:11, 471.46it/s]
Adding requests:  32%|███▏      | 2623/8192 [00:05<00:11, 469.72it/s]
Adding requests:  33%|███▎      | 2670/8192 [00:05<00:11, 460.62it/s]
Adding requests:  33%|███▎      | 2717/8192 [00:05<00:11, 457.98it/s]
Adding requests:  34%|███▍      | 2765/8192 [00:05<00:11, 462.28it/s]
Adding requests:  34%|███▍      | 2812/8192 [00:06<00:11, 458.33it/s]
Adding requests:  35%|███▍      | 2860/8192 [00:06<00:11, 461.84it/s]
Adding requests:  35%|███▌      | 2907/8192 [00:06<00:11, 461.47it/s]
Adding requests:  36%|███▌      | 2954/8192 [00:06<00:11, 459.37it/s]
Adding requests:  37%|███▋      | 3001/8192 [00:06<00:11, 461.69it/s]
Adding requests:  37%|███▋      | 3049/8192 [00:06<00:11, 465.15it/s]
Adding requests:  38%|███▊      | 3096/8192 [00:06<00:11, 450.37it/s]
Adding requests:  38%|███▊      | 3143/8192 [00:06<00:11, 455.41it/s]
Adding requests:  39%|███▉      | 3191/8192 [00:06<00:10, 462.24it/s]
Adding requests:  40%|███▉      | 3240/8192 [00:06<00:10, 468.34it/s]
Adding requests:  40%|████      | 3288/8192 [00:07<00:10, 470.12it/s]
Adding requests:  41%|████      | 3336/8192 [00:07<00:10, 472.69it/s]
Adding requests:  41%|████▏     | 3386/8192 [00:07<00:10, 477.87it/s]
Adding requests:  42%|████▏     | 3435/8192 [00:07<00:09, 481.33it/s]
Adding requests:  43%|████▎     | 3484/8192 [00:07<00:10, 470.05it/s]
Adding requests:  43%|████▎     | 3532/8192 [00:07<00:09, 472.35it/s]
Adding requests:  44%|████▎     | 3580/8192 [00:07<00:09, 473.40it/s]
Adding requests:  44%|████▍     | 3628/8192 [00:07<00:09, 472.71it/s]
Adding requests:  45%|████▍     | 3676/8192 [00:07<00:09, 471.99it/s]
Adding requests:  45%|████▌     | 3724/8192 [00:07<00:09, 468.70it/s]
Adding requests:  46%|████▌     | 3774/8192 [00:08<00:09, 476.03it/s]
Adding requests:  47%|████▋     | 3823/8192 [00:08<00:09, 479.14it/s]
Adding requests:  47%|████▋     | 3871/8192 [00:08<00:09, 471.58it/s]
Adding requests:  48%|████▊     | 3920/8192 [00:08<00:09, 474.46it/s]
Adding requests:  48%|████▊     | 3968/8192 [00:08<00:08, 475.31it/s]
Adding requests:  49%|████▉     | 4016/8192 [00:08<00:08, 476.19it/s]
Adding requests:  50%|████▉     | 4064/8192 [00:08<00:08, 470.88it/s]
Adding requests:  50%|█████     | 4113/8192 [00:08<00:08, 475.24it/s]
Adding requests:  51%|█████     | 4162/8192 [00:08<00:08, 477.70it/s]
Adding requests:  51%|█████▏    | 4211/8192 [00:09<00:08, 481.15it/s]
Adding requests:  52%|█████▏    | 4260/8192 [00:09<00:08, 480.57it/s]
Adding requests:  53%|█████▎    | 4309/8192 [00:09<00:08, 482.06it/s]
Adding requests:  53%|█████▎    | 4359/8192 [00:09<00:07, 485.73it/s]
Adding requests:  54%|█████▍    | 4410/8192 [00:09<00:07, 491.27it/s]
Adding requests:  54%|█████▍    | 4460/8192 [00:09<00:07, 472.12it/s]
Adding requests:  55%|█████▌    | 4508/8192 [00:09<00:07, 471.55it/s]
Adding requests:  56%|█████▌    | 4556/8192 [00:09<00:07, 471.38it/s]
Adding requests:  56%|█████▌    | 4606/8192 [00:09<00:07, 479.42it/s]
Adding requests:  57%|█████▋    | 4655/8192 [00:09<00:07, 479.04it/s]
Adding requests:  57%|█████▋    | 4703/8192 [00:10<00:07, 477.60it/s]
Adding requests:  58%|█████▊    | 4752/8192 [00:10<00:07, 479.80it/s]
Adding requests:  59%|█████▊    | 4801/8192 [00:10<00:07, 480.47it/s]
Adding requests:  59%|█████▉    | 4850/8192 [00:10<00:06, 479.49it/s]
Adding requests:  60%|█████▉    | 4898/8192 [00:10<00:06, 475.48it/s]
Adding requests:  60%|██████    | 4948/8192 [00:10<00:06, 480.34it/s]
Adding requests:  61%|██████    | 4997/8192 [00:10<00:06, 480.09it/s]
Adding requests:  62%|██████▏   | 5046/8192 [00:10<00:06, 465.92it/s]
Adding requests:  62%|██████▏   | 5096/8192 [00:10<00:06, 475.72it/s]
Adding requests:  63%|██████▎   | 5145/8192 [00:10<00:06, 477.32it/s]
Adding requests:  63%|██████▎   | 5195/8192 [00:11<00:06, 481.35it/s]
Adding requests:  64%|██████▍   | 5244/8192 [00:11<00:06, 479.71it/s]
Adding requests:  65%|██████▍   | 5293/8192 [00:11<00:06, 478.94it/s]
Adding requests:  65%|██████▌   | 5342/8192 [00:11<00:05, 482.13it/s]
Adding requests:  66%|██████▌   | 5391/8192 [00:11<00:05, 484.16it/s]
Adding requests:  66%|██████▋   | 5440/8192 [00:11<00:05, 481.28it/s]
Adding requests:  67%|██████▋   | 5489/8192 [00:11<00:05, 478.88it/s]
Adding requests:  68%|██████▊   | 5538/8192 [00:11<00:05, 479.78it/s]
Adding requests:  68%|██████▊   | 5587/8192 [00:11<00:05, 480.80it/s]
Adding requests:  69%|██████▉   | 5636/8192 [00:11<00:05, 478.50it/s]
Adding requests:  69%|██████▉   | 5684/8192 [00:12<00:05, 476.65it/s]
Adding requests:  70%|██████▉   | 5733/8192 [00:12<00:05, 478.88it/s]
Adding requests:  71%|███████   | 5782/8192 [00:12<00:05, 480.15it/s]
Adding requests:  71%|███████   | 5831/8192 [00:12<00:05, 470.92it/s]
Adding requests:  72%|███████▏  | 5880/8192 [00:12<00:04, 474.08it/s]
Adding requests:  72%|███████▏  | 5928/8192 [00:12<00:04, 475.31it/s]
Adding requests:  73%|███████▎  | 5976/8192 [00:12<00:04, 475.00it/s]
Adding requests:  74%|███████▎  | 6025/8192 [00:12<00:04, 477.67it/s]
Adding requests:  74%|███████▍  | 6074/8192 [00:12<00:04, 479.94it/s]
Adding requests:  75%|███████▍  | 6123/8192 [00:13<00:04, 479.73it/s]
Adding requests:  75%|███████▌  | 6171/8192 [00:13<00:04, 479.04it/s]
Adding requests:  76%|███████▌  | 6219/8192 [00:13<00:04, 474.17it/s]
Adding requests:  77%|███████▋  | 6269/8192 [00:13<00:03, 481.46it/s]
Adding requests:  77%|███████▋  | 6319/8192 [00:13<00:03, 485.30it/s]
Adding requests:  78%|███████▊  | 6369/8192 [00:13<00:03, 487.83it/s]
Adding requests:  78%|███████▊  | 6418/8192 [00:13<00:03, 487.45it/s]
Adding requests:  79%|███████▉  | 6469/8192 [00:13<00:03, 491.29it/s]
Adding requests:  80%|███████▉  | 6521/8192 [00:13<00:03, 496.82it/s]
Adding requests:  80%|████████  | 6571/8192 [00:13<00:03, 496.05it/s]
Adding requests:  81%|████████  | 6621/8192 [00:14<00:03, 489.98it/s]
Adding requests:  81%|████████▏ | 6671/8192 [00:14<00:03, 488.60it/s]
Adding requests:  82%|████████▏ | 6721/8192 [00:14<00:03, 489.11it/s]
Adding requests:  83%|████████▎ | 6770/8192 [00:14<00:02, 486.52it/s]
Adding requests:  83%|████████▎ | 6820/8192 [00:14<00:02, 488.09it/s]
Adding requests:  84%|████████▍ | 6871/8192 [00:14<00:02, 492.17it/s]
Adding requests:  84%|████████▍ | 6921/8192 [00:14<00:02, 491.15it/s]
Adding requests:  85%|████████▌ | 6971/8192 [00:14<00:02, 493.20it/s]
Adding requests:  86%|████████▌ | 7021/8192 [00:14<00:02, 486.75it/s]
Adding requests:  86%|████████▋ | 7070/8192 [00:14<00:02, 485.36it/s]
Adding requests:  87%|████████▋ | 7121/8192 [00:15<00:02, 490.49it/s]
Adding requests:  88%|████████▊ | 7171/8192 [00:15<00:02, 485.48it/s]
Adding requests:  88%|████████▊ | 7220/8192 [00:15<00:01, 486.49it/s]
Adding requests:  89%|████████▊ | 7270/8192 [00:15<00:01, 488.35it/s]
Adding requests:  89%|████████▉ | 7320/8192 [00:15<00:01, 488.94it/s]
Adding requests:  90%|████████▉ | 7369/8192 [00:15<00:01, 475.39it/s]
Adding requests:  91%|█████████ | 7420/8192 [00:15<00:01, 484.50it/s]
Adding requests:  91%|█████████ | 7470/8192 [00:15<00:01, 487.12it/s]
Adding requests:  92%|█████████▏| 7519/8192 [00:15<00:01, 487.46it/s]
Adding requests:  92%|█████████▏| 7568/8192 [00:15<00:01, 486.38it/s]
Adding requests:  93%|█████████▎| 7617/8192 [00:16<00:01, 482.33it/s]
Adding requests:  94%|█████████▎| 7668/8192 [00:16<00:01, 486.51it/s]
Adding requests:  94%|█████████▍| 7718/8192 [00:16<00:00, 489.11it/s]
Adding requests:  95%|█████████▍| 7767/8192 [00:16<00:00, 484.68it/s]
Adding requests:  95%|█████████▌| 7816/8192 [00:16<00:00, 482.89it/s]
Adding requests:  96%|█████████▌| 7865/8192 [00:16<00:00, 483.65it/s]
Adding requests:  97%|█████████▋| 7914/8192 [00:16<00:00, 478.87it/s]
Adding requests:  97%|█████████▋| 7962/8192 [00:16<00:00, 478.51it/s]
Adding requests:  98%|█████████▊| 8010/8192 [00:16<00:00, 477.76it/s]
Adding requests:  98%|█████████▊| 8058/8192 [00:16<00:00, 477.12it/s]
Adding requests:  99%|█████████▉| 8108/8192 [00:17<00:00, 482.83it/s]
Adding requests: 100%|█████████▉| 8157/8192 [00:17<00:00, 482.42it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 474.55it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  66%|██████▌   | 5407/8192 [00:00<00:00, 54069.54it/s, est. speed input: 55370901.15 toks/s, output: 54070.31 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 54069.54it/s, est. speed input: 56885512.47 toks/s, output: 55550.46 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 55509.00it/s, est. speed input: 56885512.47 toks/s, output: 55550.46 toks/s]
[rank0]:[W126 13:44:23.675239501 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 14:41:51
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:41:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=609402) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=609402) WARNING 01-26 14:42:14 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=609402) WARNING 01-26 14:42:23 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 34.24 requests/s, 17566.03 total tokens/s, 34.24 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 14:41:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:41:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:41:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:41:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:41:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:41:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:41:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:41:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:41:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:41:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:41:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:41:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:41:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:41:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:42:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:42:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:42:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:42:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:42:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:42:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:42:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:42:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:42:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:42:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:42:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:42:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:42:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:42:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=609402) [2026-01-26 14:42:06] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=609402) [2026-01-26 14:42:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=609402) [2026-01-26 14:42:06] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=609402) [2026-01-26 14:42:06] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=609402) [2026-01-26 14:42:06] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=609402) [2026-01-26 14:42:06] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=609402) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=609402) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.16it/s]
(EngineCore_DP0 pid=609402) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.15it/s]
(EngineCore_DP0 pid=609402) 
(EngineCore_DP0 pid=609402) [2026-01-26 14:42:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=609402) [2026-01-26 14:42:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13107200 bytes
(EngineCore_DP0 pid=609402) [2026-01-26 14:42:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=609402) [2026-01-26 14:42:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=609402) [2026-01-26 14:42:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=609402) [2026-01-26 14:42:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41943040 bytes
(EngineCore_DP0 pid=609402) [2026-01-26 14:42:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=609402) [2026-01-26 14:42:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21037056 bytes
(EngineCore_DP0 pid=609402) 2026-01-26 14:42:23,843 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=609402) 2026-01-26 14:42:23,866 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=609402) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.03it/s]
(EngineCore_DP0 pid=609402) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.83it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  55%|█████▌    | 71/128 [00:00<00:00, 704.89it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 328.34it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:02, 57.91it/s, est. speed input: 29650.96 toks/s, output: 57.91 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:02, 44.58it/s, est. speed input: 23704.92 toks/s, output: 46.30 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 41.45it/s, est. speed input: 22244.33 toks/s, output: 43.45 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 39.93it/s, est. speed input: 21502.21 toks/s, output: 42.00 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 39.11it/s, est. speed input: 21064.74 toks/s, output: 41.14 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 38.66it/s, est. speed input: 20814.68 toks/s, output: 40.65 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 38.30it/s, est. speed input: 20618.96 toks/s, output: 40.27 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:02, 38.07it/s, est. speed input: 20468.05 toks/s, output: 39.98 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 37.90it/s, est. speed input: 20345.34 toks/s, output: 39.74 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 37.75it/s, est. speed input: 20239.97 toks/s, output: 39.53 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 37.67it/s, est. speed input: 20155.69 toks/s, output: 39.37 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 37.59it/s, est. speed input: 20080.32 toks/s, output: 39.22 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 37.58it/s, est. speed input: 20021.40 toks/s, output: 39.10 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 37.60it/s, est. speed input: 19973.01 toks/s, output: 39.01 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 37.63it/s, est. speed input: 19932.38 toks/s, output: 38.93 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 37.60it/s, est. speed input: 19890.99 toks/s, output: 38.85 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 37.52it/s, est. speed input: 19848.84 toks/s, output: 38.77 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 37.56it/s, est. speed input: 19819.33 toks/s, output: 38.71 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 37.55it/s, est. speed input: 19789.26 toks/s, output: 38.65 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 37.58it/s, est. speed input: 19765.41 toks/s, output: 38.60 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 37.59it/s, est. speed input: 19742.93 toks/s, output: 38.56 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 37.53it/s, est. speed input: 19717.42 toks/s, output: 38.51 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 37.56it/s, est. speed input: 19698.73 toks/s, output: 38.47 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 37.62it/s, est. speed input: 19684.33 toks/s, output: 38.45 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 37.62it/s, est. speed input: 19668.42 toks/s, output: 38.41 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 37.08it/s, est. speed input: 19618.83 toks/s, output: 38.32 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 37.27it/s, est. speed input: 19608.21 toks/s, output: 38.30 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 37.41it/s, est. speed input: 19598.65 toks/s, output: 38.28 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 37.50it/s, est. speed input: 19588.84 toks/s, output: 38.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.56it/s, est. speed input: 19579.84 toks/s, output: 38.24 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.56it/s, est. speed input: 19579.84 toks/s, output: 38.24 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.24it/s, est. speed input: 19579.84 toks/s, output: 38.24 toks/s]
[rank0]:[W126 14:42:30.280775369 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 14:42:31
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:42:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=610599) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=610599) WARNING 01-26 14:42:54 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=610599) WARNING 01-26 14:43:03 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 35.24 requests/s, 36120.09 total tokens/s, 35.24 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 14:42:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:42:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:42:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:42:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:42:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:42:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:42:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:42:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:42:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:42:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:42:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:42:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:42:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:42:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:42:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:42:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:42:45] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:42:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:42:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:42:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:42:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:42:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:42:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:42:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:42:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:42:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:42:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:42:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=610599) [2026-01-26 14:42:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=610599) [2026-01-26 14:42:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=610599) [2026-01-26 14:42:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=610599) [2026-01-26 14:42:46] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=610599) [2026-01-26 14:42:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=610599) [2026-01-26 14:42:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=610599) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=610599) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.28it/s]
(EngineCore_DP0 pid=610599) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.28it/s]
(EngineCore_DP0 pid=610599) 
(EngineCore_DP0 pid=610599) [2026-01-26 14:42:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=610599) [2026-01-26 14:42:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13107200 bytes
(EngineCore_DP0 pid=610599) [2026-01-26 14:42:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=610599) [2026-01-26 14:42:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=610599) [2026-01-26 14:42:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=610599) [2026-01-26 14:42:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41943040 bytes
(EngineCore_DP0 pid=610599) [2026-01-26 14:42:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=610599) [2026-01-26 14:42:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21037056 bytes
(EngineCore_DP0 pid=610599) 2026-01-26 14:43:03,723 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=610599) 2026-01-26 14:43:03,746 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=610599) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.67it/s]
(EngineCore_DP0 pid=610599) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.22it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:00, 397.26it/s]
Adding requests:  66%|██████▋   | 85/128 [00:00<00:00, 422.94it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 232.15it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 262.59it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:00, 141.13it/s, est. speed input: 144531.24 toks/s, output: 141.14 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:01, 53.34it/s, est. speed input: 60240.96 toks/s, output: 58.83 toks/s]   
Processed prompts:  30%|██▉       | 38/128 [00:00<00:01, 46.94it/s, est. speed input: 53596.14 toks/s, output: 52.34 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 43.79it/s, est. speed input: 50438.40 toks/s, output: 49.26 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 41.86it/s, est. speed input: 48562.78 toks/s, output: 47.42 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 40.67it/s, est. speed input: 47398.06 toks/s, output: 46.29 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 39.77it/s, est. speed input: 46476.58 toks/s, output: 45.39 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 39.06it/s, est. speed input: 45712.94 toks/s, output: 44.64 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 38.58it/s, est. speed input: 45185.87 toks/s, output: 44.13 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 38.24it/s, est. speed input: 44738.53 toks/s, output: 43.69 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 37.91it/s, est. speed input: 44329.73 toks/s, output: 43.29 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 37.72it/s, est. speed input: 43977.88 toks/s, output: 42.95 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 37.50it/s, est. speed input: 43648.10 toks/s, output: 42.62 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:01, 37.44it/s, est. speed input: 43373.67 toks/s, output: 42.36 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 37.45it/s, est. speed input: 43134.61 toks/s, output: 42.12 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 37.41it/s, est. speed input: 42908.02 toks/s, output: 41.90 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 37.42it/s, est. speed input: 42709.66 toks/s, output: 41.71 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 37.38it/s, est. speed input: 42519.53 toks/s, output: 41.52 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 37.36it/s, est. speed input: 42345.21 toks/s, output: 41.35 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 37.31it/s, est. speed input: 42180.32 toks/s, output: 41.19 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 37.27it/s, est. speed input: 42025.58 toks/s, output: 41.04 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 37.17it/s, est. speed input: 41872.91 toks/s, output: 40.89 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 37.23it/s, est. speed input: 41747.73 toks/s, output: 40.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.23it/s, est. speed input: 41691.86 toks/s, output: 40.71 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.71it/s, est. speed input: 41691.86 toks/s, output: 40.71 toks/s]
[rank0]:[W126 14:43:08.155872478 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 14:43:10
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:43:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=611704) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=611704) WARNING 01-26 14:43:33 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=611704) WARNING 01-26 14:43:43 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 68.51 requests/s, 70224.36 total tokens/s, 68.51 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 14:43:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:43:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:43:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:43:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:43:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:43:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:43:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:43:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:43:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:43:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:43:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:43:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:43:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:43:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:43:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:43:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:43:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:43:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:43:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:43:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:43:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:43:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:43:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:43:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:43:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:43:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:43:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:43:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=611704) [2026-01-26 14:43:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=611704) [2026-01-26 14:43:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=611704) [2026-01-26 14:43:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=611704) [2026-01-26 14:43:25] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=611704) [2026-01-26 14:43:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=611704) [2026-01-26 14:43:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=611704) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=611704) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.17it/s]
(EngineCore_DP0 pid=611704) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.17it/s]
(EngineCore_DP0 pid=611704) 
(EngineCore_DP0 pid=611704) [2026-01-26 14:43:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=611704) [2026-01-26 14:43:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13107200 bytes
(EngineCore_DP0 pid=611704) [2026-01-26 14:43:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=611704) [2026-01-26 14:43:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=611704) [2026-01-26 14:43:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=611704) [2026-01-26 14:43:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41943040 bytes
(EngineCore_DP0 pid=611704) [2026-01-26 14:43:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=611704) [2026-01-26 14:43:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21037056 bytes
(EngineCore_DP0 pid=611704) 2026-01-26 14:43:42,998 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=611704) 2026-01-26 14:43:43,020 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=611704) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.79it/s]
(EngineCore_DP0 pid=611704) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.91it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:47,  5.34it/s]
Adding requests:  16%|█▌        | 41/256 [00:00<00:01, 175.72it/s]
Adding requests:  33%|███▎      | 84/256 [00:00<00:00, 271.93it/s]
Adding requests:  50%|█████     | 128/256 [00:00<00:00, 330.64it/s]
Adding requests:  67%|██████▋   | 172/256 [00:00<00:00, 365.56it/s]
Adding requests:  84%|████████▍ | 216/256 [00:00<00:00, 389.43it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 329.29it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:00<00:00, 367.64it/s, est. speed input: 376493.60 toks/s, output: 367.65 toks/s]
Processed prompts:  31%|███       | 79/256 [00:00<00:01, 118.83it/s, est. speed input: 136405.01 toks/s, output: 133.21 toks/s]
Processed prompts:  39%|███▊      | 99/256 [00:00<00:01, 101.25it/s, est. speed input: 118082.34 toks/s, output: 115.31 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:01<00:01, 91.46it/s, est. speed input: 108922.24 toks/s, output: 106.37 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:01<00:01, 87.37it/s, est. speed input: 104804.90 toks/s, output: 102.35 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:01<00:01, 84.48it/s, est. speed input: 102055.19 toks/s, output: 99.66 toks/s] 
Processed prompts:  57%|█████▋    | 146/256 [00:01<00:01, 82.30it/s, est. speed input: 99887.53 toks/s, output: 97.55 toks/s] 
Processed prompts:  61%|██████    | 155/256 [00:01<00:01, 82.83it/s, est. speed input: 99016.02 toks/s, output: 96.70 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:01<00:01, 78.48it/s, est. speed input: 96719.18 toks/s, output: 94.45 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:01, 77.77it/s, est. speed input: 95607.88 toks/s, output: 93.37 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:01<00:00, 77.15it/s, est. speed input: 94604.36 toks/s, output: 92.39 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:02<00:00, 76.72it/s, est. speed input: 93715.62 toks/s, output: 91.52 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:02<00:00, 76.33it/s, est. speed input: 92899.57 toks/s, output: 90.72 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:02<00:00, 76.09it/s, est. speed input: 92169.92 toks/s, output: 90.01 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:02<00:00, 75.83it/s, est. speed input: 91490.30 toks/s, output: 89.35 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:02<00:00, 75.61it/s, est. speed input: 90861.73 toks/s, output: 88.73 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 75.56it/s, est. speed input: 90304.34 toks/s, output: 88.19 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 75.42it/s, est. speed input: 89773.09 toks/s, output: 87.67 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:02<00:00, 75.24it/s, est. speed input: 89270.45 toks/s, output: 87.18 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:02<00:00, 75.24it/s, est. speed input: 88823.27 toks/s, output: 86.74 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 75.24it/s, est. speed input: 88622.72 toks/s, output: 86.55 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 86.54it/s, est. speed input: 88622.72 toks/s, output: 86.55 toks/s]
[rank0]:[W126 14:43:48.702314894 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 14:43:50
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:43:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=612837) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=612837) WARNING 01-26 14:44:14 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=612837) WARNING 01-26 14:44:23 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 134.40 requests/s, 137763.29 total tokens/s, 134.40 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 14:43:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:43:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:43:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:43:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:43:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:43:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:43:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:43:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:43:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:43:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:43:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:43:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:43:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:43:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:44:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:44:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:44:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:44:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:44:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:44:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:44:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:44:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:44:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:44:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:44:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:44:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:44:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:44:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=612837) [2026-01-26 14:44:06] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=612837) [2026-01-26 14:44:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=612837) [2026-01-26 14:44:06] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=612837) [2026-01-26 14:44:06] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=612837) [2026-01-26 14:44:06] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=612837) [2026-01-26 14:44:06] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=612837) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=612837) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.16it/s]
(EngineCore_DP0 pid=612837) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.16it/s]
(EngineCore_DP0 pid=612837) 
(EngineCore_DP0 pid=612837) [2026-01-26 14:44:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=612837) [2026-01-26 14:44:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13107200 bytes
(EngineCore_DP0 pid=612837) [2026-01-26 14:44:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=612837) [2026-01-26 14:44:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=612837) [2026-01-26 14:44:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=612837) [2026-01-26 14:44:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41943040 bytes
(EngineCore_DP0 pid=612837) [2026-01-26 14:44:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=612837) [2026-01-26 14:44:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21037056 bytes
(EngineCore_DP0 pid=612837) 2026-01-26 14:44:23,801 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=612837) 2026-01-26 14:44:23,825 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=612837) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 15.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.01it/s]
(EngineCore_DP0 pid=612837) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 17.56it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 17.77it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 40/512 [00:00<00:01, 398.78it/s]
Adding requests:  17%|█▋        | 85/512 [00:00<00:01, 423.69it/s]
Adding requests:  25%|██▌       | 128/512 [00:00<00:00, 412.71it/s]
Adding requests:  33%|███▎      | 170/512 [00:00<00:00, 403.28it/s]
Adding requests:  41%|████      | 211/512 [00:00<00:00, 404.35it/s]
Adding requests:  50%|████▉     | 255/512 [00:00<00:00, 415.42it/s]
Adding requests:  58%|█████▊    | 298/512 [00:00<00:00, 420.08it/s]
Adding requests:  67%|██████▋   | 342/512 [00:00<00:00, 423.13it/s]
Adding requests:  76%|███████▌  | 388/512 [00:00<00:00, 432.84it/s]
Adding requests:  85%|████████▍ | 434/512 [00:01<00:00, 438.75it/s]
Adding requests:  94%|█████████▍| 480/512 [00:01<00:00, 443.74it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 427.08it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  31%|███       | 158/512 [00:00<00:00, 1349.08it/s, est. speed input: 1381549.81 toks/s, output: 1349.11 toks/s]
Processed prompts:  57%|█████▋    | 293/512 [00:01<00:00, 244.51it/s, est. speed input: 288601.62 toks/s, output: 281.84 toks/s]   
Processed prompts:  70%|██████▉   | 358/512 [00:01<00:00, 198.83it/s, est. speed input: 239913.70 toks/s, output: 234.29 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [00:01<00:00, 185.68it/s, est. speed input: 225993.94 toks/s, output: 220.70 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:02<00:00, 175.40it/s, est. speed input: 216870.98 toks/s, output: 211.79 toks/s]
Processed prompts:  89%|████████▉ | 455/512 [00:02<00:00, 168.74it/s, est. speed input: 211187.72 toks/s, output: 206.24 toks/s]
Processed prompts:  93%|█████████▎| 477/512 [00:02<00:00, 166.61it/s, est. speed input: 208195.91 toks/s, output: 203.32 toks/s]
Processed prompts:  97%|█████████▋| 497/512 [00:02<00:00, 161.38it/s, est. speed input: 204625.83 toks/s, output: 199.83 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 161.38it/s, est. speed input: 200922.67 toks/s, output: 196.21 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 196.21it/s, est. speed input: 200922.67 toks/s, output: 196.21 toks/s]
[rank0]:[W126 14:44:29.966705685 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 14:44:31
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:44:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=613987) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=613987) WARNING 01-26 14:44:58 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=613987) WARNING 01-26 14:45:07 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 221.71 requests/s, 227251.22 total tokens/s, 221.71 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 14:44:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:44:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:44:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:44:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:44:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:44:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:44:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:44:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:44:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:44:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:44:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:44:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:44:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:44:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:44:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:44:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:44:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:44:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:44:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:44:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:44:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:44:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:44:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:44:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:44:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:44:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:44:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:44:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=613987) [2026-01-26 14:44:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=613987) [2026-01-26 14:44:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=613987) [2026-01-26 14:44:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=613987) [2026-01-26 14:44:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=613987) [2026-01-26 14:44:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=613987) [2026-01-26 14:44:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=613987) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=613987) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=613987) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=613987) 
(EngineCore_DP0 pid=613987) [2026-01-26 14:44:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=613987) [2026-01-26 14:44:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13107200 bytes
(EngineCore_DP0 pid=613987) [2026-01-26 14:44:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=613987) [2026-01-26 14:44:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=613987) [2026-01-26 14:44:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=613987) [2026-01-26 14:44:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41943040 bytes
(EngineCore_DP0 pid=613987) [2026-01-26 14:44:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=613987) [2026-01-26 14:44:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21037056 bytes
(EngineCore_DP0 pid=613987) 2026-01-26 14:45:07,887 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=613987) 2026-01-26 14:45:07,912 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=613987) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 16.05it/s]
(EngineCore_DP0 pid=613987) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 17.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.36it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.29it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 398.98it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 426.73it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 435.63it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:01, 437.02it/s]
Adding requests:  21%|██▏       | 220/1024 [00:00<00:01, 443.64it/s]
Adding requests:  26%|██▌       | 268/1024 [00:00<00:01, 455.64it/s]
Adding requests:  31%|███       | 314/1024 [00:00<00:01, 453.90it/s]
Adding requests:  35%|███▌      | 361/1024 [00:00<00:01, 457.25it/s]
Adding requests:  40%|███▉      | 408/1024 [00:00<00:01, 459.20it/s]
Adding requests:  44%|████▍     | 455/1024 [00:01<00:01, 462.08it/s]
Adding requests:  49%|████▉     | 502/1024 [00:01<00:01, 459.23it/s]
Adding requests:  54%|█████▎    | 548/1024 [00:01<00:01, 453.62it/s]
Adding requests:  58%|█████▊    | 596/1024 [00:01<00:00, 459.88it/s]
Adding requests:  63%|██████▎   | 644/1024 [00:01<00:00, 464.94it/s]
Adding requests:  68%|██████▊   | 693/1024 [00:01<00:00, 472.13it/s]
Adding requests:  72%|███████▏  | 741/1024 [00:01<00:00, 470.40it/s]
Adding requests:  77%|███████▋  | 789/1024 [00:01<00:00, 469.17it/s]
Adding requests:  82%|████████▏ | 836/1024 [00:01<00:00, 460.23it/s]
Adding requests:  86%|████████▋ | 885/1024 [00:01<00:00, 467.39it/s]
Adding requests:  91%|█████████ | 934/1024 [00:02<00:00, 473.00it/s]
Adding requests:  96%|█████████▌| 982/1024 [00:02<00:00, 472.48it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 461.17it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:00<00:00, 3754.01it/s, est. speed input: 3844303.74 toks/s, output: 3754.05 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:01<00:00, 418.69it/s, est. speed input: 505519.04 toks/s, output: 493.67 toks/s]   
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 418.69it/s, est. speed input: 437456.12 toks/s, output: 427.20 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 427.19it/s, est. speed input: 437456.12 toks/s, output: 427.20 toks/s]
[rank0]:[W126 14:45:14.145723326 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 14:45:16
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:45:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=615234) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=615234) WARNING 01-26 14:45:48 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=615234) WARNING 01-26 14:45:57 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 245.20 requests/s, 251325.34 total tokens/s, 245.20 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 14:45:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:45:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:45:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:45:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:45:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:45:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:45:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:45:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:45:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:45:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:45:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:45:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:45:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:45:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:45:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:45:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:45:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:45:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:45:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:45:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:45:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:45:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:45:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:45:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:45:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:45:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:45:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:45:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=615234) [2026-01-26 14:45:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=615234) [2026-01-26 14:45:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=615234) [2026-01-26 14:45:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=615234) [2026-01-26 14:45:40] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=615234) [2026-01-26 14:45:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=615234) [2026-01-26 14:45:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=615234) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=615234) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=615234) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=615234) 
(EngineCore_DP0 pid=615234) [2026-01-26 14:45:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=615234) [2026-01-26 14:45:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13107200 bytes
(EngineCore_DP0 pid=615234) [2026-01-26 14:45:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=615234) [2026-01-26 14:45:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=615234) [2026-01-26 14:45:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=615234) [2026-01-26 14:45:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41943040 bytes
(EngineCore_DP0 pid=615234) [2026-01-26 14:45:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=615234) [2026-01-26 14:45:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21037056 bytes
(EngineCore_DP0 pid=615234) 2026-01-26 14:45:57,667 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=615234) 2026-01-26 14:45:57,690 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=615234) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 16.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 17.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 15.83it/s]
(EngineCore_DP0 pid=615234) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 18.09it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 18.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.22it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 40/2048 [00:00<00:05, 395.69it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 421.42it/s]
Adding requests:   6%|▌         | 127/2048 [00:00<00:04, 425.32it/s]
Adding requests:   8%|▊         | 171/2048 [00:00<00:04, 428.51it/s]
Adding requests:  11%|█         | 216/2048 [00:00<00:04, 435.48it/s]
Adding requests:  13%|█▎        | 263/2048 [00:00<00:03, 447.02it/s]
Adding requests:  15%|█▌        | 308/2048 [00:00<00:03, 444.14it/s]
Adding requests:  17%|█▋        | 355/2048 [00:00<00:03, 450.63it/s]
Adding requests:  20%|█▉        | 402/2048 [00:00<00:03, 455.13it/s]
Adding requests:  22%|██▏       | 449/2048 [00:01<00:03, 456.96it/s]
Adding requests:  24%|██▍       | 496/2048 [00:01<00:03, 458.54it/s]
Adding requests:  26%|██▋       | 542/2048 [00:01<00:03, 446.18it/s]
Adding requests:  29%|██▉       | 591/2048 [00:01<00:03, 458.53it/s]
Adding requests:  31%|███       | 637/2048 [00:01<00:03, 451.94it/s]
Adding requests:  33%|███▎      | 685/2048 [00:01<00:02, 458.38it/s]
Adding requests:  36%|███▌      | 733/2048 [00:01<00:02, 464.73it/s]
Adding requests:  38%|███▊      | 780/2048 [00:01<00:02, 460.28it/s]
Adding requests:  40%|████      | 827/2048 [00:01<00:02, 453.40it/s]
Adding requests:  43%|████▎     | 875/2048 [00:01<00:02, 457.77it/s]
Adding requests:  45%|████▌     | 924/2048 [00:02<00:02, 464.28it/s]
Adding requests:  47%|████▋     | 971/2048 [00:02<00:02, 465.26it/s]
Adding requests:  50%|████▉     | 1019/2048 [00:02<00:02, 469.37it/s]
Adding requests:  52%|█████▏    | 1067/2048 [00:02<00:02, 469.66it/s]
Adding requests:  54%|█████▍    | 1114/2048 [00:02<00:02, 463.89it/s]
Adding requests:  57%|█████▋    | 1163/2048 [00:02<00:01, 468.75it/s]
Adding requests:  59%|█████▉    | 1213/2048 [00:02<00:01, 476.78it/s]
Adding requests:  62%|██████▏   | 1261/2048 [00:02<00:01, 470.39it/s]
Adding requests:  64%|██████▍   | 1309/2048 [00:02<00:01, 471.66it/s]
Adding requests:  66%|██████▋   | 1358/2048 [00:02<00:01, 474.13it/s]
Adding requests:  69%|██████▉   | 1408/2048 [00:03<00:01, 478.99it/s]
Adding requests:  71%|███████   | 1456/2048 [00:03<00:01, 474.55it/s]
Adding requests:  74%|███████▎  | 1506/2048 [00:03<00:01, 477.23it/s]
Adding requests:  76%|███████▌  | 1554/2048 [00:03<00:01, 475.81it/s]
Adding requests:  78%|███████▊  | 1605/2048 [00:03<00:00, 483.29it/s]
Adding requests:  81%|████████  | 1654/2048 [00:03<00:00, 468.24it/s]
Adding requests:  83%|████████▎ | 1701/2048 [00:03<00:00, 465.69it/s]
Adding requests:  85%|████████▌ | 1748/2048 [00:03<00:00, 449.91it/s]
Adding requests:  88%|████████▊ | 1795/2048 [00:03<00:00, 454.19it/s]
Adding requests:  90%|████████▉ | 1843/2048 [00:04<00:00, 459.70it/s]
Adding requests:  92%|█████████▏| 1890/2048 [00:04<00:00, 462.12it/s]
Adding requests:  95%|█████████▍| 1938/2048 [00:04<00:00, 464.37it/s]
Adding requests:  97%|█████████▋| 1985/2048 [00:04<00:00, 465.76it/s]
Adding requests:  99%|█████████▉| 2034/2048 [00:04<00:00, 470.65it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 461.27it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:00<00:00, 8110.69it/s, est. speed input: 8305829.50 toks/s, output: 8110.83 toks/s]
Processed prompts:  93%|█████████▎| 1902/2048 [00:03<00:00, 478.04it/s, est. speed input: 583997.47 toks/s, output: 570.31 toks/s]   
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 478.04it/s, est. speed input: 536235.02 toks/s, output: 523.67 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 523.65it/s, est. speed input: 536235.02 toks/s, output: 523.67 toks/s]
[rank0]:[W126 14:46:08.676454863 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 14:46:10
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:46:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=616664) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=616664) WARNING 01-26 14:46:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=616664) WARNING 01-26 14:47:00 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 255.69 requests/s, 262079.05 total tokens/s, 255.69 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 14:46:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:46:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:46:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:46:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:46:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:46:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:46:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:46:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:46:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:46:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:46:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:46:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:46:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:46:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:46:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:46:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:46:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:46:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:46:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:46:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:46:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:46:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:46:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:46:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:46:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:46:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:46:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:46:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=616664) [2026-01-26 14:46:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=616664) [2026-01-26 14:46:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=616664) [2026-01-26 14:46:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=616664) [2026-01-26 14:46:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=616664) [2026-01-26 14:46:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=616664) [2026-01-26 14:46:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=616664) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=616664) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.14it/s]
(EngineCore_DP0 pid=616664) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.14it/s]
(EngineCore_DP0 pid=616664) 
(EngineCore_DP0 pid=616664) [2026-01-26 14:46:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=616664) [2026-01-26 14:46:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13107200 bytes
(EngineCore_DP0 pid=616664) [2026-01-26 14:46:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=616664) [2026-01-26 14:46:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=616664) [2026-01-26 14:46:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=616664) [2026-01-26 14:46:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41943040 bytes
(EngineCore_DP0 pid=616664) [2026-01-26 14:46:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=616664) [2026-01-26 14:46:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21037056 bytes
(EngineCore_DP0 pid=616664) [rank0]:W0126 14:46:55.874000 616664 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=616664) [rank0]:W0126 14:46:55.958000 616664 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=616664) [rank0]:W0126 14:46:56.870000 616664 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=616664) [rank0]:W0126 14:46:56.992000 616664 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=616664) 2026-01-26 14:47:00,331 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=616664) 2026-01-26 14:47:00,357 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=616664) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  5.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  9.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 11.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 13.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 15.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 13.07it/s]
(EngineCore_DP0 pid=616664) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 16.94it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.51it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 17.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 17.77it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 40/4096 [00:00<00:10, 398.76it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 422.07it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:09, 430.73it/s]
Adding requests:   4%|▍         | 173/4096 [00:00<00:09, 432.63it/s]
Adding requests:   5%|▌         | 218/4096 [00:00<00:08, 437.80it/s]
Adding requests:   6%|▋         | 266/4096 [00:00<00:08, 451.17it/s]
Adding requests:   8%|▊         | 312/4096 [00:00<00:08, 430.64it/s]
Adding requests:   9%|▊         | 358/4096 [00:00<00:08, 439.21it/s]
Adding requests:  10%|▉         | 405/4096 [00:00<00:08, 446.45it/s]
Adding requests:  11%|█         | 452/4096 [00:01<00:08, 451.85it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:07, 453.29it/s]
Adding requests:  13%|█▎        | 544/4096 [00:01<00:07, 448.24it/s]
Adding requests:  14%|█▍        | 593/4096 [00:01<00:07, 457.57it/s]
Adding requests:  16%|█▌        | 641/4096 [00:01<00:07, 461.90it/s]
Adding requests:  17%|█▋        | 689/4096 [00:01<00:07, 467.17it/s]
Adding requests:  18%|█▊        | 738/4096 [00:01<00:07, 473.01it/s]
Adding requests:  19%|█▉        | 786/4096 [00:01<00:07, 467.67it/s]
Adding requests:  20%|██        | 833/4096 [00:01<00:07, 459.17it/s]
Adding requests:  22%|██▏       | 881/4096 [00:01<00:06, 462.99it/s]
Adding requests:  23%|██▎       | 928/4096 [00:02<00:06, 463.27it/s]
Adding requests:  24%|██▍       | 975/4096 [00:02<00:06, 457.05it/s]
Adding requests:  25%|██▌       | 1024/4096 [00:02<00:06, 463.09it/s]
Adding requests:  26%|██▌       | 1071/4096 [00:02<00:06, 460.40it/s]
Adding requests:  27%|██▋       | 1118/4096 [00:02<00:06, 459.85it/s]
Adding requests:  28%|██▊       | 1167/4096 [00:02<00:06, 468.46it/s]
Adding requests:  30%|██▉       | 1216/4096 [00:02<00:06, 473.52it/s]
Adding requests:  31%|███       | 1264/4096 [00:02<00:06, 468.51it/s]
Adding requests:  32%|███▏      | 1312/4096 [00:02<00:05, 469.05it/s]
Adding requests:  33%|███▎      | 1361/4096 [00:02<00:05, 474.37it/s]
Adding requests:  34%|███▍      | 1411/4096 [00:03<00:05, 479.41it/s]
Adding requests:  36%|███▌      | 1459/4096 [00:03<00:05, 476.48it/s]
Adding requests:  37%|███▋      | 1508/4096 [00:03<00:05, 478.60it/s]
Adding requests:  38%|███▊      | 1556/4096 [00:03<00:05, 477.14it/s]
Adding requests:  39%|███▉      | 1606/4096 [00:03<00:05, 482.27it/s]
Adding requests:  40%|████      | 1655/4096 [00:03<00:05, 479.74it/s]
Adding requests:  42%|████▏     | 1703/4096 [00:03<00:05, 476.04it/s]
Adding requests:  43%|████▎     | 1751/4096 [00:03<00:04, 476.29it/s]
Adding requests:  44%|████▍     | 1799/4096 [00:03<00:04, 475.15it/s]
Adding requests:  45%|████▌     | 1848/4096 [00:03<00:04, 474.81it/s]
Adding requests:  46%|████▋     | 1896/4096 [00:04<00:04, 474.32it/s]
Adding requests:  47%|████▋     | 1944/4096 [00:04<00:04, 474.08it/s]
Adding requests:  49%|████▊     | 1992/4096 [00:04<00:04, 473.75it/s]
Adding requests:  50%|████▉     | 2040/4096 [00:04<00:04, 466.06it/s]
Adding requests:  51%|█████     | 2089/4096 [00:04<00:04, 471.74it/s]
Adding requests:  52%|█████▏    | 2137/4096 [00:04<00:04, 468.77it/s]
Adding requests:  53%|█████▎    | 2184/4096 [00:04<00:04, 463.13it/s]
Adding requests:  54%|█████▍    | 2231/4096 [00:04<00:04, 460.22it/s]
Adding requests:  56%|█████▌    | 2279/4096 [00:04<00:03, 463.10it/s]
Adding requests:  57%|█████▋    | 2327/4096 [00:05<00:03, 466.35it/s]
Adding requests:  58%|█████▊    | 2375/4096 [00:05<00:03, 468.03it/s]
Adding requests:  59%|█████▉    | 2423/4096 [00:05<00:03, 471.23it/s]
Adding requests:  60%|██████    | 2471/4096 [00:05<00:03, 470.37it/s]
Adding requests:  61%|██████▏   | 2519/4096 [00:05<00:03, 470.90it/s]
Adding requests:  63%|██████▎   | 2568/4096 [00:05<00:03, 475.28it/s]
Adding requests:  64%|██████▍   | 2616/4096 [00:05<00:03, 474.93it/s]
Adding requests:  65%|██████▌   | 2665/4096 [00:05<00:02, 477.91it/s]
Adding requests:  66%|██████▌   | 2713/4096 [00:05<00:02, 472.07it/s]
Adding requests:  67%|██████▋   | 2761/4096 [00:05<00:02, 473.76it/s]
Adding requests:  69%|██████▊   | 2809/4096 [00:06<00:02, 470.02it/s]
Adding requests:  70%|██████▉   | 2857/4096 [00:06<00:02, 471.53it/s]
Adding requests:  71%|███████   | 2906/4096 [00:06<00:02, 474.14it/s]
Adding requests:  72%|███████▏  | 2954/4096 [00:06<00:02, 471.37it/s]
Adding requests:  73%|███████▎  | 3002/4096 [00:06<00:02, 471.84it/s]
Adding requests:  74%|███████▍  | 3050/4096 [00:06<00:02, 473.24it/s]
Adding requests:  76%|███████▌  | 3098/4096 [00:06<00:02, 470.04it/s]
Adding requests:  77%|███████▋  | 3146/4096 [00:06<00:02, 470.13it/s]
Adding requests:  78%|███████▊  | 3194/4096 [00:06<00:01, 472.31it/s]
Adding requests:  79%|███████▉  | 3243/4096 [00:06<00:01, 475.99it/s]
Adding requests:  80%|████████  | 3292/4096 [00:07<00:01, 478.06it/s]
Adding requests:  82%|████████▏ | 3340/4096 [00:07<00:01, 464.18it/s]
Adding requests:  83%|████████▎ | 3388/4096 [00:07<00:01, 467.95it/s]
Adding requests:  84%|████████▍ | 3437/4096 [00:07<00:01, 472.55it/s]
Adding requests:  85%|████████▌ | 3485/4096 [00:07<00:01, 449.70it/s]
Adding requests:  86%|████████▌ | 3532/4096 [00:07<00:01, 454.68it/s]
Adding requests:  87%|████████▋ | 3578/4096 [00:07<00:01, 453.43it/s]
Adding requests:  88%|████████▊ | 3624/4096 [00:07<00:01, 444.71it/s]
Adding requests:  90%|████████▉ | 3672/4096 [00:07<00:00, 454.05it/s]
Adding requests:  91%|█████████ | 3719/4096 [00:08<00:00, 458.07it/s]
Adding requests:  92%|█████████▏| 3768/4096 [00:08<00:00, 464.73it/s]
Adding requests:  93%|█████████▎| 3816/4096 [00:08<00:00, 469.00it/s]
Adding requests:  94%|█████████▍| 3865/4096 [00:08<00:00, 474.19it/s]
Adding requests:  96%|█████████▌| 3913/4096 [00:08<00:00, 473.26it/s]
Adding requests:  97%|█████████▋| 3961/4096 [00:08<00:00, 472.63it/s]
Adding requests:  98%|█████████▊| 4009/4096 [00:08<00:00, 472.07it/s]
Adding requests:  99%|█████████▉| 4057/4096 [00:08<00:00, 465.89it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 465.65it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  54%|█████▍    | 2228/4096 [00:00<00:00, 20274.56it/s, est. speed input: 20762725.32 toks/s, output: 20275.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:07<00:00, 20274.56it/s, est. speed input: 580828.37 toks/s, output: 567.21 toks/s]    
Processed prompts: 100%|██████████| 4096/4096 [00:07<00:00, 567.20it/s, est. speed input: 580828.37 toks/s, output: 567.21 toks/s]  
[rank0]:[W126 14:47:19.569912008 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 14:47:21
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:48:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=618439) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=618439) WARNING 01-26 14:48:20 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=618439) WARNING 01-26 14:48:30 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 261.60 requests/s, 268136.38 total tokens/s, 261.60 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 14:48:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:48:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:48:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:48:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:48:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:48:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:48:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:48:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:48:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:48:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:48:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:48:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:48:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:48:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:48:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:48:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:48:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:48:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:48:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:48:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:48:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:48:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:48:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:48:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:48:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:48:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:48:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:48:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=618439) [2026-01-26 14:48:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=618439) [2026-01-26 14:48:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=618439) [2026-01-26 14:48:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=618439) [2026-01-26 14:48:12] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=618439) [2026-01-26 14:48:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=618439) [2026-01-26 14:48:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=618439) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=618439) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.23it/s]
(EngineCore_DP0 pid=618439) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.23it/s]
(EngineCore_DP0 pid=618439) 
(EngineCore_DP0 pid=618439) [2026-01-26 14:48:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=618439) [2026-01-26 14:48:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13107200 bytes
(EngineCore_DP0 pid=618439) [2026-01-26 14:48:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=618439) [2026-01-26 14:48:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=618439) [2026-01-26 14:48:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=618439) [2026-01-26 14:48:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41943040 bytes
(EngineCore_DP0 pid=618439) [2026-01-26 14:48:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=618439) [2026-01-26 14:48:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21037056 bytes
(EngineCore_DP0 pid=618439) [rank0]:W0126 14:48:25.554000 618439 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=618439) [rank0]:W0126 14:48:25.642000 618439 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=618439) [rank0]:W0126 14:48:26.567000 618439 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=618439) [rank0]:W0126 14:48:26.693000 618439 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=618439) 2026-01-26 14:48:30,084 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=618439) 2026-01-26 14:48:30,110 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=618439) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:06,  2.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:06,  2.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:02,  5.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01, 10.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:00, 11.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00, 12.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00, 14.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00, 15.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:01<00:00, 15.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 10.46it/s]
(EngineCore_DP0 pid=618439) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 17.46it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 17.81it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 17.87it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 18.03it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 18.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 17.98it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 40/8192 [00:00<00:20, 395.36it/s]
Adding requests:   1%|          | 85/8192 [00:00<00:19, 422.86it/s]
Adding requests:   2%|▏         | 129/8192 [00:00<00:18, 427.46it/s]
Adding requests:   2%|▏         | 173/8192 [00:00<00:18, 431.41it/s]
Adding requests:   3%|▎         | 219/8192 [00:00<00:18, 439.17it/s]
Adding requests:   3%|▎         | 267/8192 [00:00<00:17, 451.51it/s]
Adding requests:   4%|▍         | 313/8192 [00:00<00:17, 450.05it/s]
Adding requests:   4%|▍         | 359/8192 [00:00<00:17, 452.26it/s]
Adding requests:   5%|▍         | 406/8192 [00:00<00:17, 455.33it/s]
Adding requests:   6%|▌         | 453/8192 [00:01<00:16, 458.18it/s]
Adding requests:   6%|▌         | 500/8192 [00:01<00:16, 458.06it/s]
Adding requests:   7%|▋         | 546/8192 [00:01<00:16, 453.28it/s]
Adding requests:   7%|▋         | 595/8192 [00:01<00:16, 462.47it/s]
Adding requests:   8%|▊         | 643/8192 [00:01<00:16, 465.21it/s]
Adding requests:   8%|▊         | 692/8192 [00:01<00:15, 472.01it/s]
Adding requests:   9%|▉         | 740/8192 [00:01<00:15, 472.17it/s]
Adding requests:  10%|▉         | 788/8192 [00:01<00:15, 469.43it/s]
Adding requests:  10%|█         | 835/8192 [00:01<00:16, 458.03it/s]
Adding requests:  11%|█         | 883/8192 [00:01<00:15, 464.19it/s]
Adding requests:  11%|█▏        | 932/8192 [00:02<00:15, 469.62it/s]
Adding requests:  12%|█▏        | 980/8192 [00:02<00:15, 470.11it/s]
Adding requests:  13%|█▎        | 1029/8192 [00:02<00:15, 474.41it/s]
Adding requests:  13%|█▎        | 1077/8192 [00:02<00:15, 470.38it/s]
Adding requests:  14%|█▎        | 1125/8192 [00:02<00:15, 468.12it/s]
Adding requests:  14%|█▍        | 1174/8192 [00:02<00:14, 474.52it/s]
Adding requests:  15%|█▍        | 1224/8192 [00:02<00:14, 480.65it/s]
Adding requests:  16%|█▌        | 1273/8192 [00:02<00:14, 475.02it/s]
Adding requests:  16%|█▌        | 1321/8192 [00:02<00:14, 476.01it/s]
Adding requests:  17%|█▋        | 1370/8192 [00:02<00:14, 478.28it/s]
Adding requests:  17%|█▋        | 1418/8192 [00:03<00:14, 478.74it/s]
Adding requests:  18%|█▊        | 1468/8192 [00:03<00:13, 481.92it/s]
Adding requests:  19%|█▊        | 1517/8192 [00:03<00:14, 470.27it/s]
Adding requests:  19%|█▉        | 1565/8192 [00:03<00:14, 473.01it/s]
Adding requests:  20%|█▉        | 1614/8192 [00:03<00:13, 476.83it/s]
Adding requests:  20%|██        | 1662/8192 [00:03<00:13, 474.76it/s]
Adding requests:  21%|██        | 1710/8192 [00:03<00:13, 474.42it/s]
Adding requests:  21%|██▏       | 1758/8192 [00:03<00:13, 473.80it/s]
Adding requests:  22%|██▏       | 1806/8192 [00:03<00:13, 475.38it/s]
Adding requests:  23%|██▎       | 1854/8192 [00:03<00:13, 476.47it/s]
Adding requests:  23%|██▎       | 1902/8192 [00:04<00:13, 476.46it/s]
Adding requests:  24%|██▍       | 1950/8192 [00:04<00:13, 475.58it/s]
Adding requests:  24%|██▍       | 1998/8192 [00:04<00:13, 476.07it/s]
Adding requests:  25%|██▍       | 2046/8192 [00:04<00:12, 477.03it/s]
Adding requests:  26%|██▌       | 2096/8192 [00:04<00:12, 480.73it/s]
Adding requests:  26%|██▌       | 2145/8192 [00:04<00:12, 473.23it/s]
Adding requests:  27%|██▋       | 2193/8192 [00:04<00:12, 469.99it/s]
Adding requests:  27%|██▋       | 2242/8192 [00:04<00:12, 475.01it/s]
Adding requests:  28%|██▊       | 2290/8192 [00:04<00:12, 473.16it/s]
Adding requests:  29%|██▊       | 2338/8192 [00:04<00:12, 474.42it/s]
Adding requests:  29%|██▉       | 2386/8192 [00:05<00:12, 475.90it/s]
Adding requests:  30%|██▉       | 2434/8192 [00:05<00:12, 476.95it/s]
Adding requests:  30%|███       | 2482/8192 [00:05<00:12, 475.73it/s]
Adding requests:  31%|███       | 2530/8192 [00:05<00:11, 474.36it/s]
Adding requests:  31%|███▏      | 2579/8192 [00:05<00:11, 478.83it/s]
Adding requests:  32%|███▏      | 2627/8192 [00:05<00:11, 476.84it/s]
Adding requests:  33%|███▎      | 2675/8192 [00:05<00:11, 465.74it/s]
Adding requests:  33%|███▎      | 2723/8192 [00:05<00:11, 468.24it/s]
Adding requests:  34%|███▍      | 2771/8192 [00:05<00:11, 469.83it/s]
Adding requests:  34%|███▍      | 2819/8192 [00:06<00:11, 467.37it/s]
Adding requests:  35%|███▍      | 2866/8192 [00:06<00:11, 460.48it/s]
Adding requests:  36%|███▌      | 2914/8192 [00:06<00:11, 465.60it/s]
Adding requests:  36%|███▌      | 2961/8192 [00:06<00:11, 464.82it/s]
Adding requests:  37%|███▋      | 3010/8192 [00:06<00:11, 469.83it/s]
Adding requests:  37%|███▋      | 3058/8192 [00:06<00:10, 469.81it/s]
Adding requests:  38%|███▊      | 3105/8192 [00:06<00:10, 467.31it/s]
Adding requests:  38%|███▊      | 3152/8192 [00:06<00:10, 467.84it/s]
Adding requests:  39%|███▉      | 3200/8192 [00:06<00:10, 471.10it/s]
Adding requests:  40%|███▉      | 3248/8192 [00:06<00:10, 468.22it/s]
Adding requests:  40%|████      | 3295/8192 [00:07<00:10, 461.60it/s]
Adding requests:  41%|████      | 3342/8192 [00:07<00:10, 460.65it/s]
Adding requests:  41%|████▏     | 3391/8192 [00:07<00:10, 466.69it/s]
Adding requests:  42%|████▏     | 3441/8192 [00:07<00:10, 473.17it/s]
Adding requests:  43%|████▎     | 3489/8192 [00:07<00:10, 466.64it/s]
Adding requests:  43%|████▎     | 3537/8192 [00:07<00:09, 468.61it/s]
Adding requests:  44%|████▍     | 3585/8192 [00:07<00:09, 470.11it/s]
Adding requests:  44%|████▍     | 3633/8192 [00:07<00:09, 469.84it/s]
Adding requests:  45%|████▍     | 3681/8192 [00:07<00:09, 472.58it/s]
Adding requests:  46%|████▌     | 3729/8192 [00:07<00:09, 470.78it/s]
Adding requests:  46%|████▌     | 3779/8192 [00:08<00:09, 479.13it/s]
Adding requests:  47%|████▋     | 3827/8192 [00:08<00:09, 478.81it/s]
Adding requests:  47%|████▋     | 3877/8192 [00:08<00:08, 484.57it/s]
Adding requests:  48%|████▊     | 3926/8192 [00:08<00:09, 465.33it/s]
Adding requests:  48%|████▊     | 3973/8192 [00:08<00:09, 452.28it/s]
Adding requests:  49%|████▉     | 4021/8192 [00:08<00:09, 459.77it/s]
Adding requests:  50%|████▉     | 4068/8192 [00:08<00:09, 457.51it/s]
Adding requests:  50%|█████     | 4117/8192 [00:08<00:08, 464.42it/s]
Adding requests:  51%|█████     | 4166/8192 [00:08<00:08, 469.78it/s]
Adding requests:  51%|█████▏    | 4215/8192 [00:08<00:08, 475.43it/s]
Adding requests:  52%|█████▏    | 4263/8192 [00:09<00:08, 476.62it/s]
Adding requests:  53%|█████▎    | 4311/8192 [00:09<00:08, 477.04it/s]
Adding requests:  53%|█████▎    | 4361/8192 [00:09<00:07, 483.00it/s]
Adding requests:  54%|█████▍    | 4411/8192 [00:09<00:07, 487.97it/s]
Adding requests:  54%|█████▍    | 4460/8192 [00:09<00:07, 485.89it/s]
Adding requests:  55%|█████▌    | 4509/8192 [00:09<00:07, 480.46it/s]
Adding requests:  56%|█████▌    | 4558/8192 [00:09<00:07, 477.57it/s]
Adding requests:  56%|█████▋    | 4608/8192 [00:09<00:07, 482.81it/s]
Adding requests:  57%|█████▋    | 4657/8192 [00:09<00:07, 480.92it/s]
Adding requests:  57%|█████▋    | 4706/8192 [00:10<00:07, 477.83it/s]
Adding requests:  58%|█████▊    | 4755/8192 [00:10<00:07, 480.30it/s]
Adding requests:  59%|█████▊    | 4804/8192 [00:10<00:07, 478.60it/s]
Adding requests:  59%|█████▉    | 4853/8192 [00:10<00:06, 479.10it/s]
Adding requests:  60%|█████▉    | 4901/8192 [00:10<00:06, 474.98it/s]
Adding requests:  60%|██████    | 4951/8192 [00:10<00:06, 480.02it/s]
Adding requests:  61%|██████    | 5000/8192 [00:10<00:06, 478.68it/s]
Adding requests:  62%|██████▏   | 5050/8192 [00:10<00:06, 483.19it/s]
Adding requests:  62%|██████▏   | 5100/8192 [00:10<00:06, 487.71it/s]
Adding requests:  63%|██████▎   | 5149/8192 [00:10<00:06, 487.30it/s]
Adding requests:  63%|██████▎   | 5199/8192 [00:11<00:06, 488.45it/s]
Adding requests:  64%|██████▍   | 5248/8192 [00:11<00:06, 468.53it/s]
Adding requests:  65%|██████▍   | 5297/8192 [00:11<00:06, 472.52it/s]
Adding requests:  65%|██████▌   | 5345/8192 [00:11<00:06, 474.46it/s]
Adding requests:  66%|██████▌   | 5394/8192 [00:11<00:05, 477.45it/s]
Adding requests:  66%|██████▋   | 5442/8192 [00:11<00:05, 473.96it/s]
Adding requests:  67%|██████▋   | 5490/8192 [00:11<00:05, 471.37it/s]
Adding requests:  68%|██████▊   | 5538/8192 [00:11<00:05, 472.23it/s]
Adding requests:  68%|██████▊   | 5586/8192 [00:11<00:05, 473.75it/s]
Adding requests:  69%|██████▉   | 5634/8192 [00:11<00:05, 471.61it/s]
Adding requests:  69%|██████▉   | 5682/8192 [00:12<00:05, 470.61it/s]
Adding requests:  70%|██████▉   | 5730/8192 [00:12<00:05, 468.38it/s]
Adding requests:  71%|███████   | 5779/8192 [00:12<00:05, 474.42it/s]
Adding requests:  71%|███████   | 5827/8192 [00:12<00:05, 468.87it/s]
Adding requests:  72%|███████▏  | 5876/8192 [00:12<00:04, 473.69it/s]
Adding requests:  72%|███████▏  | 5925/8192 [00:12<00:04, 477.84it/s]
Adding requests:  73%|███████▎  | 5973/8192 [00:12<00:04, 478.45it/s]
Adding requests:  74%|███████▎  | 6023/8192 [00:12<00:04, 482.02it/s]
Adding requests:  74%|███████▍  | 6072/8192 [00:12<00:04, 484.20it/s]
Adding requests:  75%|███████▍  | 6121/8192 [00:12<00:04, 484.15it/s]
Adding requests:  75%|███████▌  | 6170/8192 [00:13<00:04, 481.34it/s]
Adding requests:  76%|███████▌  | 6222/8192 [00:13<00:04, 491.97it/s]
Adding requests:  77%|███████▋  | 6272/8192 [00:13<00:03, 493.46it/s]
Adding requests:  77%|███████▋  | 6323/8192 [00:13<00:03, 497.24it/s]
Adding requests:  78%|███████▊  | 6373/8192 [00:13<00:03, 495.00it/s]
Adding requests:  78%|███████▊  | 6423/8192 [00:13<00:03, 494.29it/s]
Adding requests:  79%|███████▉  | 6473/8192 [00:13<00:03, 481.66it/s]
Adding requests:  80%|███████▉  | 6524/8192 [00:13<00:03, 487.95it/s]
Adding requests:  80%|████████  | 6573/8192 [00:13<00:03, 487.03it/s]
Adding requests:  81%|████████  | 6622/8192 [00:14<00:03, 482.23it/s]
Adding requests:  81%|████████▏ | 6671/8192 [00:14<00:03, 478.30it/s]
Adding requests:  82%|████████▏ | 6720/8192 [00:14<00:03, 478.92it/s]
Adding requests:  83%|████████▎ | 6768/8192 [00:14<00:02, 478.15it/s]
Adding requests:  83%|████████▎ | 6817/8192 [00:14<00:02, 481.29it/s]
Adding requests:  84%|████████▍ | 6868/8192 [00:14<00:02, 487.62it/s]
Adding requests:  84%|████████▍ | 6917/8192 [00:14<00:02, 485.77it/s]
Adding requests:  85%|████████▌ | 6967/8192 [00:14<00:02, 489.88it/s]
Adding requests:  86%|████████▌ | 7016/8192 [00:14<00:02, 464.75it/s]
Adding requests:  86%|████████▌ | 7065/8192 [00:14<00:02, 471.21it/s]
Adding requests:  87%|████████▋ | 7114/8192 [00:15<00:02, 474.59it/s]
Adding requests:  87%|████████▋ | 7162/8192 [00:15<00:02, 472.92it/s]
Adding requests:  88%|████████▊ | 7210/8192 [00:15<00:02, 474.35it/s]
Adding requests:  89%|████████▊ | 7259/8192 [00:15<00:01, 478.32it/s]
Adding requests:  89%|████████▉ | 7308/8192 [00:15<00:01, 480.92it/s]
Adding requests:  90%|████████▉ | 7357/8192 [00:15<00:01, 477.64it/s]
Adding requests:  90%|█████████ | 7408/8192 [00:15<00:01, 484.69it/s]
Adding requests:  91%|█████████ | 7458/8192 [00:15<00:01, 488.23it/s]
Adding requests:  92%|█████████▏| 7507/8192 [00:15<00:01, 486.59it/s]
Adding requests:  92%|█████████▏| 7556/8192 [00:15<00:01, 485.54it/s]
Adding requests:  93%|█████████▎| 7605/8192 [00:16<00:01, 484.19it/s]
Adding requests:  93%|█████████▎| 7654/8192 [00:16<00:01, 485.78it/s]
Adding requests:  94%|█████████▍| 7704/8192 [00:16<00:01, 487.16it/s]
Adding requests:  95%|█████████▍| 7753/8192 [00:16<00:00, 470.07it/s]
Adding requests:  95%|█████████▌| 7801/8192 [00:16<00:00, 464.10it/s]
Adding requests:  96%|█████████▌| 7849/8192 [00:16<00:00, 467.91it/s]
Adding requests:  96%|█████████▋| 7897/8192 [00:16<00:00, 471.22it/s]
Adding requests:  97%|█████████▋| 7945/8192 [00:16<00:00, 465.73it/s]
Adding requests:  98%|█████████▊| 7993/8192 [00:16<00:00, 467.32it/s]
Adding requests:  98%|█████████▊| 8040/8192 [00:16<00:00, 465.05it/s]
Adding requests:  99%|█████████▊| 8089/8192 [00:17<00:00, 471.27it/s]
Adding requests:  99%|█████████▉| 8137/8192 [00:17<00:00, 472.57it/s]
Adding requests: 100%|█████████▉| 8186/8192 [00:17<00:00, 477.21it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 473.59it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  55%|█████▌    | 4535/8192 [00:00<00:00, 14192.03it/s, est. speed input: 14533009.23 toks/s, output: 14192.12 toks/s]
Processed prompts:  73%|███████▎  | 5955/8192 [00:05<00:02, 825.23it/s, est. speed input: 1076727.19 toks/s, output: 1051.49 toks/s]    
Processed prompts:  80%|████████  | 6556/8192 [00:07<00:02, 626.20it/s, est. speed input: 853911.69 toks/s, output: 833.90 toks/s]  
Processed prompts:  84%|████████▍ | 6895/8192 [00:09<00:02, 548.16it/s, est. speed input: 776922.70 toks/s, output: 758.71 toks/s]
Processed prompts:  87%|████████▋ | 7111/8192 [00:10<00:02, 482.38it/s, est. speed input: 723832.50 toks/s, output: 706.87 toks/s]
Processed prompts:  89%|████████▊ | 7257/8192 [00:10<00:02, 459.09it/s, est. speed input: 704443.17 toks/s, output: 687.93 toks/s]
Processed prompts:  90%|████████▉ | 7364/8192 [00:11<00:01, 422.08it/s, est. speed input: 683072.31 toks/s, output: 667.06 toks/s]
Processed prompts:  91%|█████████ | 7443/8192 [00:11<00:01, 411.89it/s, est. speed input: 675283.39 toks/s, output: 659.46 toks/s]
Processed prompts:  92%|█████████▏| 7507/8192 [00:11<00:01, 393.26it/s, est. speed input: 666617.33 toks/s, output: 650.99 toks/s]
Processed prompts:  92%|█████████▏| 7560/8192 [00:11<00:01, 367.82it/s, est. speed input: 657630.68 toks/s, output: 642.22 toks/s]
Processed prompts:  93%|█████████▎| 7607/8192 [00:12<00:01, 336.94it/s, est. speed input: 648285.30 toks/s, output: 633.09 toks/s]
Processed prompts:  94%|█████████▎| 7671/8192 [00:12<00:01, 321.05it/s, est. speed input: 640556.48 toks/s, output: 625.54 toks/s]
Processed prompts:  94%|█████████▍| 7735/8192 [00:12<00:01, 307.65it/s, est. speed input: 633245.43 toks/s, output: 618.40 toks/s]
Processed prompts:  95%|█████████▌| 7799/8192 [00:12<00:01, 296.47it/s, est. speed input: 626228.23 toks/s, output: 611.55 toks/s]
Processed prompts:  96%|█████████▌| 7863/8192 [00:12<00:01, 287.28it/s, est. speed input: 619444.76 toks/s, output: 604.93 toks/s]
Processed prompts:  97%|█████████▋| 7927/8192 [00:13<00:00, 281.74it/s, est. speed input: 613151.70 toks/s, output: 598.78 toks/s]
Processed prompts:  98%|█████████▊| 7991/8192 [00:13<00:00, 276.13it/s, est. speed input: 606884.65 toks/s, output: 592.66 toks/s]
Processed prompts:  98%|█████████▊| 8055/8192 [00:13<00:00, 273.93it/s, est. speed input: 601108.98 toks/s, output: 587.02 toks/s]
Processed prompts:  99%|█████████▉| 8119/8192 [00:13<00:00, 271.36it/s, est. speed input: 595405.48 toks/s, output: 581.45 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:14<00:00, 271.36it/s, est. speed input: 598556.02 toks/s, output: 584.53 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:14<00:00, 584.52it/s, est. speed input: 598556.02 toks/s, output: 584.53 toks/s]
[rank0]:[W126 14:49:06.277115101 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 15:56:17
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:56:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=717046) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=717046) WARNING 01-26 15:56:40 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=717046) WARNING 01-26 15:56:50 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 37.42 requests/s, 19195.68 total tokens/s, 37.42 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 15:56:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:56:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:56:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:56:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:56:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:56:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:56:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:56:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:56:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:56:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:56:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:56:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:56:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:56:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:56:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:56:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:56:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=717046) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=717046) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.14it/s]
(EngineCore_DP0 pid=717046) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.06s/it]
(EngineCore_DP0 pid=717046) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.03s/it]
(EngineCore_DP0 pid=717046) 
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=717046) [2026-01-26 15:56:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=717046) 2026-01-26 15:56:50,418 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=717046) 2026-01-26 15:56:50,442 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=717046) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.69it/s]
(EngineCore_DP0 pid=717046) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.22it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 510.56it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 255.00it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 303.29it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:00, 114.05it/s, est. speed input: 58395.66 toks/s, output: 114.05 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:01, 56.05it/s, est. speed input: 31268.47 toks/s, output: 61.07 toks/s]  
Processed prompts:  27%|██▋       | 34/128 [00:00<00:01, 49.08it/s, est. speed input: 27745.47 toks/s, output: 54.19 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 46.11it/s, est. speed input: 26294.93 toks/s, output: 51.36 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 44.39it/s, est. speed input: 25468.60 toks/s, output: 49.74 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:01, 43.04it/s, est. speed input: 24830.87 toks/s, output: 48.50 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 42.06it/s, est. speed input: 24334.04 toks/s, output: 47.53 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 41.35it/s, est. speed input: 23935.77 toks/s, output: 46.75 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 40.82it/s, est. speed input: 23604.99 toks/s, output: 46.10 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 40.45it/s, est. speed input: 23329.81 toks/s, output: 45.57 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 40.20it/s, est. speed input: 23097.16 toks/s, output: 45.11 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 40.02it/s, est. speed input: 22897.70 toks/s, output: 44.72 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:01, 39.88it/s, est. speed input: 22723.84 toks/s, output: 44.38 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:00, 39.78it/s, est. speed input: 22597.05 toks/s, output: 44.13 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 39.73it/s, est. speed input: 22486.73 toks/s, output: 43.92 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 39.73it/s, est. speed input: 22389.35 toks/s, output: 43.73 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 39.72it/s, est. speed input: 22299.55 toks/s, output: 43.55 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 39.73it/s, est. speed input: 22218.29 toks/s, output: 43.39 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 39.72it/s, est. speed input: 22142.80 toks/s, output: 43.25 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 39.72it/s, est. speed input: 22073.41 toks/s, output: 43.11 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 39.75it/s, est. speed input: 22011.37 toks/s, output: 42.99 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 39.75it/s, est. speed input: 21952.38 toks/s, output: 42.88 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 39.82it/s, est. speed input: 21901.28 toks/s, output: 42.78 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 39.82it/s, est. speed input: 21862.13 toks/s, output: 42.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 42.70it/s, est. speed input: 21862.13 toks/s, output: 42.70 toks/s]
[rank0]:[W126 15:56:56.557202459 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 15:56:58
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:57:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=718247) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=718247) WARNING 01-26 15:57:22 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=718247) WARNING 01-26 15:57:31 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 38.90 requests/s, 39877.29 total tokens/s, 38.90 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 15:57:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:57:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:57:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:57:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:57:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:57:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:57:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:57:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:57:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:57:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:57:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:57:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=718247) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=718247) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.08it/s]
(EngineCore_DP0 pid=718247) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.07s/it]
(EngineCore_DP0 pid=718247) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.05s/it]
(EngineCore_DP0 pid=718247) 
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=718247) [2026-01-26 15:57:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=718247) 2026-01-26 15:57:31,461 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=718247) 2026-01-26 15:57:31,484 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=718247) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 13.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.99it/s]
(EngineCore_DP0 pid=718247) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.03it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  21%|██        | 27/128 [00:00<00:00, 265.42it/s]
Adding requests:  48%|████▊     | 62/128 [00:00<00:00, 311.51it/s]
Adding requests:  73%|███████▎  | 94/128 [00:00<00:00, 311.92it/s]
Adding requests:  99%|█████████▉| 127/128 [00:00<00:00, 314.76it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 309.89it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:00, 177.81it/s, est. speed input: 182093.02 toks/s, output: 177.82 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 58.27it/s, est. speed input: 66361.55 toks/s, output: 64.81 toks/s]   
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 50.75it/s, est. speed input: 58370.19 toks/s, output: 57.00 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 47.24it/s, est. speed input: 54829.34 toks/s, output: 53.54 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 45.32it/s, est. speed input: 52978.11 toks/s, output: 51.74 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 43.81it/s, est. speed input: 51546.44 toks/s, output: 50.34 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 42.82it/s, est. speed input: 50592.45 toks/s, output: 49.41 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 42.01it/s, est. speed input: 49785.39 toks/s, output: 48.62 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 41.41it/s, est. speed input: 49109.92 toks/s, output: 47.96 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:01, 40.94it/s, est. speed input: 48524.55 toks/s, output: 47.39 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 40.53it/s, est. speed input: 47996.88 toks/s, output: 46.87 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 40.27it/s, est. speed input: 47544.68 toks/s, output: 46.43 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 40.08it/s, est. speed input: 47144.43 toks/s, output: 46.04 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 39.92it/s, est. speed input: 46780.67 toks/s, output: 45.68 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 39.79it/s, est. speed input: 46452.22 toks/s, output: 45.36 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 39.80it/s, est. speed input: 46227.88 toks/s, output: 45.14 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 39.76it/s, est. speed input: 46014.03 toks/s, output: 44.94 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 39.66it/s, est. speed input: 45804.96 toks/s, output: 44.73 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 39.65it/s, est. speed input: 45620.18 toks/s, output: 44.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 39.65it/s, est. speed input: 45577.04 toks/s, output: 44.51 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.51it/s, est. speed input: 45577.04 toks/s, output: 44.51 toks/s]
[rank0]:[W126 15:57:36.651712956 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 15:57:38
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:57:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=719363) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=719363) WARNING 01-26 15:58:03 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=719363) WARNING 01-26 15:58:12 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 77.95 requests/s, 79896.56 total tokens/s, 77.95 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 15:57:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:57:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:57:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:57:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:57:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:57:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:57:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:57:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:57:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:57:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:57:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:57:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:57:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:57:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=719363) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=719363) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.13it/s]
(EngineCore_DP0 pid=719363) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.13s/it]
(EngineCore_DP0 pid=719363) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]
(EngineCore_DP0 pid=719363) 
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=719363) [2026-01-26 15:57:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=719363) 2026-01-26 15:58:12,855 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=719363) 2026-01-26 15:58:12,878 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=719363) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 11.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 11.14it/s]
(EngineCore_DP0 pid=719363) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.86it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█         | 27/256 [00:00<00:00, 266.18it/s]
Adding requests:  24%|██▍       | 62/256 [00:00<00:00, 311.27it/s]
Adding requests:  37%|███▋      | 94/256 [00:00<00:01, 139.14it/s]
Adding requests:  49%|████▉     | 125/256 [00:00<00:00, 176.75it/s]
Adding requests:  62%|██████▏   | 158/256 [00:00<00:00, 212.89it/s]
Adding requests:  76%|███████▌  | 194/256 [00:00<00:00, 248.53it/s]
Adding requests:  89%|████████▉ | 229/256 [00:00<00:00, 274.45it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 238.68it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:00<00:00, 770.03it/s, est. speed input: 788563.72 toks/s, output: 770.05 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:01<00:00, 133.35it/s, est. speed input: 156966.47 toks/s, output: 153.29 toks/s]
Processed prompts:  79%|███████▊  | 201/256 [00:01<00:00, 114.63it/s, est. speed input: 136201.42 toks/s, output: 133.01 toks/s]
Processed prompts:  88%|████████▊ | 225/256 [00:01<00:00, 104.77it/s, est. speed input: 126797.39 toks/s, output: 123.83 toks/s]
Processed prompts:  95%|█████████▍| 243/256 [00:02<00:00, 99.57it/s, est. speed input: 122041.20 toks/s, output: 119.18 toks/s] 
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 99.57it/s, est. speed input: 118599.88 toks/s, output: 115.82 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 115.81it/s, est. speed input: 118599.88 toks/s, output: 115.82 toks/s]
[rank0]:[W126 15:58:18.298867977 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 15:58:19
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:58:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=720517) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=720517) WARNING 01-26 15:58:46 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=720517) WARNING 01-26 15:58:55 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 119.54 requests/s, 122528.16 total tokens/s, 119.54 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 15:58:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:58:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:58:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:58:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:58:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:58:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:58:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:58:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:58:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:58:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:58:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:58:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:58:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:58:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:58:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:58:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:58:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=720517) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=720517) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.14it/s]
(EngineCore_DP0 pid=720517) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.05s/it]
(EngineCore_DP0 pid=720517) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.03s/it]
(EngineCore_DP0 pid=720517) 
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=720517) [2026-01-26 15:58:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=720517) 2026-01-26 15:58:55,883 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=720517) 2026-01-26 15:58:55,907 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=720517) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.54it/s]
(EngineCore_DP0 pid=720517) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 17.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 18.02it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 27/512 [00:00<00:01, 266.62it/s]
Adding requests:  12%|█▏        | 62/512 [00:00<00:01, 312.05it/s]
Adding requests:  18%|█▊        | 94/512 [00:00<00:01, 311.66it/s]
Adding requests:  25%|██▍       | 126/512 [00:00<00:01, 309.78it/s]
Adding requests:  31%|███▏      | 160/512 [00:00<00:01, 317.54it/s]
Adding requests:  38%|███▊      | 196/512 [00:00<00:00, 331.68it/s]
Adding requests:  45%|████▌     | 232/512 [00:00<00:00, 338.21it/s]
Adding requests:  52%|█████▏    | 267/512 [00:00<00:00, 338.90it/s]
Adding requests:  59%|█████▉    | 301/512 [00:00<00:00, 339.04it/s]
Adding requests:  66%|██████▌   | 338/512 [00:01<00:00, 348.19it/s]
Adding requests:  73%|███████▎  | 375/512 [00:01<00:00, 351.70it/s]
Adding requests:  81%|████████  | 413/512 [00:01<00:00, 357.42it/s]
Adding requests:  88%|████████▊ | 449/512 [00:01<00:00, 354.72it/s]
Adding requests:  96%|█████████▌| 489/512 [00:01<00:00, 367.12it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 343.42it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:00<00:00, 1455.13it/s, est. speed input: 1490145.30 toks/s, output: 1455.15 toks/s]
Processed prompts:  65%|██████▍   | 332/512 [00:01<00:00, 216.92it/s, est. speed input: 259200.18 toks/s, output: 253.12 toks/s]   
Processed prompts:  78%|███████▊  | 400/512 [00:01<00:00, 180.22it/s, est. speed input: 218988.84 toks/s, output: 213.86 toks/s]
Processed prompts:  87%|████████▋ | 443/512 [00:02<00:00, 164.67it/s, est. speed input: 203656.64 toks/s, output: 198.88 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:02<00:00, 154.46it/s, est. speed input: 194805.41 toks/s, output: 190.24 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:02<00:00, 148.13it/s, est. speed input: 189557.62 toks/s, output: 185.11 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 148.13it/s, est. speed input: 187856.38 toks/s, output: 183.45 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 183.45it/s, est. speed input: 187856.38 toks/s, output: 183.45 toks/s]
[rank0]:[W126 15:59:02.698490215 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 15:59:04
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:59:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=721743) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=721743) WARNING 01-26 15:59:34 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=721743) WARNING 01-26 15:59:43 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 128.48 requests/s, 131688.49 total tokens/s, 128.48 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 15:59:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:59:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:59:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:59:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:59:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:59:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:59:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:59:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:59:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:59:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:59:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:59:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:59:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:59:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:59:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:59:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:59:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:25] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=721743) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=721743) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=721743) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
(EngineCore_DP0 pid=721743) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.07it/s]
(EngineCore_DP0 pid=721743) 
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=721743) [2026-01-26 15:59:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=721743) 2026-01-26 15:59:43,887 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=721743) 2026-01-26 15:59:43,912 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=721743) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 15.82it/s]
(EngineCore_DP0 pid=721743) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 17.70it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.96it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 277.22it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 319.52it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 316.89it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 322.02it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 331.27it/s]
Adding requests:  20%|█▉        | 200/1024 [00:00<00:02, 341.77it/s]
Adding requests:  23%|██▎       | 236/1024 [00:00<00:02, 345.98it/s]
Adding requests:  26%|██▋       | 271/1024 [00:00<00:02, 344.35it/s]
Adding requests:  30%|███       | 308/1024 [00:00<00:02, 350.71it/s]
Adding requests:  34%|███▍      | 346/1024 [00:01<00:01, 356.31it/s]
Adding requests:  37%|███▋      | 383/1024 [00:01<00:01, 360.06it/s]
Adding requests:  41%|████      | 421/1024 [00:01<00:01, 363.09it/s]
Adding requests:  45%|████▍     | 458/1024 [00:01<00:01, 361.27it/s]
Adding requests:  49%|████▊     | 498/1024 [00:01<00:01, 369.94it/s]
Adding requests:  52%|█████▏    | 537/1024 [00:01<00:01, 374.20it/s]
Adding requests:  56%|█████▌    | 575/1024 [00:01<00:01, 372.11it/s]
Adding requests:  60%|█████▉    | 613/1024 [00:01<00:01, 361.81it/s]
Adding requests:  63%|██████▎   | 650/1024 [00:01<00:01, 355.94it/s]
Adding requests:  67%|██████▋   | 687/1024 [00:01<00:00, 359.42it/s]
Adding requests:  71%|███████   | 723/1024 [00:02<00:00, 355.73it/s]
Adding requests:  74%|███████▍  | 759/1024 [00:02<00:00, 354.28it/s]
Adding requests:  78%|███████▊  | 795/1024 [00:02<00:00, 354.16it/s]
Adding requests:  81%|████████▏ | 833/1024 [00:02<00:00, 358.87it/s]
Adding requests:  85%|████████▍ | 870/1024 [00:02<00:00, 360.83it/s]
Adding requests:  89%|████████▊ | 907/1024 [00:02<00:00, 362.57it/s]
Adding requests:  92%|█████████▏| 944/1024 [00:02<00:00, 351.44it/s]
Adding requests:  96%|█████████▌| 980/1024 [00:02<00:00, 352.44it/s]
Adding requests:  99%|█████████▉| 1016/1024 [00:02<00:00, 350.64it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 352.76it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:00<00:00, 2478.35it/s, est. speed input: 2537956.28 toks/s, output: 2478.38 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:02<00:01, 255.62it/s, est. speed input: 312534.85 toks/s, output: 305.21 toks/s]   
Processed prompts:  72%|███████▏  | 736/1024 [00:02<00:01, 212.59it/s, est. speed input: 264201.48 toks/s, output: 258.01 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:03<00:01, 188.48it/s, est. speed input: 241524.37 toks/s, output: 235.86 toks/s]
Processed prompts:  83%|████████▎ | 847/1024 [00:03<00:00, 180.94it/s, est. speed input: 233785.34 toks/s, output: 228.31 toks/s]
Processed prompts:  86%|████████▌ | 881/1024 [00:03<00:00, 173.82it/s, est. speed input: 228020.86 toks/s, output: 222.68 toks/s]
Processed prompts:  89%|████████▊ | 908/1024 [00:04<00:00, 161.50it/s, est. speed input: 221220.14 toks/s, output: 216.04 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:04<00:00, 154.33it/s, est. speed input: 217053.31 toks/s, output: 211.97 toks/s]
Processed prompts:  93%|█████████▎| 949/1024 [00:04<00:00, 154.36it/s, est. speed input: 215451.09 toks/s, output: 210.40 toks/s]
Processed prompts:  94%|█████████▍| 967/1024 [00:04<00:00, 153.07it/s, est. speed input: 213718.30 toks/s, output: 208.71 toks/s]
Processed prompts:  96%|█████████▌| 984/1024 [00:04<00:00, 150.19it/s, est. speed input: 211844.79 toks/s, output: 206.88 toks/s]
Processed prompts:  98%|█████████▊| 1000/1024 [00:04<00:00, 145.87it/s, est. speed input: 209862.71 toks/s, output: 204.94 toks/s]
Processed prompts:  99%|█████████▉| 1015/1024 [00:05<00:00, 140.10it/s, est. speed input: 207750.13 toks/s, output: 202.88 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 140.10it/s, est. speed input: 206978.23 toks/s, output: 202.13 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 202.12it/s, est. speed input: 206978.23 toks/s, output: 202.13 toks/s]
[rank0]:[W126 15:59:54.370040787 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 15:59:56
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:00:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=723104) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=723104) WARNING 01-26 16:00:31 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=723104) WARNING 01-26 16:00:41 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 134.50 requests/s, 137866.07 total tokens/s, 134.50 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 16:00:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:00:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:00:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:00:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:00:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:00:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:00:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:00:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:00:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:00:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:00:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:00:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:00:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:00:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:00:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:00:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:00:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=723104) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=723104) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=723104) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
(EngineCore_DP0 pid=723104) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.07it/s]
(EngineCore_DP0 pid=723104) 
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=723104) [2026-01-26 16:00:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=723104) [rank0]:W0126 16:00:36.450000 723104 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=723104) [rank0]:W0126 16:00:36.533000 723104 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=723104) [rank0]:W0126 16:00:37.535000 723104 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=723104) [rank0]:W0126 16:00:37.665000 723104 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=723104) 2026-01-26 16:00:41,145 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=723104) 2026-01-26 16:00:41,171 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=723104) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 16.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 17.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 15.89it/s]
(EngineCore_DP0 pid=723104) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 17.98it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 15.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 15.45it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 278.32it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 318.28it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 315.35it/s]
Adding requests:   6%|▌         | 127/2048 [00:00<00:06, 308.86it/s]
Adding requests:   8%|▊         | 162/2048 [00:00<00:05, 320.72it/s]
Adding requests:  10%|▉         | 199/2048 [00:00<00:05, 333.71it/s]
Adding requests:  12%|█▏        | 236/2048 [00:00<00:05, 342.10it/s]
Adding requests:  13%|█▎        | 271/2048 [00:00<00:05, 341.00it/s]
Adding requests:  15%|█▌        | 308/2048 [00:00<00:04, 348.50it/s]
Adding requests:  17%|█▋        | 344/2048 [00:01<00:04, 350.59it/s]
Adding requests:  19%|█▊        | 380/2048 [00:01<00:04, 353.39it/s]
Adding requests:  20%|██        | 419/2048 [00:01<00:04, 363.48it/s]
Adding requests:  22%|██▏       | 456/2048 [00:01<00:04, 360.52it/s]
Adding requests:  24%|██▍       | 496/2048 [00:01<00:04, 371.78it/s]
Adding requests:  26%|██▌       | 535/2048 [00:01<00:04, 373.54it/s]
Adding requests:  28%|██▊       | 573/2048 [00:01<00:03, 373.05it/s]
Adding requests:  30%|██▉       | 611/2048 [00:01<00:04, 359.18it/s]
Adding requests:  32%|███▏      | 648/2048 [00:01<00:03, 355.60it/s]
Adding requests:  33%|███▎      | 685/2048 [00:01<00:03, 357.85it/s]
Adding requests:  35%|███▌      | 721/2048 [00:02<00:03, 356.79it/s]
Adding requests:  37%|███▋      | 757/2048 [00:02<00:03, 351.10it/s]
Adding requests:  39%|███▊      | 793/2048 [00:02<00:03, 349.95it/s]
Adding requests:  41%|████      | 831/2048 [00:02<00:03, 356.99it/s]
Adding requests:  42%|████▏     | 868/2048 [00:02<00:03, 357.62it/s]
Adding requests:  44%|████▍     | 906/2048 [00:02<00:03, 363.98it/s]
Adding requests:  46%|████▌     | 943/2048 [00:02<00:03, 355.52it/s]
Adding requests:  48%|████▊     | 979/2048 [00:02<00:02, 356.56it/s]
Adding requests:  50%|████▉     | 1015/2048 [00:02<00:02, 350.72it/s]
Adding requests:  51%|█████▏    | 1051/2048 [00:02<00:02, 352.33it/s]
Adding requests:  53%|█████▎    | 1087/2048 [00:03<00:02, 352.75it/s]
Adding requests:  55%|█████▍    | 1123/2048 [00:03<00:02, 354.18it/s]
Adding requests:  57%|█████▋    | 1159/2048 [00:03<00:02, 353.10it/s]
Adding requests:  58%|█████▊    | 1195/2048 [00:03<00:02, 354.46it/s]
Adding requests:  60%|██████    | 1233/2048 [00:03<00:02, 361.90it/s]
Adding requests:  62%|██████▏   | 1270/2048 [00:03<00:02, 357.04it/s]
Adding requests:  64%|██████▍   | 1306/2048 [00:03<00:02, 355.91it/s]
Adding requests:  66%|██████▌   | 1343/2048 [00:03<00:01, 356.92it/s]
Adding requests:  67%|██████▋   | 1380/2048 [00:03<00:01, 359.03it/s]
Adding requests:  69%|██████▉   | 1416/2048 [00:04<00:01, 356.83it/s]
Adding requests:  71%|███████   | 1453/2048 [00:04<00:01, 359.76it/s]
Adding requests:  73%|███████▎  | 1490/2048 [00:04<00:01, 362.38it/s]
Adding requests:  75%|███████▍  | 1527/2048 [00:04<00:01, 353.07it/s]
Adding requests:  76%|███████▋  | 1563/2048 [00:04<00:01, 349.82it/s]
Adding requests:  78%|███████▊  | 1599/2048 [00:04<00:01, 346.45it/s]
Adding requests:  80%|███████▉  | 1634/2048 [00:04<00:01, 344.19it/s]
Adding requests:  81%|████████▏ | 1669/2048 [00:04<00:01, 330.37it/s]
Adding requests:  83%|████████▎ | 1706/2048 [00:04<00:01, 339.72it/s]
Adding requests:  85%|████████▌ | 1742/2048 [00:04<00:00, 345.43it/s]
Adding requests:  87%|████████▋ | 1780/2048 [00:05<00:00, 353.29it/s]
Adding requests:  89%|████████▊ | 1816/2048 [00:05<00:00, 350.60it/s]
Adding requests:  90%|█████████ | 1852/2048 [00:05<00:00, 352.92it/s]
Adding requests:  92%|█████████▏| 1889/2048 [00:05<00:00, 357.02it/s]
Adding requests:  94%|█████████▍| 1927/2048 [00:05<00:00, 362.78it/s]
Adding requests:  96%|█████████▌| 1964/2048 [00:05<00:00, 362.90it/s]
Adding requests:  98%|█████████▊| 2001/2048 [00:05<00:00, 355.12it/s]
Adding requests:  99%|█████████▉| 2037/2048 [00:05<00:00, 348.61it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 351.95it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  38%|███▊      | 781/2048 [00:00<00:00, 5562.65it/s, est. speed input: 5696494.61 toks/s, output: 5562.76 toks/s]
Processed prompts:  65%|██████▌   | 1338/2048 [00:04<00:02, 266.81it/s, est. speed input: 327871.15 toks/s, output: 320.19 toks/s]  
Processed prompts:  77%|███████▋  | 1576/2048 [00:05<00:02, 216.91it/s, est. speed input: 271592.30 toks/s, output: 265.23 toks/s]
Processed prompts:  84%|████████▎ | 1712/2048 [00:07<00:01, 194.93it/s, est. speed input: 250171.91 toks/s, output: 244.31 toks/s]
Processed prompts:  88%|████████▊ | 1799/2048 [00:07<00:01, 188.10it/s, est. speed input: 243030.29 toks/s, output: 237.33 toks/s]
Processed prompts:  91%|█████████ | 1860/2048 [00:08<00:01, 178.35it/s, est. speed input: 236313.53 toks/s, output: 230.77 toks/s]
Processed prompts:  93%|█████████▎| 1905/2048 [00:08<00:00, 171.12it/s, est. speed input: 231908.62 toks/s, output: 226.47 toks/s]
Processed prompts:  95%|█████████▍| 1940/2048 [00:08<00:00, 167.88it/s, est. speed input: 229576.92 toks/s, output: 224.20 toks/s]
Processed prompts:  96%|█████████▌| 1968/2048 [00:08<00:00, 160.71it/s, est. speed input: 226722.52 toks/s, output: 221.41 toks/s]
Processed prompts:  97%|█████████▋| 1991/2048 [00:09<00:00, 164.51it/s, est. speed input: 226471.44 toks/s, output: 221.16 toks/s]
Processed prompts:  98%|█████████▊| 2014/2048 [00:09<00:00, 149.57it/s, est. speed input: 223130.66 toks/s, output: 217.90 toks/s]
Processed prompts:  99%|█████████▉| 2033/2048 [00:09<00:00, 151.29it/s, est. speed input: 222425.05 toks/s, output: 217.21 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:09<00:00, 151.29it/s, est. speed input: 222966.48 toks/s, output: 217.74 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:09<00:00, 217.74it/s, est. speed input: 222966.48 toks/s, output: 217.74 toks/s]
[rank0]:[W126 16:00:58.206826161 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 16:01:00
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:01:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=724688) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=724688) WARNING 01-26 16:01:48 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=724688) WARNING 01-26 16:01:58 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 139.82 requests/s, 143315.09 total tokens/s, 139.82 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 16:01:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:01:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:01:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:01:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:01:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:01:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:01:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:01:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:01:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:01:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:01:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:01:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:01:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:01:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:01:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:01:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:01:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:39] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:39] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:39] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:39] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:39] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=724688) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=724688) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=724688) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
(EngineCore_DP0 pid=724688) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.07it/s]
(EngineCore_DP0 pid=724688) 
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=724688) [2026-01-26 16:01:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=724688) [rank0]:W0126 16:01:53.394000 724688 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=724688) [rank0]:W0126 16:01:53.478000 724688 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=724688) [rank0]:W0126 16:01:54.447000 724688 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=724688) [rank0]:W0126 16:01:54.578000 724688 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=724688) 2026-01-26 16:01:58,329 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=724688) 2026-01-26 16:01:58,355 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=724688) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 15.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 14.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 14.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 15.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.51it/s]
(EngineCore_DP0 pid=724688) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.67it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.80it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 18.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 17.98it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 277.88it/s]
Adding requests:   2%|▏         | 64/4096 [00:00<00:12, 319.60it/s]
Adding requests:   2%|▏         | 96/4096 [00:00<00:12, 316.95it/s]
Adding requests:   3%|▎         | 128/4096 [00:00<00:12, 308.44it/s]
Adding requests:   4%|▍         | 163/4096 [00:00<00:12, 321.54it/s]
Adding requests:   5%|▍         | 200/4096 [00:00<00:11, 335.18it/s]
Adding requests:   6%|▌         | 236/4096 [00:00<00:11, 341.62it/s]
Adding requests:   7%|▋         | 271/4096 [00:00<00:11, 341.34it/s]
Adding requests:   7%|▋         | 307/4096 [00:00<00:10, 346.43it/s]
Adding requests:   8%|▊         | 345/4096 [00:01<00:10, 356.02it/s]
Adding requests:   9%|▉         | 382/4096 [00:01<00:10, 357.59it/s]
Adding requests:  10%|█         | 421/4096 [00:01<00:10, 365.42it/s]
Adding requests:  11%|█         | 458/4096 [00:01<00:10, 363.32it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:09, 371.46it/s]
Adding requests:  13%|█▎        | 537/4096 [00:01<00:09, 376.86it/s]
Adding requests:  14%|█▍        | 575/4096 [00:01<00:09, 375.94it/s]
Adding requests:  15%|█▍        | 613/4096 [00:01<00:09, 360.20it/s]
Adding requests:  16%|█▌        | 650/4096 [00:01<00:09, 355.22it/s]
Adding requests:  17%|█▋        | 687/4096 [00:01<00:09, 358.90it/s]
Adding requests:  18%|█▊        | 723/4096 [00:02<00:09, 356.68it/s]
Adding requests:  19%|█▊        | 759/4096 [00:02<00:09, 355.46it/s]
Adding requests:  19%|█▉        | 795/4096 [00:02<00:09, 355.73it/s]
Adding requests:  20%|██        | 833/4096 [00:02<00:09, 360.66it/s]
Adding requests:  21%|██        | 870/4096 [00:02<00:09, 357.73it/s]
Adding requests:  22%|██▏       | 908/4096 [00:02<00:08, 361.26it/s]
Adding requests:  23%|██▎       | 945/4096 [00:02<00:08, 355.45it/s]
Adding requests:  24%|██▍       | 982/4096 [00:02<00:08, 356.63it/s]
Adding requests:  25%|██▍       | 1018/4096 [00:02<00:08, 354.02it/s]
Adding requests:  26%|██▌       | 1054/4096 [00:02<00:08, 352.78it/s]
Adding requests:  27%|██▋       | 1090/4096 [00:03<00:08, 351.83it/s]
Adding requests:  28%|██▊       | 1128/4096 [00:03<00:08, 358.04it/s]
Adding requests:  28%|██▊       | 1164/4096 [00:03<00:08, 356.04it/s]
Adding requests:  29%|██▉       | 1200/4096 [00:03<00:08, 354.43it/s]
Adding requests:  30%|███       | 1238/4096 [00:03<00:07, 361.48it/s]
Adding requests:  31%|███       | 1275/4096 [00:03<00:07, 356.12it/s]
Adding requests:  32%|███▏      | 1311/4096 [00:03<00:07, 356.97it/s]
Adding requests:  33%|███▎      | 1347/4096 [00:03<00:07, 357.74it/s]
Adding requests:  34%|███▍      | 1384/4096 [00:03<00:07, 360.11it/s]
Adding requests:  35%|███▍      | 1421/4096 [00:04<00:07, 358.10it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:04<00:07, 358.53it/s]
Adding requests:  36%|███▋      | 1495/4096 [00:04<00:07, 364.50it/s]
Adding requests:  37%|███▋      | 1532/4096 [00:04<00:07, 363.15it/s]
Adding requests:  38%|███▊      | 1569/4096 [00:04<00:07, 357.14it/s]
Adding requests:  39%|███▉      | 1605/4096 [00:04<00:07, 355.07it/s]
Adding requests:  40%|████      | 1641/4096 [00:04<00:07, 347.10it/s]
Adding requests:  41%|████      | 1676/4096 [00:04<00:07, 343.31it/s]
Adding requests:  42%|████▏     | 1713/4096 [00:04<00:06, 350.38it/s]
Adding requests:  43%|████▎     | 1750/4096 [00:04<00:06, 355.55it/s]
Adding requests:  44%|████▎     | 1786/4096 [00:05<00:06, 356.35it/s]
Adding requests:  44%|████▍     | 1822/4096 [00:05<00:06, 354.23it/s]
Adding requests:  45%|████▌     | 1859/4096 [00:05<00:06, 357.33it/s]
Adding requests:  46%|████▋     | 1895/4096 [00:05<00:06, 351.98it/s]
Adding requests:  47%|████▋     | 1933/4096 [00:05<00:06, 359.05it/s]
Adding requests:  48%|████▊     | 1970/4096 [00:05<00:05, 359.94it/s]
Adding requests:  49%|████▉     | 2007/4096 [00:05<00:05, 349.45it/s]
Adding requests:  50%|████▉     | 2043/4096 [00:05<00:05, 347.16it/s]
Adding requests:  51%|█████     | 2078/4096 [00:05<00:05, 341.42it/s]
Adding requests:  52%|█████▏    | 2115/4096 [00:05<00:05, 347.49it/s]
Adding requests:  53%|█████▎    | 2151/4096 [00:06<00:05, 348.56it/s]
Adding requests:  53%|█████▎    | 2186/4096 [00:06<00:05, 342.51it/s]
Adding requests:  54%|█████▍    | 2222/4096 [00:06<00:05, 344.78it/s]
Adding requests:  55%|█████▌    | 2259/4096 [00:06<00:05, 349.49it/s]
Adding requests:  56%|█████▌    | 2297/4096 [00:06<00:05, 356.05it/s]
Adding requests:  57%|█████▋    | 2334/4096 [00:06<00:04, 358.07it/s]
Adding requests:  58%|█████▊    | 2371/4096 [00:06<00:04, 360.47it/s]
Adding requests:  59%|█████▉    | 2410/4096 [00:06<00:04, 366.88it/s]
Adding requests:  60%|█████▉    | 2447/4096 [00:06<00:04, 357.29it/s]
Adding requests:  61%|██████    | 2483/4096 [00:07<00:04, 357.42it/s]
Adding requests:  62%|██████▏   | 2521/4096 [00:07<00:04, 361.97it/s]
Adding requests:  62%|██████▎   | 2560/4096 [00:07<00:04, 369.52it/s]
Adding requests:  63%|██████▎   | 2600/4096 [00:07<00:03, 375.38it/s]
Adding requests:  64%|██████▍   | 2638/4096 [00:07<00:04, 361.72it/s]
Adding requests:  65%|██████▌   | 2675/4096 [00:07<00:03, 361.17it/s]
Adding requests:  66%|██████▌   | 2712/4096 [00:07<00:03, 355.93it/s]
Adding requests:  67%|██████▋   | 2749/4096 [00:07<00:03, 359.59it/s]
Adding requests:  68%|██████▊   | 2788/4096 [00:07<00:03, 365.29it/s]
Adding requests:  69%|██████▉   | 2826/4096 [00:07<00:03, 367.84it/s]
Adding requests:  70%|██████▉   | 2863/4096 [00:08<00:03, 366.75it/s]
Adding requests:  71%|███████   | 2900/4096 [00:08<00:03, 365.06it/s]
Adding requests:  72%|███████▏  | 2939/4096 [00:08<00:03, 368.89it/s]
Adding requests:  73%|███████▎  | 2976/4096 [00:08<00:03, 365.41it/s]
Adding requests:  74%|███████▎  | 3015/4096 [00:08<00:02, 369.16it/s]
Adding requests:  75%|███████▍  | 3053/4096 [00:08<00:02, 371.72it/s]
Adding requests:  75%|███████▌  | 3091/4096 [00:08<00:02, 369.46it/s]
Adding requests:  76%|███████▋  | 3130/4096 [00:08<00:02, 373.20it/s]
Adding requests:  77%|███████▋  | 3168/4096 [00:08<00:02, 368.96it/s]
Adding requests:  78%|███████▊  | 3205/4096 [00:08<00:02, 363.97it/s]
Adding requests:  79%|███████▉  | 3243/4096 [00:09<00:02, 365.94it/s]
Adding requests:  80%|████████  | 3280/4096 [00:09<00:02, 360.03it/s]
Adding requests:  81%|████████  | 3317/4096 [00:09<00:02, 343.59it/s]
Adding requests:  82%|████████▏ | 3354/4096 [00:09<00:02, 348.52it/s]
Adding requests:  83%|████████▎ | 3391/4096 [00:09<00:01, 353.60it/s]
Adding requests:  84%|████████▎ | 3428/4096 [00:09<00:01, 356.64it/s]
Adding requests:  85%|████████▍ | 3466/4096 [00:09<00:01, 361.38it/s]
Adding requests:  86%|████████▌ | 3503/4096 [00:09<00:01, 357.05it/s]
Adding requests:  86%|████████▋ | 3543/4096 [00:09<00:01, 367.41it/s]
Adding requests:  87%|████████▋ | 3580/4096 [00:10<00:01, 366.37it/s]
Adding requests:  88%|████████▊ | 3617/4096 [00:10<00:01, 365.56it/s]
Adding requests:  89%|████████▉ | 3654/4096 [00:10<00:01, 364.48it/s]
Adding requests:  90%|█████████ | 3691/4096 [00:10<00:01, 357.49it/s]
Adding requests:  91%|█████████ | 3728/4096 [00:10<00:01, 358.10it/s]
Adding requests:  92%|█████████▏| 3764/4096 [00:10<00:00, 351.86it/s]
Adding requests:  93%|█████████▎| 3800/4096 [00:10<00:00, 342.32it/s]
Adding requests:  94%|█████████▎| 3835/4096 [00:10<00:00, 342.96it/s]
Adding requests:  95%|█████████▍| 3871/4096 [00:10<00:00, 347.49it/s]
Adding requests:  95%|█████████▌| 3906/4096 [00:10<00:00, 343.36it/s]
Adding requests:  96%|█████████▌| 3941/4096 [00:11<00:00, 344.42it/s]
Adding requests:  97%|█████████▋| 3976/4096 [00:11<00:00, 344.99it/s]
Adding requests:  98%|█████████▊| 4012/4096 [00:11<00:00, 348.10it/s]
Adding requests:  99%|█████████▉| 4047/4096 [00:11<00:00, 344.54it/s]
Adding requests: 100%|█████████▉| 4083/4096 [00:11<00:00, 346.22it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 355.58it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  39%|███▊      | 1578/4096 [00:00<00:00, 12760.20it/s, est. speed input: 13067272.32 toks/s, output: 12760.37 toks/s]
Processed prompts:  70%|██████▉   | 2855/4096 [00:08<00:04, 266.08it/s, est. speed input: 325271.82 toks/s, output: 317.65 toks/s]      
Processed prompts:  83%|████████▎ | 3389/4096 [00:12<00:03, 217.24it/s, est. speed input: 269971.62 toks/s, output: 263.64 toks/s]
Processed prompts:  90%|████████▉ | 3686/4096 [00:14<00:02, 200.81it/s, est. speed input: 253083.01 toks/s, output: 247.15 toks/s]
Processed prompts:  95%|█████████▍| 3872/4096 [00:16<00:01, 189.53it/s, est. speed input: 243596.58 toks/s, output: 237.89 toks/s]
Processed prompts:  98%|█████████▊| 3996/4096 [00:17<00:00, 182.13it/s, est. speed input: 238200.90 toks/s, output: 232.62 toks/s]
Processed prompts: 100%|█████████▉| 4083/4096 [00:17<00:00, 177.89it/s, est. speed input: 235241.11 toks/s, output: 229.73 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:17<00:00, 177.89it/s, est. speed input: 235987.39 toks/s, output: 230.46 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:17<00:00, 230.46it/s, est. speed input: 235987.39 toks/s, output: 230.46 toks/s]
[rank0]:[W126 16:02:30.070290381 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 16:02:32
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:03:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=726849) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=726849) WARNING 01-26 16:03:44 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     def forward(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     raise e
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/tmp/torchinductor_root/2w/c2wdcwqri7mchk5gopfhtl34ozq3lcknjbygpd6cgdwtsk2mgubx.py", line 1093, in call
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 6)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 327, in quant_slide_int8_triton
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) ERROR 01-26 16:03:51 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered

STDERR:
[2026-01-26 16:03:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:03:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:03:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:03:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:03:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:03:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:03:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:03:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:03:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:03:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:03:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:03:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:03:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:03:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:03:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:03:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:03:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:35] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:35] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=726849) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=726849) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=726849) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]
(EngineCore_DP0 pid=726849) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.07it/s]
(EngineCore_DP0 pid=726849) 
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13860864 bytes
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10780672 bytes
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113967104 bytes
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=726849) [2026-01-26 16:03:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56655872 bytes
(EngineCore_DP0 pid=726849) [rank0]:W0126 16:03:49.584000 726849 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=726849) [rank0]:W0126 16:03:49.665000 726849 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=726849) [rank0]:W0126 16:03:50.769000 726849 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=726849) [rank0]:W0126 16:03:50.892000 726849 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=726849) Process EngineCore_DP0:
(EngineCore_DP0 pid=726849) Traceback (most recent call last):
(EngineCore_DP0 pid=726849)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=726849)     self.run()
(EngineCore_DP0 pid=726849)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=726849)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=726849)     raise e
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=726849)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=726849)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=726849)     super().__init__(
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=726849)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=726849)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=726849)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=726849)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=726849)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=726849)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=726849)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=726849)     return func(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=726849)     return func(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=726849)     self.model_runner.profile_run()
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=726849)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=726849)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=726849)     return func(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=726849)     outputs = self.model(
(EngineCore_DP0 pid=726849)               ^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=726849)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=726849)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=726849)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=726849)     hidden_states = self.model(
(EngineCore_DP0 pid=726849)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=726849)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=726849)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=726849)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=726849)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=726849)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=726849)     def forward(
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=726849)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=726849)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=726849)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=726849)     raise e
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=726849)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=726849)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=726849)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=726849)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=726849)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=726849)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=726849)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=726849)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=726849)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=726849)     return compiled_fn(full_args)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=726849)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=726849)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=726849)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=726849)                             ^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=726849)     outs = compiled_fn(args)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=726849)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=726849)     return self.current_callable(inputs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=726849)     out = model(new_inputs)
(EngineCore_DP0 pid=726849)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/tmp/torchinductor_root/2w/c2wdcwqri7mchk5gopfhtl34ozq3lcknjbygpd6cgdwtsk2mgubx.py", line 1093, in call
(EngineCore_DP0 pid=726849)     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 6)
(EngineCore_DP0 pid=726849)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=726849)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=726849)     return fn(input, L)
(EngineCore_DP0 pid=726849)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 327, in quant_slide_int8_triton
(EngineCore_DP0 pid=726849)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=726849)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=726849)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=726849)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=726849)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=726849)     self._init_handles()
(EngineCore_DP0 pid=726849)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=726849)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=726849)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=726849) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 16:03:51.964286872 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-26 17:22:58
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:23:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=834283) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=834283) WARNING 01-26 17:23:27 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=834283) WARNING 01-26 17:23:42 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 22.72 requests/s, 11653.59 total tokens/s, 22.72 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 17:23:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:23:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:23:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:23:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:23:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:23:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:23:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:23:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:23:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:23:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:23:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:23:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:23:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:23:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:23:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:23:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=834283) [2026-01-26 17:23:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=834283) [2026-01-26 17:23:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=834283) [2026-01-26 17:23:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=834283) [2026-01-26 17:23:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=834283) [2026-01-26 17:23:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=834283) [2026-01-26 17:23:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=834283) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=834283) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.23s/it]
(EngineCore_DP0 pid=834283) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.38it/s]
(EngineCore_DP0 pid=834283) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.13it/s]
(EngineCore_DP0 pid=834283) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.04s/it]
(EngineCore_DP0 pid=834283) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.01it/s]
(EngineCore_DP0 pid=834283) 
(EngineCore_DP0 pid=834283) [2026-01-26 17:23:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=834283) [2026-01-26 17:23:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30736384 bytes
(EngineCore_DP0 pid=834283) [2026-01-26 17:23:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=834283) [2026-01-26 17:23:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21954560 bytes
(EngineCore_DP0 pid=834283) [2026-01-26 17:23:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=834283) [2026-01-26 17:23:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 118554624 bytes
(EngineCore_DP0 pid=834283) [2026-01-26 17:23:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=834283) [2026-01-26 17:23:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 58982400 bytes
(EngineCore_DP0 pid=834283) 2026-01-26 17:23:41,991 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=834283) 2026-01-26 17:23:42,030 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=834283) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.24it/s]
(EngineCore_DP0 pid=834283) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.65it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.64it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████▏     | 53/128 [00:00<00:00, 521.70it/s]
Adding requests:  89%|████████▉ | 114/128 [00:00<00:00, 570.27it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 568.59it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:18,  6.71it/s, est. speed input: 3436.61 toks/s, output: 6.71 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:07, 15.97it/s, est. speed input: 7408.41 toks/s, output: 14.47 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:06, 19.52it/s, est. speed input: 8948.08 toks/s, output: 17.48 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:05, 21.32it/s, est. speed input: 9765.12 toks/s, output: 19.07 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:05, 22.30it/s, est. speed input: 10257.38 toks/s, output: 20.03 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:04, 22.93it/s, est. speed input: 10597.03 toks/s, output: 20.70 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:04, 23.31it/s, est. speed input: 10838.75 toks/s, output: 21.17 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:04, 23.60it/s, est. speed input: 11028.28 toks/s, output: 21.54 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:04, 23.77it/s, est. speed input: 11173.69 toks/s, output: 21.82 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:04, 23.87it/s, est. speed input: 11287.24 toks/s, output: 22.05 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:04, 23.96it/s, est. speed input: 11383.45 toks/s, output: 22.23 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:03, 24.02it/s, est. speed input: 11464.57 toks/s, output: 22.39 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:03, 24.05it/s, est. speed input: 11531.49 toks/s, output: 22.52 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:03, 24.10it/s, est. speed input: 11591.97 toks/s, output: 22.64 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:03, 24.09it/s, est. speed input: 11639.85 toks/s, output: 22.73 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:03, 24.11it/s, est. speed input: 11685.18 toks/s, output: 22.82 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:03, 24.06it/s, est. speed input: 11718.96 toks/s, output: 22.89 toks/s]
Processed prompts:  41%|████      | 52/128 [00:02<00:03, 24.10it/s, est. speed input: 11755.58 toks/s, output: 22.96 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:02<00:03, 24.15it/s, est. speed input: 11789.72 toks/s, output: 23.03 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:02<00:02, 24.15it/s, est. speed input: 11818.12 toks/s, output: 23.08 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:02, 24.14it/s, est. speed input: 11843.00 toks/s, output: 23.13 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:02<00:02, 24.16it/s, est. speed input: 11867.77 toks/s, output: 23.18 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:02<00:02, 24.16it/s, est. speed input: 11889.72 toks/s, output: 23.22 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:03<00:02, 24.06it/s, est. speed input: 11902.61 toks/s, output: 23.25 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:03<00:02, 24.18it/s, est. speed input: 11926.73 toks/s, output: 23.29 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:03<00:02, 24.20it/s, est. speed input: 11945.69 toks/s, output: 23.33 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:03<00:02, 24.19it/s, est. speed input: 11961.48 toks/s, output: 23.36 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:03<00:01, 24.18it/s, est. speed input: 11975.85 toks/s, output: 23.39 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:03<00:01, 24.17it/s, est. speed input: 11989.06 toks/s, output: 23.42 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:03<00:01, 24.16it/s, est. speed input: 12001.24 toks/s, output: 23.44 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:03<00:01, 24.15it/s, est. speed input: 12012.68 toks/s, output: 23.46 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:04<00:01, 24.16it/s, est. speed input: 12023.99 toks/s, output: 23.48 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:04<00:01, 24.14it/s, est. speed input: 12033.29 toks/s, output: 23.50 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:04<00:01, 24.11it/s, est. speed input: 12041.42 toks/s, output: 23.52 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:04<00:01, 24.15it/s, est. speed input: 12052.04 toks/s, output: 23.54 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:04<00:00, 24.15it/s, est. speed input: 12060.47 toks/s, output: 23.56 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:04<00:00, 24.17it/s, est. speed input: 12069.61 toks/s, output: 23.57 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:04<00:00, 24.19it/s, est. speed input: 12078.51 toks/s, output: 23.59 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:04<00:00, 24.22it/s, est. speed input: 12087.45 toks/s, output: 23.61 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:04<00:00, 24.20it/s, est. speed input: 12094.67 toks/s, output: 23.62 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:05<00:00, 24.21it/s, est. speed input: 12102.04 toks/s, output: 23.64 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:05<00:00, 24.24it/s, est. speed input: 12110.10 toks/s, output: 23.65 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:05<00:00, 24.19it/s, est. speed input: 12115.02 toks/s, output: 23.66 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 24.19it/s, est. speed input: 12117.45 toks/s, output: 23.67 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.67it/s, est. speed input: 12117.45 toks/s, output: 23.67 toks/s]
[rank0]:[W126 17:23:50.669081563 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 17:23:52
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:24:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=835647) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=835647) WARNING 01-26 17:24:21 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=835647) WARNING 01-26 17:24:36 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 22.73 requests/s, 23299.75 total tokens/s, 22.73 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 17:23:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:23:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:23:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:23:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:23:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:23:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:23:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:23:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:23:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:24:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:24:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:24:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:24:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:24:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:24:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:24:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:24:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:24:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:24:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:24:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:24:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:24:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:24:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=835647) [2026-01-26 17:24:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=835647) [2026-01-26 17:24:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=835647) [2026-01-26 17:24:07] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=835647) [2026-01-26 17:24:07] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=835647) [2026-01-26 17:24:07] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=835647) [2026-01-26 17:24:07] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=835647) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=835647) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.15s/it]
(EngineCore_DP0 pid=835647) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.52it/s]
(EngineCore_DP0 pid=835647) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.25it/s]
(EngineCore_DP0 pid=835647) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.01it/s]
(EngineCore_DP0 pid=835647) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.08it/s]
(EngineCore_DP0 pid=835647) 
(EngineCore_DP0 pid=835647) [2026-01-26 17:24:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=835647) [2026-01-26 17:24:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30736384 bytes
(EngineCore_DP0 pid=835647) [2026-01-26 17:24:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=835647) [2026-01-26 17:24:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21954560 bytes
(EngineCore_DP0 pid=835647) [2026-01-26 17:24:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=835647) [2026-01-26 17:24:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 118554624 bytes
(EngineCore_DP0 pid=835647) [2026-01-26 17:24:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=835647) [2026-01-26 17:24:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 58982400 bytes
(EngineCore_DP0 pid=835647) 2026-01-26 17:24:36,283 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=835647) 2026-01-26 17:24:36,322 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=835647) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.63it/s]
(EngineCore_DP0 pid=835647) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 10.04it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  21%|██        | 27/128 [00:00<00:00, 268.94it/s]
Adding requests:  48%|████▊     | 62/128 [00:00<00:00, 312.78it/s]
Adding requests:  73%|███████▎  | 94/128 [00:00<00:00, 311.65it/s]
Adding requests:  98%|█████████▊| 126/128 [00:00<00:00, 176.75it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 208.59it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:00, 141.52it/s, est. speed input: 144928.55 toks/s, output: 141.52 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 35.29it/s, est. speed input: 40719.62 toks/s, output: 39.76 toks/s]   
Processed prompts:  30%|██▉       | 38/128 [00:01<00:02, 30.34it/s, est. speed input: 35357.58 toks/s, output: 34.53 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 28.40it/s, est. speed input: 33427.07 toks/s, output: 32.64 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 27.19it/s, est. speed input: 32308.22 toks/s, output: 31.55 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 26.19it/s, est. speed input: 31415.98 toks/s, output: 30.68 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:02, 25.54it/s, est. speed input: 30855.22 toks/s, output: 30.13 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 24.99it/s, est. speed input: 30374.17 toks/s, output: 29.66 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:02<00:02, 24.54it/s, est. speed input: 29953.94 toks/s, output: 29.25 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:02<00:02, 24.15it/s, est. speed input: 29575.83 toks/s, output: 28.88 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:02<00:02, 23.85it/s, est. speed input: 29239.67 toks/s, output: 28.55 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:02, 23.63it/s, est. speed input: 28939.66 toks/s, output: 28.26 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:02<00:02, 23.48it/s, est. speed input: 28674.53 toks/s, output: 28.00 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:02<00:02, 23.38it/s, est. speed input: 28434.22 toks/s, output: 27.77 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:02, 23.30it/s, est. speed input: 28215.95 toks/s, output: 27.55 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:02, 23.23it/s, est. speed input: 28014.46 toks/s, output: 27.36 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:03<00:01, 23.06it/s, est. speed input: 27808.96 toks/s, output: 27.16 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:03<00:01, 23.05it/s, est. speed input: 27637.30 toks/s, output: 26.99 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:03<00:01, 22.91it/s, est. speed input: 27458.30 toks/s, output: 26.81 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:03<00:01, 22.96it/s, est. speed input: 27315.94 toks/s, output: 26.68 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:03<00:01, 23.01it/s, est. speed input: 27186.31 toks/s, output: 26.55 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:03<00:01, 22.98it/s, est. speed input: 27055.87 toks/s, output: 26.42 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:03<00:01, 22.99it/s, est. speed input: 26939.03 toks/s, output: 26.31 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:04<00:01, 22.99it/s, est. speed input: 26828.19 toks/s, output: 26.20 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:04<00:00, 23.02it/s, est. speed input: 26727.86 toks/s, output: 26.10 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:04<00:00, 22.64it/s, est. speed input: 26586.78 toks/s, output: 25.96 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:04<00:00, 22.78it/s, est. speed input: 26500.56 toks/s, output: 25.88 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:04<00:00, 22.84it/s, est. speed input: 26415.09 toks/s, output: 25.80 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:04<00:00, 22.75it/s, est. speed input: 26320.30 toks/s, output: 25.70 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:04<00:00, 22.82it/s, est. speed input: 26244.64 toks/s, output: 25.63 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:04<00:00, 22.92it/s, est. speed input: 26178.16 toks/s, output: 25.56 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 22.92it/s, est. speed input: 26129.56 toks/s, output: 25.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 25.52it/s, est. speed input: 26129.56 toks/s, output: 25.52 toks/s]
[rank0]:[W126 17:24:43.146236693 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 17:24:45
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:24:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=836938) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=836938) WARNING 01-26 17:25:15 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=836938) WARNING 01-26 17:25:30 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 47.43 requests/s, 48617.58 total tokens/s, 47.43 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 17:24:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:24:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:24:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:24:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:24:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:24:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:24:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:24:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:24:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:24:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:24:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:24:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:24:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:24:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:25:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:25:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:25:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:25:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:25:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:25:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:25:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:25:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=836938) [2026-01-26 17:25:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=836938) [2026-01-26 17:25:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=836938) [2026-01-26 17:25:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=836938) [2026-01-26 17:25:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=836938) [2026-01-26 17:25:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=836938) [2026-01-26 17:25:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=836938) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=836938) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.13s/it]
(EngineCore_DP0 pid=836938) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.54it/s]
(EngineCore_DP0 pid=836938) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.25it/s]
(EngineCore_DP0 pid=836938) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.03it/s]
(EngineCore_DP0 pid=836938) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.09it/s]
(EngineCore_DP0 pid=836938) 
(EngineCore_DP0 pid=836938) [2026-01-26 17:25:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=836938) [2026-01-26 17:25:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30736384 bytes
(EngineCore_DP0 pid=836938) [2026-01-26 17:25:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=836938) [2026-01-26 17:25:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21954560 bytes
(EngineCore_DP0 pid=836938) [2026-01-26 17:25:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=836938) [2026-01-26 17:25:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 118554624 bytes
(EngineCore_DP0 pid=836938) [2026-01-26 17:25:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=836938) [2026-01-26 17:25:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 58982400 bytes
(EngineCore_DP0 pid=836938) 2026-01-26 17:25:30,352 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=836938) 2026-01-26 17:25:30,390 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=836938) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  9.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.55it/s]
(EngineCore_DP0 pid=836938) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 11.02it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█         | 28/256 [00:00<00:00, 274.88it/s]
Adding requests:  25%|██▌       | 64/256 [00:00<00:00, 319.48it/s]
Adding requests:  38%|███▊      | 96/256 [00:00<00:01, 152.92it/s]
Adding requests:  50%|████▉     | 127/256 [00:00<00:00, 189.02it/s]
Adding requests:  63%|██████▎   | 161/256 [00:00<00:00, 226.85it/s]
Adding requests:  77%|███████▋  | 197/256 [00:00<00:00, 260.90it/s]
Adding requests:  91%|█████████ | 233/256 [00:00<00:00, 286.31it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 250.68it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:00<00:00, 442.30it/s, est. speed input: 452950.11 toks/s, output: 442.31 toks/s]
Processed prompts:  36%|███▋      | 93/256 [00:01<00:02, 80.42it/s, est. speed input: 94295.10 toks/s, output: 92.08 toks/s]   
Processed prompts:  45%|████▍     | 115/256 [00:01<00:02, 67.87it/s, est. speed input: 80620.96 toks/s, output: 78.73 toks/s]
Processed prompts:  50%|█████     | 129/256 [00:01<00:02, 62.78it/s, est. speed input: 75565.74 toks/s, output: 73.79 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:01<00:01, 58.34it/s, est. speed input: 71886.12 toks/s, output: 70.20 toks/s]
Processed prompts:  58%|█████▊    | 149/256 [00:02<00:01, 57.71it/s, est. speed input: 70705.64 toks/s, output: 69.05 toks/s]
Processed prompts:  61%|██████▏   | 157/256 [00:02<00:01, 55.85it/s, est. speed input: 69256.59 toks/s, output: 67.63 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:02<00:01, 52.81it/s, est. speed input: 67585.23 toks/s, output: 66.00 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:02<00:01, 52.06it/s, est. speed input: 66769.76 toks/s, output: 65.20 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:02<00:01, 51.40it/s, est. speed input: 66028.98 toks/s, output: 64.48 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:02<00:01, 50.81it/s, est. speed input: 65346.99 toks/s, output: 63.82 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:02<00:01, 50.38it/s, est. speed input: 64728.49 toks/s, output: 63.21 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:03<00:01, 49.96it/s, est. speed input: 64142.93 toks/s, output: 62.64 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:03<00:01, 49.70it/s, est. speed input: 63613.04 toks/s, output: 62.12 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:03<00:01, 48.25it/s, est. speed input: 62910.17 toks/s, output: 61.44 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:03<00:00, 48.46it/s, est. speed input: 62460.74 toks/s, output: 61.00 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:03<00:00, 48.61it/s, est. speed input: 62041.66 toks/s, output: 60.59 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:03<00:00, 48.71it/s, est. speed input: 61649.13 toks/s, output: 60.20 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:03<00:00, 48.74it/s, est. speed input: 61276.54 toks/s, output: 59.84 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:03<00:00, 48.80it/s, est. speed input: 60931.54 toks/s, output: 59.50 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:04<00:00, 48.86it/s, est. speed input: 60609.10 toks/s, output: 59.19 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:04<00:00, 48.89it/s, est. speed input: 60304.55 toks/s, output: 58.89 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:04<00:00, 48.99it/s, est. speed input: 60026.28 toks/s, output: 58.62 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 48.99it/s, est. speed input: 59923.75 toks/s, output: 58.52 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 58.52it/s, est. speed input: 59923.75 toks/s, output: 58.52 toks/s]
[rank0]:[W126 17:25:37.224942917 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 17:25:39
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:25:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=838257) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=838257) WARNING 01-26 17:26:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=838257) WARNING 01-26 17:26:26 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 66.58 requests/s, 68240.58 total tokens/s, 66.58 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 17:25:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:25:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:25:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:25:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:25:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:25:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:25:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:25:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:25:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:25:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:25:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:25:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:25:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:25:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:25:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:25:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:25:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=838257) [2026-01-26 17:25:57] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=838257) [2026-01-26 17:25:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=838257) [2026-01-26 17:25:57] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=838257) [2026-01-26 17:25:57] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=838257) [2026-01-26 17:25:57] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=838257) [2026-01-26 17:25:57] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=838257) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=838257) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.03s/it]
(EngineCore_DP0 pid=838257) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.65it/s]
(EngineCore_DP0 pid=838257) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.30it/s]
(EngineCore_DP0 pid=838257) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.05it/s]
(EngineCore_DP0 pid=838257) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.13it/s]
(EngineCore_DP0 pid=838257) 
(EngineCore_DP0 pid=838257) [2026-01-26 17:26:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=838257) [2026-01-26 17:26:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30736384 bytes
(EngineCore_DP0 pid=838257) [2026-01-26 17:26:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=838257) [2026-01-26 17:26:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21954560 bytes
(EngineCore_DP0 pid=838257) [2026-01-26 17:26:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=838257) [2026-01-26 17:26:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 118554624 bytes
(EngineCore_DP0 pid=838257) [2026-01-26 17:26:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=838257) [2026-01-26 17:26:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 58982400 bytes
(EngineCore_DP0 pid=838257) 2026-01-26 17:26:25,955 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=838257) 2026-01-26 17:26:25,992 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=838257) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  8.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.75it/s]
(EngineCore_DP0 pid=838257) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 10.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 11.04it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 28/512 [00:00<00:01, 277.00it/s]
Adding requests:  12%|█▏        | 63/512 [00:00<00:01, 316.55it/s]
Adding requests:  19%|█▊        | 95/512 [00:00<00:01, 315.50it/s]
Adding requests:  25%|██▌       | 129/512 [00:00<00:01, 322.71it/s]
Adding requests:  32%|███▏      | 165/512 [00:00<00:01, 332.32it/s]
Adding requests:  39%|███▉      | 200/512 [00:00<00:00, 338.20it/s]
Adding requests:  46%|████▌     | 236/512 [00:00<00:00, 344.19it/s]
Adding requests:  53%|█████▎    | 271/512 [00:00<00:00, 342.35it/s]
Adding requests:  60%|██████    | 308/512 [00:00<00:00, 349.54it/s]
Adding requests:  68%|██████▊   | 346/512 [00:01<00:00, 356.35it/s]
Adding requests:  75%|███████▍  | 383/512 [00:01<00:00, 359.23it/s]
Adding requests:  82%|████████▏ | 421/512 [00:01<00:00, 365.34it/s]
Adding requests:  89%|████████▉ | 458/512 [00:01<00:00, 362.27it/s]
Adding requests:  97%|█████████▋| 498/512 [00:01<00:00, 370.52it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 349.77it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:00<00:00, 835.47it/s, est. speed input: 855578.13 toks/s, output: 835.48 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:01<00:02, 113.92it/s, est. speed input: 135572.85 toks/s, output: 132.39 toks/s]
Processed prompts:  43%|████▎     | 221/512 [00:01<00:02, 98.90it/s, est. speed input: 118391.28 toks/s, output: 115.62 toks/s] 
Processed prompts:  48%|████▊     | 246/512 [00:02<00:03, 87.79it/s, est. speed input: 108257.54 toks/s, output: 105.72 toks/s]
Processed prompts:  51%|█████▏    | 263/512 [00:02<00:02, 84.71it/s, est. speed input: 105021.42 toks/s, output: 102.56 toks/s]
Processed prompts:  54%|█████▍    | 277/512 [00:02<00:02, 83.82it/s, est. speed input: 103495.94 toks/s, output: 101.07 toks/s]
Processed prompts:  56%|█████▋    | 289/512 [00:02<00:02, 80.69it/s, est. speed input: 101417.20 toks/s, output: 99.04 toks/s] 
Processed prompts:  59%|█████▊    | 300/512 [00:03<00:02, 76.49it/s, est. speed input: 99196.12 toks/s, output: 96.87 toks/s] 
Processed prompts:  60%|██████    | 309/512 [00:03<00:02, 77.41it/s, est. speed input: 98684.88 toks/s, output: 96.37 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:03<00:02, 70.26it/s, est. speed input: 96254.48 toks/s, output: 94.00 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:03<00:02, 69.61it/s, est. speed input: 95317.76 toks/s, output: 93.08 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:03<00:02, 68.95it/s, est. speed input: 94422.93 toks/s, output: 92.21 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:03<00:02, 68.61it/s, est. speed input: 93623.93 toks/s, output: 91.43 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:03<00:02, 68.38it/s, est. speed input: 92880.14 toks/s, output: 90.70 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:03<00:02, 68.18it/s, est. speed input: 92177.71 toks/s, output: 90.02 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:04<00:02, 68.13it/s, est. speed input: 91530.01 toks/s, output: 89.38 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:04<00:02, 67.75it/s, est. speed input: 90872.98 toks/s, output: 88.74 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:04<00:01, 67.73it/s, est. speed input: 90284.23 toks/s, output: 88.17 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:04<00:01, 67.42it/s, est. speed input: 89691.80 toks/s, output: 87.59 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:04<00:01, 67.59it/s, est. speed input: 89175.71 toks/s, output: 87.09 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:04<00:01, 67.34it/s, est. speed input: 88643.04 toks/s, output: 86.57 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:04<00:01, 67.50it/s, est. speed input: 88174.18 toks/s, output: 86.11 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:04<00:01, 67.41it/s, est. speed input: 87706.18 toks/s, output: 85.65 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:05<00:01, 67.56it/s, est. speed input: 87281.81 toks/s, output: 85.24 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:05<00:01, 69.27it/s, est. speed input: 87031.93 toks/s, output: 84.99 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:05<00:00, 68.72it/s, est. speed input: 86627.75 toks/s, output: 84.60 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:05<00:00, 68.41it/s, est. speed input: 86248.91 toks/s, output: 84.23 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:05<00:00, 67.98it/s, est. speed input: 85865.98 toks/s, output: 83.85 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:05<00:00, 67.83it/s, est. speed input: 85513.11 toks/s, output: 83.51 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:05<00:00, 67.28it/s, est. speed input: 85136.34 toks/s, output: 83.14 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:05<00:00, 66.86it/s, est. speed input: 84770.95 toks/s, output: 82.78 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:05<00:00, 66.88it/s, est. speed input: 84446.79 toks/s, output: 82.47 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:06<00:00, 67.00it/s, est. speed input: 84144.00 toks/s, output: 82.17 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:06<00:00, 67.54it/s, est. speed input: 83889.46 toks/s, output: 81.92 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 67.54it/s, est. speed input: 84217.46 toks/s, output: 82.24 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 82.24it/s, est. speed input: 84217.46 toks/s, output: 82.24 toks/s]
[rank0]:[W126 17:26:36.755735155 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 17:26:38
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:26:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=839652) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=839652) WARNING 01-26 17:27:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=839652) WARNING 01-26 17:27:28 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 69.04 requests/s, 70766.07 total tokens/s, 69.04 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 17:26:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:26:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:26:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:26:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:26:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:26:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:26:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:26:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:26:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:26:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:26:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:26:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:26:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:26:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:26:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:26:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:26:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:26:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:26:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:26:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:26:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:26:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:26:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:26:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:26:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:26:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:26:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:26:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=839652) [2026-01-26 17:26:58] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=839652) [2026-01-26 17:26:58] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=839652) [2026-01-26 17:26:58] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=839652) [2026-01-26 17:26:58] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=839652) [2026-01-26 17:26:58] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=839652) [2026-01-26 17:26:58] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=839652) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=839652) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.01s/it]
(EngineCore_DP0 pid=839652) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.67it/s]
(EngineCore_DP0 pid=839652) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.31it/s]
(EngineCore_DP0 pid=839652) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.05it/s]
(EngineCore_DP0 pid=839652) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.14it/s]
(EngineCore_DP0 pid=839652) 
(EngineCore_DP0 pid=839652) [2026-01-26 17:27:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=839652) [2026-01-26 17:27:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30736384 bytes
(EngineCore_DP0 pid=839652) [2026-01-26 17:27:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=839652) [2026-01-26 17:27:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21954560 bytes
(EngineCore_DP0 pid=839652) [2026-01-26 17:27:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=839652) [2026-01-26 17:27:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 118554624 bytes
(EngineCore_DP0 pid=839652) [2026-01-26 17:27:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=839652) [2026-01-26 17:27:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 58982400 bytes
(EngineCore_DP0 pid=839652) 2026-01-26 17:27:28,611 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=839652) 2026-01-26 17:27:28,650 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=839652) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 10.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 10.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 10.08it/s]
(EngineCore_DP0 pid=839652) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 10.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 10.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 10.93it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 26/1024 [00:00<00:03, 252.28it/s]
Adding requests:   6%|▌         | 60/1024 [00:00<00:03, 300.16it/s]
Adding requests:   9%|▉         | 92/1024 [00:00<00:03, 306.37it/s]
Adding requests:  12%|█▏        | 126/1024 [00:00<00:02, 316.79it/s]
Adding requests:  16%|█▌        | 161/1024 [00:00<00:02, 325.89it/s]
Adding requests:  19%|█▉        | 198/1024 [00:00<00:02, 337.71it/s]
Adding requests:  23%|██▎       | 234/1024 [00:00<00:02, 341.55it/s]
Adding requests:  26%|██▋       | 269/1024 [00:00<00:02, 342.67it/s]
Adding requests:  30%|██▉       | 305/1024 [00:00<00:02, 347.03it/s]
Adding requests:  33%|███▎      | 342/1024 [00:01<00:01, 352.64it/s]
Adding requests:  37%|███▋      | 378/1024 [00:01<00:01, 349.94it/s]
Adding requests:  41%|████      | 417/1024 [00:01<00:01, 358.94it/s]
Adding requests:  44%|████▍     | 453/1024 [00:01<00:01, 357.37it/s]
Adding requests:  48%|████▊     | 493/1024 [00:01<00:01, 368.31it/s]
Adding requests:  52%|█████▏    | 532/1024 [00:01<00:01, 374.51it/s]
Adding requests:  56%|█████▌    | 570/1024 [00:01<00:01, 371.23it/s]
Adding requests:  59%|█████▉    | 608/1024 [00:01<00:01, 360.93it/s]
Adding requests:  63%|██████▎   | 645/1024 [00:01<00:01, 355.81it/s]
Adding requests:  67%|██████▋   | 681/1024 [00:01<00:00, 355.42it/s]
Adding requests:  70%|███████   | 718/1024 [00:02<00:00, 357.29it/s]
Adding requests:  74%|███████▎  | 754/1024 [00:02<00:00, 354.13it/s]
Adding requests:  77%|███████▋  | 790/1024 [00:02<00:00, 355.05it/s]
Adding requests:  81%|████████  | 827/1024 [00:02<00:00, 357.05it/s]
Adding requests:  84%|████████▍ | 864/1024 [00:02<00:00, 358.46it/s]
Adding requests:  88%|████████▊ | 901/1024 [00:02<00:00, 361.14it/s]
Adding requests:  92%|█████████▏| 938/1024 [00:02<00:00, 356.84it/s]
Adding requests:  95%|█████████▌| 975/1024 [00:02<00:00, 357.75it/s]
Adding requests:  99%|█████████▊| 1011/1024 [00:02<00:00, 352.75it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 350.58it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:00<00:00, 1257.14it/s, est. speed input: 1287366.61 toks/s, output: 1257.15 toks/s]
Processed prompts:  32%|███▏      | 328/1024 [00:01<00:04, 145.62it/s, est. speed input: 178234.03 toks/s, output: 174.06 toks/s]   
Processed prompts:  38%|███▊      | 385/1024 [00:02<00:05, 116.33it/s, est. speed input: 146117.27 toks/s, output: 142.69 toks/s]
Processed prompts:  41%|████      | 419/1024 [00:03<00:06, 100.14it/s, est. speed input: 130998.92 toks/s, output: 127.93 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:03<00:06, 94.26it/s, est. speed input: 125423.53 toks/s, output: 122.48 toks/s] 
Processed prompts:  45%|████▍     | 460/1024 [00:03<00:06, 91.58it/s, est. speed input: 122626.42 toks/s, output: 119.75 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:04<00:06, 86.01it/s, est. speed input: 119237.63 toks/s, output: 116.44 toks/s]
Processed prompts:  47%|████▋     | 486/1024 [00:04<00:06, 88.26it/s, est. speed input: 118940.54 toks/s, output: 116.15 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:04<00:06, 79.23it/s, est. speed input: 115507.99 toks/s, output: 112.80 toks/s]
Processed prompts:  50%|████▉     | 508/1024 [00:04<00:06, 80.40it/s, est. speed input: 114837.87 toks/s, output: 112.15 toks/s]
Processed prompts:  51%|█████     | 518/1024 [00:04<00:06, 81.30it/s, est. speed input: 114146.50 toks/s, output: 111.47 toks/s]
Processed prompts:  51%|█████▏    | 527/1024 [00:04<00:06, 80.34it/s, est. speed input: 113266.37 toks/s, output: 110.61 toks/s]
Processed prompts:  52%|█████▏    | 536/1024 [00:04<00:06, 79.48it/s, est. speed input: 112426.80 toks/s, output: 109.79 toks/s]
Processed prompts:  53%|█████▎    | 545/1024 [00:05<00:06, 78.41it/s, est. speed input: 111578.33 toks/s, output: 108.96 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:05<00:07, 62.49it/s, est. speed input: 108430.84 toks/s, output: 105.89 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:05<00:07, 64.03it/s, est. speed input: 107622.67 toks/s, output: 105.10 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:05<00:06, 65.09it/s, est. speed input: 106817.23 toks/s, output: 104.31 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:05<00:06, 65.99it/s, est. speed input: 106052.24 toks/s, output: 103.57 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:05<00:06, 67.35it/s, est. speed input: 105396.80 toks/s, output: 102.93 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:05<00:06, 67.78it/s, est. speed input: 104700.23 toks/s, output: 102.25 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:05<00:06, 67.66it/s, est. speed input: 103984.39 toks/s, output: 101.55 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:06<00:06, 67.65it/s, est. speed input: 103304.46 toks/s, output: 100.88 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:06<00:05, 68.41it/s, est. speed input: 102725.84 toks/s, output: 100.32 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:06<00:05, 68.94it/s, est. speed input: 102166.56 toks/s, output: 99.77 toks/s] 
Processed prompts:  62%|██████▏   | 634/1024 [00:06<00:05, 68.89it/s, est. speed input: 101588.64 toks/s, output: 99.21 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:06<00:05, 68.27it/s, est. speed input: 100979.89 toks/s, output: 98.61 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:06<00:05, 68.62it/s, est. speed input: 100461.25 toks/s, output: 98.11 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:06<00:05, 69.11it/s, est. speed input: 99979.91 toks/s, output: 97.64 toks/s] 
Processed prompts:  65%|██████▌   | 666/1024 [00:06<00:05, 69.15it/s, est. speed input: 99490.20 toks/s, output: 97.16 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:06<00:05, 69.05it/s, est. speed input: 99006.22 toks/s, output: 96.69 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:07<00:04, 68.99it/s, est. speed input: 98538.49 toks/s, output: 96.23 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:07<00:04, 68.86it/s, est. speed input: 98079.82 toks/s, output: 95.78 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:07<00:04, 69.19it/s, est. speed input: 97666.91 toks/s, output: 95.38 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:07<00:04, 69.30it/s, est. speed input: 97257.50 toks/s, output: 94.98 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:07<00:04, 69.53it/s, est. speed input: 96872.24 toks/s, output: 94.60 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:07<00:04, 69.29it/s, est. speed input: 96470.08 toks/s, output: 94.21 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:07<00:04, 69.49it/s, est. speed input: 96104.54 toks/s, output: 93.85 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:07<00:04, 69.28it/s, est. speed input: 95726.55 toks/s, output: 93.48 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:08<00:04, 69.36it/s, est. speed input: 95374.78 toks/s, output: 93.14 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:08<00:03, 69.10it/s, est. speed input: 95012.14 toks/s, output: 92.79 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:08<00:03, 69.02it/s, est. speed input: 94666.45 toks/s, output: 92.45 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:08<00:03, 68.99it/s, est. speed input: 94331.98 toks/s, output: 92.12 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:08<00:03, 69.20it/s, est. speed input: 94020.67 toks/s, output: 91.82 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:08<00:03, 71.25it/s, est. speed input: 93830.34 toks/s, output: 91.63 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:08<00:03, 70.71it/s, est. speed input: 93529.68 toks/s, output: 91.34 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:08<00:03, 70.61it/s, est. speed input: 93252.81 toks/s, output: 91.07 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:08<00:03, 70.08it/s, est. speed input: 92956.65 toks/s, output: 90.78 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:09<00:02, 69.77it/s, est. speed input: 92671.72 toks/s, output: 90.50 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:09<00:02, 69.53it/s, est. speed input: 92392.74 toks/s, output: 90.23 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:09<00:02, 69.85it/s, est. speed input: 92146.81 toks/s, output: 89.99 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:09<00:02, 70.09it/s, est. speed input: 91908.24 toks/s, output: 89.75 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:09<00:02, 69.85it/s, est. speed input: 91653.40 toks/s, output: 89.51 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:09<00:02, 69.95it/s, est. speed input: 91418.68 toks/s, output: 89.28 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:09<00:02, 69.40it/s, est. speed input: 91157.89 toks/s, output: 89.02 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:09<00:02, 69.04it/s, est. speed input: 90903.97 toks/s, output: 88.77 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:09<00:02, 69.32it/s, est. speed input: 90682.96 toks/s, output: 88.56 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:10<00:01, 69.41it/s, est. speed input: 90461.75 toks/s, output: 88.34 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:10<00:01, 69.23it/s, est. speed input: 90233.84 toks/s, output: 88.12 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:10<00:01, 69.39it/s, est. speed input: 90024.55 toks/s, output: 87.91 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:10<00:01, 69.22it/s, est. speed input: 89806.60 toks/s, output: 87.70 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:10<00:01, 69.65it/s, est. speed input: 89619.46 toks/s, output: 87.52 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:10<00:01, 69.19it/s, est. speed input: 89400.51 toks/s, output: 87.31 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:10<00:01, 69.28it/s, est. speed input: 89205.58 toks/s, output: 87.11 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:10<00:01, 69.37it/s, est. speed input: 89016.01 toks/s, output: 86.93 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:10<00:01, 69.64it/s, est. speed input: 88839.59 toks/s, output: 86.76 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:11<00:00, 69.65it/s, est. speed input: 88658.66 toks/s, output: 86.58 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:11<00:00, 69.58it/s, est. speed input: 88478.13 toks/s, output: 86.40 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:11<00:00, 69.41it/s, est. speed input: 88296.33 toks/s, output: 86.23 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:11<00:00, 69.20it/s, est. speed input: 88113.88 toks/s, output: 86.05 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:11<00:00, 69.43it/s, est. speed input: 87951.48 toks/s, output: 85.89 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:11<00:00, 69.64it/s, est. speed input: 87793.84 toks/s, output: 85.74 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:11<00:00, 69.73it/s, est. speed input: 87637.28 toks/s, output: 85.58 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:11<00:00, 70.93it/s, est. speed input: 87528.28 toks/s, output: 85.48 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:11<00:00, 70.93it/s, est. speed input: 88043.35 toks/s, output: 85.98 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:11<00:00, 85.98it/s, est. speed input: 88043.35 toks/s, output: 85.98 toks/s]
[rank0]:[W126 17:27:46.655326141 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 17:27:48
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:28:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=841243) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=841243) WARNING 01-26 17:28:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=841243) WARNING 01-26 17:28:46 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 73.05 requests/s, 74876.55 total tokens/s, 73.05 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 17:28:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:28:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:28:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:28:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:28:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:28:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:28:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:28:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:28:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:28:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:28:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:28:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:28:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:28:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:28:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:28:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:28:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:28:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:28:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:28:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:28:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:28:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:28:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:28:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:28:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:28:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:28:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:28:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=841243) [2026-01-26 17:28:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=841243) [2026-01-26 17:28:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=841243) [2026-01-26 17:28:15] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=841243) [2026-01-26 17:28:15] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=841243) [2026-01-26 17:28:15] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=841243) [2026-01-26 17:28:15] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=841243) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=841243) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.02s/it]
(EngineCore_DP0 pid=841243) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.65it/s]
(EngineCore_DP0 pid=841243) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.30it/s]
(EngineCore_DP0 pid=841243) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.05it/s]
(EngineCore_DP0 pid=841243) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.13it/s]
(EngineCore_DP0 pid=841243) 
(EngineCore_DP0 pid=841243) [2026-01-26 17:28:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=841243) [2026-01-26 17:28:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30736384 bytes
(EngineCore_DP0 pid=841243) [2026-01-26 17:28:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=841243) [2026-01-26 17:28:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21954560 bytes
(EngineCore_DP0 pid=841243) [2026-01-26 17:28:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=841243) [2026-01-26 17:28:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 118554624 bytes
(EngineCore_DP0 pid=841243) [2026-01-26 17:28:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=841243) [2026-01-26 17:28:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 58982400 bytes
(EngineCore_DP0 pid=841243) [rank0]:W0126 17:28:36.565000 841243 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=841243) [rank0]:W0126 17:28:36.635000 841243 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=841243) [rank0]:W0126 17:28:37.447000 841243 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=841243) [rank0]:W0126 17:28:37.555000 841243 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=841243) 2026-01-26 17:28:46,234 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=841243) 2026-01-26 17:28:46,285 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=841243) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  9.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 10.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  9.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  9.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.44it/s]
(EngineCore_DP0 pid=841243) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  9.77it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  9.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 10.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.87it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 274.88it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 314.95it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 312.54it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:06, 319.75it/s]
Adding requests:   8%|▊         | 164/2048 [00:00<00:05, 329.48it/s]
Adding requests:  10%|▉         | 201/2048 [00:00<00:05, 340.04it/s]
Adding requests:  12%|█▏        | 237/2048 [00:00<00:05, 345.22it/s]
Adding requests:  13%|█▎        | 272/2048 [00:00<00:05, 342.68it/s]
Adding requests:  15%|█▌        | 309/2048 [00:00<00:04, 349.06it/s]
Adding requests:  17%|█▋        | 347/2048 [00:01<00:04, 355.86it/s]
Adding requests:  19%|█▉        | 384/2048 [00:01<00:04, 357.93it/s]
Adding requests:  21%|██        | 423/2048 [00:01<00:04, 365.61it/s]
Adding requests:  22%|██▏       | 460/2048 [00:01<00:04, 360.31it/s]
Adding requests:  24%|██▍       | 500/2048 [00:01<00:04, 369.19it/s]
Adding requests:  26%|██▋       | 539/2048 [00:01<00:04, 374.25it/s]
Adding requests:  28%|██▊       | 577/2048 [00:01<00:03, 369.08it/s]
Adding requests:  30%|██▉       | 614/2048 [00:01<00:04, 358.39it/s]
Adding requests:  32%|███▏      | 650/2048 [00:01<00:03, 352.62it/s]
Adding requests:  34%|███▎      | 687/2048 [00:01<00:03, 356.46it/s]
Adding requests:  35%|███▌      | 723/2048 [00:02<00:03, 353.54it/s]
Adding requests:  37%|███▋      | 759/2048 [00:02<00:03, 351.35it/s]
Adding requests:  39%|███▉      | 795/2048 [00:02<00:03, 352.16it/s]
Adding requests:  41%|████      | 832/2048 [00:02<00:03, 357.29it/s]
Adding requests:  42%|████▏     | 868/2048 [00:02<00:03, 356.99it/s]
Adding requests:  44%|████▍     | 906/2048 [00:02<00:03, 363.59it/s]
Adding requests:  46%|████▌     | 943/2048 [00:02<00:03, 353.28it/s]
Adding requests:  48%|████▊     | 979/2048 [00:02<00:03, 354.16it/s]
Adding requests:  50%|████▉     | 1015/2048 [00:02<00:02, 349.15it/s]
Adding requests:  51%|█████▏    | 1051/2048 [00:02<00:02, 350.12it/s]
Adding requests:  53%|█████▎    | 1087/2048 [00:03<00:02, 350.11it/s]
Adding requests:  55%|█████▍    | 1123/2048 [00:03<00:02, 351.32it/s]
Adding requests:  57%|█████▋    | 1159/2048 [00:03<00:02, 345.83it/s]
Adding requests:  58%|█████▊    | 1195/2048 [00:03<00:02, 349.22it/s]
Adding requests:  60%|██████    | 1232/2048 [00:03<00:02, 355.08it/s]
Adding requests:  62%|██████▏   | 1268/2048 [00:03<00:02, 353.45it/s]
Adding requests:  64%|██████▎   | 1304/2048 [00:03<00:02, 352.76it/s]
Adding requests:  65%|██████▌   | 1340/2048 [00:03<00:02, 351.92it/s]
Adding requests:  67%|██████▋   | 1378/2048 [00:03<00:01, 357.13it/s]
Adding requests:  69%|██████▉   | 1414/2048 [00:04<00:01, 353.12it/s]
Adding requests:  71%|███████   | 1450/2048 [00:04<00:01, 351.93it/s]
Adding requests:  73%|███████▎  | 1488/2048 [00:04<00:01, 358.37it/s]
Adding requests:  74%|███████▍  | 1525/2048 [00:04<00:01, 359.50it/s]
Adding requests:  76%|███████▌  | 1561/2048 [00:04<00:01, 354.07it/s]
Adding requests:  78%|███████▊  | 1597/2048 [00:04<00:01, 344.62it/s]
Adding requests:  80%|███████▉  | 1632/2048 [00:04<00:01, 339.85it/s]
Adding requests:  81%|████████▏ | 1667/2048 [00:04<00:01, 334.76it/s]
Adding requests:  83%|████████▎ | 1701/2048 [00:04<00:01, 332.51it/s]
Adding requests:  85%|████████▍ | 1738/2048 [00:04<00:00, 340.24it/s]
Adding requests:  87%|████████▋ | 1776/2048 [00:05<00:00, 349.66it/s]
Adding requests:  88%|████████▊ | 1812/2048 [00:05<00:00, 347.50it/s]
Adding requests:  90%|█████████ | 1848/2048 [00:05<00:00, 350.03it/s]
Adding requests:  92%|█████████▏| 1884/2048 [00:05<00:00, 352.31it/s]
Adding requests:  94%|█████████▍| 1921/2048 [00:05<00:00, 356.00it/s]
Adding requests:  96%|█████████▌| 1959/2048 [00:05<00:00, 361.85it/s]
Adding requests:  97%|█████████▋| 1996/2048 [00:05<00:00, 352.48it/s]
Adding requests:  99%|█████████▉| 2032/2048 [00:05<00:00, 345.64it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 350.57it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:00<00:00, 3221.19it/s, est. speed input: 3298685.39 toks/s, output: 3221.24 toks/s]
Processed prompts:  36%|███▌      | 741/2048 [00:04<00:09, 137.40it/s, est. speed input: 167899.86 toks/s, output: 163.96 toks/s]   
Processed prompts:  43%|████▎     | 878/2048 [00:06<00:10, 116.42it/s, est. speed input: 143592.95 toks/s, output: 140.23 toks/s]
Processed prompts:  47%|████▋     | 957/2048 [00:07<00:10, 105.55it/s, est. speed input: 133121.76 toks/s, output: 130.00 toks/s]
Processed prompts:  49%|████▉     | 1008/2048 [00:08<00:10, 100.71it/s, est. speed input: 128707.05 toks/s, output: 125.69 toks/s]
Processed prompts:  51%|█████     | 1043/2048 [00:08<00:11, 91.23it/s, est. speed input: 123029.01 toks/s, output: 120.15 toks/s] 
Processed prompts:  52%|█████▏    | 1068/2048 [00:08<00:10, 93.21it/s, est. speed input: 122883.03 toks/s, output: 120.00 toks/s]
Processed prompts:  53%|█████▎    | 1089/2048 [00:09<00:10, 93.43it/s, est. speed input: 122270.85 toks/s, output: 119.41 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:09<00:11, 80.25it/s, est. speed input: 118430.07 toks/s, output: 115.65 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:09<00:11, 79.15it/s, est. speed input: 117441.36 toks/s, output: 114.69 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:10<00:11, 78.14it/s, est. speed input: 116514.67 toks/s, output: 113.78 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:10<00:11, 77.05it/s, est. speed input: 115605.72 toks/s, output: 112.90 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:10<00:11, 76.07it/s, est. speed input: 114735.25 toks/s, output: 112.05 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:10<00:11, 75.33it/s, est. speed input: 113912.03 toks/s, output: 111.24 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:10<00:11, 75.86it/s, est. speed input: 113256.03 toks/s, output: 110.60 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:11<00:11, 75.19it/s, est. speed input: 112505.50 toks/s, output: 109.87 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:11<00:10, 74.53it/s, est. speed input: 111768.28 toks/s, output: 109.15 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:11<00:10, 74.18it/s, est. speed input: 111072.60 toks/s, output: 108.47 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:11<00:10, 73.62it/s, est. speed input: 110373.45 toks/s, output: 107.79 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:11<00:10, 73.36it/s, est. speed input: 109713.90 toks/s, output: 107.14 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:12<00:10, 73.18it/s, est. speed input: 109077.83 toks/s, output: 106.52 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:12<00:10, 73.20it/s, est. speed input: 108477.89 toks/s, output: 105.94 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:12<00:09, 73.04it/s, est. speed input: 107883.44 toks/s, output: 105.35 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:12<00:09, 72.98it/s, est. speed input: 107314.33 toks/s, output: 104.80 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:13<00:09, 72.95it/s, est. speed input: 106764.67 toks/s, output: 104.26 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:13<00:09, 73.03it/s, est. speed input: 106242.10 toks/s, output: 103.75 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:13<00:08, 73.10it/s, est. speed input: 105737.16 toks/s, output: 103.26 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:13<00:08, 73.05it/s, est. speed input: 105240.84 toks/s, output: 102.77 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:13<00:08, 72.97it/s, est. speed input: 104756.24 toks/s, output: 102.30 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:14<00:08, 72.88it/s, est. speed input: 104284.07 toks/s, output: 101.84 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:14<00:07, 73.89it/s, est. speed input: 103904.01 toks/s, output: 101.47 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:14<00:07, 73.70it/s, est. speed input: 103471.30 toks/s, output: 101.05 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:14<00:07, 73.67it/s, est. speed input: 103058.38 toks/s, output: 100.64 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:15<00:07, 73.27it/s, est. speed input: 102632.26 toks/s, output: 100.23 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:15<00:07, 74.27it/s, est. speed input: 102302.51 toks/s, output: 99.90 toks/s] 
Processed prompts:  75%|███████▌  | 1538/2048 [00:15<00:06, 73.81it/s, est. speed input: 101906.93 toks/s, output: 99.52 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:15<00:06, 74.84it/s, est. speed input: 101607.16 toks/s, output: 99.23 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:15<00:06, 74.05it/s, est. speed input: 101222.46 toks/s, output: 98.85 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:16<00:06, 73.61it/s, est. speed input: 100854.78 toks/s, output: 98.49 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:16<00:06, 73.34it/s, est. speed input: 100498.99 toks/s, output: 98.14 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:16<00:05, 74.62it/s, est. speed input: 100239.44 toks/s, output: 97.89 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:16<00:05, 74.46it/s, est. speed input: 99925.10 toks/s, output: 97.58 toks/s] 
Processed prompts:  81%|████████  | 1650/2048 [00:16<00:05, 73.96it/s, est. speed input: 99596.68 toks/s, output: 97.26 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:17<00:05, 73.82it/s, est. speed input: 99288.17 toks/s, output: 96.96 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:17<00:04, 73.51it/s, est. speed input: 98975.68 toks/s, output: 96.66 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:17<00:04, 73.24it/s, est. speed input: 98667.86 toks/s, output: 96.36 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:17<00:04, 73.08it/s, est. speed input: 98369.13 toks/s, output: 96.06 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:18<00:04, 74.12it/s, est. speed input: 98139.77 toks/s, output: 95.84 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:18<00:04, 74.95it/s, est. speed input: 97919.75 toks/s, output: 95.62 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:18<00:03, 74.10it/s, est. speed input: 97631.96 toks/s, output: 95.34 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:18<00:03, 73.73it/s, est. speed input: 97361.86 toks/s, output: 95.08 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:18<00:03, 73.43it/s, est. speed input: 97096.03 toks/s, output: 94.82 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:19<00:03, 73.27it/s, est. speed input: 96838.49 toks/s, output: 94.57 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:19<00:03, 73.07it/s, est. speed input: 96582.42 toks/s, output: 94.32 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:19<00:02, 72.86it/s, est. speed input: 96328.88 toks/s, output: 94.07 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:19<00:02, 72.72it/s, est. speed input: 96081.06 toks/s, output: 93.83 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:20<00:02, 72.57it/s, est. speed input: 95836.46 toks/s, output: 93.59 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:20<00:02, 74.14it/s, est. speed input: 95675.57 toks/s, output: 93.43 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:20<00:01, 73.59it/s, est. speed input: 95441.75 toks/s, output: 93.20 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:20<00:01, 73.29it/s, est. speed input: 95216.96 toks/s, output: 92.99 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:20<00:01, 73.17it/s, est. speed input: 95000.45 toks/s, output: 92.77 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:21<00:01, 72.87it/s, est. speed input: 94779.17 toks/s, output: 92.56 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:21<00:01, 72.76it/s, est. speed input: 94566.89 toks/s, output: 92.35 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:21<00:00, 73.93it/s, est. speed input: 94412.88 toks/s, output: 92.20 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:21<00:00, 73.73it/s, est. speed input: 94218.53 toks/s, output: 92.01 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:21<00:00, 73.48it/s, est. speed input: 94023.01 toks/s, output: 91.82 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:22<00:00, 73.88it/s, est. speed input: 93855.49 toks/s, output: 91.66 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:22<00:00, 73.88it/s, est. speed input: 94500.63 toks/s, output: 92.29 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:22<00:00, 92.29it/s, est. speed input: 94500.63 toks/s, output: 92.29 toks/s]
[rank0]:[W126 17:29:17.756663980 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 17:29:19
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:29:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=843208) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=843208) WARNING 01-26 17:30:13 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=843208) WARNING 01-26 17:30:32 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 74.29 requests/s, 76143.46 total tokens/s, 74.29 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 17:29:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:29:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:29:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:29:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:29:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:29:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:29:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:29:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:29:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:29:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:29:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:29:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:29:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:29:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:29:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:29:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:29:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:29:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:29:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:29:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:29:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:29:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:29:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:29:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:29:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:29:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:29:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:29:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=843208) [2026-01-26 17:29:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=843208) [2026-01-26 17:29:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=843208) [2026-01-26 17:29:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=843208) [2026-01-26 17:29:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=843208) [2026-01-26 17:29:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=843208) [2026-01-26 17:29:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=843208) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=843208) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.24s/it]
(EngineCore_DP0 pid=843208) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.36it/s]
(EngineCore_DP0 pid=843208) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.06it/s]
(EngineCore_DP0 pid=843208) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.04s/it]
(EngineCore_DP0 pid=843208) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.00s/it]
(EngineCore_DP0 pid=843208) 
(EngineCore_DP0 pid=843208) [2026-01-26 17:30:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=843208) [2026-01-26 17:30:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30736384 bytes
(EngineCore_DP0 pid=843208) [2026-01-26 17:30:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=843208) [2026-01-26 17:30:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21954560 bytes
(EngineCore_DP0 pid=843208) [2026-01-26 17:30:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=843208) [2026-01-26 17:30:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 118554624 bytes
(EngineCore_DP0 pid=843208) [2026-01-26 17:30:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=843208) [2026-01-26 17:30:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 58982400 bytes
(EngineCore_DP0 pid=843208) [rank0]:W0126 17:30:21.252000 843208 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=843208) [rank0]:W0126 17:30:21.321000 843208 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=843208) [rank0]:W0126 17:30:22.250000 843208 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=843208) [rank0]:W0126 17:30:22.359000 843208 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=843208) 2026-01-26 17:30:31,722 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=843208) 2026-01-26 17:30:31,819 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=843208) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  9.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  9.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 10.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  9.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  9.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00, 10.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.65it/s]
(EngineCore_DP0 pid=843208) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 10.94it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 11.05it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 11.24it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 11.17it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 274.01it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 317.89it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 314.35it/s]
Adding requests:   3%|▎         | 127/4096 [00:00<00:12, 315.51it/s]
Adding requests:   4%|▍         | 162/4096 [00:00<00:12, 325.60it/s]
Adding requests:   5%|▍         | 199/4096 [00:00<00:11, 337.40it/s]
Adding requests:   6%|▌         | 235/4096 [00:00<00:11, 344.05it/s]
Adding requests:   7%|▋         | 270/4096 [00:00<00:11, 342.99it/s]
Adding requests:   7%|▋         | 307/4096 [00:00<00:10, 348.22it/s]
Adding requests:   8%|▊         | 345/4096 [00:01<00:10, 357.11it/s]
Adding requests:   9%|▉         | 382/4096 [00:01<00:10, 359.14it/s]
Adding requests:  10%|█         | 421/4096 [00:01<00:10, 366.13it/s]
Adding requests:  11%|█         | 458/4096 [00:01<00:10, 363.65it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:09, 372.34it/s]
Adding requests:  13%|█▎        | 537/4096 [00:01<00:09, 375.80it/s]
Adding requests:  14%|█▍        | 575/4096 [00:01<00:09, 375.01it/s]
Adding requests:  15%|█▍        | 613/4096 [00:01<00:09, 365.07it/s]
Adding requests:  16%|█▌        | 650/4096 [00:01<00:09, 357.62it/s]
Adding requests:  17%|█▋        | 687/4096 [00:01<00:09, 361.16it/s]
Adding requests:  18%|█▊        | 724/4096 [00:02<00:09, 356.89it/s]
Adding requests:  19%|█▊        | 760/4096 [00:02<00:09, 354.97it/s]
Adding requests:  19%|█▉        | 796/4096 [00:02<00:09, 356.25it/s]
Adding requests:  20%|██        | 833/4096 [00:02<00:09, 360.21it/s]
Adding requests:  21%|██▏       | 871/4096 [00:02<00:08, 365.45it/s]
Adding requests:  22%|██▏       | 908/4096 [00:02<00:08, 363.95it/s]
Adding requests:  23%|██▎       | 945/4096 [00:02<00:09, 346.48it/s]
Adding requests:  24%|██▍       | 981/4096 [00:02<00:08, 349.88it/s]
Adding requests:  25%|██▍       | 1017/4096 [00:02<00:09, 339.70it/s]
Adding requests:  26%|██▌       | 1052/4096 [00:02<00:08, 338.57it/s]
Adding requests:  27%|██▋       | 1087/4096 [00:03<00:08, 341.79it/s]
Adding requests:  27%|██▋       | 1124/4096 [00:03<00:08, 346.89it/s]
Adding requests:  28%|██▊       | 1159/4096 [00:03<00:08, 347.08it/s]
Adding requests:  29%|██▉       | 1195/4096 [00:03<00:08, 350.17it/s]
Adding requests:  30%|███       | 1233/4096 [00:03<00:07, 358.24it/s]
Adding requests:  31%|███       | 1269/4096 [00:03<00:07, 353.90it/s]
Adding requests:  32%|███▏      | 1305/4096 [00:03<00:07, 353.30it/s]
Adding requests:  33%|███▎      | 1341/4096 [00:03<00:07, 353.99it/s]
Adding requests:  34%|███▎      | 1379/4096 [00:03<00:07, 358.67it/s]
Adding requests:  35%|███▍      | 1415/4096 [00:04<00:07, 356.18it/s]
Adding requests:  35%|███▌      | 1451/4096 [00:04<00:07, 357.05it/s]
Adding requests:  36%|███▋      | 1488/4096 [00:04<00:07, 360.45it/s]
Adding requests:  37%|███▋      | 1525/4096 [00:04<00:07, 363.00it/s]
Adding requests:  38%|███▊      | 1562/4096 [00:04<00:07, 356.82it/s]
Adding requests:  39%|███▉      | 1598/4096 [00:04<00:07, 353.22it/s]
Adding requests:  40%|███▉      | 1634/4096 [00:04<00:07, 349.18it/s]
Adding requests:  41%|████      | 1669/4096 [00:04<00:07, 343.36it/s]
Adding requests:  42%|████▏     | 1706/4096 [00:04<00:06, 349.05it/s]
Adding requests:  43%|████▎     | 1743/4096 [00:04<00:06, 352.50it/s]
Adding requests:  43%|████▎     | 1780/4096 [00:05<00:06, 356.81it/s]
Adding requests:  44%|████▍     | 1816/4096 [00:05<00:06, 352.06it/s]
Adding requests:  45%|████▌     | 1852/4096 [00:05<00:06, 354.24it/s]
Adding requests:  46%|████▌     | 1889/4096 [00:05<00:06, 357.66it/s]
Adding requests:  47%|████▋     | 1925/4096 [00:05<00:06, 352.29it/s]
Adding requests:  48%|████▊     | 1962/4096 [00:05<00:05, 356.92it/s]
Adding requests:  49%|████▉     | 1998/4096 [00:05<00:05, 354.13it/s]
Adding requests:  50%|████▉     | 2034/4096 [00:05<00:05, 346.41it/s]
Adding requests:  51%|█████     | 2069/4096 [00:05<00:05, 342.16it/s]
Adding requests:  51%|█████▏    | 2106/4096 [00:05<00:05, 347.47it/s]
Adding requests:  52%|█████▏    | 2142/4096 [00:06<00:05, 348.05it/s]
Adding requests:  53%|█████▎    | 2177/4096 [00:06<00:05, 329.31it/s]
Adding requests:  54%|█████▍    | 2211/4096 [00:06<00:05, 332.31it/s]
Adding requests:  55%|█████▍    | 2247/4096 [00:06<00:05, 338.81it/s]
Adding requests:  56%|█████▌    | 2284/4096 [00:06<00:05, 346.97it/s]
Adding requests:  57%|█████▋    | 2320/4096 [00:06<00:05, 350.27it/s]
Adding requests:  58%|█████▊    | 2357/4096 [00:06<00:04, 355.64it/s]
Adding requests:  58%|█████▊    | 2394/4096 [00:06<00:04, 359.07it/s]
Adding requests:  59%|█████▉    | 2432/4096 [00:06<00:04, 363.04it/s]
Adding requests:  60%|██████    | 2469/4096 [00:07<00:04, 357.35it/s]
Adding requests:  61%|██████    | 2505/4096 [00:07<00:04, 354.50it/s]
Adding requests:  62%|██████▏   | 2543/4096 [00:07<00:04, 361.58it/s]
Adding requests:  63%|██████▎   | 2583/4096 [00:07<00:04, 370.17it/s]
Adding requests:  64%|██████▍   | 2621/4096 [00:07<00:04, 365.71it/s]
Adding requests:  65%|██████▍   | 2658/4096 [00:07<00:04, 358.10it/s]
Adding requests:  66%|██████▌   | 2694/4096 [00:07<00:03, 354.42it/s]
Adding requests:  67%|██████▋   | 2730/4096 [00:07<00:03, 354.66it/s]
Adding requests:  68%|██████▊   | 2768/4096 [00:07<00:03, 361.22it/s]
Adding requests:  69%|██████▊   | 2806/4096 [00:07<00:03, 366.07it/s]
Adding requests:  69%|██████▉   | 2843/4096 [00:08<00:03, 365.13it/s]
Adding requests:  70%|███████   | 2880/4096 [00:08<00:03, 361.03it/s]
Adding requests:  71%|███████   | 2917/4096 [00:08<00:03, 363.06it/s]
Adding requests:  72%|███████▏  | 2955/4096 [00:08<00:03, 364.85it/s]
Adding requests:  73%|███████▎  | 2992/4096 [00:08<00:03, 363.79it/s]
Adding requests:  74%|███████▍  | 3030/4096 [00:08<00:02, 368.42it/s]
Adding requests:  75%|███████▍  | 3068/4096 [00:08<00:02, 369.04it/s]
Adding requests:  76%|███████▌  | 3106/4096 [00:08<00:02, 369.84it/s]
Adding requests:  77%|███████▋  | 3145/4096 [00:08<00:02, 374.22it/s]
Adding requests:  78%|███████▊  | 3183/4096 [00:08<00:02, 364.46it/s]
Adding requests:  79%|███████▊  | 3220/4096 [00:09<00:02, 364.36it/s]
Adding requests:  80%|███████▉  | 3258/4096 [00:09<00:02, 366.36it/s]
Adding requests:  80%|████████  | 3295/4096 [00:09<00:02, 352.81it/s]
Adding requests:  81%|████████▏ | 3331/4096 [00:09<00:02, 351.19it/s]
Adding requests:  82%|████████▏ | 3369/4096 [00:09<00:02, 357.76it/s]
Adding requests:  83%|████████▎ | 3406/4096 [00:09<00:01, 361.00it/s]
Adding requests:  84%|████████▍ | 3443/4096 [00:09<00:01, 363.15it/s]
Adding requests:  85%|████████▍ | 3480/4096 [00:09<00:01, 364.38it/s]
Adding requests:  86%|████████▌ | 3517/4096 [00:09<00:01, 350.31it/s]
Adding requests:  87%|████████▋ | 3557/4096 [00:10<00:01, 362.51it/s]
Adding requests:  88%|████████▊ | 3594/4096 [00:10<00:01, 359.89it/s]
Adding requests:  89%|████████▊ | 3631/4096 [00:10<00:01, 361.98it/s]
Adding requests:  90%|████████▉ | 3668/4096 [00:10<00:01, 357.67it/s]
Adding requests:  90%|█████████ | 3704/4096 [00:10<00:01, 356.07it/s]
Adding requests:  91%|█████████▏| 3740/4096 [00:10<00:00, 356.36it/s]
Adding requests:  92%|█████████▏| 3776/4096 [00:10<00:00, 348.06it/s]
Adding requests:  93%|█████████▎| 3811/4096 [00:10<00:00, 338.75it/s]
Adding requests:  94%|█████████▍| 3847/4096 [00:10<00:00, 343.39it/s]
Adding requests:  95%|█████████▍| 3883/4096 [00:10<00:00, 347.72it/s]
Adding requests:  96%|█████████▌| 3918/4096 [00:11<00:00, 343.79it/s]
Adding requests:  97%|█████████▋| 3953/4096 [00:11<00:00, 344.66it/s]
Adding requests:  97%|█████████▋| 3988/4096 [00:11<00:00, 346.13it/s]
Adding requests:  98%|█████████▊| 4024/4096 [00:11<00:00, 348.54it/s]
Adding requests:  99%|█████████▉| 4059/4096 [00:11<00:00, 345.27it/s]
Adding requests: 100%|█████████▉| 4094/4096 [00:11<00:00, 346.15it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 353.93it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  21%|██        | 857/4096 [00:00<00:01, 1822.22it/s, est. speed input: 1866000.61 toks/s, output: 1822.23 toks/s]
Processed prompts:  25%|██▌       | 1040/4096 [00:02<00:09, 315.74it/s, est. speed input: 406370.13 toks/s, output: 396.84 toks/s]  
Processed prompts:  27%|██▋       | 1121/4096 [00:03<00:14, 207.44it/s, est. speed input: 293030.07 toks/s, output: 286.16 toks/s]
Processed prompts:  29%|██▊       | 1168/4096 [00:04<00:15, 191.11it/s, est. speed input: 275290.87 toks/s, output: 268.84 toks/s]
Processed prompts:  29%|██▉       | 1201/4096 [00:04<00:17, 168.47it/s, est. speed input: 257267.14 toks/s, output: 251.24 toks/s]
Processed prompts:  30%|██▉       | 1225/4096 [00:05<00:19, 144.88it/s, est. speed input: 241272.75 toks/s, output: 235.62 toks/s]
Processed prompts:  30%|███       | 1243/4096 [00:05<00:23, 120.50it/s, est. speed input: 226159.93 toks/s, output: 220.86 toks/s]
Processed prompts:  31%|███       | 1273/4096 [00:06<00:26, 107.49it/s, est. speed input: 215092.01 toks/s, output: 210.05 toks/s]
Processed prompts:  32%|███▏      | 1305/4096 [00:06<00:28, 98.52it/s, est. speed input: 205793.92 toks/s, output: 200.97 toks/s] 
Processed prompts:  33%|███▎      | 1337/4096 [00:06<00:30, 91.74it/s, est. speed input: 197663.69 toks/s, output: 193.03 toks/s]
Processed prompts:  33%|███▎      | 1369/4096 [00:07<00:31, 86.74it/s, est. speed input: 190508.74 toks/s, output: 186.04 toks/s]
Processed prompts:  34%|███▍      | 1401/4096 [00:07<00:32, 83.05it/s, est. speed input: 184126.61 toks/s, output: 179.81 toks/s]
Processed prompts:  35%|███▍      | 1433/4096 [00:08<00:32, 81.26it/s, est. speed input: 178765.51 toks/s, output: 174.58 toks/s]
Processed prompts:  36%|███▌      | 1465/4096 [00:08<00:33, 79.03it/s, est. speed input: 173582.81 toks/s, output: 169.51 toks/s]
Processed prompts:  37%|███▋      | 1497/4096 [00:09<00:33, 77.48it/s, est. speed input: 168901.12 toks/s, output: 164.94 toks/s]
Processed prompts:  37%|███▋      | 1529/4096 [00:09<00:33, 77.02it/s, est. speed input: 164855.25 toks/s, output: 160.99 toks/s]
Processed prompts:  38%|███▊      | 1561/4096 [00:09<00:33, 76.71it/s, est. speed input: 161156.06 toks/s, output: 157.38 toks/s]
Processed prompts:  39%|███▉      | 1593/4096 [00:10<00:32, 75.90it/s, est. speed input: 157591.67 toks/s, output: 153.90 toks/s]
Processed prompts:  40%|███▉      | 1625/4096 [00:10<00:32, 76.06it/s, est. speed input: 154509.58 toks/s, output: 150.89 toks/s]
Processed prompts:  40%|████      | 1657/4096 [00:11<00:32, 75.54it/s, est. speed input: 151496.26 toks/s, output: 147.95 toks/s]
Processed prompts:  41%|████      | 1689/4096 [00:11<00:31, 75.37it/s, est. speed input: 148753.51 toks/s, output: 145.27 toks/s]
Processed prompts:  42%|████▏     | 1721/4096 [00:12<00:31, 75.51it/s, est. speed input: 146263.75 toks/s, output: 142.84 toks/s]
Processed prompts:  43%|████▎     | 1753/4096 [00:12<00:31, 75.42it/s, est. speed input: 143902.73 toks/s, output: 140.53 toks/s]
Processed prompts:  44%|████▎     | 1785/4096 [00:12<00:30, 75.03it/s, est. speed input: 141628.56 toks/s, output: 138.31 toks/s]
Processed prompts:  44%|████▍     | 1817/4096 [00:13<00:30, 74.87it/s, est. speed input: 139523.48 toks/s, output: 136.25 toks/s]
Processed prompts:  45%|████▌     | 1849/4096 [00:13<00:30, 74.55it/s, est. speed input: 137510.30 toks/s, output: 134.29 toks/s]
Processed prompts:  46%|████▌     | 1881/4096 [00:14<00:29, 74.93it/s, est. speed input: 135730.47 toks/s, output: 132.55 toks/s]
Processed prompts:  47%|████▋     | 1913/4096 [00:14<00:29, 74.53it/s, est. speed input: 133936.08 toks/s, output: 130.80 toks/s]
Processed prompts:  47%|████▋     | 1945/4096 [00:15<00:28, 74.35it/s, est. speed input: 132262.85 toks/s, output: 129.16 toks/s]
Processed prompts:  48%|████▊     | 1977/4096 [00:15<00:28, 74.84it/s, est. speed input: 130783.20 toks/s, output: 127.72 toks/s]
Processed prompts:  49%|████▉     | 2009/4096 [00:15<00:27, 74.72it/s, est. speed input: 129307.94 toks/s, output: 126.28 toks/s]
Processed prompts:  50%|████▉     | 2041/4096 [00:16<00:27, 75.05it/s, est. speed input: 127973.36 toks/s, output: 124.97 toks/s]
Processed prompts:  51%|█████     | 2073/4096 [00:16<00:27, 74.50it/s, est. speed input: 126593.00 toks/s, output: 123.63 toks/s]
Processed prompts:  51%|█████▏    | 2105/4096 [00:17<00:26, 74.39it/s, est. speed input: 125321.61 toks/s, output: 122.38 toks/s]
Processed prompts:  52%|█████▏    | 2137/4096 [00:17<00:26, 74.30it/s, est. speed input: 124109.48 toks/s, output: 121.20 toks/s]
Processed prompts:  53%|█████▎    | 2169/4096 [00:18<00:25, 74.81it/s, est. speed input: 123031.08 toks/s, output: 120.15 toks/s]
Processed prompts:  54%|█████▎    | 2201/4096 [00:18<00:25, 74.61it/s, est. speed input: 121931.44 toks/s, output: 119.07 toks/s]
Processed prompts:  55%|█████▍    | 2233/4096 [00:18<00:25, 74.42it/s, est. speed input: 120875.69 toks/s, output: 118.04 toks/s]
Processed prompts:  55%|█████▌    | 2265/4096 [00:19<00:24, 74.33it/s, est. speed input: 119871.89 toks/s, output: 117.06 toks/s]
Processed prompts:  56%|█████▌    | 2297/4096 [00:19<00:24, 74.20it/s, est. speed input: 118904.56 toks/s, output: 116.12 toks/s]
Processed prompts:  57%|█████▋    | 2329/4096 [00:20<00:23, 74.20it/s, est. speed input: 117989.18 toks/s, output: 115.22 toks/s]
Processed prompts:  58%|█████▊    | 2361/4096 [00:20<00:23, 74.16it/s, est. speed input: 117106.82 toks/s, output: 114.36 toks/s]
Processed prompts:  58%|█████▊    | 2393/4096 [00:21<00:22, 74.26it/s, est. speed input: 116274.63 toks/s, output: 113.55 toks/s]
Processed prompts:  59%|█████▉    | 2425/4096 [00:21<00:22, 74.26it/s, est. speed input: 115468.08 toks/s, output: 112.76 toks/s]
Processed prompts:  60%|█████▉    | 2457/4096 [00:21<00:22, 74.29it/s, est. speed input: 114697.31 toks/s, output: 112.01 toks/s]
Processed prompts:  61%|██████    | 2489/4096 [00:22<00:21, 74.30it/s, est. speed input: 113954.40 toks/s, output: 111.28 toks/s]
Processed prompts:  62%|██████▏   | 2521/4096 [00:22<00:21, 74.69it/s, est. speed input: 113275.61 toks/s, output: 110.62 toks/s]
Processed prompts:  62%|██████▏   | 2553/4096 [00:23<00:20, 74.51it/s, est. speed input: 112580.29 toks/s, output: 109.94 toks/s]
Processed prompts:  63%|██████▎   | 2585/4096 [00:23<00:20, 74.94it/s, est. speed input: 111960.33 toks/s, output: 109.34 toks/s]
Processed prompts:  64%|██████▍   | 2617/4096 [00:24<00:19, 74.55it/s, est. speed input: 111301.16 toks/s, output: 108.69 toks/s]
Processed prompts:  65%|██████▍   | 2649/4096 [00:24<00:19, 74.32it/s, est. speed input: 110668.82 toks/s, output: 108.07 toks/s]
Processed prompts:  65%|██████▌   | 2681/4096 [00:24<00:19, 74.40it/s, est. speed input: 110079.22 toks/s, output: 107.50 toks/s]
Processed prompts:  66%|██████▌   | 2713/4096 [00:25<00:18, 74.43it/s, est. speed input: 109507.05 toks/s, output: 106.94 toks/s]
Processed prompts:  67%|██████▋   | 2745/4096 [00:25<00:18, 74.92it/s, est. speed input: 108991.84 toks/s, output: 106.44 toks/s]
Processed prompts:  68%|██████▊   | 2777/4096 [00:26<00:17, 74.72it/s, est. speed input: 108450.08 toks/s, output: 105.91 toks/s]
Processed prompts:  69%|██████▊   | 2809/4096 [00:26<00:17, 74.55it/s, est. speed input: 107923.83 toks/s, output: 105.39 toks/s]
Processed prompts:  69%|██████▉   | 2841/4096 [00:27<00:16, 74.38it/s, est. speed input: 107410.35 toks/s, output: 104.89 toks/s]
Processed prompts:  70%|███████   | 2873/4096 [00:27<00:16, 74.28it/s, est. speed input: 106914.49 toks/s, output: 104.41 toks/s]
Processed prompts:  71%|███████   | 2905/4096 [00:27<00:15, 75.29it/s, est. speed input: 106511.99 toks/s, output: 104.02 toks/s]
Processed prompts:  72%|███████▏  | 2937/4096 [00:28<00:15, 74.84it/s, est. speed input: 106039.18 toks/s, output: 103.55 toks/s]
Processed prompts:  72%|███████▏  | 2969/4096 [00:28<00:15, 74.67it/s, est. speed input: 105590.46 toks/s, output: 103.12 toks/s]
Processed prompts:  73%|███████▎  | 3001/4096 [00:29<00:14, 74.54it/s, est. speed input: 105154.73 toks/s, output: 102.69 toks/s]
Processed prompts:  74%|███████▍  | 3033/4096 [00:29<00:14, 74.44it/s, est. speed input: 104730.60 toks/s, output: 102.28 toks/s]
Processed prompts:  75%|███████▍  | 3065/4096 [00:30<00:13, 74.54it/s, est. speed input: 104329.59 toks/s, output: 101.88 toks/s]
Processed prompts:  76%|███████▌  | 3097/4096 [00:30<00:13, 74.39it/s, est. speed input: 103925.69 toks/s, output: 101.49 toks/s]
Processed prompts:  76%|███████▋  | 3129/4096 [00:30<00:12, 74.52it/s, est. speed input: 103548.42 toks/s, output: 101.12 toks/s]
Processed prompts:  77%|███████▋  | 3161/4096 [00:31<00:12, 74.38it/s, est. speed input: 103166.68 toks/s, output: 100.75 toks/s]
Processed prompts:  78%|███████▊  | 3193/4096 [00:31<00:12, 74.37it/s, est. speed input: 102801.03 toks/s, output: 100.39 toks/s]
Processed prompts:  79%|███████▊  | 3225/4096 [00:32<00:11, 74.46it/s, est. speed input: 102450.87 toks/s, output: 100.05 toks/s]
Processed prompts:  80%|███████▉  | 3257/4096 [00:32<00:11, 74.31it/s, est. speed input: 102096.85 toks/s, output: 99.70 toks/s] 
Processed prompts:  80%|████████  | 3289/4096 [00:33<00:10, 74.18it/s, est. speed input: 101750.72 toks/s, output: 99.37 toks/s]
Processed prompts:  81%|████████  | 3321/4096 [00:33<00:10, 74.02it/s, est. speed input: 101409.48 toks/s, output: 99.03 toks/s]
Processed prompts:  82%|████████▏ | 3353/4096 [00:33<00:10, 73.80it/s, est. speed input: 101070.95 toks/s, output: 98.70 toks/s]
Processed prompts:  83%|████████▎ | 3385/4096 [00:34<00:09, 73.91it/s, est. speed input: 100755.95 toks/s, output: 98.39 toks/s]
Processed prompts:  83%|████████▎ | 3417/4096 [00:34<00:09, 73.83it/s, est. speed input: 100439.93 toks/s, output: 98.09 toks/s]
Processed prompts:  84%|████████▍ | 3449/4096 [00:35<00:08, 74.01it/s, est. speed input: 100144.50 toks/s, output: 97.80 toks/s]
Processed prompts:  85%|████████▍ | 3481/4096 [00:35<00:08, 74.04it/s, est. speed input: 99851.28 toks/s, output: 97.51 toks/s] 
Processed prompts:  86%|████████▌ | 3513/4096 [00:36<00:07, 74.09it/s, est. speed input: 99566.53 toks/s, output: 97.23 toks/s]
Processed prompts:  87%|████████▋ | 3545/4096 [00:36<00:07, 74.69it/s, est. speed input: 99317.71 toks/s, output: 96.99 toks/s]
Processed prompts:  87%|████████▋ | 3577/4096 [00:36<00:06, 74.36it/s, est. speed input: 99036.41 toks/s, output: 96.72 toks/s]
Processed prompts:  88%|████████▊ | 3609/4096 [00:37<00:06, 74.33it/s, est. speed input: 98771.65 toks/s, output: 96.46 toks/s]
Processed prompts:  89%|████████▉ | 3641/4096 [00:37<00:06, 74.15it/s, est. speed input: 98504.61 toks/s, output: 96.20 toks/s]
Processed prompts:  90%|████████▉ | 3673/4096 [00:38<00:05, 74.69it/s, est. speed input: 98276.89 toks/s, output: 95.97 toks/s]
Processed prompts:  90%|█████████ | 3705/4096 [00:38<00:05, 74.51it/s, est. speed input: 98027.01 toks/s, output: 95.73 toks/s]
Processed prompts:  91%|█████████ | 3737/4096 [00:39<00:04, 74.42it/s, est. speed input: 97784.28 toks/s, output: 95.49 toks/s]
Processed prompts:  92%|█████████▏| 3769/4096 [00:39<00:04, 74.43it/s, est. speed input: 97550.21 toks/s, output: 95.26 toks/s]
Processed prompts:  93%|█████████▎| 3801/4096 [00:39<00:03, 74.20it/s, est. speed input: 97310.21 toks/s, output: 95.03 toks/s]
Processed prompts:  94%|█████████▎| 3833/4096 [00:40<00:03, 74.20it/s, est. speed input: 97082.59 toks/s, output: 94.81 toks/s]
Processed prompts:  94%|█████████▍| 3865/4096 [00:40<00:03, 74.10it/s, est. speed input: 96855.11 toks/s, output: 94.59 toks/s]
Processed prompts:  95%|█████████▌| 3897/4096 [00:41<00:02, 74.14it/s, est. speed input: 96637.51 toks/s, output: 94.37 toks/s]
Processed prompts:  96%|█████████▌| 3929/4096 [00:41<00:02, 74.50it/s, est. speed input: 96439.55 toks/s, output: 94.18 toks/s]
Processed prompts:  97%|█████████▋| 3961/4096 [00:42<00:01, 74.36it/s, est. speed input: 96227.76 toks/s, output: 93.97 toks/s]
Processed prompts:  97%|█████████▋| 3993/4096 [00:42<00:01, 74.84it/s, est. speed input: 96045.43 toks/s, output: 93.79 toks/s]
Processed prompts:  98%|█████████▊| 4025/4096 [00:43<00:00, 74.47it/s, est. speed input: 95836.78 toks/s, output: 93.59 toks/s]
Processed prompts:  99%|█████████▉| 4057/4096 [00:43<00:00, 74.40it/s, est. speed input: 95640.28 toks/s, output: 93.40 toks/s]
Processed prompts: 100%|█████████▉| 4089/4096 [00:43<00:00, 94.50it/s, est. speed input: 96117.59 toks/s, output: 93.86 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:43<00:00, 94.50it/s, est. speed input: 96281.85 toks/s, output: 94.03 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:43<00:00, 94.03it/s, est. speed input: 96281.85 toks/s, output: 94.03 toks/s]
[rank0]:[W126 17:31:30.188359545 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 17:31:33
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:32:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=845888) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=845888) WARNING 01-26 17:32:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=845888) WARNING 01-26 17:33:09 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 76.63 requests/s, 78548.46 total tokens/s, 76.63 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 17:32:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:32:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:32:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:32:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:32:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:32:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:32:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:32:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:32:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:32:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:32:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:32:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:32:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:32:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:32:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:32:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:32:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:32:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:32:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:32:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:32:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:32:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:32:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:32:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:32:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:32:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:32:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:32:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=845888) [2026-01-26 17:32:36] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=845888) [2026-01-26 17:32:36] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=845888) [2026-01-26 17:32:36] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=845888) [2026-01-26 17:32:36] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=845888) [2026-01-26 17:32:36] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=845888) [2026-01-26 17:32:36] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=845888) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=845888) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.15s/it]
(EngineCore_DP0 pid=845888) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.48it/s]
(EngineCore_DP0 pid=845888) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.16it/s]
(EngineCore_DP0 pid=845888) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.02s/it]
(EngineCore_DP0 pid=845888) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.04it/s]
(EngineCore_DP0 pid=845888) 
(EngineCore_DP0 pid=845888) [2026-01-26 17:32:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=845888) [2026-01-26 17:32:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30736384 bytes
(EngineCore_DP0 pid=845888) [2026-01-26 17:32:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=845888) [2026-01-26 17:32:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21954560 bytes
(EngineCore_DP0 pid=845888) [2026-01-26 17:32:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=845888) [2026-01-26 17:32:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 118554624 bytes
(EngineCore_DP0 pid=845888) [2026-01-26 17:32:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=845888) [2026-01-26 17:32:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 58982400 bytes
(EngineCore_DP0 pid=845888) [rank0]:W0126 17:32:58.363000 845888 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=845888) [rank0]:W0126 17:32:58.433000 845888 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=845888) [rank0]:W0126 17:32:59.221000 845888 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=845888) [rank0]:W0126 17:32:59.329000 845888 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=845888) 2026-01-26 17:33:09,159 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=845888) 2026-01-26 17:33:09,349 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=845888) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:05,  3.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  3.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:02,  5.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  7.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  8.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  8.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  8.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:01<00:00,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  9.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.18it/s]
(EngineCore_DP0 pid=845888) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 10.75it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  9.51it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 10.00it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 10.37it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 10.66it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00, 10.47it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 27/8192 [00:00<00:30, 268.00it/s]
Adding requests:   1%|          | 61/8192 [00:00<00:26, 309.30it/s]
Adding requests:   1%|          | 93/8192 [00:00<00:26, 310.09it/s]
Adding requests:   2%|▏         | 127/8192 [00:00<00:25, 317.69it/s]
Adding requests:   2%|▏         | 162/8192 [00:00<00:24, 327.03it/s]
Adding requests:   2%|▏         | 198/8192 [00:00<00:23, 336.17it/s]
Adding requests:   3%|▎         | 233/8192 [00:00<00:23, 339.88it/s]
Adding requests:   3%|▎         | 268/8192 [00:00<00:23, 339.37it/s]
Adding requests:   4%|▎         | 304/8192 [00:00<00:23, 342.39it/s]
Adding requests:   4%|▍         | 341/8192 [00:01<00:22, 349.12it/s]
Adding requests:   5%|▍         | 377/8192 [00:01<00:22, 351.35it/s]
Adding requests:   5%|▌         | 414/8192 [00:01<00:21, 356.64it/s]
Adding requests:   5%|▌         | 450/8192 [00:01<00:21, 352.09it/s]
Adding requests:   6%|▌         | 490/8192 [00:01<00:21, 366.23it/s]
Adding requests:   6%|▋         | 529/8192 [00:01<00:20, 369.95it/s]
Adding requests:   7%|▋         | 567/8192 [00:01<00:20, 368.50it/s]
Adding requests:   7%|▋         | 604/8192 [00:01<00:21, 354.88it/s]
Adding requests:   8%|▊         | 640/8192 [00:01<00:21, 353.95it/s]
Adding requests:   8%|▊         | 676/8192 [00:01<00:21, 348.16it/s]
Adding requests:   9%|▊         | 713/8192 [00:02<00:21, 353.97it/s]
Adding requests:   9%|▉         | 749/8192 [00:02<00:21, 347.14it/s]
Adding requests:  10%|▉         | 785/8192 [00:02<00:21, 350.40it/s]
Adding requests:  10%|█         | 821/8192 [00:02<00:21, 349.49it/s]
Adding requests:  10%|█         | 859/8192 [00:02<00:20, 357.14it/s]
Adding requests:  11%|█         | 896/8192 [00:02<00:20, 358.96it/s]
Adding requests:  11%|█▏        | 932/8192 [00:02<00:20, 351.59it/s]
Adding requests:  12%|█▏        | 969/8192 [00:02<00:20, 355.58it/s]
Adding requests:  12%|█▏        | 1005/8192 [00:02<00:20, 347.09it/s]
Adding requests:  13%|█▎        | 1041/8192 [00:02<00:20, 350.80it/s]
Adding requests:  13%|█▎        | 1077/8192 [00:03<00:20, 347.78it/s]
Adding requests:  14%|█▎        | 1112/8192 [00:03<00:20, 347.89it/s]
Adding requests:  14%|█▍        | 1149/8192 [00:03<00:20, 351.41it/s]
Adding requests:  14%|█▍        | 1185/8192 [00:03<00:20, 350.16it/s]
Adding requests:  15%|█▍        | 1223/8192 [00:03<00:19, 357.35it/s]
Adding requests:  15%|█▌        | 1259/8192 [00:03<00:19, 347.09it/s]
Adding requests:  16%|█▌        | 1294/8192 [00:03<00:20, 343.93it/s]
Adding requests:  16%|█▌        | 1330/8192 [00:03<00:19, 348.04it/s]
Adding requests:  17%|█▋        | 1367/8192 [00:03<00:19, 352.63it/s]
Adding requests:  17%|█▋        | 1403/8192 [00:04<00:19, 350.60it/s]
Adding requests:  18%|█▊        | 1439/8192 [00:04<00:19, 353.24it/s]
Adding requests:  18%|█▊        | 1475/8192 [00:04<00:19, 350.89it/s]
Adding requests:  18%|█▊        | 1513/8192 [00:04<00:18, 357.15it/s]
Adding requests:  19%|█▉        | 1549/8192 [00:04<00:18, 354.81it/s]
Adding requests:  19%|█▉        | 1585/8192 [00:04<00:19, 346.50it/s]
Adding requests:  20%|█▉        | 1620/8192 [00:04<00:19, 342.56it/s]
Adding requests:  20%|██        | 1655/8192 [00:04<00:19, 328.88it/s]
Adding requests:  21%|██        | 1690/8192 [00:04<00:19, 332.66it/s]
Adding requests:  21%|██        | 1727/8192 [00:04<00:18, 341.29it/s]
Adding requests:  22%|██▏       | 1763/8192 [00:05<00:18, 345.28it/s]
Adding requests:  22%|██▏       | 1798/8192 [00:05<00:18, 345.79it/s]
Adding requests:  22%|██▏       | 1834/8192 [00:05<00:18, 349.39it/s]
Adding requests:  23%|██▎       | 1869/8192 [00:05<00:18, 348.41it/s]
Adding requests:  23%|██▎       | 1906/8192 [00:05<00:17, 352.57it/s]
Adding requests:  24%|██▎       | 1944/8192 [00:05<00:17, 358.56it/s]
Adding requests:  24%|██▍       | 1981/8192 [00:05<00:17, 357.47it/s]
Adding requests:  25%|██▍       | 2017/8192 [00:05<00:17, 349.36it/s]
Adding requests:  25%|██▌       | 2052/8192 [00:05<00:17, 346.70it/s]
Adding requests:  25%|██▌       | 2087/8192 [00:06<00:18, 339.17it/s]
Adding requests:  26%|██▌       | 2125/8192 [00:06<00:17, 348.47it/s]
Adding requests:  26%|██▋       | 2160/8192 [00:06<00:17, 344.26it/s]
Adding requests:  27%|██▋       | 2195/8192 [00:06<00:18, 328.64it/s]
Adding requests:  27%|██▋       | 2230/8192 [00:06<00:17, 332.03it/s]
Adding requests:  28%|██▊       | 2267/8192 [00:06<00:17, 340.05it/s]
Adding requests:  28%|██▊       | 2304/8192 [00:06<00:16, 346.92it/s]
Adding requests:  29%|██▊       | 2341/8192 [00:06<00:16, 352.36it/s]
Adding requests:  29%|██▉       | 2377/8192 [00:06<00:16, 350.60it/s]
Adding requests:  29%|██▉       | 2415/8192 [00:06<00:16, 359.06it/s]
Adding requests:  30%|██▉       | 2451/8192 [00:07<00:16, 356.40it/s]
Adding requests:  30%|███       | 2488/8192 [00:07<00:15, 357.89it/s]
Adding requests:  31%|███       | 2525/8192 [00:07<00:15, 358.85it/s]
Adding requests:  31%|███▏      | 2566/8192 [00:07<00:15, 370.64it/s]
Adding requests:  32%|███▏      | 2604/8192 [00:07<00:15, 368.79it/s]
Adding requests:  32%|███▏      | 2641/8192 [00:07<00:15, 358.37it/s]
Adding requests:  33%|███▎      | 2677/8192 [00:07<00:15, 357.35it/s]
Adding requests:  33%|███▎      | 2713/8192 [00:07<00:15, 349.64it/s]
Adding requests:  34%|███▎      | 2750/8192 [00:07<00:15, 353.85it/s]
Adding requests:  34%|███▍      | 2788/8192 [00:07<00:15, 359.94it/s]
Adding requests:  34%|███▍      | 2825/8192 [00:08<00:14, 362.16it/s]
Adding requests:  35%|███▍      | 2862/8192 [00:08<00:14, 360.97it/s]
Adding requests:  35%|███▌      | 2899/8192 [00:08<00:14, 358.78it/s]
Adding requests:  36%|███▌      | 2935/8192 [00:08<00:15, 349.19it/s]
Adding requests:  36%|███▋      | 2972/8192 [00:08<00:14, 352.16it/s]
Adding requests:  37%|███▋      | 3010/8192 [00:08<00:14, 358.51it/s]
Adding requests:  37%|███▋      | 3047/8192 [00:08<00:14, 360.66it/s]
Adding requests:  38%|███▊      | 3084/8192 [00:08<00:14, 362.88it/s]
Adding requests:  38%|███▊      | 3123/8192 [00:08<00:13, 367.41it/s]
Adding requests:  39%|███▊      | 3160/8192 [00:09<00:14, 356.11it/s]
Adding requests:  39%|███▉      | 3196/8192 [00:09<00:14, 354.82it/s]
Adding requests:  39%|███▉      | 3234/8192 [00:09<00:13, 359.40it/s]
Adding requests:  40%|███▉      | 3270/8192 [00:09<00:13, 353.47it/s]
Adding requests:  40%|████      | 3306/8192 [00:09<00:14, 345.99it/s]
Adding requests:  41%|████      | 3342/8192 [00:09<00:13, 348.52it/s]
Adding requests:  41%|████      | 3379/8192 [00:09<00:13, 353.20it/s]
Adding requests:  42%|████▏     | 3415/8192 [00:09<00:13, 352.66it/s]
Adding requests:  42%|████▏     | 3452/8192 [00:09<00:13, 356.36it/s]
Adding requests:  43%|████▎     | 3488/8192 [00:09<00:13, 351.31it/s]
Adding requests:  43%|████▎     | 3527/8192 [00:10<00:12, 361.51it/s]
Adding requests:  44%|████▎     | 3566/8192 [00:10<00:12, 368.59it/s]
Adding requests:  44%|████▍     | 3603/8192 [00:10<00:12, 362.13it/s]
Adding requests:  44%|████▍     | 3641/8192 [00:10<00:12, 364.50it/s]
Adding requests:  45%|████▍     | 3678/8192 [00:10<00:12, 357.31it/s]
Adding requests:  45%|████▌     | 3714/8192 [00:10<00:12, 354.38it/s]
Adding requests:  46%|████▌     | 3750/8192 [00:10<00:12, 350.23it/s]
Adding requests:  46%|████▌     | 3786/8192 [00:10<00:12, 341.23it/s]
Adding requests:  47%|████▋     | 3821/8192 [00:10<00:13, 335.86it/s]
Adding requests:  47%|████▋     | 3857/8192 [00:11<00:12, 340.72it/s]
Adding requests:  48%|████▊     | 3892/8192 [00:11<00:12, 342.48it/s]
Adding requests:  48%|████▊     | 3927/8192 [00:11<00:12, 338.71it/s]
Adding requests:  48%|████▊     | 3962/8192 [00:11<00:12, 341.63it/s]
Adding requests:  49%|████▉     | 3997/8192 [00:11<00:12, 340.13it/s]
Adding requests:  49%|████▉     | 4032/8192 [00:11<00:12, 342.38it/s]
Adding requests:  50%|████▉     | 4067/8192 [00:11<00:12, 341.97it/s]
Adding requests:  50%|█████     | 4102/8192 [00:11<00:11, 341.98it/s]
Adding requests:  51%|█████     | 4138/8192 [00:11<00:11, 345.43it/s]
Adding requests:  51%|█████     | 4175/8192 [00:11<00:11, 351.27it/s]
Adding requests:  51%|█████▏    | 4211/8192 [00:12<00:11, 344.14it/s]
Adding requests:  52%|█████▏    | 4246/8192 [00:12<00:11, 339.50it/s]
Adding requests:  52%|█████▏    | 4280/8192 [00:12<00:11, 338.38it/s]
Adding requests:  53%|█████▎    | 4316/8192 [00:12<00:11, 343.73it/s]
Adding requests:  53%|█████▎    | 4351/8192 [00:12<00:11, 345.56it/s]
Adding requests:  54%|█████▎    | 4386/8192 [00:12<00:10, 346.44it/s]
Adding requests:  54%|█████▍    | 4422/8192 [00:12<00:10, 349.46it/s]
Adding requests:  54%|█████▍    | 4458/8192 [00:12<00:10, 351.31it/s]
Adding requests:  55%|█████▍    | 4494/8192 [00:12<00:10, 350.21it/s]
Adding requests:  55%|█████▌    | 4532/8192 [00:12<00:10, 357.28it/s]
Adding requests:  56%|█████▌    | 4568/8192 [00:13<00:10, 350.77it/s]
Adding requests:  56%|█████▌    | 4604/8192 [00:13<00:10, 351.17it/s]
Adding requests:  57%|█████▋    | 4640/8192 [00:13<00:10, 349.79it/s]
Adding requests:  57%|█████▋    | 4675/8192 [00:13<00:10, 341.38it/s]
Adding requests:  57%|█████▋    | 4710/8192 [00:13<00:10, 337.35it/s]
Adding requests:  58%|█████▊    | 4748/8192 [00:13<00:09, 349.40it/s]
Adding requests:  58%|█████▊    | 4784/8192 [00:13<00:09, 346.72it/s]
Adding requests:  59%|█████▉    | 4819/8192 [00:13<00:09, 347.41it/s]
Adding requests:  59%|█████▉    | 4854/8192 [00:13<00:09, 343.06it/s]
Adding requests:  60%|█████▉    | 4889/8192 [00:13<00:09, 337.77it/s]
Adding requests:  60%|██████    | 4923/8192 [00:14<00:09, 336.84it/s]
Adding requests:  61%|██████    | 4959/8192 [00:14<00:09, 341.95it/s]
Adding requests:  61%|██████    | 4995/8192 [00:14<00:09, 346.35it/s]
Adding requests:  61%|██████▏   | 5033/8192 [00:14<00:08, 354.17it/s]
Adding requests:  62%|██████▏   | 5069/8192 [00:14<00:08, 353.15it/s]
Adding requests:  62%|██████▏   | 5105/8192 [00:14<00:08, 351.75it/s]
Adding requests:  63%|██████▎   | 5141/8192 [00:14<00:08, 350.41it/s]
Adding requests:  63%|██████▎   | 5177/8192 [00:14<00:08, 350.51it/s]
Adding requests:  64%|██████▎   | 5213/8192 [00:14<00:08, 347.58it/s]
Adding requests:  64%|██████▍   | 5248/8192 [00:15<00:08, 347.68it/s]
Adding requests:  64%|██████▍   | 5283/8192 [00:15<00:08, 346.96it/s]
Adding requests:  65%|██████▍   | 5318/8192 [00:15<00:08, 346.54it/s]
Adding requests:  65%|██████▌   | 5355/8192 [00:15<00:08, 351.55it/s]
Adding requests:  66%|██████▌   | 5391/8192 [00:15<00:08, 348.41it/s]
Adding requests:  66%|██████▌   | 5426/8192 [00:15<00:08, 341.73it/s]
Adding requests:  67%|██████▋   | 5464/8192 [00:15<00:07, 351.70it/s]
Adding requests:  67%|██████▋   | 5500/8192 [00:15<00:07, 351.05it/s]
Adding requests:  68%|██████▊   | 5536/8192 [00:15<00:07, 350.68it/s]
Adding requests:  68%|██████▊   | 5572/8192 [00:15<00:07, 348.28it/s]
Adding requests:  68%|██████▊   | 5607/8192 [00:16<00:07, 337.25it/s]
Adding requests:  69%|██████▉   | 5644/8192 [00:16<00:07, 345.16it/s]
Adding requests:  69%|██████▉   | 5679/8192 [00:16<00:07, 345.39it/s]
Adding requests:  70%|██████▉   | 5714/8192 [00:16<00:07, 346.72it/s]
Adding requests:  70%|███████   | 5749/8192 [00:16<00:07, 347.23it/s]
Adding requests:  71%|███████   | 5785/8192 [00:16<00:06, 350.11it/s]
Adding requests:  71%|███████   | 5822/8192 [00:16<00:06, 353.85it/s]
Adding requests:  72%|███████▏  | 5858/8192 [00:16<00:06, 350.30it/s]
Adding requests:  72%|███████▏  | 5896/8192 [00:16<00:06, 357.48it/s]
Adding requests:  72%|███████▏  | 5932/8192 [00:16<00:06, 356.29it/s]
Adding requests:  73%|███████▎  | 5968/8192 [00:17<00:06, 351.21it/s]
Adding requests:  73%|███████▎  | 6004/8192 [00:17<00:06, 349.87it/s]
Adding requests:  74%|███████▎  | 6040/8192 [00:17<00:06, 350.52it/s]
Adding requests:  74%|███████▍  | 6076/8192 [00:17<00:06, 351.35it/s]
Adding requests:  75%|███████▍  | 6112/8192 [00:17<00:05, 347.13it/s]
Adding requests:  75%|███████▌  | 6149/8192 [00:17<00:05, 352.13it/s]
Adding requests:  76%|███████▌  | 6185/8192 [00:17<00:05, 350.62it/s]
Adding requests:  76%|███████▌  | 6221/8192 [00:17<00:05, 345.96it/s]
Adding requests:  76%|███████▋  | 6257/8192 [00:17<00:05, 348.84it/s]
Adding requests:  77%|███████▋  | 6292/8192 [00:18<00:05, 348.67it/s]
Adding requests:  77%|███████▋  | 6331/8192 [00:18<00:05, 357.88it/s]
Adding requests:  78%|███████▊  | 6368/8192 [00:18<00:05, 361.24it/s]
Adding requests:  78%|███████▊  | 6405/8192 [00:18<00:05, 354.80it/s]
Adding requests:  79%|███████▊  | 6441/8192 [00:18<00:05, 345.95it/s]
Adding requests:  79%|███████▉  | 6476/8192 [00:18<00:05, 342.15it/s]
Adding requests:  79%|███████▉  | 6512/8192 [00:18<00:04, 347.26it/s]
Adding requests:  80%|███████▉  | 6547/8192 [00:18<00:04, 347.62it/s]
Adding requests:  80%|████████  | 6582/8192 [00:18<00:04, 347.69it/s]
Adding requests:  81%|████████  | 6618/8192 [00:18<00:04, 348.97it/s]
Adding requests:  81%|████████  | 6655/8192 [00:19<00:04, 352.06it/s]
Adding requests:  82%|████████▏ | 6691/8192 [00:19<00:04, 351.30it/s]
Adding requests:  82%|████████▏ | 6727/8192 [00:19<00:04, 350.23it/s]
Adding requests:  83%|████████▎ | 6764/8192 [00:19<00:04, 354.13it/s]
Adding requests:  83%|████████▎ | 6800/8192 [00:19<00:03, 350.41it/s]
Adding requests:  83%|████████▎ | 6836/8192 [00:19<00:03, 346.80it/s]
Adding requests:  84%|████████▍ | 6873/8192 [00:19<00:03, 352.18it/s]
Adding requests:  84%|████████▍ | 6909/8192 [00:19<00:03, 350.00it/s]
Adding requests:  85%|████████▍ | 6945/8192 [00:19<00:03, 341.58it/s]
Adding requests:  85%|████████▌ | 6980/8192 [00:19<00:03, 341.78it/s]
Adding requests:  86%|████████▌ | 7017/8192 [00:20<00:03, 349.75it/s]
Adding requests:  86%|████████▌ | 7053/8192 [00:20<00:03, 346.77it/s]
Adding requests:  87%|████████▋ | 7089/8192 [00:20<00:03, 348.78it/s]
Adding requests:  87%|████████▋ | 7124/8192 [00:20<00:03, 348.27it/s]
Adding requests:  87%|████████▋ | 7159/8192 [00:20<00:02, 345.84it/s]
Adding requests:  88%|████████▊ | 7197/8192 [00:20<00:02, 354.15it/s]
Adding requests:  88%|████████▊ | 7235/8192 [00:20<00:02, 358.92it/s]
Adding requests:  89%|████████▉ | 7272/8192 [00:20<00:02, 359.55it/s]
Adding requests:  89%|████████▉ | 7308/8192 [00:20<00:02, 357.06it/s]
Adding requests:  90%|████████▉ | 7344/8192 [00:21<00:02, 357.64it/s]
Adding requests:  90%|█████████ | 7380/8192 [00:21<00:02, 356.74it/s]
Adding requests:  91%|█████████ | 7416/8192 [00:21<00:02, 353.92it/s]
Adding requests:  91%|█████████ | 7452/8192 [00:21<00:02, 352.39it/s]
Adding requests:  91%|█████████▏| 7488/8192 [00:21<00:02, 347.27it/s]
Adding requests:  92%|█████████▏| 7525/8192 [00:21<00:01, 351.57it/s]
Adding requests:  92%|█████████▏| 7561/8192 [00:21<00:01, 353.48it/s]
Adding requests:  93%|█████████▎| 7597/8192 [00:21<00:01, 347.32it/s]
Adding requests:  93%|█████████▎| 7633/8192 [00:21<00:01, 350.46it/s]
Adding requests:  94%|█████████▎| 7672/8192 [00:21<00:01, 360.46it/s]
Adding requests:  94%|█████████▍| 7709/8192 [00:22<00:01, 361.75it/s]
Adding requests:  95%|█████████▍| 7746/8192 [00:22<00:01, 357.75it/s]
Adding requests:  95%|█████████▍| 7782/8192 [00:22<00:01, 351.13it/s]
Adding requests:  95%|█████████▌| 7818/8192 [00:22<00:01, 351.61it/s]
Adding requests:  96%|█████████▌| 7854/8192 [00:22<00:00, 347.96it/s]
Adding requests:  96%|█████████▋| 7889/8192 [00:22<00:00, 346.77it/s]
Adding requests:  97%|█████████▋| 7927/8192 [00:22<00:00, 355.25it/s]
Adding requests:  97%|█████████▋| 7964/8192 [00:22<00:00, 358.05it/s]
Adding requests:  98%|█████████▊| 8002/8192 [00:22<00:00, 363.37it/s]
Adding requests:  98%|█████████▊| 8039/8192 [00:22<00:00, 353.51it/s]
Adding requests:  99%|█████████▊| 8078/8192 [00:23<00:00, 363.34it/s]
Adding requests:  99%|█████████▉| 8115/8192 [00:23<00:00, 357.66it/s]
Adding requests:  99%|█████████▉| 8151/8192 [00:23<00:00, 351.08it/s]
Adding requests: 100%|█████████▉| 8187/8192 [00:23<00:00, 346.60it/s]
Adding requests: 100%|██████████| 8192/8192 [00:23<00:00, 349.91it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  21%|██▏       | 1748/8192 [00:00<00:00, 7574.32it/s, est. speed input: 7756397.02 toks/s, output: 7574.41 toks/s]
Processed prompts:  31%|███       | 2506/8192 [00:09<00:26, 212.35it/s, est. speed input: 272962.51 toks/s, output: 266.56 toks/s]   
Processed prompts:  34%|███▍      | 2823/8192 [00:13<00:33, 159.52it/s, est. speed input: 213313.10 toks/s, output: 208.31 toks/s]
Processed prompts:  37%|███▋      | 2999/8192 [00:16<00:37, 137.24it/s, est. speed input: 191363.26 toks/s, output: 186.88 toks/s]
Processed prompts:  38%|███▊      | 3109/8192 [00:17<00:41, 123.42it/s, est. speed input: 179653.85 toks/s, output: 175.44 toks/s]
Processed prompts:  39%|███▉      | 3182/8192 [00:18<00:42, 119.10it/s, est. speed input: 175618.09 toks/s, output: 171.50 toks/s]
Processed prompts:  39%|███▉      | 3233/8192 [00:19<00:44, 110.55it/s, est. speed input: 170752.00 toks/s, output: 166.75 toks/s]
Processed prompts:  40%|████      | 3284/8192 [00:20<00:48, 101.90it/s, est. speed input: 166270.75 toks/s, output: 162.37 toks/s]
Processed prompts:  41%|████      | 3348/8192 [00:21<00:50, 96.79it/s, est. speed input: 162762.89 toks/s, output: 158.95 toks/s] 
Processed prompts:  42%|████▏     | 3412/8192 [00:21<00:51, 92.29it/s, est. speed input: 159537.82 toks/s, output: 155.80 toks/s]
Processed prompts:  42%|████▏     | 3476/8192 [00:22<00:53, 88.48it/s, est. speed input: 156547.17 toks/s, output: 152.88 toks/s]
Processed prompts:  43%|████▎     | 3540/8192 [00:23<00:54, 85.45it/s, est. speed input: 153779.31 toks/s, output: 150.18 toks/s]
Processed prompts:  44%|████▍     | 3604/8192 [00:24<00:55, 83.06it/s, est. speed input: 151194.29 toks/s, output: 147.65 toks/s]
Processed prompts:  45%|████▍     | 3668/8192 [00:25<00:55, 81.52it/s, est. speed input: 148841.09 toks/s, output: 145.35 toks/s]
Processed prompts:  46%|████▌     | 3732/8192 [00:26<00:55, 80.04it/s, est. speed input: 146567.65 toks/s, output: 143.13 toks/s]
Processed prompts:  46%|████▋     | 3796/8192 [00:26<00:55, 79.02it/s, est. speed input: 144446.96 toks/s, output: 141.06 toks/s]
Processed prompts:  47%|████▋     | 3860/8192 [00:27<00:55, 78.17it/s, est. speed input: 142430.94 toks/s, output: 139.09 toks/s]
Processed prompts:  48%|████▊     | 3924/8192 [00:28<00:54, 77.92it/s, est. speed input: 140598.67 toks/s, output: 137.30 toks/s]
Processed prompts:  49%|████▊     | 3988/8192 [00:29<00:53, 77.90it/s, est. speed input: 138897.26 toks/s, output: 135.64 toks/s]
Processed prompts:  49%|████▉     | 4052/8192 [00:30<00:53, 77.79it/s, est. speed input: 137272.14 toks/s, output: 134.05 toks/s]
Processed prompts:  50%|█████     | 4116/8192 [00:31<00:52, 77.42it/s, est. speed input: 135687.75 toks/s, output: 132.51 toks/s]
Processed prompts:  51%|█████     | 4180/8192 [00:31<00:52, 77.02it/s, est. speed input: 134164.22 toks/s, output: 131.02 toks/s]
Processed prompts:  52%|█████▏    | 4244/8192 [00:32<00:51, 76.85it/s, est. speed input: 132735.93 toks/s, output: 129.62 toks/s]
Processed prompts:  53%|█████▎    | 4308/8192 [00:33<00:50, 76.61it/s, est. speed input: 131361.61 toks/s, output: 128.28 toks/s]
Processed prompts:  53%|█████▎    | 4372/8192 [00:34<00:49, 76.49it/s, est. speed input: 130060.54 toks/s, output: 127.01 toks/s]
Processed prompts:  54%|█████▍    | 4436/8192 [00:35<00:49, 76.54it/s, est. speed input: 128839.33 toks/s, output: 125.82 toks/s]
Processed prompts:  55%|█████▍    | 4500/8192 [00:36<00:48, 76.84it/s, est. speed input: 127708.84 toks/s, output: 124.72 toks/s]
Processed prompts:  56%|█████▌    | 4564/8192 [00:36<00:47, 76.92it/s, est. speed input: 126612.80 toks/s, output: 123.65 toks/s]
Processed prompts:  56%|█████▋    | 4628/8192 [00:37<00:46, 76.72it/s, est. speed input: 125533.99 toks/s, output: 122.59 toks/s]
Processed prompts:  57%|█████▋    | 4692/8192 [00:38<00:45, 76.66it/s, est. speed input: 124510.99 toks/s, output: 121.59 toks/s]
Processed prompts:  58%|█████▊    | 4756/8192 [00:39<00:44, 76.64it/s, est. speed input: 123534.03 toks/s, output: 120.64 toks/s]
Processed prompts:  59%|█████▉    | 4820/8192 [00:40<00:44, 76.56it/s, est. speed input: 122591.09 toks/s, output: 119.72 toks/s]
Processed prompts:  60%|█████▉    | 4884/8192 [00:41<00:43, 76.82it/s, est. speed input: 121720.10 toks/s, output: 118.87 toks/s]
Processed prompts:  60%|██████    | 4948/8192 [00:41<00:42, 76.87it/s, est. speed input: 120869.60 toks/s, output: 118.04 toks/s]
Processed prompts:  61%|██████    | 5012/8192 [00:42<00:41, 76.60it/s, est. speed input: 120021.38 toks/s, output: 117.21 toks/s]
Processed prompts:  62%|██████▏   | 5076/8192 [00:43<00:40, 76.58it/s, est. speed input: 119221.87 toks/s, output: 116.43 toks/s]
Processed prompts:  63%|██████▎   | 5140/8192 [00:44<00:39, 76.47it/s, est. speed input: 118443.63 toks/s, output: 115.67 toks/s]
Processed prompts:  64%|██████▎   | 5204/8192 [00:45<00:38, 76.78it/s, est. speed input: 117731.67 toks/s, output: 114.97 toks/s]
Processed prompts:  64%|██████▍   | 5268/8192 [00:46<00:38, 76.94it/s, est. speed input: 117039.01 toks/s, output: 114.30 toks/s]
Processed prompts:  65%|██████▌   | 5332/8192 [00:46<00:37, 76.84it/s, est. speed input: 116351.50 toks/s, output: 113.62 toks/s]
Processed prompts:  66%|██████▌   | 5396/8192 [00:47<00:36, 76.67it/s, est. speed input: 115679.81 toks/s, output: 112.97 toks/s]
Processed prompts:  67%|██████▋   | 5460/8192 [00:48<00:35, 76.87it/s, est. speed input: 115058.93 toks/s, output: 112.36 toks/s]
Processed prompts:  67%|██████▋   | 5524/8192 [00:49<00:34, 77.03it/s, est. speed input: 114459.56 toks/s, output: 111.78 toks/s]
Processed prompts:  68%|██████▊   | 5588/8192 [00:50<00:33, 76.93it/s, est. speed input: 113863.10 toks/s, output: 111.19 toks/s]
Processed prompts:  69%|██████▉   | 5652/8192 [00:51<00:33, 76.68it/s, est. speed input: 113271.68 toks/s, output: 110.62 toks/s]
Processed prompts:  70%|██████▉   | 5716/8192 [00:51<00:32, 76.58it/s, est. speed input: 112705.61 toks/s, output: 110.06 toks/s]
Processed prompts:  71%|███████   | 5780/8192 [00:52<00:31, 76.48it/s, est. speed input: 112154.27 toks/s, output: 109.53 toks/s]
Processed prompts:  71%|███████▏  | 5844/8192 [00:53<00:30, 76.42it/s, est. speed input: 111621.54 toks/s, output: 109.01 toks/s]
Processed prompts:  72%|███████▏  | 5908/8192 [00:54<00:29, 76.72it/s, est. speed input: 111130.75 toks/s, output: 108.53 toks/s]
Processed prompts:  73%|███████▎  | 5972/8192 [00:55<00:28, 76.89it/s, est. speed input: 110651.24 toks/s, output: 108.06 toks/s]
Processed prompts:  74%|███████▎  | 6036/8192 [00:56<00:28, 76.58it/s, est. speed input: 110155.78 toks/s, output: 107.57 toks/s]
Processed prompts:  74%|███████▍  | 6100/8192 [00:56<00:27, 76.51it/s, est. speed input: 109685.07 toks/s, output: 107.11 toks/s]
Processed prompts:  75%|███████▌  | 6164/8192 [00:57<00:26, 76.40it/s, est. speed input: 109223.79 toks/s, output: 106.66 toks/s]
Processed prompts:  76%|███████▌  | 6228/8192 [00:58<00:25, 76.68it/s, est. speed input: 108800.19 toks/s, output: 106.25 toks/s]
Processed prompts:  77%|███████▋  | 6292/8192 [00:59<00:24, 76.51it/s, est. speed input: 108363.57 toks/s, output: 105.82 toks/s]
Processed prompts:  78%|███████▊  | 6356/8192 [01:00<00:23, 76.51it/s, est. speed input: 107946.89 toks/s, output: 105.42 toks/s]
Processed prompts:  78%|███████▊  | 6420/8192 [01:01<00:23, 76.44it/s, est. speed input: 107537.68 toks/s, output: 105.02 toks/s]
Processed prompts:  79%|███████▉  | 6484/8192 [01:01<00:22, 76.24it/s, est. speed input: 107129.51 toks/s, output: 104.62 toks/s]
Processed prompts:  80%|███████▉  | 6548/8192 [01:02<00:21, 76.35it/s, est. speed input: 106747.77 toks/s, output: 104.25 toks/s]
Processed prompts:  81%|████████  | 6612/8192 [01:03<00:20, 76.34it/s, est. speed input: 106370.97 toks/s, output: 103.88 toks/s]
Processed prompts:  81%|████████▏ | 6676/8192 [01:04<00:19, 76.25it/s, est. speed input: 105998.85 toks/s, output: 103.51 toks/s]
Processed prompts:  82%|████████▏ | 6740/8192 [01:05<00:19, 76.32it/s, est. speed input: 105644.72 toks/s, output: 103.17 toks/s]
Processed prompts:  83%|████████▎ | 6804/8192 [01:06<00:18, 76.38it/s, est. speed input: 105299.55 toks/s, output: 102.83 toks/s]
Processed prompts:  84%|████████▍ | 6868/8192 [01:07<00:17, 76.24it/s, est. speed input: 104953.07 toks/s, output: 102.49 toks/s]
Processed prompts:  85%|████████▍ | 6932/8192 [01:07<00:16, 76.26it/s, est. speed input: 104621.21 toks/s, output: 102.17 toks/s]
Processed prompts:  85%|████████▌ | 6996/8192 [01:08<00:15, 76.22it/s, est. speed input: 104295.09 toks/s, output: 101.85 toks/s]
Processed prompts:  86%|████████▌ | 7060/8192 [01:09<00:14, 76.58it/s, est. speed input: 103997.62 toks/s, output: 101.56 toks/s]
Processed prompts:  87%|████████▋ | 7124/8192 [01:10<00:13, 76.70it/s, est. speed input: 103700.25 toks/s, output: 101.27 toks/s]
Processed prompts:  88%|████████▊ | 7188/8192 [01:11<00:13, 76.55it/s, est. speed input: 103397.46 toks/s, output: 100.97 toks/s]
Processed prompts:  89%|████████▊ | 7252/8192 [01:12<00:12, 76.80it/s, est. speed input: 103119.89 toks/s, output: 100.70 toks/s]
Processed prompts:  89%|████████▉ | 7316/8192 [01:12<00:11, 76.63it/s, est. speed input: 102831.18 toks/s, output: 100.42 toks/s]
Processed prompts:  90%|█████████ | 7380/8192 [01:13<00:10, 76.61it/s, est. speed input: 102554.13 toks/s, output: 100.15 toks/s]
Processed prompts:  91%|█████████ | 7444/8192 [01:14<00:09, 76.83it/s, est. speed input: 102294.66 toks/s, output: 99.90 toks/s] 
Processed prompts:  92%|█████████▏| 7508/8192 [01:15<00:08, 76.69it/s, est. speed input: 102026.68 toks/s, output: 99.64 toks/s]
Processed prompts:  92%|█████████▏| 7572/8192 [01:16<00:08, 76.57it/s, est. speed input: 101763.68 toks/s, output: 99.38 toks/s]
Processed prompts:  93%|█████████▎| 7636/8192 [01:17<00:07, 76.75it/s, est. speed input: 101518.78 toks/s, output: 99.14 toks/s]
Processed prompts:  94%|█████████▍| 7700/8192 [01:17<00:06, 77.05it/s, est. speed input: 101287.12 toks/s, output: 98.91 toks/s]
Processed prompts:  95%|█████████▍| 7764/8192 [01:18<00:05, 76.81it/s, est. speed input: 101039.87 toks/s, output: 98.67 toks/s]
Processed prompts:  96%|█████████▌| 7828/8192 [01:19<00:04, 76.60it/s, est. speed input: 100795.53 toks/s, output: 98.43 toks/s]
Processed prompts:  96%|█████████▋| 7892/8192 [01:20<00:03, 76.44it/s, est. speed input: 100555.63 toks/s, output: 98.20 toks/s]
Processed prompts:  97%|█████████▋| 7956/8192 [01:21<00:03, 76.28it/s, est. speed input: 100318.94 toks/s, output: 97.97 toks/s]
Processed prompts:  98%|█████████▊| 8020/8192 [01:22<00:02, 76.31it/s, est. speed input: 100092.99 toks/s, output: 97.75 toks/s]
Processed prompts:  99%|█████████▊| 8084/8192 [01:22<00:01, 76.16it/s, est. speed input: 99864.23 toks/s, output: 97.52 toks/s] 
Processed prompts:  99%|█████████▉| 8148/8192 [01:23<00:00, 83.60it/s, est. speed input: 99942.16 toks/s, output: 97.60 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:23<00:00, 83.60it/s, est. speed input: 100481.22 toks/s, output: 98.13 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:23<00:00, 98.13it/s, est. speed input: 100481.22 toks/s, output: 98.13 toks/s]
[rank0]:[W126 17:35:02.619594731 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

