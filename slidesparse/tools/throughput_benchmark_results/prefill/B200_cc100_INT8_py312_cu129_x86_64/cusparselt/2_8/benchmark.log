
========== M=512 ==========
Time: 2026-01-26 13:44:25
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:44:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=520682) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=520682) WARNING 01-26 13:44:44 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=520682) WARNING 01-26 13:44:51 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 39.60 requests/s, 20312.70 total tokens/s, 39.60 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 13:44:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:44:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:44:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:44:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:44:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:44:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:44:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:44:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:44:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:44:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:44:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:44:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:44:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:44:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:44:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:44:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:44:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:44:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=520682) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=520682) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=520682) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=520682) 
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=520682) 2026-01-26 13:44:51,557 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=520682) 2026-01-26 13:44:51,578 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=520682) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.25it/s]
(EngineCore_DP0 pid=520682) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 21.81it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  59%|█████▊    | 75/128 [00:00<00:00, 749.15it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 760.62it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:22,  5.63it/s, est. speed input: 2884.83 toks/s, output: 5.63 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:05, 23.79it/s, est. speed input: 10491.96 toks/s, output: 20.49 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 31.98it/s, est. speed input: 13863.59 toks/s, output: 27.08 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 36.44it/s, est. speed input: 15772.66 toks/s, output: 30.81 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 39.08it/s, est. speed input: 16995.64 toks/s, output: 33.19 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 40.72it/s, est. speed input: 17840.58 toks/s, output: 34.84 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 41.76it/s, est. speed input: 18458.27 toks/s, output: 36.05 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 42.47it/s, est. speed input: 18934.15 toks/s, output: 36.98 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 42.99it/s, est. speed input: 19316.52 toks/s, output: 37.73 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:01, 43.35it/s, est. speed input: 19628.60 toks/s, output: 38.34 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 43.66it/s, est. speed input: 19893.97 toks/s, output: 38.86 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 43.89it/s, est. speed input: 20118.25 toks/s, output: 39.29 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 44.01it/s, est. speed input: 20306.41 toks/s, output: 39.66 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 44.19it/s, est. speed input: 20478.27 toks/s, output: 40.00 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 43.53it/s, est. speed input: 20550.36 toks/s, output: 40.14 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 43.74it/s, est. speed input: 20675.72 toks/s, output: 40.38 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 43.91it/s, est. speed input: 20789.44 toks/s, output: 40.60 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:00, 43.99it/s, est. speed input: 20887.40 toks/s, output: 40.80 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 43.61it/s, est. speed input: 20940.60 toks/s, output: 40.90 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 43.72it/s, est. speed input: 21017.37 toks/s, output: 41.05 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 43.82it/s, est. speed input: 21088.69 toks/s, output: 41.19 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 43.93it/s, est. speed input: 21156.30 toks/s, output: 41.32 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 44.01it/s, est. speed input: 21218.45 toks/s, output: 41.44 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 44.05it/s, est. speed input: 21274.39 toks/s, output: 41.55 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 44.14it/s, est. speed input: 21330.05 toks/s, output: 41.66 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 44.08it/s, est. speed input: 21374.38 toks/s, output: 41.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 44.08it/s, est. speed input: 21394.34 toks/s, output: 41.79 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.78it/s, est. speed input: 21394.34 toks/s, output: 41.79 toks/s]
[rank0]:[W126 13:44:57.737629466 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 13:44:59
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:45:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=521782) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=521782) WARNING 01-26 13:45:19 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=521782) WARNING 01-26 13:45:26 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 37.49 requests/s, 38426.89 total tokens/s, 37.49 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 13:45:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:45:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:45:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:45:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:45:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:45:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:45:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:45:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:45:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:45:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:45:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:45:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:45:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:45:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=521782) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=521782) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.47it/s]
(EngineCore_DP0 pid=521782) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.47it/s]
(EngineCore_DP0 pid=521782) 
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=521782) 2026-01-26 13:45:25,998 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=521782) 2026-01-26 13:45:26,019 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=521782) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 21.40it/s]
(EngineCore_DP0 pid=521782) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 21.00it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  32%|███▏      | 41/128 [00:00<00:00, 402.60it/s]
Adding requests:  68%|██████▊   | 87/128 [00:00<00:00, 432.14it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 406.79it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:01, 79.99it/s, est. speed input: 81914.69 toks/s, output: 79.99 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 52.12it/s, est. speed input: 56662.59 toks/s, output: 55.33 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 46.83it/s, est. speed input: 51611.17 toks/s, output: 50.40 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 44.54it/s, est. speed input: 49402.02 toks/s, output: 48.24 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 42.94it/s, est. speed input: 47871.45 toks/s, output: 46.75 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:02, 41.99it/s, est. speed input: 46849.21 toks/s, output: 45.75 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:00<00:02, 41.28it/s, est. speed input: 46053.68 toks/s, output: 44.97 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:01, 40.79it/s, est. speed input: 45436.71 toks/s, output: 44.37 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 40.54it/s, est. speed input: 44976.58 toks/s, output: 43.92 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 40.20it/s, est. speed input: 44546.26 toks/s, output: 43.50 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 39.96it/s, est. speed input: 44189.62 toks/s, output: 43.15 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 39.76it/s, est. speed input: 43927.45 toks/s, output: 42.90 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 39.68it/s, est. speed input: 43715.40 toks/s, output: 42.69 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 39.39it/s, est. speed input: 43476.22 toks/s, output: 42.46 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 39.38it/s, est. speed input: 43305.08 toks/s, output: 42.29 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:01, 39.43it/s, est. speed input: 43162.98 toks/s, output: 42.15 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 39.22it/s, est. speed input: 42990.09 toks/s, output: 41.98 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 39.43it/s, est. speed input: 42895.63 toks/s, output: 41.89 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 39.64it/s, est. speed input: 42794.04 toks/s, output: 41.79 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 39.71it/s, est. speed input: 42712.67 toks/s, output: 41.71 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 39.83it/s, est. speed input: 42631.03 toks/s, output: 41.63 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 39.82it/s, est. speed input: 42558.67 toks/s, output: 41.56 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 39.84it/s, est. speed input: 42497.16 toks/s, output: 41.50 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 39.83it/s, est. speed input: 42435.59 toks/s, output: 41.44 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 39.91it/s, est. speed input: 42376.77 toks/s, output: 41.38 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 39.87it/s, est. speed input: 42322.35 toks/s, output: 41.33 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.87it/s, est. speed input: 42307.59 toks/s, output: 41.32 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.31it/s, est. speed input: 42307.59 toks/s, output: 41.32 toks/s]
[rank0]:[W126 13:45:31.396389062 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 13:45:33
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:45:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=522833) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=522833) WARNING 01-26 13:45:53 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=522833) WARNING 01-26 13:46:00 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 75.18 requests/s, 77057.03 total tokens/s, 75.18 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 13:45:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:45:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:45:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:45:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:45:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:45:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:45:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:45:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:45:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:45:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:45:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:45:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:45:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:45:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:48] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=522833) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=522833) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.75it/s]
(EngineCore_DP0 pid=522833) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.75it/s]
(EngineCore_DP0 pid=522833) 
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=522833) 2026-01-26 13:46:00,154 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=522833) 2026-01-26 13:46:00,176 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=522833) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 13.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 12.79it/s]
(EngineCore_DP0 pid=522833) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.13it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:56,  4.53it/s]
Adding requests:  16%|█▌        | 40/256 [00:00<00:01, 155.99it/s]
Adding requests:  32%|███▏      | 83/256 [00:00<00:00, 254.08it/s]
Adding requests:  50%|████▉     | 127/256 [00:00<00:00, 315.93it/s]
Adding requests:  66%|██████▋   | 170/256 [00:00<00:00, 351.21it/s]
Adding requests:  84%|████████▎ | 214/256 [00:00<00:00, 377.31it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 313.57it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:00<00:00, 461.38it/s, est. speed input: 472532.12 toks/s, output: 461.40 toks/s]
Processed prompts:  38%|███▊      | 97/256 [00:00<00:01, 129.92it/s, est. speed input: 149666.83 toks/s, output: 146.16 toks/s]
Processed prompts:  47%|████▋     | 121/256 [00:00<00:01, 111.33it/s, est. speed input: 130009.80 toks/s, output: 126.96 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:01<00:01, 101.48it/s, est. speed input: 120753.86 toks/s, output: 117.92 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:01<00:01, 96.94it/s, est. speed input: 116280.09 toks/s, output: 113.55 toks/s] 
Processed prompts:  64%|██████▍   | 164/256 [00:01<00:00, 93.85it/s, est. speed input: 113315.14 toks/s, output: 110.66 toks/s]
Processed prompts:  68%|██████▊   | 175/256 [00:01<00:00, 93.33it/s, est. speed input: 111838.75 toks/s, output: 109.22 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:01<00:00, 88.70it/s, est. speed input: 109050.24 toks/s, output: 106.49 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:01<00:00, 87.38it/s, est. speed input: 107524.32 toks/s, output: 105.00 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:01<00:00, 86.25it/s, est. speed input: 106158.70 toks/s, output: 103.67 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:02<00:00, 85.34it/s, est. speed input: 104941.27 toks/s, output: 102.48 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:02<00:00, 84.73it/s, est. speed input: 103872.05 toks/s, output: 101.44 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 84.28it/s, est. speed input: 102913.15 toks/s, output: 100.50 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:02<00:00, 84.06it/s, est. speed input: 102069.66 toks/s, output: 99.68 toks/s] 
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 83.90it/s, est. speed input: 101302.70 toks/s, output: 98.93 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 83.90it/s, est. speed input: 101302.70 toks/s, output: 98.93 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 98.92it/s, est. speed input: 101302.70 toks/s, output: 98.93 toks/s]
[rank0]:[W126 13:46:05.851895891 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 13:46:07
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:46:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=523880) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=523880) WARNING 01-26 13:46:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=523880) WARNING 01-26 13:46:35 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 159.33 requests/s, 163312.06 total tokens/s, 159.33 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 13:46:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:46:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:46:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:46:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:46:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:46:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:46:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:46:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:46:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:46:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:46:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:46:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:46:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:46:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:46:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:46:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:46:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:46:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=523880) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=523880) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.15it/s]
(EngineCore_DP0 pid=523880) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.14it/s]
(EngineCore_DP0 pid=523880) 
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=523880) 2026-01-26 13:46:35,722 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=523880) 2026-01-26 13:46:35,743 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=523880) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 19.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 18.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 18.80it/s]
(EngineCore_DP0 pid=523880) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.40it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 34/512 [00:00<00:01, 334.42it/s]
Adding requests:  15%|█▌        | 78/512 [00:00<00:01, 395.01it/s]
Adding requests:  24%|██▍       | 123/512 [00:00<00:00, 418.48it/s]
Adding requests:  33%|███▎      | 167/512 [00:00<00:00, 425.51it/s]
Adding requests:  41%|████▏     | 212/512 [00:00<00:00, 433.67it/s]
Adding requests:  51%|█████     | 260/512 [00:00<00:00, 447.11it/s]
Adding requests:  60%|█████▉    | 305/512 [00:00<00:00, 445.64it/s]
Adding requests:  69%|██████▉   | 352/512 [00:00<00:00, 451.51it/s]
Adding requests:  78%|███████▊  | 400/512 [00:00<00:00, 457.65it/s]
Adding requests:  87%|████████▋ | 447/512 [00:01<00:00, 458.66it/s]
Adding requests:  96%|█████████▋| 494/512 [00:01<00:00, 460.57it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 443.93it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:00<00:00, 1755.94it/s, est. speed input: 1798214.97 toks/s, output: 1755.97 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:01<00:00, 273.75it/s, est. speed input: 321745.24 toks/s, output: 314.20 toks/s]   
Processed prompts:  86%|████████▌ | 441/512 [00:01<00:00, 237.93it/s, est. speed input: 281350.42 toks/s, output: 274.76 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:01<00:00, 212.97it/s, est. speed input: 258651.78 toks/s, output: 252.59 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 212.97it/s, est. speed input: 254638.24 toks/s, output: 248.67 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 248.66it/s, est. speed input: 254638.24 toks/s, output: 248.67 toks/s]
[rank0]:[W126 13:46:41.554749497 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 13:46:43
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:46:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=524991) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=524991) WARNING 01-26 13:47:07 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=524991) WARNING 01-26 13:47:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 302.44 requests/s, 309996.49 total tokens/s, 302.44 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 13:46:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:46:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:46:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:46:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:46:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:46:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:46:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:46:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:46:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:47:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:47:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:47:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:47:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:47:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:47:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:47:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:47:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:47:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=524991) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=524991) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=524991) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=524991) 
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=524991) 2026-01-26 13:47:13,940 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=524991) 2026-01-26 13:47:13,962 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=524991) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 22.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 20.59it/s]
(EngineCore_DP0 pid=524991) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 23.13it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 396.67it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 424.06it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 432.72it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:01, 434.56it/s]
Adding requests:  21%|██▏       | 220/1024 [00:00<00:01, 440.77it/s]
Adding requests:  26%|██▌       | 268/1024 [00:00<00:01, 452.34it/s]
Adding requests:  31%|███       | 314/1024 [00:00<00:01, 449.90it/s]
Adding requests:  35%|███▌      | 360/1024 [00:00<00:01, 452.26it/s]
Adding requests:  40%|███▉      | 407/1024 [00:00<00:01, 454.79it/s]
Adding requests:  44%|████▍     | 454/1024 [00:01<00:01, 458.18it/s]
Adding requests:  49%|████▉     | 500/1024 [00:01<00:01, 457.17it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 453.16it/s]
Adding requests:  58%|█████▊    | 595/1024 [00:01<00:00, 462.18it/s]
Adding requests:  63%|██████▎   | 643/1024 [00:01<00:00, 465.02it/s]
Adding requests:  68%|██████▊   | 692/1024 [00:01<00:00, 471.20it/s]
Adding requests:  72%|███████▏  | 740/1024 [00:01<00:00, 471.40it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:01<00:00, 469.31it/s]
Adding requests:  82%|████████▏ | 835/1024 [00:01<00:00, 457.98it/s]
Adding requests:  86%|████████▋ | 884/1024 [00:01<00:00, 465.02it/s]
Adding requests:  91%|█████████ | 933/1024 [00:02<00:00, 470.62it/s]
Adding requests:  96%|█████████▌| 981/1024 [00:02<00:00, 471.14it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 459.06it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:00<00:00, 6058.53it/s, est. speed input: 6204405.78 toks/s, output: 6058.65 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 6058.53it/s, est. speed input: 908717.52 toks/s, output: 887.42 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 887.33it/s, est. speed input: 908717.52 toks/s, output: 887.42 toks/s] 
[rank0]:[W126 13:47:19.120881552 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 13:47:21
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:47:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=526137) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=526137) WARNING 01-26 13:47:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=526137) WARNING 01-26 13:47:57 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 459.87 requests/s, 471368.34 total tokens/s, 459.87 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 13:47:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:47:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:47:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:47:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:47:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:47:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:47:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:47:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:47:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:47:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:47:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:47:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:47:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:47:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:47:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:47:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:47:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:47:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=526137) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=526137) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=526137) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=526137) 
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=526137) 2026-01-26 13:47:56,970 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=526137) 2026-01-26 13:47:56,993 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=526137) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 19.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 22.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 20.81it/s]
(EngineCore_DP0 pid=526137) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 19.64it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.48it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 40/2048 [00:00<00:05, 397.59it/s]
Adding requests:   4%|▍         | 85/2048 [00:00<00:04, 423.05it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:04, 429.75it/s]
Adding requests:   8%|▊         | 173/2048 [00:00<00:04, 431.90it/s]
Adding requests:  11%|█         | 219/2048 [00:00<00:04, 439.12it/s]
Adding requests:  13%|█▎        | 267/2048 [00:00<00:03, 449.89it/s]
Adding requests:  15%|█▌        | 312/2048 [00:00<00:03, 448.89it/s]
Adding requests:  17%|█▋        | 358/2048 [00:00<00:03, 451.66it/s]
Adding requests:  20%|█▉        | 405/2048 [00:00<00:03, 457.04it/s]
Adding requests:  22%|██▏       | 452/2048 [00:01<00:03, 460.89it/s]
Adding requests:  24%|██▍       | 499/2048 [00:01<00:03, 461.50it/s]
Adding requests:  27%|██▋       | 546/2048 [00:01<00:03, 455.33it/s]
Adding requests:  29%|██▉       | 594/2048 [00:01<00:03, 460.77it/s]
Adding requests:  31%|███▏      | 642/2048 [00:01<00:03, 464.87it/s]
Adding requests:  34%|███▎      | 689/2048 [00:01<00:02, 462.80it/s]
Adding requests:  36%|███▌      | 736/2048 [00:01<00:02, 462.40it/s]
Adding requests:  38%|███▊      | 783/2048 [00:01<00:02, 452.82it/s]
Adding requests:  40%|████      | 829/2048 [00:01<00:02, 441.01it/s]
Adding requests:  43%|████▎     | 874/2048 [00:01<00:02, 441.64it/s]
Adding requests:  45%|████▍     | 920/2048 [00:02<00:02, 444.82it/s]
Adding requests:  47%|████▋     | 966/2048 [00:02<00:02, 449.19it/s]
Adding requests:  50%|████▉     | 1014/2048 [00:02<00:02, 457.04it/s]
Adding requests:  52%|█████▏    | 1061/2048 [00:02<00:02, 459.29it/s]
Adding requests:  54%|█████▍    | 1107/2048 [00:02<00:02, 459.01it/s]
Adding requests:  56%|█████▋    | 1155/2048 [00:02<00:01, 463.82it/s]
Adding requests:  59%|█████▉    | 1206/2048 [00:02<00:01, 475.11it/s]
Adding requests:  61%|██████    | 1254/2048 [00:02<00:01, 472.33it/s]
Adding requests:  64%|██████▎   | 1302/2048 [00:02<00:01, 467.53it/s]
Adding requests:  66%|██████▌   | 1350/2048 [00:02<00:01, 470.96it/s]
Adding requests:  68%|██████▊   | 1400/2048 [00:03<00:01, 476.88it/s]
Adding requests:  71%|███████   | 1448/2048 [00:03<00:01, 475.01it/s]
Adding requests:  73%|███████▎  | 1498/2048 [00:03<00:01, 481.04it/s]
Adding requests:  76%|███████▌  | 1547/2048 [00:03<00:01, 480.55it/s]
Adding requests:  78%|███████▊  | 1597/2048 [00:03<00:00, 486.06it/s]
Adding requests:  80%|████████  | 1646/2048 [00:03<00:00, 477.66it/s]
Adding requests:  83%|████████▎ | 1694/2048 [00:03<00:00, 473.87it/s]
Adding requests:  85%|████████▌ | 1744/2048 [00:03<00:00, 476.64it/s]
Adding requests:  88%|████████▊ | 1792/2048 [00:03<00:00, 474.92it/s]
Adding requests:  90%|████████▉ | 1841/2048 [00:03<00:00, 476.76it/s]
Adding requests:  92%|█████████▏| 1889/2048 [00:04<00:00, 476.82it/s]
Adding requests:  95%|█████████▍| 1937/2048 [00:04<00:00, 477.23it/s]
Adding requests:  97%|█████████▋| 1985/2048 [00:04<00:00, 477.02it/s]
Adding requests:  99%|█████████▉| 2035/2048 [00:04<00:00, 480.10it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 463.65it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 58989.92it/s, est. speed input: 60430988.91 toks/s, output: 59007.34 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 58818.65it/s, est. speed input: 60430988.91 toks/s, output: 59007.34 toks/s]
[rank0]:[W126 13:48:03.129322754 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 13:48:05
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:48:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=527424) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=527424) WARNING 01-26 13:48:43 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=527424) WARNING 01-26 13:48:49 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 467.92 requests/s, 479616.98 total tokens/s, 467.92 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 13:48:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:48:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:48:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:48:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:48:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:48:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:48:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:48:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:48:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:48:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:48:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:48:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:48:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:48:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:48:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:48:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:48:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:48:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=527424) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=527424) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.14it/s]
(EngineCore_DP0 pid=527424) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.14it/s]
(EngineCore_DP0 pid=527424) 
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=527424) [rank0]:W0126 13:48:46.388000 527424 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=527424) [rank0]:W0126 13:48:46.457000 527424 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=527424) [rank0]:W0126 13:48:47.263000 527424 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=527424) [rank0]:W0126 13:48:47.360000 527424 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=527424) 2026-01-26 13:48:49,713 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=527424) 2026-01-26 13:48:49,737 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=527424) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 13.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 17.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 20.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 20.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 19.47it/s]
(EngineCore_DP0 pid=527424) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 23.00it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 23.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 23.10it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 41/4096 [00:00<00:10, 402.00it/s]
Adding requests:   2%|▏         | 86/4096 [00:00<00:09, 425.07it/s]
Adding requests:   3%|▎         | 131/4096 [00:00<00:09, 434.47it/s]
Adding requests:   4%|▍         | 176/4096 [00:00<00:08, 437.73it/s]
Adding requests:   5%|▌         | 223/4096 [00:00<00:08, 447.43it/s]
Adding requests:   7%|▋         | 271/4096 [00:00<00:08, 457.10it/s]
Adding requests:   8%|▊         | 317/4096 [00:00<00:08, 452.99it/s]
Adding requests:   9%|▉         | 365/4096 [00:00<00:08, 459.43it/s]
Adding requests:  10%|█         | 412/4096 [00:00<00:07, 462.38it/s]
Adding requests:  11%|█         | 460/4096 [00:01<00:07, 465.02it/s]
Adding requests:  12%|█▏        | 507/4096 [00:01<00:07, 465.58it/s]
Adding requests:  14%|█▎        | 554/4096 [00:01<00:07, 460.36it/s]
Adding requests:  15%|█▍        | 602/4096 [00:01<00:07, 464.31it/s]
Adding requests:  16%|█▌        | 652/4096 [00:01<00:07, 471.95it/s]
Adding requests:  17%|█▋        | 703/4096 [00:01<00:07, 481.02it/s]
Adding requests:  18%|█▊        | 752/4096 [00:01<00:06, 478.23it/s]
Adding requests:  20%|█▉        | 800/4096 [00:01<00:07, 468.50it/s]
Adding requests:  21%|██        | 847/4096 [00:01<00:07, 461.74it/s]
Adding requests:  22%|██▏       | 897/4096 [00:01<00:06, 470.92it/s]
Adding requests:  23%|██▎       | 945/4096 [00:02<00:06, 473.21it/s]
Adding requests:  24%|██▍       | 994/4096 [00:02<00:06, 475.27it/s]
Adding requests:  25%|██▌       | 1043/4096 [00:02<00:06, 478.64it/s]
Adding requests:  27%|██▋       | 1091/4096 [00:02<00:06, 476.17it/s]
Adding requests:  28%|██▊       | 1139/4096 [00:02<00:06, 469.67it/s]
Adding requests:  29%|██▉       | 1191/4096 [00:02<00:06, 483.56it/s]
Adding requests:  30%|███       | 1240/4096 [00:02<00:05, 483.37it/s]
Adding requests:  31%|███▏      | 1289/4096 [00:02<00:05, 479.83it/s]
Adding requests:  33%|███▎      | 1339/4096 [00:02<00:05, 484.31it/s]
Adding requests:  34%|███▍      | 1388/4096 [00:02<00:05, 480.35it/s]
Adding requests:  35%|███▌      | 1437/4096 [00:03<00:05, 478.81it/s]
Adding requests:  36%|███▋      | 1487/4096 [00:03<00:05, 482.14it/s]
Adding requests:  38%|███▊      | 1537/4096 [00:03<00:05, 485.31it/s]
Adding requests:  39%|███▊      | 1586/4096 [00:03<00:05, 485.53it/s]
Adding requests:  40%|███▉      | 1637/4096 [00:03<00:05, 489.99it/s]
Adding requests:  41%|████      | 1687/4096 [00:03<00:04, 482.93it/s]
Adding requests:  42%|████▏     | 1736/4096 [00:03<00:04, 484.47it/s]
Adding requests:  44%|████▎     | 1785/4096 [00:03<00:04, 479.76it/s]
Adding requests:  45%|████▍     | 1833/4096 [00:03<00:04, 472.09it/s]
Adding requests:  46%|████▌     | 1881/4096 [00:03<00:04, 474.34it/s]
Adding requests:  47%|████▋     | 1929/4096 [00:04<00:04, 465.14it/s]
Adding requests:  48%|████▊     | 1977/4096 [00:04<00:04, 468.82it/s]
Adding requests:  49%|████▉     | 2026/4096 [00:04<00:04, 473.23it/s]
Adding requests:  51%|█████     | 2076/4096 [00:04<00:04, 479.42it/s]
Adding requests:  52%|█████▏    | 2124/4096 [00:04<00:04, 476.19it/s]
Adding requests:  53%|█████▎    | 2172/4096 [00:04<00:04, 472.96it/s]
Adding requests:  54%|█████▍    | 2220/4096 [00:04<00:03, 472.06it/s]
Adding requests:  55%|█████▌    | 2269/4096 [00:04<00:03, 474.53it/s]
Adding requests:  57%|█████▋    | 2319/4096 [00:04<00:03, 480.10it/s]
Adding requests:  58%|█████▊    | 2368/4096 [00:05<00:03, 477.01it/s]
Adding requests:  59%|█████▉    | 2416/4096 [00:05<00:03, 477.89it/s]
Adding requests:  60%|██████    | 2465/4096 [00:05<00:03, 480.02it/s]
Adding requests:  61%|██████▏   | 2514/4096 [00:05<00:03, 477.23it/s]
Adding requests:  63%|██████▎   | 2564/4096 [00:05<00:03, 483.75it/s]
Adding requests:  64%|██████▍   | 2613/4096 [00:05<00:03, 481.57it/s]
Adding requests:  65%|██████▌   | 2663/4096 [00:05<00:02, 485.81it/s]
Adding requests:  66%|██████▌   | 2712/4096 [00:05<00:02, 477.57it/s]
Adding requests:  67%|██████▋   | 2760/4096 [00:05<00:02, 478.08it/s]
Adding requests:  69%|██████▊   | 2808/4096 [00:05<00:02, 475.18it/s]
Adding requests:  70%|██████▉   | 2856/4096 [00:06<00:02, 475.33it/s]
Adding requests:  71%|███████   | 2905/4096 [00:06<00:02, 477.95it/s]
Adding requests:  72%|███████▏  | 2953/4096 [00:06<00:02, 474.93it/s]
Adding requests:  73%|███████▎  | 3001/4096 [00:06<00:02, 444.50it/s]
Adding requests:  74%|███████▍  | 3048/4096 [00:06<00:02, 450.93it/s]
Adding requests:  76%|███████▌  | 3095/4096 [00:06<00:02, 454.00it/s]
Adding requests:  77%|███████▋  | 3143/4096 [00:06<00:02, 457.01it/s]
Adding requests:  78%|███████▊  | 3191/4096 [00:06<00:01, 461.95it/s]
Adding requests:  79%|███████▉  | 3239/4096 [00:06<00:01, 467.15it/s]
Adding requests:  80%|████████  | 3287/4096 [00:06<00:01, 468.83it/s]
Adding requests:  81%|████████▏ | 3335/4096 [00:07<00:01, 470.16it/s]
Adding requests:  83%|████████▎ | 3383/4096 [00:07<00:01, 473.01it/s]
Adding requests:  84%|████████▍ | 3432/4096 [00:07<00:01, 475.90it/s]
Adding requests:  85%|████████▍ | 3480/4096 [00:07<00:01, 465.36it/s]
Adding requests:  86%|████████▌ | 3528/4096 [00:07<00:01, 468.21it/s]
Adding requests:  87%|████████▋ | 3576/4096 [00:07<00:01, 470.72it/s]
Adding requests:  88%|████████▊ | 3624/4096 [00:07<00:01, 468.62it/s]
Adding requests:  90%|████████▉ | 3672/4096 [00:07<00:00, 470.59it/s]
Adding requests:  91%|█████████ | 3720/4096 [00:07<00:00, 472.19it/s]
Adding requests:  92%|█████████▏| 3769/4096 [00:07<00:00, 476.87it/s]
Adding requests:  93%|█████████▎| 3819/4096 [00:08<00:00, 482.99it/s]
Adding requests:  94%|█████████▍| 3869/4096 [00:08<00:00, 486.35it/s]
Adding requests:  96%|█████████▌| 3918/4096 [00:08<00:00, 484.67it/s]
Adding requests:  97%|█████████▋| 3967/4096 [00:08<00:00, 483.85it/s]
Adding requests:  98%|█████████▊| 4016/4096 [00:08<00:00, 482.90it/s]
Adding requests:  99%|█████████▉| 4065/4096 [00:08<00:00, 476.72it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 472.45it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 49915.65it/s, est. speed input: 51122093.81 toks/s, output: 49921.31 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 49853.80it/s, est. speed input: 51122093.81 toks/s, output: 49921.31 toks/s]
[rank0]:[W126 13:49:01.368439142 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 13:49:02
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:49:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=529009) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=529009) WARNING 01-26 13:49:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=529009) WARNING 01-26 13:50:05 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 466.02 requests/s, 477670.73 total tokens/s, 466.02 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 13:49:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:49:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:49:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:49:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:49:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:49:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:49:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:49:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:49:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:49:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:49:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:49:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:49:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:49:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:49:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:49:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:49:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:49:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=529009) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=529009) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.16it/s]
(EngineCore_DP0 pid=529009) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.16it/s]
(EngineCore_DP0 pid=529009) 
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=529009) [rank0]:W0126 13:50:02.311000 529009 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=529009) [rank0]:W0126 13:50:02.379000 529009 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=529009) [rank0]:W0126 13:50:03.179000 529009 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=529009) [rank0]:W0126 13:50:03.276000 529009 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=529009) 2026-01-26 13:50:05,602 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=529009) 2026-01-26 13:50:05,625 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=529009) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:03,  4.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:03,  5.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:01,  9.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:00, 14.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:00, 14.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 14.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00, 17.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:01<00:00, 19.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 15.14it/s]
(EngineCore_DP0 pid=529009) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00, 23.02it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 23.55it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 23.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 23.66it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 40/8192 [00:00<00:20, 394.21it/s]
Adding requests:   1%|          | 85/8192 [00:00<00:19, 421.82it/s]
Adding requests:   2%|▏         | 129/8192 [00:00<00:18, 426.44it/s]
Adding requests:   2%|▏         | 173/8192 [00:00<00:18, 431.02it/s]
Adding requests:   3%|▎         | 218/8192 [00:00<00:18, 436.84it/s]
Adding requests:   3%|▎         | 266/8192 [00:00<00:17, 449.41it/s]
Adding requests:   4%|▍         | 311/8192 [00:00<00:17, 447.69it/s]
Adding requests:   4%|▍         | 357/8192 [00:00<00:17, 450.88it/s]
Adding requests:   5%|▍         | 404/8192 [00:00<00:17, 454.33it/s]
Adding requests:   5%|▌         | 450/8192 [00:01<00:17, 452.22it/s]
Adding requests:   6%|▌         | 497/8192 [00:01<00:16, 454.71it/s]
Adding requests:   7%|▋         | 543/8192 [00:01<00:17, 447.91it/s]
Adding requests:   7%|▋         | 592/8192 [00:01<00:16, 459.06it/s]
Adding requests:   8%|▊         | 639/8192 [00:01<00:16, 461.14it/s]
Adding requests:   8%|▊         | 688/8192 [00:01<00:16, 467.29it/s]
Adding requests:   9%|▉         | 737/8192 [00:01<00:15, 472.24it/s]
Adding requests:  10%|▉         | 785/8192 [00:01<00:15, 467.72it/s]
Adding requests:  10%|█         | 832/8192 [00:01<00:16, 457.19it/s]
Adding requests:  11%|█         | 880/8192 [00:01<00:15, 460.96it/s]
Adding requests:  11%|█▏        | 928/8192 [00:02<00:15, 466.19it/s]
Adding requests:  12%|█▏        | 975/8192 [00:02<00:15, 465.35it/s]
Adding requests:  12%|█▏        | 1023/8192 [00:02<00:15, 467.00it/s]
Adding requests:  13%|█▎        | 1070/8192 [00:02<00:15, 462.50it/s]
Adding requests:  14%|█▎        | 1117/8192 [00:02<00:15, 459.09it/s]
Adding requests:  14%|█▍        | 1166/8192 [00:02<00:15, 466.06it/s]
Adding requests:  15%|█▍        | 1215/8192 [00:02<00:14, 472.91it/s]
Adding requests:  15%|█▌        | 1263/8192 [00:02<00:14, 466.30it/s]
Adding requests:  16%|█▌        | 1310/8192 [00:02<00:14, 466.91it/s]
Adding requests:  17%|█▋        | 1358/8192 [00:02<00:14, 470.25it/s]
Adding requests:  17%|█▋        | 1407/8192 [00:03<00:14, 474.67it/s]
Adding requests:  18%|█▊        | 1455/8192 [00:03<00:14, 472.86it/s]
Adding requests:  18%|█▊        | 1503/8192 [00:03<00:14, 465.94it/s]
Adding requests:  19%|█▉        | 1551/8192 [00:03<00:14, 467.80it/s]
Adding requests:  20%|█▉        | 1601/8192 [00:03<00:13, 475.06it/s]
Adding requests:  20%|██        | 1649/8192 [00:03<00:13, 473.81it/s]
Adding requests:  21%|██        | 1697/8192 [00:03<00:13, 471.43it/s]
Adding requests:  21%|██▏       | 1745/8192 [00:03<00:13, 470.43it/s]
Adding requests:  22%|██▏       | 1794/8192 [00:03<00:13, 473.84it/s]
Adding requests:  22%|██▏       | 1842/8192 [00:03<00:13, 472.77it/s]
Adding requests:  23%|██▎       | 1890/8192 [00:04<00:13, 473.84it/s]
Adding requests:  24%|██▎       | 1938/8192 [00:04<00:13, 473.01it/s]
Adding requests:  24%|██▍       | 1986/8192 [00:04<00:13, 473.48it/s]
Adding requests:  25%|██▍       | 2035/8192 [00:04<00:12, 476.69it/s]
Adding requests:  25%|██▌       | 2084/8192 [00:04<00:12, 479.97it/s]
Adding requests:  26%|██▌       | 2133/8192 [00:04<00:12, 474.10it/s]
Adding requests:  27%|██▋       | 2181/8192 [00:04<00:12, 467.44it/s]
Adding requests:  27%|██▋       | 2230/8192 [00:04<00:12, 471.22it/s]
Adding requests:  28%|██▊       | 2278/8192 [00:04<00:12, 473.62it/s]
Adding requests:  28%|██▊       | 2326/8192 [00:05<00:12, 473.07it/s]
Adding requests:  29%|██▉       | 2374/8192 [00:05<00:12, 474.12it/s]
Adding requests:  30%|██▉       | 2422/8192 [00:05<00:12, 474.73it/s]
Adding requests:  30%|███       | 2470/8192 [00:05<00:12, 474.28it/s]
Adding requests:  31%|███       | 2518/8192 [00:05<00:12, 472.68it/s]
Adding requests:  31%|███▏      | 2568/8192 [00:05<00:11, 478.17it/s]
Adding requests:  32%|███▏      | 2616/8192 [00:05<00:11, 465.69it/s]
Adding requests:  33%|███▎      | 2665/8192 [00:05<00:11, 471.45it/s]
Adding requests:  33%|███▎      | 2713/8192 [00:05<00:11, 465.86it/s]
Adding requests:  34%|███▎      | 2761/8192 [00:05<00:11, 468.92it/s]
Adding requests:  34%|███▍      | 2808/8192 [00:06<00:11, 466.50it/s]
Adding requests:  35%|███▍      | 2856/8192 [00:06<00:11, 468.06it/s]
Adding requests:  35%|███▌      | 2905/8192 [00:06<00:11, 471.58it/s]
Adding requests:  36%|███▌      | 2953/8192 [00:06<00:11, 469.02it/s]
Adding requests:  37%|███▋      | 3000/8192 [00:06<00:11, 468.78it/s]
Adding requests:  37%|███▋      | 3048/8192 [00:06<00:10, 470.10it/s]
Adding requests:  38%|███▊      | 3096/8192 [00:06<00:10, 468.71it/s]
Adding requests:  38%|███▊      | 3143/8192 [00:06<00:10, 468.55it/s]
Adding requests:  39%|███▉      | 3191/8192 [00:06<00:10, 471.43it/s]
Adding requests:  40%|███▉      | 3240/8192 [00:06<00:10, 474.51it/s]
Adding requests:  40%|████      | 3288/8192 [00:07<00:10, 475.71it/s]
Adding requests:  41%|████      | 3336/8192 [00:07<00:10, 475.85it/s]
Adding requests:  41%|████▏     | 3385/8192 [00:07<00:10, 479.79it/s]
Adding requests:  42%|████▏     | 3434/8192 [00:07<00:09, 482.52it/s]
Adding requests:  43%|████▎     | 3483/8192 [00:07<00:10, 470.49it/s]
Adding requests:  43%|████▎     | 3531/8192 [00:07<00:09, 472.48it/s]
Adding requests:  44%|████▎     | 3579/8192 [00:07<00:09, 473.62it/s]
Adding requests:  44%|████▍     | 3627/8192 [00:07<00:09, 461.69it/s]
Adding requests:  45%|████▍     | 3674/8192 [00:07<00:09, 459.20it/s]
Adding requests:  45%|████▌     | 3720/8192 [00:07<00:09, 454.02it/s]
Adding requests:  46%|████▌     | 3766/8192 [00:08<00:09, 445.45it/s]
Adding requests:  47%|████▋     | 3813/8192 [00:08<00:09, 450.73it/s]
Adding requests:  47%|████▋     | 3860/8192 [00:08<00:09, 455.80it/s]
Adding requests:  48%|████▊     | 3906/8192 [00:08<00:09, 452.82it/s]
Adding requests:  48%|████▊     | 3952/8192 [00:08<00:09, 451.72it/s]
Adding requests:  49%|████▉     | 3998/8192 [00:08<00:09, 450.11it/s]
Adding requests:  49%|████▉     | 4044/8192 [00:08<00:09, 447.64it/s]
Adding requests:  50%|████▉     | 4090/8192 [00:08<00:09, 449.84it/s]
Adding requests:  50%|█████     | 4136/8192 [00:08<00:08, 452.70it/s]
Adding requests:  51%|█████     | 4182/8192 [00:09<00:08, 454.31it/s]
Adding requests:  52%|█████▏    | 4228/8192 [00:09<00:08, 453.53it/s]
Adding requests:  52%|█████▏    | 4274/8192 [00:09<00:08, 447.20it/s]
Adding requests:  53%|█████▎    | 4322/8192 [00:09<00:08, 456.45it/s]
Adding requests:  53%|█████▎    | 4372/8192 [00:09<00:08, 466.90it/s]
Adding requests:  54%|█████▍    | 4421/8192 [00:09<00:07, 471.86it/s]
Adding requests:  55%|█████▍    | 4471/8192 [00:09<00:07, 478.89it/s]
Adding requests:  55%|█████▌    | 4519/8192 [00:09<00:07, 469.51it/s]
Adding requests:  56%|█████▌    | 4568/8192 [00:09<00:07, 475.33it/s]
Adding requests:  56%|█████▋    | 4617/8192 [00:09<00:07, 478.63it/s]
Adding requests:  57%|█████▋    | 4666/8192 [00:10<00:07, 479.04it/s]
Adding requests:  58%|█████▊    | 4714/8192 [00:10<00:07, 477.37it/s]
Adding requests:  58%|█████▊    | 4763/8192 [00:10<00:07, 480.58it/s]
Adding requests:  59%|█████▊    | 4812/8192 [00:10<00:07, 479.42it/s]
Adding requests:  59%|█████▉    | 4860/8192 [00:10<00:06, 479.41it/s]
Adding requests:  60%|█████▉    | 4908/8192 [00:10<00:06, 476.00it/s]
Adding requests:  60%|██████    | 4956/8192 [00:10<00:06, 469.17it/s]
Adding requests:  61%|██████    | 5004/8192 [00:10<00:06, 471.11it/s]
Adding requests:  62%|██████▏   | 5053/8192 [00:10<00:06, 474.70it/s]
Adding requests:  62%|██████▏   | 5104/8192 [00:10<00:06, 482.13it/s]
Adding requests:  63%|██████▎   | 5153/8192 [00:11<00:06, 481.23it/s]
Adding requests:  64%|██████▎   | 5202/8192 [00:11<00:06, 481.13it/s]
Adding requests:  64%|██████▍   | 5251/8192 [00:11<00:06, 474.81it/s]
Adding requests:  65%|██████▍   | 5301/8192 [00:11<00:06, 481.00it/s]
Adding requests:  65%|██████▌   | 5350/8192 [00:11<00:05, 481.58it/s]
Adding requests:  66%|██████▌   | 5399/8192 [00:11<00:05, 482.48it/s]
Adding requests:  67%|██████▋   | 5448/8192 [00:11<00:05, 479.86it/s]
Adding requests:  67%|██████▋   | 5496/8192 [00:11<00:05, 475.98it/s]
Adding requests:  68%|██████▊   | 5544/8192 [00:11<00:05, 475.06it/s]
Adding requests:  68%|██████▊   | 5592/8192 [00:11<00:05, 474.69it/s]
Adding requests:  69%|██████▉   | 5640/8192 [00:12<00:05, 474.84it/s]
Adding requests:  69%|██████▉   | 5688/8192 [00:12<00:05, 472.50it/s]
Adding requests:  70%|███████   | 5736/8192 [00:12<00:05, 462.11it/s]
Adding requests:  71%|███████   | 5783/8192 [00:12<00:05, 462.21it/s]
Adding requests:  71%|███████   | 5830/8192 [00:12<00:05, 460.14it/s]
Adding requests:  72%|███████▏  | 5879/8192 [00:12<00:04, 467.29it/s]
Adding requests:  72%|███████▏  | 5927/8192 [00:12<00:04, 470.74it/s]
Adding requests:  73%|███████▎  | 5975/8192 [00:12<00:04, 467.32it/s]
Adding requests:  74%|███████▎  | 6024/8192 [00:12<00:04, 471.53it/s]
Adding requests:  74%|███████▍  | 6073/8192 [00:12<00:04, 475.27it/s]
Adding requests:  75%|███████▍  | 6121/8192 [00:13<00:04, 462.79it/s]
Adding requests:  75%|███████▌  | 6169/8192 [00:13<00:04, 465.87it/s]
Adding requests:  76%|███████▌  | 6220/8192 [00:13<00:04, 476.66it/s]
Adding requests:  77%|███████▋  | 6270/8192 [00:13<00:03, 482.12it/s]
Adding requests:  77%|███████▋  | 6320/8192 [00:13<00:03, 485.66it/s]
Adding requests:  78%|███████▊  | 6370/8192 [00:13<00:03, 487.74it/s]
Adding requests:  78%|███████▊  | 6419/8192 [00:13<00:03, 486.86it/s]
Adding requests:  79%|███████▉  | 6469/8192 [00:13<00:03, 490.26it/s]
Adding requests:  80%|███████▉  | 6520/8192 [00:13<00:03, 495.12it/s]
Adding requests:  80%|████████  | 6570/8192 [00:14<00:03, 493.04it/s]
Adding requests:  81%|████████  | 6620/8192 [00:14<00:03, 487.34it/s]
Adding requests:  81%|████████▏ | 6669/8192 [00:14<00:03, 485.56it/s]
Adding requests:  82%|████████▏ | 6718/8192 [00:14<00:03, 486.67it/s]
Adding requests:  83%|████████▎ | 6767/8192 [00:14<00:03, 474.21it/s]
Adding requests:  83%|████████▎ | 6817/8192 [00:14<00:02, 480.07it/s]
Adding requests:  84%|████████▍ | 6868/8192 [00:14<00:02, 486.93it/s]
Adding requests:  84%|████████▍ | 6918/8192 [00:14<00:02, 488.33it/s]
Adding requests:  85%|████████▌ | 6968/8192 [00:14<00:02, 491.13it/s]
Adding requests:  86%|████████▌ | 7018/8192 [00:14<00:02, 484.38it/s]
Adding requests:  86%|████████▋ | 7067/8192 [00:15<00:02, 484.63it/s]
Adding requests:  87%|████████▋ | 7117/8192 [00:15<00:02, 487.04it/s]
Adding requests:  87%|████████▋ | 7166/8192 [00:15<00:02, 481.80it/s]
Adding requests:  88%|████████▊ | 7216/8192 [00:15<00:02, 482.83it/s]
Adding requests:  89%|████████▊ | 7265/8192 [00:15<00:01, 476.61it/s]
Adding requests:  89%|████████▉ | 7313/8192 [00:15<00:01, 476.23it/s]
Adding requests:  90%|████████▉ | 7361/8192 [00:15<00:01, 474.42it/s]
Adding requests:  90%|█████████ | 7411/8192 [00:15<00:01, 481.20it/s]
Adding requests:  91%|█████████ | 7461/8192 [00:15<00:01, 484.53it/s]
Adding requests:  92%|█████████▏| 7510/8192 [00:15<00:01, 484.09it/s]
Adding requests:  92%|█████████▏| 7559/8192 [00:16<00:01, 483.47it/s]
Adding requests:  93%|█████████▎| 7608/8192 [00:16<00:01, 478.92it/s]
Adding requests:  93%|█████████▎| 7656/8192 [00:16<00:01, 468.59it/s]
Adding requests:  94%|█████████▍| 7705/8192 [00:16<00:01, 472.66it/s]
Adding requests:  95%|█████████▍| 7753/8192 [00:16<00:00, 472.79it/s]
Adding requests:  95%|█████████▌| 7801/8192 [00:16<00:00, 469.23it/s]
Adding requests:  96%|█████████▌| 7849/8192 [00:16<00:00, 470.45it/s]
Adding requests:  96%|█████████▋| 7898/8192 [00:16<00:00, 473.50it/s]
Adding requests:  97%|█████████▋| 7946/8192 [00:16<00:00, 468.19it/s]
Adding requests:  98%|█████████▊| 7994/8192 [00:16<00:00, 469.27it/s]
Adding requests:  98%|█████████▊| 8041/8192 [00:17<00:00, 467.64it/s]
Adding requests:  99%|█████████▉| 8091/8192 [00:17<00:00, 475.50it/s]
Adding requests:  99%|█████████▉| 8139/8192 [00:17<00:00, 476.21it/s]
Adding requests: 100%|█████████▉| 8189/8192 [00:17<00:00, 480.30it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 470.55it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  58%|█████▊    | 4766/8192 [00:00<00:00, 47608.85it/s, est. speed input: 48755645.85 toks/s, output: 47609.87 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 47608.85it/s, est. speed input: 50529389.84 toks/s, output: 49343.55 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 49309.13it/s, est. speed input: 50529389.84 toks/s, output: 49343.55 toks/s]
[rank0]:[W126 13:50:26.055403359 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 14:49:08
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:49:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=619982) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=619982) WARNING 01-26 14:49:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=619982) WARNING 01-26 14:49:40 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 34.10 requests/s, 17490.88 total tokens/s, 34.10 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 14:49:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:49:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:49:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:49:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:49:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:49:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:49:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:49:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:49:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:49:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:49:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:49:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:49:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:49:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:49:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:49:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:49:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:49:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:49:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:49:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:49:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:49:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:49:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:49:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:49:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:49:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:49:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:49:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=619982) [2026-01-26 14:49:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=619982) [2026-01-26 14:49:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=619982) [2026-01-26 14:49:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=619982) [2026-01-26 14:49:22] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=619982) [2026-01-26 14:49:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=619982) [2026-01-26 14:49:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=619982) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=619982) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.05it/s]
(EngineCore_DP0 pid=619982) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.05it/s]
(EngineCore_DP0 pid=619982) 
(EngineCore_DP0 pid=619982) [2026-01-26 14:49:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=619982) [2026-01-26 14:49:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=619982) [2026-01-26 14:49:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=619982) [2026-01-26 14:49:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8847360 bytes
(EngineCore_DP0 pid=619982) [2026-01-26 14:49:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=619982) [2026-01-26 14:49:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 47185920 bytes
(EngineCore_DP0 pid=619982) [2026-01-26 14:49:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=619982) [2026-01-26 14:49:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 23592960 bytes
(EngineCore_DP0 pid=619982) 2026-01-26 14:49:40,178 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=619982) 2026-01-26 14:49:40,201 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=619982) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.07it/s]
(EngineCore_DP0 pid=619982) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.77it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  55%|█████▌    | 71/128 [00:00<00:00, 701.85it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 326.10it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 51.69it/s, est. speed input: 26469.39 toks/s, output: 51.70 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:02, 42.19it/s, est. speed input: 22213.82 toks/s, output: 43.39 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 40.17it/s, est. speed input: 21250.54 toks/s, output: 41.50 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 39.24it/s, est. speed input: 20783.70 toks/s, output: 40.59 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 38.71it/s, est. speed input: 20522.65 toks/s, output: 40.08 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 38.37it/s, est. speed input: 20344.54 toks/s, output: 39.74 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 38.16it/s, est. speed input: 20214.09 toks/s, output: 39.48 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:02, 37.96it/s, est. speed input: 20101.35 toks/s, output: 39.26 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 37.90it/s, est. speed input: 20026.07 toks/s, output: 39.11 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 37.80it/s, est. speed input: 19955.25 toks/s, output: 38.97 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 37.73it/s, est. speed input: 19895.12 toks/s, output: 38.86 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 37.63it/s, est. speed input: 19838.38 toks/s, output: 38.75 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 37.60it/s, est. speed input: 19793.16 toks/s, output: 38.66 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 37.64it/s, est. speed input: 19762.36 toks/s, output: 38.60 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 37.64it/s, est. speed input: 19732.09 toks/s, output: 38.54 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 37.61it/s, est. speed input: 19702.43 toks/s, output: 38.48 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 37.57it/s, est. speed input: 19674.09 toks/s, output: 38.43 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 37.59it/s, est. speed input: 19652.73 toks/s, output: 38.38 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 37.65it/s, est. speed input: 19637.22 toks/s, output: 38.35 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 37.63it/s, est. speed input: 19618.86 toks/s, output: 38.32 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:01, 37.62it/s, est. speed input: 19601.84 toks/s, output: 38.28 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 37.60it/s, est. speed input: 19585.95 toks/s, output: 38.25 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 37.65it/s, est. speed input: 19575.45 toks/s, output: 38.23 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 37.71it/s, est. speed input: 19567.72 toks/s, output: 38.22 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 37.71it/s, est. speed input: 19557.56 toks/s, output: 38.20 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 37.74it/s, est. speed input: 19550.59 toks/s, output: 38.18 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 37.79it/s, est. speed input: 19545.41 toks/s, output: 38.17 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 37.74it/s, est. speed input: 19535.54 toks/s, output: 38.16 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 37.60it/s, est. speed input: 19520.37 toks/s, output: 38.13 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 37.50it/s, est. speed input: 19506.44 toks/s, output: 38.10 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.50it/s, est. speed input: 19501.98 toks/s, output: 38.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.09it/s, est. speed input: 19501.98 toks/s, output: 38.09 toks/s]
[rank0]:[W126 14:49:46.583036199 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 14:49:48
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:49:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=621177) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=621177) WARNING 01-26 14:50:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=621177) WARNING 01-26 14:50:20 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 34.83 requests/s, 35702.72 total tokens/s, 34.83 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 14:49:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:49:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:49:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:49:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:49:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:49:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:49:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:49:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:49:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:49:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:49:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:49:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:49:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:49:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:50:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:50:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:50:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:50:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:50:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:50:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:50:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:50:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:50:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:50:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:50:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:50:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:50:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:50:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=621177) [2026-01-26 14:50:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=621177) [2026-01-26 14:50:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=621177) [2026-01-26 14:50:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=621177) [2026-01-26 14:50:03] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=621177) [2026-01-26 14:50:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=621177) [2026-01-26 14:50:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=621177) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=621177) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.07it/s]
(EngineCore_DP0 pid=621177) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.07it/s]
(EngineCore_DP0 pid=621177) 
(EngineCore_DP0 pid=621177) [2026-01-26 14:50:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=621177) [2026-01-26 14:50:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=621177) [2026-01-26 14:50:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=621177) [2026-01-26 14:50:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8847360 bytes
(EngineCore_DP0 pid=621177) [2026-01-26 14:50:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=621177) [2026-01-26 14:50:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 47185920 bytes
(EngineCore_DP0 pid=621177) [2026-01-26 14:50:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=621177) [2026-01-26 14:50:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 23592960 bytes
(EngineCore_DP0 pid=621177) 2026-01-26 14:50:20,598 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=621177) 2026-01-26 14:50:20,622 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=621177) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.73it/s]
(EngineCore_DP0 pid=621177) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.08it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:00, 398.11it/s]
Adding requests:  67%|██████▋   | 86/128 [00:00<00:00, 428.78it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 260.54it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:00, 136.00it/s, est. speed input: 139277.57 toks/s, output: 136.01 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 53.47it/s, est. speed input: 60448.08 toks/s, output: 59.03 toks/s]   
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 46.74it/s, est. speed input: 53469.42 toks/s, output: 52.22 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 43.81it/s, est. speed input: 50569.16 toks/s, output: 49.38 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:01, 41.68it/s, est. speed input: 48526.06 toks/s, output: 47.39 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 40.43it/s, est. speed input: 47289.24 toks/s, output: 46.18 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 39.45it/s, est. speed input: 46293.12 toks/s, output: 45.21 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 38.66it/s, est. speed input: 45467.18 toks/s, output: 44.40 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 38.22it/s, est. speed input: 44923.44 toks/s, output: 43.87 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 37.84it/s, est. speed input: 44444.59 toks/s, output: 43.40 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 37.56it/s, est. speed input: 44030.39 toks/s, output: 43.00 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 37.35it/s, est. speed input: 43662.93 toks/s, output: 42.64 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:01, 37.19it/s, est. speed input: 43333.29 toks/s, output: 42.32 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 37.06it/s, est. speed input: 43037.51 toks/s, output: 42.03 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 36.96it/s, est. speed input: 42767.22 toks/s, output: 41.76 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 36.86it/s, est. speed input: 42519.57 toks/s, output: 41.52 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 36.89it/s, est. speed input: 42310.05 toks/s, output: 41.32 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 36.87it/s, est. speed input: 42112.76 toks/s, output: 41.13 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 36.84it/s, est. speed input: 41928.43 toks/s, output: 40.95 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 36.79it/s, est. speed input: 41755.14 toks/s, output: 40.78 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 36.77it/s, est. speed input: 41595.87 toks/s, output: 40.62 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 36.74it/s, est. speed input: 41447.91 toks/s, output: 40.48 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 36.77it/s, est. speed input: 41316.47 toks/s, output: 40.35 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.73it/s, est. speed input: 41186.15 toks/s, output: 40.22 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.73it/s, est. speed input: 41186.15 toks/s, output: 40.22 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.22it/s, est. speed input: 41186.15 toks/s, output: 40.22 toks/s]
[rank0]:[W126 14:50:25.159903261 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 14:50:27
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:50:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=622297) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=622297) WARNING 01-26 14:50:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=622297) WARNING 01-26 14:51:00 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 67.20 requests/s, 68875.28 total tokens/s, 67.20 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 14:50:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:50:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:50:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:50:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:50:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:50:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:50:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:50:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:50:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:50:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:50:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:50:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:50:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:50:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:50:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:50:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:50:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:50:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:50:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:50:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:50:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:50:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:50:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:50:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:50:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:50:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:50:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:50:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=622297) [2026-01-26 14:50:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=622297) [2026-01-26 14:50:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=622297) [2026-01-26 14:50:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=622297) [2026-01-26 14:50:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=622297) [2026-01-26 14:50:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=622297) [2026-01-26 14:50:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=622297) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=622297) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.08it/s]
(EngineCore_DP0 pid=622297) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.08it/s]
(EngineCore_DP0 pid=622297) 
(EngineCore_DP0 pid=622297) [2026-01-26 14:50:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=622297) [2026-01-26 14:50:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=622297) [2026-01-26 14:50:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=622297) [2026-01-26 14:50:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8847360 bytes
(EngineCore_DP0 pid=622297) [2026-01-26 14:50:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=622297) [2026-01-26 14:50:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 47185920 bytes
(EngineCore_DP0 pid=622297) [2026-01-26 14:50:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=622297) [2026-01-26 14:50:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 23592960 bytes
(EngineCore_DP0 pid=622297) 2026-01-26 14:51:00,220 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=622297) 2026-01-26 14:51:00,244 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=622297) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 17.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.86it/s]
(EngineCore_DP0 pid=622297) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.90it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.89it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<01:08,  3.73it/s]
Adding requests:  13%|█▎        | 34/256 [00:00<00:01, 116.33it/s]
Adding requests:  30%|███       | 78/256 [00:00<00:00, 223.12it/s]
Adding requests:  48%|████▊     | 123/256 [00:00<00:00, 294.42it/s]
Adding requests:  65%|██████▌   | 167/256 [00:00<00:00, 339.95it/s]
Adding requests:  83%|████████▎ | 212/256 [00:00<00:00, 372.39it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 295.90it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:00<00:00, 407.75it/s, est. speed input: 417563.89 toks/s, output: 407.76 toks/s]
Processed prompts:  33%|███▎      | 85/256 [00:00<00:01, 117.75it/s, est. speed input: 135550.51 toks/s, output: 132.37 toks/s]
Processed prompts:  42%|████▏     | 107/256 [00:00<00:01, 100.25it/s, est. speed input: 117046.91 toks/s, output: 114.30 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:01<00:01, 91.24it/s, est. speed input: 108599.37 toks/s, output: 106.05 toks/s] 
Processed prompts:  52%|█████▏    | 134/256 [00:01<00:01, 87.21it/s, est. speed input: 104650.04 toks/s, output: 102.20 toks/s]
Processed prompts:  57%|█████▋    | 145/256 [00:01<00:01, 86.07it/s, est. speed input: 102745.94 toks/s, output: 100.34 toks/s]
Processed prompts:  61%|██████    | 155/256 [00:01<00:01, 83.37it/s, est. speed input: 100541.92 toks/s, output: 98.19 toks/s] 
Processed prompts:  64%|██████▍   | 164/256 [00:01<00:01, 79.22it/s, est. speed input: 98064.31 toks/s, output: 95.77 toks/s] 
Processed prompts:  68%|██████▊   | 173/256 [00:01<00:01, 80.41it/s, est. speed input: 97387.89 toks/s, output: 95.11 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:01<00:00, 76.66it/s, est. speed input: 95446.15 toks/s, output: 93.21 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:02<00:00, 76.23it/s, est. speed input: 94474.83 toks/s, output: 92.26 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:02<00:00, 75.78it/s, est. speed input: 93574.62 toks/s, output: 91.38 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:02<00:00, 75.36it/s, est. speed input: 92744.61 toks/s, output: 90.57 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:02<00:00, 75.25it/s, est. speed input: 92027.34 toks/s, output: 89.87 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:02<00:00, 75.18it/s, est. speed input: 91374.66 toks/s, output: 89.23 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:02<00:00, 75.07it/s, est. speed input: 90766.56 toks/s, output: 88.64 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:02<00:00, 74.96it/s, est. speed input: 90199.26 toks/s, output: 88.09 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:02<00:00, 74.78it/s, est. speed input: 89661.61 toks/s, output: 87.56 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:02<00:00, 74.79it/s, est. speed input: 89182.65 toks/s, output: 87.09 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 74.79it/s, est. speed input: 89072.00 toks/s, output: 86.98 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 86.98it/s, est. speed input: 89072.00 toks/s, output: 86.98 toks/s]
[rank0]:[W126 14:51:05.115497260 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 14:51:07
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:51:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=623425) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=623425) WARNING 01-26 14:51:31 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=623425) WARNING 01-26 14:51:41 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 138.85 requests/s, 142323.46 total tokens/s, 138.85 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 14:51:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:51:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:51:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:51:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:51:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:51:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:51:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:51:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:51:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:51:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:51:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:51:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:51:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:51:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:51:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:51:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:51:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:51:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:51:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:51:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:51:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:51:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:51:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:51:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:51:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:51:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:51:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:51:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=623425) [2026-01-26 14:51:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=623425) [2026-01-26 14:51:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=623425) [2026-01-26 14:51:24] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=623425) [2026-01-26 14:51:24] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=623425) [2026-01-26 14:51:24] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=623425) [2026-01-26 14:51:24] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=623425) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=623425) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=623425) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=623425) 
(EngineCore_DP0 pid=623425) [2026-01-26 14:51:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=623425) [2026-01-26 14:51:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=623425) [2026-01-26 14:51:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=623425) [2026-01-26 14:51:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8847360 bytes
(EngineCore_DP0 pid=623425) [2026-01-26 14:51:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=623425) [2026-01-26 14:51:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 47185920 bytes
(EngineCore_DP0 pid=623425) [2026-01-26 14:51:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=623425) [2026-01-26 14:51:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 23592960 bytes
(EngineCore_DP0 pid=623425) 2026-01-26 14:51:41,147 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=623425) 2026-01-26 14:51:41,171 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=623425) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.43it/s]
(EngineCore_DP0 pid=623425) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 17.83it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 18.02it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 40/512 [00:00<00:01, 398.33it/s]
Adding requests:  17%|█▋        | 85/512 [00:00<00:01, 422.92it/s]
Adding requests:  25%|██▌       | 130/512 [00:00<00:00, 432.48it/s]
Adding requests:  34%|███▍      | 174/512 [00:00<00:00, 432.05it/s]
Adding requests:  43%|████▎     | 220/512 [00:00<00:00, 440.02it/s]
Adding requests:  52%|█████▏    | 268/512 [00:00<00:00, 450.71it/s]
Adding requests:  61%|██████▏   | 314/512 [00:00<00:00, 449.24it/s]
Adding requests:  70%|███████   | 360/512 [00:00<00:00, 450.70it/s]
Adding requests:  80%|███████▉  | 408/512 [00:00<00:00, 456.91it/s]
Adding requests:  89%|████████▉ | 455/512 [00:01<00:00, 458.66it/s]
Adding requests:  98%|█████████▊| 501/512 [00:01<00:00, 458.68it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 448.72it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  30%|███       | 154/512 [00:00<00:00, 1420.79it/s, est. speed input: 1454998.73 toks/s, output: 1420.82 toks/s]
Processed prompts:  58%|█████▊    | 297/512 [00:01<00:00, 245.14it/s, est. speed input: 288106.94 toks/s, output: 281.35 toks/s]   
Processed prompts:  71%|███████▏  | 365/512 [00:01<00:00, 204.98it/s, est. speed input: 244661.79 toks/s, output: 238.93 toks/s]
Processed prompts:  80%|███████▉  | 409/512 [00:01<00:00, 189.96it/s, est. speed input: 229478.59 toks/s, output: 224.10 toks/s]
Processed prompts:  86%|████████▌ | 441/512 [00:02<00:00, 181.10it/s, est. speed input: 221319.32 toks/s, output: 216.13 toks/s]
Processed prompts:  91%|█████████ | 467/512 [00:02<00:00, 169.80it/s, est. speed input: 213450.20 toks/s, output: 208.45 toks/s]
Processed prompts:  96%|█████████▌| 489/512 [00:02<00:00, 168.66it/s, est. speed input: 210824.97 toks/s, output: 205.88 toks/s]
Processed prompts:  99%|█████████▉| 509/512 [00:02<00:00, 164.54it/s, est. speed input: 207644.97 toks/s, output: 202.78 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 164.54it/s, est. speed input: 205987.98 toks/s, output: 201.16 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 201.15it/s, est. speed input: 205987.98 toks/s, output: 201.16 toks/s]
[rank0]:[W126 14:51:46.208741551 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 14:51:48
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:51:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=624580) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=624580) WARNING 01-26 14:52:15 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=624580) WARNING 01-26 14:52:24 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 211.30 requests/s, 216584.21 total tokens/s, 211.30 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 14:51:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:51:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:51:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:51:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:51:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:51:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:51:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:51:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:51:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:51:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:51:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:51:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:51:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:51:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:52:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:52:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:52:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:52:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:52:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:52:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:52:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:52:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:52:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:52:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:52:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:52:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:52:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:52:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=624580) [2026-01-26 14:52:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=624580) [2026-01-26 14:52:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=624580) [2026-01-26 14:52:07] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=624580) [2026-01-26 14:52:07] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=624580) [2026-01-26 14:52:07] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=624580) [2026-01-26 14:52:07] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=624580) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=624580) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=624580) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=624580) 
(EngineCore_DP0 pid=624580) [2026-01-26 14:52:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=624580) [2026-01-26 14:52:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=624580) [2026-01-26 14:52:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=624580) [2026-01-26 14:52:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8847360 bytes
(EngineCore_DP0 pid=624580) [2026-01-26 14:52:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=624580) [2026-01-26 14:52:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 47185920 bytes
(EngineCore_DP0 pid=624580) [2026-01-26 14:52:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=624580) [2026-01-26 14:52:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 23592960 bytes
(EngineCore_DP0 pid=624580) 2026-01-26 14:52:24,597 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=624580) 2026-01-26 14:52:24,622 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=624580) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 16.04it/s]
(EngineCore_DP0 pid=624580) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 18.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.35it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.29it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 41/1024 [00:00<00:02, 399.98it/s]
Adding requests:   8%|▊         | 86/1024 [00:00<00:02, 427.28it/s]
Adding requests:  13%|█▎        | 131/1024 [00:00<00:02, 436.10it/s]
Adding requests:  17%|█▋        | 175/1024 [00:00<00:01, 436.51it/s]
Adding requests:  22%|██▏       | 221/1024 [00:00<00:01, 443.82it/s]
Adding requests:  26%|██▋       | 269/1024 [00:00<00:01, 455.65it/s]
Adding requests:  31%|███       | 315/1024 [00:00<00:01, 450.70it/s]
Adding requests:  35%|███▌      | 362/1024 [00:00<00:01, 456.49it/s]
Adding requests:  40%|███▉      | 408/1024 [00:00<00:01, 457.56it/s]
Adding requests:  45%|████▍     | 456/1024 [00:01<00:01, 461.52it/s]
Adding requests:  49%|████▉     | 503/1024 [00:01<00:01, 461.52it/s]
Adding requests:  54%|█████▎    | 550/1024 [00:01<00:01, 456.06it/s]
Adding requests:  58%|█████▊    | 598/1024 [00:01<00:00, 462.69it/s]
Adding requests:  63%|██████▎   | 646/1024 [00:01<00:00, 466.87it/s]
Adding requests:  68%|██████▊   | 695/1024 [00:01<00:00, 472.40it/s]
Adding requests:  73%|███████▎  | 743/1024 [00:01<00:00, 471.95it/s]
Adding requests:  77%|███████▋  | 791/1024 [00:01<00:00, 469.52it/s]
Adding requests:  82%|████████▏ | 838/1024 [00:01<00:00, 460.81it/s]
Adding requests:  87%|████████▋ | 887/1024 [00:01<00:00, 467.73it/s]
Adding requests:  91%|█████████▏| 935/1024 [00:02<00:00, 471.17it/s]
Adding requests:  96%|█████████▌| 983/1024 [00:02<00:00, 472.73it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 461.26it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:00<00:00, 4207.09it/s, est. speed input: 4308319.45 toks/s, output: 4207.17 toks/s]
Processed prompts:  87%|████████▋ | 895/1024 [00:02<00:00, 376.68it/s, est. speed input: 450957.04 toks/s, output: 440.39 toks/s]   
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 376.68it/s, est. speed input: 399475.85 toks/s, output: 390.11 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 390.10it/s, est. speed input: 399475.85 toks/s, output: 390.11 toks/s]
[rank0]:[W126 14:52:31.110455511 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 14:52:33
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:52:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=625813) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=625813) WARNING 01-26 14:53:05 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=625813) WARNING 01-26 14:53:15 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 234.03 requests/s, 239879.16 total tokens/s, 234.03 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 14:52:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:52:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:52:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:52:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:52:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:52:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:52:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:52:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:52:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:52:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:52:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:52:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:52:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:52:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:52:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:52:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:52:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:52:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:52:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:52:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:52:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:52:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:52:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:52:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:52:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:52:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:52:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:52:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=625813) [2026-01-26 14:52:57] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=625813) [2026-01-26 14:52:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=625813) [2026-01-26 14:52:57] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=625813) [2026-01-26 14:52:57] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=625813) [2026-01-26 14:52:57] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=625813) [2026-01-26 14:52:57] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=625813) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=625813) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=625813) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=625813) 
(EngineCore_DP0 pid=625813) [2026-01-26 14:52:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=625813) [2026-01-26 14:52:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=625813) [2026-01-26 14:52:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=625813) [2026-01-26 14:52:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8847360 bytes
(EngineCore_DP0 pid=625813) [2026-01-26 14:52:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=625813) [2026-01-26 14:52:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 47185920 bytes
(EngineCore_DP0 pid=625813) [2026-01-26 14:52:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=625813) [2026-01-26 14:52:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 23592960 bytes
(EngineCore_DP0 pid=625813) 2026-01-26 14:53:15,024 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=625813) 2026-01-26 14:53:15,048 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=625813) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 15.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 17.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 17.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 16.38it/s]
(EngineCore_DP0 pid=625813) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 18.30it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 15.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 15.70it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 40/2048 [00:00<00:05, 396.26it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 421.54it/s]
Adding requests:   6%|▋         | 128/2048 [00:00<00:04, 428.84it/s]
Adding requests:   8%|▊         | 172/2048 [00:00<00:04, 429.57it/s]
Adding requests:  11%|█         | 217/2048 [00:00<00:04, 435.31it/s]
Adding requests:  13%|█▎        | 264/2048 [00:00<00:03, 446.64it/s]
Adding requests:  15%|█▌        | 309/2048 [00:00<00:03, 446.61it/s]
Adding requests:  17%|█▋        | 356/2048 [00:00<00:03, 450.51it/s]
Adding requests:  20%|█▉        | 403/2048 [00:00<00:03, 455.42it/s]
Adding requests:  22%|██▏       | 450/2048 [00:01<00:03, 456.56it/s]
Adding requests:  24%|██▍       | 497/2048 [00:01<00:03, 458.35it/s]
Adding requests:  27%|██▋       | 543/2048 [00:01<00:03, 450.94it/s]
Adding requests:  29%|██▉       | 592/2048 [00:01<00:03, 461.70it/s]
Adding requests:  31%|███       | 639/2048 [00:01<00:03, 459.11it/s]
Adding requests:  34%|███▎      | 687/2048 [00:01<00:02, 464.87it/s]
Adding requests:  36%|███▌      | 736/2048 [00:01<00:02, 470.21it/s]
Adding requests:  38%|███▊      | 784/2048 [00:01<00:02, 465.73it/s]
Adding requests:  41%|████      | 831/2048 [00:01<00:02, 456.78it/s]
Adding requests:  43%|████▎     | 879/2048 [00:01<00:02, 462.10it/s]
Adding requests:  45%|████▌     | 926/2048 [00:02<00:02, 462.31it/s]
Adding requests:  48%|████▊     | 974/2048 [00:02<00:02, 465.90it/s]
Adding requests:  50%|████▉     | 1022/2048 [00:02<00:02, 468.69it/s]
Adding requests:  52%|█████▏    | 1069/2048 [00:02<00:02, 468.21it/s]
Adding requests:  54%|█████▍    | 1116/2048 [00:02<00:02, 462.17it/s]
Adding requests:  57%|█████▋    | 1164/2048 [00:02<00:01, 466.88it/s]
Adding requests:  59%|█████▉    | 1214/2048 [00:02<00:01, 475.21it/s]
Adding requests:  62%|██████▏   | 1262/2048 [00:02<00:01, 467.89it/s]
Adding requests:  64%|██████▍   | 1310/2048 [00:02<00:01, 469.55it/s]
Adding requests:  66%|██████▋   | 1358/2048 [00:02<00:01, 469.57it/s]
Adding requests:  69%|██████▊   | 1405/2048 [00:03<00:01, 466.34it/s]
Adding requests:  71%|███████   | 1452/2048 [00:03<00:01, 450.86it/s]
Adding requests:  73%|███████▎  | 1500/2048 [00:03<00:01, 459.28it/s]
Adding requests:  76%|███████▌  | 1547/2048 [00:03<00:01, 461.14it/s]
Adding requests:  78%|███████▊  | 1597/2048 [00:03<00:00, 469.88it/s]
Adding requests:  80%|████████  | 1645/2048 [00:03<00:00, 472.64it/s]
Adding requests:  83%|████████▎ | 1693/2048 [00:03<00:00, 469.82it/s]
Adding requests:  85%|████████▌ | 1741/2048 [00:03<00:00, 461.35it/s]
Adding requests:  87%|████████▋ | 1788/2048 [00:03<00:00, 459.59it/s]
Adding requests:  90%|████████▉ | 1836/2048 [00:03<00:00, 462.77it/s]
Adding requests:  92%|█████████▏| 1883/2048 [00:04<00:00, 464.64it/s]
Adding requests:  94%|█████████▍| 1930/2048 [00:04<00:00, 464.45it/s]
Adding requests:  97%|█████████▋| 1978/2048 [00:04<00:00, 466.76it/s]
Adding requests:  99%|█████████▉| 2026/2048 [00:04<00:00, 469.56it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 460.74it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:00<00:00, 6981.71it/s, est. speed input: 7149633.40 toks/s, output: 6981.82 toks/s]
Processed prompts:  85%|████████▌ | 1741/2048 [00:03<00:00, 478.43it/s, est. speed input: 588311.09 toks/s, output: 574.52 toks/s]   
Processed prompts: 100%|█████████▉| 2043/2048 [00:04<00:00, 387.01it/s, est. speed input: 486020.32 toks/s, output: 474.63 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:04<00:00, 387.01it/s, est. speed input: 487194.15 toks/s, output: 475.78 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:04<00:00, 475.76it/s, est. speed input: 487194.15 toks/s, output: 475.78 toks/s]
[rank0]:[W126 14:53:26.411650443 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 14:53:28
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:53:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=627235) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=627235) WARNING 01-26 14:54:08 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=627235) WARNING 01-26 14:54:18 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 243.83 requests/s, 249925.67 total tokens/s, 243.83 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 14:53:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:53:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:53:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:53:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:53:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:53:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:53:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:53:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:53:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:53:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:53:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:53:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:53:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:53:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:53:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:53:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:53:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:53:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:53:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:53:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:53:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:53:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:53:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:53:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:53:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:53:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:53:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:53:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=627235) [2026-01-26 14:54:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=627235) [2026-01-26 14:54:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=627235) [2026-01-26 14:54:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=627235) [2026-01-26 14:54:00] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=627235) [2026-01-26 14:54:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=627235) [2026-01-26 14:54:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=627235) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=627235) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=627235) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=627235) 
(EngineCore_DP0 pid=627235) [2026-01-26 14:54:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=627235) [2026-01-26 14:54:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=627235) [2026-01-26 14:54:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=627235) [2026-01-26 14:54:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8847360 bytes
(EngineCore_DP0 pid=627235) [2026-01-26 14:54:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=627235) [2026-01-26 14:54:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 47185920 bytes
(EngineCore_DP0 pid=627235) [2026-01-26 14:54:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=627235) [2026-01-26 14:54:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 23592960 bytes
(EngineCore_DP0 pid=627235) [rank0]:W0126 14:54:13.435000 627235 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=627235) [rank0]:W0126 14:54:13.522000 627235 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=627235) [rank0]:W0126 14:54:14.429000 627235 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=627235) [rank0]:W0126 14:54:14.550000 627235 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=627235) 2026-01-26 14:54:17,897 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=627235) 2026-01-26 14:54:17,923 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=627235) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 15.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 16.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 16.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 17.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 17.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 16.41it/s]
(EngineCore_DP0 pid=627235) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 18.03it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.12it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 16.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 17.28it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 41/4096 [00:00<00:10, 400.53it/s]
Adding requests:   2%|▏         | 86/4096 [00:00<00:09, 423.66it/s]
Adding requests:   3%|▎         | 131/4096 [00:00<00:09, 434.43it/s]
Adding requests:   4%|▍         | 175/4096 [00:00<00:08, 436.54it/s]
Adding requests:   5%|▌         | 221/4096 [00:00<00:08, 442.89it/s]
Adding requests:   7%|▋         | 269/4096 [00:00<00:08, 454.97it/s]
Adding requests:   8%|▊         | 315/4096 [00:00<00:08, 451.18it/s]
Adding requests:   9%|▉         | 363/4096 [00:00<00:08, 458.07it/s]
Adding requests:  10%|█         | 410/4096 [00:00<00:07, 460.96it/s]
Adding requests:  11%|█         | 458/4096 [00:01<00:07, 464.20it/s]
Adding requests:  12%|█▏        | 505/4096 [00:01<00:07, 462.44it/s]
Adding requests:  13%|█▎        | 552/4096 [00:01<00:07, 458.77it/s]
Adding requests:  15%|█▍        | 600/4096 [00:01<00:07, 462.97it/s]
Adding requests:  16%|█▌        | 649/4096 [00:01<00:07, 470.74it/s]
Adding requests:  17%|█▋        | 698/4096 [00:01<00:07, 476.40it/s]
Adding requests:  18%|█▊        | 746/4096 [00:01<00:07, 475.92it/s]
Adding requests:  19%|█▉        | 794/4096 [00:01<00:06, 473.86it/s]
Adding requests:  21%|██        | 842/4096 [00:01<00:06, 464.89it/s]
Adding requests:  22%|██▏       | 892/4096 [00:01<00:06, 473.42it/s]
Adding requests:  23%|██▎       | 940/4096 [00:02<00:06, 474.00it/s]
Adding requests:  24%|██▍       | 989/4096 [00:02<00:06, 477.01it/s]
Adding requests:  25%|██▌       | 1037/4096 [00:02<00:06, 469.05it/s]
Adding requests:  26%|██▋       | 1084/4096 [00:02<00:06, 468.37it/s]
Adding requests:  28%|██▊       | 1131/4096 [00:02<00:06, 466.54it/s]
Adding requests:  29%|██▉       | 1178/4096 [00:02<00:06, 467.29it/s]
Adding requests:  30%|██▉       | 1227/4096 [00:02<00:06, 472.34it/s]
Adding requests:  31%|███       | 1275/4096 [00:02<00:06, 469.85it/s]
Adding requests:  32%|███▏      | 1324/4096 [00:02<00:05, 474.92it/s]
Adding requests:  34%|███▎      | 1373/4096 [00:02<00:05, 477.38it/s]
Adding requests:  35%|███▍      | 1422/4096 [00:03<00:05, 478.17it/s]
Adding requests:  36%|███▌      | 1471/4096 [00:03<00:05, 480.49it/s]
Adding requests:  37%|███▋      | 1521/4096 [00:03<00:05, 482.74it/s]
Adding requests:  38%|███▊      | 1570/4096 [00:03<00:05, 484.17it/s]
Adding requests:  40%|███▉      | 1620/4096 [00:03<00:05, 486.33it/s]
Adding requests:  41%|████      | 1669/4096 [00:03<00:05, 483.60it/s]
Adding requests:  42%|████▏     | 1718/4096 [00:03<00:04, 484.44it/s]
Adding requests:  43%|████▎     | 1767/4096 [00:03<00:04, 479.89it/s]
Adding requests:  44%|████▍     | 1816/4096 [00:03<00:04, 482.70it/s]
Adding requests:  46%|████▌     | 1865/4096 [00:03<00:04, 479.17it/s]
Adding requests:  47%|████▋     | 1914/4096 [00:04<00:04, 480.78it/s]
Adding requests:  48%|████▊     | 1963/4096 [00:04<00:04, 480.92it/s]
Adding requests:  49%|████▉     | 2013/4096 [00:04<00:04, 484.41it/s]
Adding requests:  50%|█████     | 2062/4096 [00:04<00:04, 484.51it/s]
Adding requests:  52%|█████▏    | 2111/4096 [00:04<00:04, 485.23it/s]
Adding requests:  53%|█████▎    | 2160/4096 [00:04<00:04, 465.74it/s]
Adding requests:  54%|█████▍    | 2207/4096 [00:04<00:04, 449.88it/s]
Adding requests:  55%|█████▌    | 2256/4096 [00:04<00:04, 459.51it/s]
Adding requests:  56%|█████▋    | 2304/4096 [00:04<00:03, 465.15it/s]
Adding requests:  57%|█████▋    | 2352/4096 [00:05<00:03, 468.62it/s]
Adding requests:  59%|█████▊    | 2400/4096 [00:05<00:03, 470.56it/s]
Adding requests:  60%|█████▉    | 2448/4096 [00:05<00:03, 472.12it/s]
Adding requests:  61%|██████    | 2497/4096 [00:05<00:03, 476.27it/s]
Adding requests:  62%|██████▏   | 2545/4096 [00:05<00:03, 475.73it/s]
Adding requests:  63%|██████▎   | 2594/4096 [00:05<00:03, 477.39it/s]
Adding requests:  65%|██████▍   | 2643/4096 [00:05<00:03, 480.94it/s]
Adding requests:  66%|██████▌   | 2692/4096 [00:05<00:02, 478.19it/s]
Adding requests:  67%|██████▋   | 2740/4096 [00:05<00:02, 477.66it/s]
Adding requests:  68%|██████▊   | 2788/4096 [00:05<00:02, 475.10it/s]
Adding requests:  69%|██████▉   | 2836/4096 [00:06<00:02, 473.46it/s]
Adding requests:  70%|███████   | 2885/4096 [00:06<00:02, 478.17it/s]
Adding requests:  72%|███████▏  | 2933/4096 [00:06<00:02, 473.42it/s]
Adding requests:  73%|███████▎  | 2982/4096 [00:06<00:02, 475.26it/s]
Adding requests:  74%|███████▍  | 3030/4096 [00:06<00:02, 474.85it/s]
Adding requests:  75%|███████▌  | 3078/4096 [00:06<00:02, 473.38it/s]
Adding requests:  76%|███████▋  | 3127/4096 [00:06<00:02, 477.74it/s]
Adding requests:  78%|███████▊  | 3175/4096 [00:06<00:01, 476.51it/s]
Adding requests:  79%|███████▊  | 3223/4096 [00:06<00:01, 475.66it/s]
Adding requests:  80%|███████▉  | 3271/4096 [00:06<00:01, 474.81it/s]
Adding requests:  81%|████████  | 3320/4096 [00:07<00:01, 476.71it/s]
Adding requests:  82%|████████▏ | 3369/4096 [00:07<00:01, 479.59it/s]
Adding requests:  83%|████████▎ | 3417/4096 [00:07<00:01, 468.01it/s]
Adding requests:  85%|████████▍ | 3464/4096 [00:07<00:01, 465.14it/s]
Adding requests:  86%|████████▌ | 3512/4096 [00:07<00:01, 469.46it/s]
Adding requests:  87%|████████▋ | 3560/4096 [00:07<00:01, 469.94it/s]
Adding requests:  88%|████████▊ | 3608/4096 [00:07<00:01, 470.46it/s]
Adding requests:  89%|████████▉ | 3656/4096 [00:07<00:00, 466.59it/s]
Adding requests:  90%|█████████ | 3705/4096 [00:07<00:00, 471.73it/s]
Adding requests:  92%|█████████▏| 3753/4096 [00:07<00:00, 470.93it/s]
Adding requests:  93%|█████████▎| 3803/4096 [00:08<00:00, 477.67it/s]
Adding requests:  94%|█████████▍| 3852/4096 [00:08<00:00, 480.07it/s]
Adding requests:  95%|█████████▌| 3901/4096 [00:08<00:00, 480.80it/s]
Adding requests:  96%|█████████▋| 3950/4096 [00:08<00:00, 479.95it/s]
Adding requests:  98%|█████████▊| 3999/4096 [00:08<00:00, 478.14it/s]
Adding requests:  99%|█████████▉| 4047/4096 [00:08<00:00, 474.33it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 478.20it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 472.02it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  51%|█████▏    | 2102/4096 [00:00<00:00, 15596.40it/s, est. speed input: 15973398.83 toks/s, output: 15596.73 toks/s]
Processed prompts:  89%|████████▉ | 3662/4096 [00:06<00:00, 478.71it/s, est. speed input: 588419.37 toks/s, output: 574.63 toks/s]      
Processed prompts: 100%|██████████| 4096/4096 [00:08<00:00, 478.71it/s, est. speed input: 516605.57 toks/s, output: 504.50 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:08<00:00, 504.49it/s, est. speed input: 516605.57 toks/s, output: 504.50 toks/s]
[rank0]:[W126 14:54:37.806594110 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 14:54:39
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:55:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=629008) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=629008) WARNING 01-26 14:55:38 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=629008) WARNING 01-26 14:55:48 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 248.59 requests/s, 254803.01 total tokens/s, 248.59 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 14:55:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:55:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:55:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:55:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:55:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:55:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:55:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:55:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:55:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:55:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:55:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:55:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:55:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:55:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:55:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:55:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:55:29] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:55:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:55:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:55:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:55:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:55:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:55:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:55:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:55:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:55:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:55:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:55:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=629008) [2026-01-26 14:55:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=629008) [2026-01-26 14:55:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=629008) [2026-01-26 14:55:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=629008) [2026-01-26 14:55:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=629008) [2026-01-26 14:55:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=629008) [2026-01-26 14:55:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=629008) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=629008) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=629008) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.27it/s]
(EngineCore_DP0 pid=629008) 
(EngineCore_DP0 pid=629008) [2026-01-26 14:55:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=629008) [2026-01-26 14:55:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=629008) [2026-01-26 14:55:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=629008) [2026-01-26 14:55:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8847360 bytes
(EngineCore_DP0 pid=629008) [2026-01-26 14:55:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=629008) [2026-01-26 14:55:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 47185920 bytes
(EngineCore_DP0 pid=629008) [2026-01-26 14:55:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=629008) [2026-01-26 14:55:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 23592960 bytes
(EngineCore_DP0 pid=629008) [rank0]:W0126 14:55:43.585000 629008 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=629008) [rank0]:W0126 14:55:43.672000 629008 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=629008) [rank0]:W0126 14:55:44.601000 629008 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=629008) [rank0]:W0126 14:55:44.726000 629008 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=629008) 2026-01-26 14:55:48,140 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=629008) 2026-01-26 14:55:48,167 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=629008) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:06,  2.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:06,  2.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:02,  5.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:01,  7.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  9.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:00, 11.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00, 12.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00, 13.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00, 15.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:01<00:00, 15.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 10.47it/s]
(EngineCore_DP0 pid=629008) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 14.52it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 15.42it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 15.85it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 16.77it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 17.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 16.75it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 40/8192 [00:00<00:20, 397.36it/s]
Adding requests:   1%|          | 85/8192 [00:00<00:18, 427.17it/s]
Adding requests:   2%|▏         | 130/8192 [00:00<00:18, 434.22it/s]
Adding requests:   2%|▏         | 175/8192 [00:00<00:18, 436.84it/s]
Adding requests:   3%|▎         | 221/8192 [00:00<00:17, 443.18it/s]
Adding requests:   3%|▎         | 270/8192 [00:00<00:17, 455.74it/s]
Adding requests:   4%|▍         | 316/8192 [00:00<00:17, 452.81it/s]
Adding requests:   4%|▍         | 364/8192 [00:00<00:17, 459.42it/s]
Adding requests:   5%|▌         | 411/8192 [00:00<00:16, 459.16it/s]
Adding requests:   6%|▌         | 459/8192 [00:01<00:16, 464.18it/s]
Adding requests:   6%|▌         | 506/8192 [00:01<00:16, 464.47it/s]
Adding requests:   7%|▋         | 553/8192 [00:01<00:16, 459.96it/s]
Adding requests:   7%|▋         | 601/8192 [00:01<00:16, 463.68it/s]
Adding requests:   8%|▊         | 651/8192 [00:01<00:15, 472.08it/s]
Adding requests:   9%|▊         | 701/8192 [00:01<00:15, 479.75it/s]
Adding requests:   9%|▉         | 749/8192 [00:01<00:15, 477.28it/s]
Adding requests:  10%|▉         | 797/8192 [00:01<00:15, 475.54it/s]
Adding requests:  10%|█         | 845/8192 [00:01<00:15, 465.59it/s]
Adding requests:  11%|█         | 895/8192 [00:01<00:15, 474.59it/s]
Adding requests:  12%|█▏        | 943/8192 [00:02<00:15, 474.87it/s]
Adding requests:  12%|█▏        | 992/8192 [00:02<00:15, 478.54it/s]
Adding requests:  13%|█▎        | 1041/8192 [00:02<00:14, 479.29it/s]
Adding requests:  13%|█▎        | 1089/8192 [00:02<00:14, 476.95it/s]
Adding requests:  14%|█▍        | 1137/8192 [00:02<00:14, 472.19it/s]
Adding requests:  14%|█▍        | 1186/8192 [00:02<00:14, 476.01it/s]
Adding requests:  15%|█▌        | 1234/8192 [00:02<00:14, 472.29it/s]
Adding requests:  16%|█▌        | 1282/8192 [00:02<00:14, 469.36it/s]
Adding requests:  16%|█▌        | 1330/8192 [00:02<00:14, 472.25it/s]
Adding requests:  17%|█▋        | 1379/8192 [00:02<00:14, 475.30it/s]
Adding requests:  17%|█▋        | 1428/8192 [00:03<00:14, 476.80it/s]
Adding requests:  18%|█▊        | 1477/8192 [00:03<00:14, 479.29it/s]
Adding requests:  19%|█▊        | 1525/8192 [00:03<00:14, 468.19it/s]
Adding requests:  19%|█▉        | 1573/8192 [00:03<00:14, 471.57it/s]
Adding requests:  20%|█▉        | 1622/8192 [00:03<00:13, 476.70it/s]
Adding requests:  20%|██        | 1670/8192 [00:03<00:13, 474.56it/s]
Adding requests:  21%|██        | 1719/8192 [00:03<00:13, 476.51it/s]
Adding requests:  22%|██▏       | 1767/8192 [00:03<00:13, 473.41it/s]
Adding requests:  22%|██▏       | 1816/8192 [00:03<00:13, 477.02it/s]
Adding requests:  23%|██▎       | 1864/8192 [00:03<00:13, 472.67it/s]
Adding requests:  23%|██▎       | 1913/8192 [00:04<00:13, 475.30it/s]
Adding requests:  24%|██▍       | 1961/8192 [00:04<00:13, 476.21it/s]
Adding requests:  25%|██▍       | 2009/8192 [00:04<00:13, 471.73it/s]
Adding requests:  25%|██▌       | 2057/8192 [00:04<00:13, 468.05it/s]
Adding requests:  26%|██▌       | 2107/8192 [00:04<00:12, 475.98it/s]
Adding requests:  26%|██▋       | 2155/8192 [00:04<00:12, 469.89it/s]
Adding requests:  27%|██▋       | 2203/8192 [00:04<00:12, 468.49it/s]
Adding requests:  28%|██▊       | 2253/8192 [00:04<00:12, 476.82it/s]
Adding requests:  28%|██▊       | 2301/8192 [00:04<00:12, 476.12it/s]
Adding requests:  29%|██▊       | 2350/8192 [00:05<00:12, 478.37it/s]
Adding requests:  29%|██▉       | 2398/8192 [00:05<00:12, 478.08it/s]
Adding requests:  30%|██▉       | 2446/8192 [00:05<00:12, 477.75it/s]
Adding requests:  30%|███       | 2495/8192 [00:05<00:11, 480.12it/s]
Adding requests:  31%|███       | 2544/8192 [00:05<00:11, 480.39it/s]
Adding requests:  32%|███▏      | 2593/8192 [00:05<00:11, 482.74it/s]
Adding requests:  32%|███▏      | 2643/8192 [00:05<00:11, 485.33it/s]
Adding requests:  33%|███▎      | 2692/8192 [00:05<00:11, 470.36it/s]
Adding requests:  33%|███▎      | 2740/8192 [00:05<00:11, 471.61it/s]
Adding requests:  34%|███▍      | 2788/8192 [00:05<00:11, 471.54it/s]
Adding requests:  35%|███▍      | 2836/8192 [00:06<00:11, 468.76it/s]
Adding requests:  35%|███▌      | 2885/8192 [00:06<00:11, 474.19it/s]
Adding requests:  36%|███▌      | 2933/8192 [00:06<00:11, 470.34it/s]
Adding requests:  36%|███▋      | 2982/8192 [00:06<00:11, 472.62it/s]
Adding requests:  37%|███▋      | 3030/8192 [00:06<00:10, 472.39it/s]
Adding requests:  38%|███▊      | 3078/8192 [00:06<00:11, 454.17it/s]
Adding requests:  38%|███▊      | 3126/8192 [00:06<00:10, 460.92it/s]
Adding requests:  39%|███▊      | 3173/8192 [00:06<00:10, 461.50it/s]
Adding requests:  39%|███▉      | 3221/8192 [00:06<00:10, 464.33it/s]
Adding requests:  40%|███▉      | 3270/8192 [00:06<00:10, 470.62it/s]
Adding requests:  41%|████      | 3318/8192 [00:07<00:10, 471.73it/s]
Adding requests:  41%|████      | 3368/8192 [00:07<00:10, 478.04it/s]
Adding requests:  42%|████▏     | 3416/8192 [00:07<00:09, 477.96it/s]
Adding requests:  42%|████▏     | 3464/8192 [00:07<00:10, 471.34it/s]
Adding requests:  43%|████▎     | 3512/8192 [00:07<00:09, 473.37it/s]
Adding requests:  43%|████▎     | 3560/8192 [00:07<00:09, 472.18it/s]
Adding requests:  44%|████▍     | 3608/8192 [00:07<00:09, 472.43it/s]
Adding requests:  45%|████▍     | 3656/8192 [00:07<00:09, 468.29it/s]
Adding requests:  45%|████▌     | 3705/8192 [00:07<00:09, 473.56it/s]
Adding requests:  46%|████▌     | 3753/8192 [00:07<00:09, 470.23it/s]
Adding requests:  46%|████▋     | 3803/8192 [00:08<00:09, 476.99it/s]
Adding requests:  47%|████▋     | 3852/8192 [00:08<00:09, 480.01it/s]
Adding requests:  48%|████▊     | 3901/8192 [00:08<00:08, 479.64it/s]
Adding requests:  48%|████▊     | 3949/8192 [00:08<00:09, 466.31it/s]
Adding requests:  49%|████▉     | 3996/8192 [00:08<00:08, 467.31it/s]
Adding requests:  49%|████▉     | 4044/8192 [00:08<00:08, 468.56it/s]
Adding requests:  50%|████▉     | 4093/8192 [00:08<00:08, 472.85it/s]
Adding requests:  51%|█████     | 4142/8192 [00:08<00:08, 475.43it/s]
Adding requests:  51%|█████     | 4190/8192 [00:08<00:08, 473.06it/s]
Adding requests:  52%|█████▏    | 4238/8192 [00:09<00:08, 475.02it/s]
Adding requests:  52%|█████▏    | 4286/8192 [00:09<00:08, 475.87it/s]
Adding requests:  53%|█████▎    | 4336/8192 [00:09<00:07, 482.28it/s]
Adding requests:  54%|█████▎    | 4386/8192 [00:09<00:07, 484.41it/s]
Adding requests:  54%|█████▍    | 4435/8192 [00:09<00:07, 485.46it/s]
Adding requests:  55%|█████▍    | 4484/8192 [00:09<00:07, 481.85it/s]
Adding requests:  55%|█████▌    | 4533/8192 [00:09<00:07, 479.70it/s]
Adding requests:  56%|█████▌    | 4582/8192 [00:09<00:07, 480.33it/s]
Adding requests:  57%|█████▋    | 4632/8192 [00:09<00:07, 484.18it/s]
Adding requests:  57%|█████▋    | 4681/8192 [00:09<00:07, 482.25it/s]
Adding requests:  58%|█████▊    | 4730/8192 [00:10<00:07, 484.04it/s]
Adding requests:  58%|█████▊    | 4779/8192 [00:10<00:07, 483.97it/s]
Adding requests:  59%|█████▉    | 4828/8192 [00:10<00:06, 482.42it/s]
Adding requests:  60%|█████▉    | 4877/8192 [00:10<00:06, 480.71it/s]
Adding requests:  60%|██████    | 4926/8192 [00:10<00:06, 481.52it/s]
Adding requests:  61%|██████    | 4975/8192 [00:10<00:06, 482.79it/s]
Adding requests:  61%|██████▏   | 5024/8192 [00:10<00:06, 483.59it/s]
Adding requests:  62%|██████▏   | 5074/8192 [00:10<00:06, 486.11it/s]
Adding requests:  63%|██████▎   | 5124/8192 [00:10<00:06, 489.09it/s]
Adding requests:  63%|██████▎   | 5173/8192 [00:10<00:06, 489.03it/s]
Adding requests:  64%|██████▎   | 5222/8192 [00:11<00:06, 472.53it/s]
Adding requests:  64%|██████▍   | 5270/8192 [00:11<00:06, 473.63it/s]
Adding requests:  65%|██████▍   | 5320/8192 [00:11<00:05, 479.60it/s]
Adding requests:  66%|██████▌   | 5369/8192 [00:11<00:05, 481.18it/s]
Adding requests:  66%|██████▌   | 5418/8192 [00:11<00:05, 481.55it/s]
Adding requests:  67%|██████▋   | 5467/8192 [00:11<00:05, 478.38it/s]
Adding requests:  67%|██████▋   | 5515/8192 [00:11<00:05, 474.15it/s]
Adding requests:  68%|██████▊   | 5563/8192 [00:11<00:05, 473.78it/s]
Adding requests:  68%|██████▊   | 5611/8192 [00:11<00:05, 475.56it/s]
Adding requests:  69%|██████▉   | 5659/8192 [00:11<00:05, 470.05it/s]
Adding requests:  70%|██████▉   | 5709/8192 [00:12<00:05, 476.86it/s]
Adding requests:  70%|███████   | 5759/8192 [00:12<00:05, 480.75it/s]
Adding requests:  71%|███████   | 5808/8192 [00:12<00:04, 479.02it/s]
Adding requests:  71%|███████▏  | 5856/8192 [00:12<00:04, 479.18it/s]
Adding requests:  72%|███████▏  | 5906/8192 [00:12<00:04, 483.28it/s]
Adding requests:  73%|███████▎  | 5955/8192 [00:12<00:04, 482.82it/s]
Adding requests:  73%|███████▎  | 6005/8192 [00:12<00:04, 486.75it/s]
Adding requests:  74%|███████▍  | 6056/8192 [00:12<00:04, 491.31it/s]
Adding requests:  75%|███████▍  | 6106/8192 [00:12<00:04, 483.77it/s]
Adding requests:  75%|███████▌  | 6155/8192 [00:12<00:04, 480.69it/s]
Adding requests:  76%|███████▌  | 6205/8192 [00:13<00:04, 486.33it/s]
Adding requests:  76%|███████▋  | 6256/8192 [00:13<00:03, 491.89it/s]
Adding requests:  77%|███████▋  | 6306/8192 [00:13<00:03, 493.36it/s]
Adding requests:  78%|███████▊  | 6357/8192 [00:13<00:03, 496.59it/s]
Adding requests:  78%|███████▊  | 6407/8192 [00:13<00:03, 479.24it/s]
Adding requests:  79%|███████▉  | 6458/8192 [00:13<00:03, 486.18it/s]
Adding requests:  79%|███████▉  | 6507/8192 [00:13<00:03, 473.14it/s]
Adding requests:  80%|████████  | 6557/8192 [00:13<00:03, 479.92it/s]
Adding requests:  81%|████████  | 6606/8192 [00:13<00:03, 477.32it/s]
Adding requests:  81%|████████▏ | 6656/8192 [00:14<00:03, 482.74it/s]
Adding requests:  82%|████████▏ | 6705/8192 [00:14<00:03, 483.89it/s]
Adding requests:  82%|████████▏ | 6754/8192 [00:14<00:02, 480.81it/s]
Adding requests:  83%|████████▎ | 6805/8192 [00:14<00:02, 487.84it/s]
Adding requests:  84%|████████▎ | 6855/8192 [00:14<00:02, 489.64it/s]
Adding requests:  84%|████████▍ | 6906/8192 [00:14<00:02, 494.05it/s]
Adding requests:  85%|████████▍ | 6956/8192 [00:14<00:02, 495.57it/s]
Adding requests:  86%|████████▌ | 7006/8192 [00:14<00:02, 488.51it/s]
Adding requests:  86%|████████▌ | 7055/8192 [00:14<00:02, 487.61it/s]
Adding requests:  87%|████████▋ | 7104/8192 [00:14<00:02, 487.87it/s]
Adding requests:  87%|████████▋ | 7153/8192 [00:15<00:02, 483.80it/s]
Adding requests:  88%|████████▊ | 7202/8192 [00:15<00:02, 482.94it/s]
Adding requests:  89%|████████▊ | 7252/8192 [00:15<00:01, 484.66it/s]
Adding requests:  89%|████████▉ | 7302/8192 [00:15<00:01, 487.97it/s]
Adding requests:  90%|████████▉ | 7351/8192 [00:15<00:01, 484.27it/s]
Adding requests:  90%|█████████ | 7402/8192 [00:15<00:01, 490.33it/s]
Adding requests:  91%|█████████ | 7453/8192 [00:15<00:01, 495.98it/s]
Adding requests:  92%|█████████▏| 7503/8192 [00:15<00:01, 494.69it/s]
Adding requests:  92%|█████████▏| 7553/8192 [00:15<00:01, 493.34it/s]
Adding requests:  93%|█████████▎| 7603/8192 [00:15<00:01, 490.33it/s]
Adding requests:  93%|█████████▎| 7653/8192 [00:16<00:01, 492.74it/s]
Adding requests:  94%|█████████▍| 7703/8192 [00:16<00:00, 493.91it/s]
Adding requests:  95%|█████████▍| 7753/8192 [00:16<00:00, 478.04it/s]
Adding requests:  95%|█████████▌| 7801/8192 [00:16<00:00, 450.20it/s]
Adding requests:  96%|█████████▌| 7851/8192 [00:16<00:00, 462.42it/s]
Adding requests:  96%|█████████▋| 7900/8192 [00:16<00:00, 465.19it/s]
Adding requests:  97%|█████████▋| 7948/8192 [00:16<00:00, 467.80it/s]
Adding requests:  98%|█████████▊| 7996/8192 [00:16<00:00, 470.55it/s]
Adding requests:  98%|█████████▊| 8044/8192 [00:16<00:00, 470.93it/s]
Adding requests:  99%|█████████▉| 8094/8192 [00:16<00:00, 476.25it/s]
Adding requests:  99%|█████████▉| 8143/8192 [00:17<00:00, 479.11it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 476.41it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  52%|█████▏    | 4280/8192 [00:00<00:00, 12993.21it/s, est. speed input: 13305360.61 toks/s, output: 12993.28 toks/s]
Processed prompts:  68%|██████▊   | 5580/8192 [00:05<00:03, 805.81it/s, est. speed input: 1052273.23 toks/s, output: 1027.61 toks/s]    
Processed prompts:  75%|███████▍  | 6131/8192 [00:07<00:03, 614.04it/s, est. speed input: 837874.14 toks/s, output: 818.24 toks/s]  
Processed prompts:  79%|███████▊  | 6443/8192 [00:08<00:03, 525.65it/s, est. speed input: 752414.15 toks/s, output: 734.78 toks/s]
Processed prompts:  81%|████████  | 6641/8192 [00:09<00:03, 479.47it/s, est. speed input: 712752.60 toks/s, output: 696.05 toks/s]
Processed prompts:  83%|████████▎ | 6777/8192 [00:10<00:03, 419.65it/s, est. speed input: 672797.72 toks/s, output: 657.03 toks/s]
Processed prompts:  84%|████████▍ | 6873/8192 [00:10<00:03, 415.56it/s, est. speed input: 665674.38 toks/s, output: 650.07 toks/s]
Processed prompts:  85%|████████▍ | 6950/8192 [00:10<00:03, 402.07it/s, est. speed input: 656845.71 toks/s, output: 641.45 toks/s]
Processed prompts:  86%|████████▌ | 7012/8192 [00:11<00:03, 380.86it/s, est. speed input: 647463.71 toks/s, output: 632.29 toks/s]
Processed prompts:  86%|████████▌ | 7062/8192 [00:11<00:03, 350.20it/s, est. speed input: 637146.22 toks/s, output: 622.21 toks/s]
Processed prompts:  87%|████████▋ | 7103/8192 [00:11<00:03, 316.20it/s, est. speed input: 627035.37 toks/s, output: 612.34 toks/s]
Processed prompts:  87%|████████▋ | 7160/8192 [00:11<00:03, 295.88it/s, est. speed input: 618292.03 toks/s, output: 603.80 toks/s]
Processed prompts:  88%|████████▊ | 7224/8192 [00:12<00:03, 284.58it/s, est. speed input: 610498.69 toks/s, output: 596.19 toks/s]
Processed prompts:  89%|████████▉ | 7288/8192 [00:12<00:03, 276.22it/s, est. speed input: 603202.77 toks/s, output: 589.06 toks/s]
Processed prompts:  90%|████████▉ | 7352/8192 [00:12<00:03, 269.08it/s, est. speed input: 596115.01 toks/s, output: 582.14 toks/s]
Processed prompts:  91%|█████████ | 7416/8192 [00:12<00:02, 262.89it/s, est. speed input: 589194.38 toks/s, output: 575.38 toks/s]
Processed prompts:  91%|█████████▏| 7480/8192 [00:13<00:02, 259.69it/s, est. speed input: 582766.03 toks/s, output: 569.11 toks/s]
Processed prompts:  92%|█████████▏| 7544/8192 [00:13<00:02, 257.35it/s, est. speed input: 576587.11 toks/s, output: 563.07 toks/s]
Processed prompts:  93%|█████████▎| 7608/8192 [00:13<00:02, 255.56it/s, est. speed input: 570625.29 toks/s, output: 557.25 toks/s]
Processed prompts:  94%|█████████▎| 7672/8192 [00:13<00:02, 253.82it/s, est. speed input: 564819.10 toks/s, output: 551.58 toks/s]
Processed prompts:  94%|█████████▍| 7736/8192 [00:14<00:01, 251.46it/s, est. speed input: 559069.39 toks/s, output: 545.97 toks/s]
Processed prompts:  95%|█████████▌| 7800/8192 [00:14<00:01, 250.16it/s, est. speed input: 553575.75 toks/s, output: 540.60 toks/s]
Processed prompts:  96%|█████████▌| 7864/8192 [00:14<00:01, 249.14it/s, est. speed input: 548262.56 toks/s, output: 535.41 toks/s]
Processed prompts:  97%|█████████▋| 7928/8192 [00:14<00:01, 250.28it/s, est. speed input: 543366.97 toks/s, output: 530.63 toks/s]
Processed prompts:  98%|█████████▊| 7992/8192 [00:15<00:00, 249.42it/s, est. speed input: 538431.66 toks/s, output: 525.81 toks/s]
Processed prompts:  98%|█████████▊| 8056/8192 [00:15<00:00, 250.93it/s, est. speed input: 533911.07 toks/s, output: 521.40 toks/s]
Processed prompts:  99%|█████████▉| 8120/8192 [00:15<00:00, 250.99it/s, est. speed input: 529420.49 toks/s, output: 517.01 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:15<00:00, 250.99it/s, est. speed input: 532417.96 toks/s, output: 519.94 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:15<00:00, 519.94it/s, est. speed input: 532417.96 toks/s, output: 519.94 toks/s]
[rank0]:[W126 14:56:25.979797381 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 16:03:53
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:04:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=727850) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=727850) WARNING 01-26 16:04:17 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=727850) WARNING 01-26 16:04:27 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 37.30 requests/s, 19135.65 total tokens/s, 37.30 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 16:04:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:04:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:04:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:04:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:04:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:04:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:04:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:04:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:04:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:04:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:04:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:04:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:08] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=727850) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=727850) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.06it/s]
(EngineCore_DP0 pid=727850) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.16s/it]
(EngineCore_DP0 pid=727850) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.13s/it]
(EngineCore_DP0 pid=727850) 
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=727850) [2026-01-26 16:04:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=727850) 2026-01-26 16:04:27,520 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=727850) 2026-01-26 16:04:27,543 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=727850) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.69it/s]
(EngineCore_DP0 pid=727850) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.35it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  40%|███▉      | 51/128 [00:00<00:00, 508.72it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 252.08it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 297.96it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:00, 117.61it/s, est. speed input: 60221.80 toks/s, output: 117.62 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:01, 56.28it/s, est. speed input: 31467.60 toks/s, output: 61.46 toks/s]  
Processed prompts:  27%|██▋       | 34/128 [00:00<00:01, 49.10it/s, est. speed input: 27824.97 toks/s, output: 54.35 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 46.14it/s, est. speed input: 26358.77 toks/s, output: 51.48 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 44.11it/s, est. speed input: 25364.08 toks/s, output: 49.54 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 42.85it/s, est. speed input: 24742.56 toks/s, output: 48.33 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 41.94it/s, est. speed input: 24263.37 toks/s, output: 47.39 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 41.27it/s, est. speed input: 23877.52 toks/s, output: 46.64 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 40.79it/s, est. speed input: 23561.14 toks/s, output: 46.02 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 40.42it/s, est. speed input: 23291.96 toks/s, output: 45.49 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 40.11it/s, est. speed input: 23056.48 toks/s, output: 45.03 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 39.98it/s, est. speed input: 22865.00 toks/s, output: 44.66 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:01, 39.88it/s, est. speed input: 22698.12 toks/s, output: 44.33 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:00, 39.79it/s, est. speed input: 22576.63 toks/s, output: 44.09 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 39.72it/s, est. speed input: 22465.64 toks/s, output: 43.88 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 39.71it/s, est. speed input: 22369.57 toks/s, output: 43.69 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 39.71it/s, est. speed input: 22281.50 toks/s, output: 43.52 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 39.71it/s, est. speed input: 22201.48 toks/s, output: 43.36 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 39.73it/s, est. speed input: 22129.17 toks/s, output: 43.22 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 39.61it/s, est. speed input: 22052.32 toks/s, output: 43.07 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 39.56it/s, est. speed input: 21984.05 toks/s, output: 42.94 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 39.55it/s, est. speed input: 21922.07 toks/s, output: 42.82 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 39.56it/s, est. speed input: 21864.92 toks/s, output: 42.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.56it/s, est. speed input: 21839.08 toks/s, output: 42.65 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 42.65it/s, est. speed input: 21839.08 toks/s, output: 42.65 toks/s]
[rank0]:[W126 16:04:33.750322262 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 16:04:35
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:04:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=729044) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=729044) WARNING 01-26 16:04:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=729044) WARNING 01-26 16:05:08 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 39.69 requests/s, 40677.24 total tokens/s, 39.69 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 16:04:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:04:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:04:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:04:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:04:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:04:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:04:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:04:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:04:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:04:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:04:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:04:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:04:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:04:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=729044) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=729044) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.05it/s]
(EngineCore_DP0 pid=729044) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.07s/it]
(EngineCore_DP0 pid=729044) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.05s/it]
(EngineCore_DP0 pid=729044) 
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=729044) [2026-01-26 16:04:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=729044) 2026-01-26 16:05:08,861 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=729044) 2026-01-26 16:05:08,884 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=729044) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.79it/s]
(EngineCore_DP0 pid=729044) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.88it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 275.42it/s]
Adding requests:  49%|████▉     | 63/128 [00:00<00:00, 315.74it/s]
Adding requests:  74%|███████▍  | 95/128 [00:00<00:00, 313.53it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 317.67it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 313.49it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:00, 178.45it/s, est. speed input: 182746.85 toks/s, output: 178.46 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 59.64it/s, est. speed input: 67844.67 toks/s, output: 66.25 toks/s]   
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 51.86it/s, est. speed input: 59625.84 toks/s, output: 58.23 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:00<00:01, 48.20it/s, est. speed input: 55954.34 toks/s, output: 54.64 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 46.28it/s, est. speed input: 54089.45 toks/s, output: 52.82 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 44.75it/s, est. speed input: 52637.30 toks/s, output: 51.40 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 43.80it/s, est. speed input: 51690.49 toks/s, output: 50.48 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 42.94it/s, est. speed input: 50860.17 toks/s, output: 49.67 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 42.28it/s, est. speed input: 50156.67 toks/s, output: 48.98 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:01, 41.76it/s, est. speed input: 49546.46 toks/s, output: 48.39 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 41.40it/s, est. speed input: 49021.04 toks/s, output: 47.87 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 41.17it/s, est. speed input: 48568.22 toks/s, output: 47.43 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 40.99it/s, est. speed input: 48164.00 toks/s, output: 47.04 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.77it/s, est. speed input: 47784.05 toks/s, output: 46.66 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 40.66it/s, est. speed input: 47453.04 toks/s, output: 46.34 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 40.58it/s, est. speed input: 47153.51 toks/s, output: 46.05 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 40.55it/s, est. speed input: 46887.43 toks/s, output: 45.79 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 40.55it/s, est. speed input: 46647.28 toks/s, output: 45.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.55it/s, est. speed input: 46546.20 toks/s, output: 45.46 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.45it/s, est. speed input: 46546.20 toks/s, output: 45.46 toks/s]
[rank0]:[W126 16:05:13.982880668 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 16:05:15
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:05:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=730166) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=730166) WARNING 01-26 16:05:40 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=730166) WARNING 01-26 16:05:50 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 77.30 requests/s, 79237.15 total tokens/s, 77.30 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 16:05:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:05:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:05:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:05:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:05:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:05:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:05:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:05:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:05:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:05:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:05:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:05:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:05:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:05:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:05:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:05:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:05:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=730166) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=730166) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.03it/s]
(EngineCore_DP0 pid=730166) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.23s/it]
(EngineCore_DP0 pid=730166) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.19s/it]
(EngineCore_DP0 pid=730166) 
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=730166) [2026-01-26 16:05:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=730166) 2026-01-26 16:05:50,414 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=730166) 2026-01-26 16:05:50,436 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=730166) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.73it/s]
(EngineCore_DP0 pid=730166) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.95it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   9%|▉         | 23/256 [00:00<00:01, 228.72it/s]
Adding requests:  21%|██        | 53/256 [00:00<00:00, 267.45it/s]
Adding requests:  32%|███▏      | 81/256 [00:00<00:00, 271.97it/s]
Adding requests:  43%|████▎     | 109/256 [00:00<00:01, 135.13it/s]
Adding requests:  51%|█████     | 131/256 [00:00<00:00, 151.71it/s]
Adding requests:  62%|██████▎   | 160/256 [00:00<00:00, 183.92it/s]
Adding requests:  76%|███████▌  | 194/256 [00:00<00:00, 222.42it/s]
Adding requests:  89%|████████▉ | 229/256 [00:01<00:00, 254.33it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 220.80it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:00<00:00, 849.60it/s, est. speed input: 870048.88 toks/s, output: 849.61 toks/s]
Processed prompts:  68%|██████▊   | 175/256 [00:01<00:00, 134.13it/s, est. speed input: 157857.51 toks/s, output: 154.16 toks/s]
Processed prompts:  84%|████████▍ | 215/256 [00:01<00:00, 112.72it/s, est. speed input: 134513.60 toks/s, output: 131.36 toks/s]
Processed prompts:  94%|█████████▍| 241/256 [00:01<00:00, 104.56it/s, est. speed input: 126233.97 toks/s, output: 123.28 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 104.56it/s, est. speed input: 121878.40 toks/s, output: 119.02 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 119.02it/s, est. speed input: 121878.40 toks/s, output: 119.02 toks/s]
[rank0]:[W126 16:05:55.817859212 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 16:05:57
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:06:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=731312) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=731312) WARNING 01-26 16:06:23 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=731312) WARNING 01-26 16:06:33 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 114.94 requests/s, 117816.03 total tokens/s, 114.94 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 16:06:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:06:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:06:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:06:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:06:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:06:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:06:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:06:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:06:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:06:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:06:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:06:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:06:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:06:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:06:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:06:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=731312) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=731312) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.16it/s]
(EngineCore_DP0 pid=731312) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.09s/it]
(EngineCore_DP0 pid=731312) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.06s/it]
(EngineCore_DP0 pid=731312) 
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=731312) [2026-01-26 16:06:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=731312) 2026-01-26 16:06:33,210 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=731312) 2026-01-26 16:06:33,233 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=731312) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 10.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 11.23it/s]
(EngineCore_DP0 pid=731312) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 15.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 16.25it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 27/512 [00:00<00:01, 265.96it/s]
Adding requests:  12%|█▏        | 61/512 [00:00<00:01, 309.06it/s]
Adding requests:  18%|█▊        | 93/512 [00:00<00:01, 310.16it/s]
Adding requests:  25%|██▍       | 127/512 [00:00<00:01, 318.73it/s]
Adding requests:  31%|███       | 159/512 [00:00<00:01, 313.93it/s]
Adding requests:  38%|███▊      | 195/512 [00:00<00:00, 326.20it/s]
Adding requests:  45%|████▌     | 231/512 [00:00<00:00, 335.43it/s]
Adding requests:  52%|█████▏    | 265/512 [00:00<00:00, 334.87it/s]
Adding requests:  59%|█████▉    | 301/512 [00:00<00:00, 340.87it/s]
Adding requests:  66%|██████▌   | 337/512 [00:01<00:00, 346.22it/s]
Adding requests:  73%|███████▎  | 373/512 [00:01<00:00, 350.35it/s]
Adding requests:  80%|████████  | 410/512 [00:01<00:00, 355.58it/s]
Adding requests:  87%|████████▋ | 446/512 [00:01<00:00, 352.78it/s]
Adding requests:  95%|█████████▍| 485/512 [00:01<00:00, 361.85it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 341.63it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:00<00:00, 1406.07it/s, est. speed input: 1439899.59 toks/s, output: 1406.09 toks/s]
Processed prompts:  63%|██████▎   | 323/512 [00:01<00:00, 209.76it/s, est. speed input: 250878.43 toks/s, output: 245.00 toks/s]   
Processed prompts:  76%|███████▌  | 388/512 [00:01<00:00, 175.47it/s, est. speed input: 213166.86 toks/s, output: 208.17 toks/s]
Processed prompts:  84%|████████▍ | 429/512 [00:02<00:00, 161.50it/s, est. speed input: 199230.97 toks/s, output: 194.56 toks/s]
Processed prompts:  90%|████████▉ | 459/512 [00:02<00:00, 150.62it/s, est. speed input: 190101.11 toks/s, output: 185.65 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:02<00:00, 143.14it/s, est. speed input: 184344.24 toks/s, output: 180.02 toks/s]
Processed prompts:  98%|█████████▊| 501/512 [00:02<00:00, 142.65it/s, est. speed input: 182363.44 toks/s, output: 178.09 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 142.65it/s, est. speed input: 177461.67 toks/s, output: 173.30 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 173.30it/s, est. speed input: 177461.67 toks/s, output: 173.30 toks/s]
[rank0]:[W126 16:06:40.263730177 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 16:06:41
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:06:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=732538) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=732538) WARNING 01-26 16:07:10 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=732538) WARNING 01-26 16:07:20 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 123.88 requests/s, 126981.26 total tokens/s, 123.88 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 16:06:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:06:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:06:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:06:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:06:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:06:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:06:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:06:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:06:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:07:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:07:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:07:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:07:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:07:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:07:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:07:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:07:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=732538) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=732538) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.30it/s]
(EngineCore_DP0 pid=732538) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]
(EngineCore_DP0 pid=732538) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.06it/s]
(EngineCore_DP0 pid=732538) 
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=732538) [2026-01-26 16:07:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=732538) 2026-01-26 16:07:20,457 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=732538) 2026-01-26 16:07:20,481 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=732538) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 15.75it/s]
(EngineCore_DP0 pid=732538) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 17.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.08it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 278.35it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 318.11it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 315.38it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 320.72it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 330.18it/s]
Adding requests:  19%|█▉        | 199/1024 [00:00<00:02, 339.99it/s]
Adding requests:  23%|██▎       | 235/1024 [00:00<00:02, 346.24it/s]
Adding requests:  26%|██▋       | 270/1024 [00:00<00:02, 344.41it/s]
Adding requests:  30%|██▉       | 306/1024 [00:00<00:02, 348.56it/s]
Adding requests:  33%|███▎      | 343/1024 [00:01<00:01, 352.14it/s]
Adding requests:  37%|███▋      | 380/1024 [00:01<00:01, 356.57it/s]
Adding requests:  41%|████      | 419/1024 [00:01<00:01, 364.16it/s]
Adding requests:  45%|████▍     | 456/1024 [00:01<00:01, 361.78it/s]
Adding requests:  48%|████▊     | 496/1024 [00:01<00:01, 371.09it/s]
Adding requests:  52%|█████▏    | 535/1024 [00:01<00:01, 375.90it/s]
Adding requests:  56%|█████▌    | 573/1024 [00:01<00:01, 373.30it/s]
Adding requests:  60%|█████▉    | 611/1024 [00:01<00:01, 356.69it/s]
Adding requests:  63%|██████▎   | 647/1024 [00:01<00:01, 350.16it/s]
Adding requests:  67%|██████▋   | 683/1024 [00:01<00:00, 350.11it/s]
Adding requests:  70%|███████   | 719/1024 [00:02<00:00, 348.77it/s]
Adding requests:  74%|███████▎  | 754/1024 [00:02<00:00, 343.96it/s]
Adding requests:  77%|███████▋  | 790/1024 [00:02<00:00, 347.57it/s]
Adding requests:  81%|████████  | 827/1024 [00:02<00:00, 352.13it/s]
Adding requests:  84%|████████▍ | 864/1024 [00:02<00:00, 355.66it/s]
Adding requests:  88%|████████▊ | 901/1024 [00:02<00:00, 358.90it/s]
Adding requests:  92%|█████████▏| 937/1024 [00:02<00:00, 355.38it/s]
Adding requests:  95%|█████████▌| 974/1024 [00:02<00:00, 356.66it/s]
Adding requests:  99%|█████████▊| 1010/1024 [00:02<00:00, 352.95it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 350.86it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:00<00:00, 3228.86it/s, est. speed input: 3306575.31 toks/s, output: 3228.92 toks/s]
Processed prompts:  67%|██████▋   | 685/1024 [00:02<00:01, 218.96it/s, est. speed input: 263103.29 toks/s, output: 256.94 toks/s]   
Processed prompts:  81%|████████  | 826/1024 [00:03<00:01, 182.25it/s, est. speed input: 221953.91 toks/s, output: 216.75 toks/s]
Processed prompts:  89%|████████▊ | 908/1024 [00:04<00:00, 169.63it/s, est. speed input: 208945.64 toks/s, output: 204.05 toks/s]
Processed prompts:  94%|█████████▍| 963/1024 [00:04<00:00, 160.89it/s, est. speed input: 201428.68 toks/s, output: 196.71 toks/s]
Processed prompts:  98%|█████████▊| 1003/1024 [00:05<00:00, 155.09it/s, est. speed input: 196937.25 toks/s, output: 192.32 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 155.09it/s, est. speed input: 196144.03 toks/s, output: 191.55 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 191.54it/s, est. speed input: 196144.03 toks/s, output: 191.55 toks/s]
[rank0]:[W126 16:07:31.476315184 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 16:07:33
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:07:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=733881) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=733881) WARNING 01-26 16:08:09 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=733881) WARNING 01-26 16:08:36 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 127.42 requests/s, 130600.86 total tokens/s, 127.42 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 16:07:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:07:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:07:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:07:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:07:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:07:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:07:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:07:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:07:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:07:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:07:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:07:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:07:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:07:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:07:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:07:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:07:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:00] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=733881) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=733881) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.07it/s]
(EngineCore_DP0 pid=733881) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.19s/it]
(EngineCore_DP0 pid=733881) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.15s/it]
(EngineCore_DP0 pid=733881) 
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=733881) [2026-01-26 16:08:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=733881) [rank0]:W0126 16:08:14.393000 733881 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=733881) [rank0]:W0126 16:08:14.476000 733881 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=733881) [rank0]:W0126 16:08:15.447000 733881 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=733881) [rank0]:W0126 16:08:15.577000 733881 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=733881) 2026-01-26 16:08:36,009 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=733881) 2026-01-26 16:08:36,099 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=733881) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  4.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:01,  4.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  4.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  4.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:01<00:00,  5.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:01<00:00,  5.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  3.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  4.18it/s]
(EngineCore_DP0 pid=733881) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  5.03it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  5.31it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  5.11it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  5.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  5.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  5.07it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 11/2048 [00:00<00:19, 105.80it/s]
Adding requests:   1%|          | 22/2048 [00:00<00:20, 96.80it/s] 
Adding requests:   2%|▏         | 35/2048 [00:00<00:18, 108.07it/s]
Adding requests:   2%|▏         | 48/2048 [00:00<00:17, 114.09it/s]
Adding requests:   3%|▎         | 69/2048 [00:00<00:13, 145.75it/s]
Adding requests:   4%|▍         | 91/2048 [00:00<00:11, 169.85it/s]
Adding requests:   6%|▌         | 114/2048 [00:00<00:10, 186.13it/s]
Adding requests:   7%|▋         | 140/2048 [00:00<00:09, 207.42it/s]
Adding requests:   8%|▊         | 170/2048 [00:00<00:08, 234.52it/s]
Adding requests:  10%|▉         | 201/2048 [00:01<00:07, 257.00it/s]
Adding requests:  11%|█         | 227/2048 [00:01<00:07, 236.40it/s]
Adding requests:  12%|█▏        | 252/2048 [00:01<00:07, 229.01it/s]
Adding requests:  14%|█▎        | 280/2048 [00:01<00:07, 242.55it/s]
Adding requests:  15%|█▌        | 312/2048 [00:01<00:06, 262.88it/s]
Adding requests:  17%|█▋        | 345/2048 [00:01<00:06, 279.90it/s]
Adding requests:  18%|█▊        | 377/2048 [00:01<00:05, 288.73it/s]
Adding requests:  20%|█▉        | 408/2048 [00:01<00:05, 294.54it/s]
Adding requests:  21%|██▏       | 440/2048 [00:01<00:05, 301.01it/s]
Adding requests:  23%|██▎       | 473/2048 [00:02<00:05, 303.55it/s]
Adding requests:  25%|██▍       | 507/2048 [00:02<00:04, 313.12it/s]
Adding requests:  26%|██▋       | 542/2048 [00:02<00:04, 321.47it/s]
Adding requests:  28%|██▊       | 575/2048 [00:02<00:04, 322.91it/s]
Adding requests:  30%|██▉       | 608/2048 [00:02<00:04, 310.52it/s]
Adding requests:  31%|███▏      | 640/2048 [00:02<00:04, 301.40it/s]
Adding requests:  33%|███▎      | 671/2048 [00:02<00:04, 297.70it/s]
Adding requests:  34%|███▍      | 704/2048 [00:02<00:04, 305.54it/s]
Adding requests:  36%|███▌      | 735/2048 [00:02<00:04, 300.40it/s]
Adding requests:  37%|███▋      | 766/2048 [00:02<00:04, 303.11it/s]
Adding requests:  39%|███▉      | 797/2048 [00:03<00:04, 302.81it/s]
Adding requests:  40%|████      | 829/2048 [00:03<00:03, 307.41it/s]
Adding requests:  42%|████▏     | 861/2048 [00:03<00:03, 310.12it/s]
Adding requests:  44%|████▎     | 893/2048 [00:03<00:03, 308.40it/s]
Adding requests:  45%|████▌     | 924/2048 [00:03<00:03, 303.68it/s]
Adding requests:  47%|████▋     | 957/2048 [00:03<00:03, 309.89it/s]
Adding requests:  48%|████▊     | 989/2048 [00:03<00:03, 307.03it/s]
Adding requests:  50%|████▉     | 1020/2048 [00:03<00:03, 303.21it/s]
Adding requests:  51%|█████▏    | 1051/2048 [00:03<00:03, 292.83it/s]
Adding requests:  53%|█████▎    | 1081/2048 [00:04<00:03, 282.12it/s]
Adding requests:  54%|█████▍    | 1111/2048 [00:04<00:03, 285.26it/s]
Adding requests:  56%|█████▌    | 1143/2048 [00:04<00:03, 294.46it/s]
Adding requests:  57%|█████▋    | 1173/2048 [00:04<00:03, 285.44it/s]
Adding requests:  59%|█████▊    | 1203/2048 [00:04<00:02, 287.72it/s]
Adding requests:  60%|██████    | 1235/2048 [00:04<00:02, 296.49it/s]
Adding requests:  62%|██████▏   | 1265/2048 [00:04<00:02, 296.77it/s]
Adding requests:  63%|██████▎   | 1295/2048 [00:04<00:02, 293.13it/s]
Adding requests:  65%|██████▍   | 1326/2048 [00:04<00:02, 297.47it/s]
Adding requests:  66%|██████▋   | 1357/2048 [00:04<00:02, 298.29it/s]
Adding requests:  68%|██████▊   | 1387/2048 [00:05<00:02, 286.42it/s]
Adding requests:  69%|██████▉   | 1416/2048 [00:05<00:02, 285.80it/s]
Adding requests:  71%|███████   | 1445/2048 [00:05<00:02, 283.87it/s]
Adding requests:  72%|███████▏  | 1474/2048 [00:05<00:02, 282.78it/s]
Adding requests:  73%|███████▎  | 1504/2048 [00:05<00:01, 287.00it/s]
Adding requests:  75%|███████▍  | 1533/2048 [00:05<00:01, 282.13it/s]
Adding requests:  76%|███████▋  | 1562/2048 [00:05<00:01, 281.54it/s]
Adding requests:  78%|███████▊  | 1591/2048 [00:05<00:01, 270.67it/s]
Adding requests:  79%|███████▉  | 1619/2048 [00:05<00:01, 235.75it/s]
Adding requests:  80%|████████  | 1644/2048 [00:06<00:01, 238.19it/s]
Adding requests:  82%|████████▏ | 1670/2048 [00:06<00:01, 242.81it/s]
Adding requests:  83%|████████▎ | 1695/2048 [00:06<00:01, 234.71it/s]
Adding requests:  84%|████████▍ | 1721/2048 [00:06<00:01, 238.56it/s]
Adding requests:  85%|████████▌ | 1749/2048 [00:06<00:01, 248.66it/s]
Adding requests:  87%|████████▋ | 1778/2048 [00:06<00:01, 260.18it/s]
Adding requests:  88%|████████▊ | 1807/2048 [00:06<00:00, 267.83it/s]
Adding requests:  90%|████████▉ | 1838/2048 [00:06<00:00, 277.77it/s]
Adding requests:  91%|█████████ | 1868/2048 [00:06<00:00, 283.24it/s]
Adding requests:  93%|█████████▎| 1897/2048 [00:07<00:00, 276.94it/s]
Adding requests:  94%|█████████▍| 1927/2048 [00:07<00:00, 282.98it/s]
Adding requests:  96%|█████████▌| 1957/2048 [00:07<00:00, 287.64it/s]
Adding requests:  97%|█████████▋| 1986/2048 [00:07<00:00, 278.71it/s]
Adding requests:  98%|█████████▊| 2014/2048 [00:07<00:00, 223.27it/s]
Adding requests: 100%|█████████▉| 2042/2048 [00:07<00:00, 236.30it/s]
Adding requests: 100%|██████████| 2048/2048 [00:07<00:00, 268.64it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  46%|████▌     | 945/2048 [00:00<00:00, 6444.56it/s, est. speed input: 6600539.09 toks/s, output: 6445.02 toks/s]
Processed prompts:  78%|███████▊  | 1590/2048 [00:05<00:01, 262.89it/s, est. speed input: 324740.79 toks/s, output: 317.13 toks/s]  
Processed prompts:  91%|█████████ | 1864/2048 [00:07<00:00, 214.50it/s, est. speed input: 269751.22 toks/s, output: 263.43 toks/s]
Processed prompts:  99%|█████████▊| 2019/2048 [00:08<00:00, 192.52it/s, est. speed input: 248374.06 toks/s, output: 242.55 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:08<00:00, 192.52it/s, est. speed input: 248365.15 toks/s, output: 242.54 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:08<00:00, 242.54it/s, est. speed input: 248365.15 toks/s, output: 242.54 toks/s]
[rank0]:[W126 16:09:03.482897780 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 16:09:07
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:09:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=735933) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=735933) WARNING 01-26 16:10:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=735933) WARNING 01-26 16:10:25 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 131.41 requests/s, 134690.99 total tokens/s, 131.41 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 16:09:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:09:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:09:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:09:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:09:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:09:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:09:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:09:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:09:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:09:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:09:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:09:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:09:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:09:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:09:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:09:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:09:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:00] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=735933) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=735933) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.23s/it]
(EngineCore_DP0 pid=735933) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.33s/it]
(EngineCore_DP0 pid=735933) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.31s/it]
(EngineCore_DP0 pid=735933) 
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=735933) [2026-01-26 16:10:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=735933) [rank0]:W0126 16:10:17.805000 735933 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=735933) [rank0]:W0126 16:10:17.900000 735933 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=735933) [rank0]:W0126 16:10:19.001000 735933 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=735933) [rank0]:W0126 16:10:19.148000 735933 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=735933) 2026-01-26 16:10:24,788 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=735933) 2026-01-26 16:10:24,858 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=735933) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 13.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 13.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 14.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 15.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 15.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.30it/s]
(EngineCore_DP0 pid=735933) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 14.54it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 12.31it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 10.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 11.88it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   0%|          | 9/4096 [00:00<00:45, 89.41it/s]
Adding requests:   1%|          | 23/4096 [00:00<00:35, 115.44it/s]
Adding requests:   1%|          | 38/4096 [00:00<00:31, 129.42it/s]
Adding requests:   1%|▏         | 52/4096 [00:00<00:30, 131.48it/s]
Adding requests:   2%|▏         | 80/4096 [00:00<00:22, 182.36it/s]
Adding requests:   3%|▎         | 104/4096 [00:00<00:20, 199.43it/s]
Adding requests:   3%|▎         | 130/4096 [00:00<00:18, 218.36it/s]
Adding requests:   4%|▍         | 159/4096 [00:00<00:16, 238.62it/s]
Adding requests:   5%|▍         | 187/4096 [00:00<00:15, 250.69it/s]
Adding requests:   5%|▌         | 213/4096 [00:01<00:15, 251.00it/s]
Adding requests:   6%|▌         | 239/4096 [00:01<00:15, 251.13it/s]
Adding requests:   6%|▋         | 265/4096 [00:01<00:15, 252.38it/s]
Adding requests:   7%|▋         | 293/4096 [00:01<00:14, 259.49it/s]
Adding requests:   8%|▊         | 323/4096 [00:01<00:13, 271.46it/s]
Adding requests:   9%|▊         | 354/4096 [00:01<00:13, 282.09it/s]
Adding requests:   9%|▉         | 384/4096 [00:01<00:13, 284.32it/s]
Adding requests:  10%|█         | 415/4096 [00:01<00:12, 289.02it/s]
Adding requests:  11%|█         | 444/4096 [00:01<00:12, 285.69it/s]
Adding requests:  12%|█▏        | 473/4096 [00:01<00:12, 279.53it/s]
Adding requests:  12%|█▏        | 501/4096 [00:02<00:13, 271.76it/s]
Adding requests:  13%|█▎        | 529/4096 [00:02<00:15, 231.53it/s]
Adding requests:  14%|█▎        | 554/4096 [00:02<00:18, 189.65it/s]
Adding requests:  14%|█▍        | 575/4096 [00:02<00:21, 166.35it/s]
Adding requests:  15%|█▍        | 594/4096 [00:02<00:21, 161.79it/s]
Adding requests:  15%|█▍        | 612/4096 [00:02<00:22, 155.96it/s]
Adding requests:  15%|█▌        | 629/4096 [00:02<00:22, 156.13it/s]
Adding requests:  16%|█▌        | 646/4096 [00:03<00:22, 152.23it/s]
Adding requests:  16%|█▌        | 662/4096 [00:03<00:23, 149.05it/s]
Adding requests:  17%|█▋        | 678/4096 [00:03<00:23, 147.08it/s]
Adding requests:  17%|█▋        | 706/4096 [00:03<00:18, 180.99it/s]
Adding requests:  18%|█▊        | 733/4096 [00:03<00:16, 204.11it/s]
Adding requests:  18%|█▊        | 754/4096 [00:03<00:16, 200.11it/s]
Adding requests:  19%|█▉        | 775/4096 [00:03<00:18, 176.21it/s]
Adding requests:  19%|█▉        | 794/4096 [00:03<00:18, 177.70it/s]
Adding requests:  20%|█▉        | 819/4096 [00:03<00:16, 196.14it/s]
Adding requests:  21%|██        | 846/4096 [00:04<00:16, 199.94it/s]
Adding requests:  21%|██        | 867/4096 [00:04<00:17, 183.52it/s]
Adding requests:  22%|██▏       | 886/4096 [00:04<00:19, 162.12it/s]
Adding requests:  22%|██▏       | 908/4096 [00:04<00:18, 175.62it/s]
Adding requests:  23%|██▎       | 935/4096 [00:04<00:15, 198.04it/s]
Adding requests:  24%|██▎       | 966/4096 [00:04<00:13, 226.50it/s]
Adding requests:  24%|██▍       | 994/4096 [00:04<00:12, 240.52it/s]
Adding requests:  25%|██▍       | 1022/4096 [00:04<00:12, 250.50it/s]
Adding requests:  26%|██▌       | 1048/4096 [00:05<00:12, 252.76it/s]
Adding requests:  26%|██▌       | 1075/4096 [00:05<00:11, 257.60it/s]
Adding requests:  27%|██▋       | 1102/4096 [00:05<00:11, 252.13it/s]
Adding requests:  28%|██▊       | 1128/4096 [00:05<00:11, 249.61it/s]
Adding requests:  28%|██▊       | 1155/4096 [00:05<00:11, 255.03it/s]
Adding requests:  29%|██▉       | 1182/4096 [00:05<00:11, 258.80it/s]
Adding requests:  30%|██▉       | 1213/4096 [00:05<00:10, 271.33it/s]
Adding requests:  30%|███       | 1243/4096 [00:05<00:10, 277.60it/s]
Adding requests:  31%|███       | 1271/4096 [00:05<00:11, 240.13it/s]
Adding requests:  32%|███▏      | 1296/4096 [00:06<00:11, 235.87it/s]
Adding requests:  32%|███▏      | 1322/4096 [00:06<00:11, 241.65it/s]
Adding requests:  33%|███▎      | 1347/4096 [00:06<00:12, 217.85it/s]
Adding requests:  33%|███▎      | 1370/4096 [00:06<00:13, 197.85it/s]
Adding requests:  34%|███▍      | 1391/4096 [00:06<00:14, 192.00it/s]
Adding requests:  34%|███▍      | 1411/4096 [00:06<00:15, 175.85it/s]
Adding requests:  35%|███▌      | 1438/4096 [00:06<00:13, 198.91it/s]
Adding requests:  36%|███▌      | 1467/4096 [00:06<00:11, 220.90it/s]
Adding requests:  36%|███▋      | 1495/4096 [00:06<00:11, 234.81it/s]
Adding requests:  37%|███▋      | 1520/4096 [00:07<00:10, 236.35it/s]
Adding requests:  38%|███▊      | 1546/4096 [00:07<00:10, 242.83it/s]
Adding requests:  38%|███▊      | 1571/4096 [00:07<00:10, 241.04it/s]
Adding requests:  39%|███▉      | 1600/4096 [00:07<00:09, 253.75it/s]
Adding requests:  40%|███▉      | 1628/4096 [00:07<00:09, 259.26it/s]
Adding requests:  40%|████      | 1655/4096 [00:07<00:09, 257.23it/s]
Adding requests:  41%|████      | 1681/4096 [00:07<00:09, 250.06it/s]
Adding requests:  42%|████▏     | 1708/4096 [00:07<00:09, 255.43it/s]
Adding requests:  42%|████▏     | 1736/4096 [00:07<00:08, 262.28it/s]
Adding requests:  43%|████▎     | 1768/4096 [00:07<00:08, 277.17it/s]
Adding requests:  44%|████▍     | 1796/4096 [00:08<00:08, 273.18it/s]
Adding requests:  45%|████▍     | 1824/4096 [00:08<00:08, 272.08it/s]
Adding requests:  45%|████▌     | 1852/4096 [00:08<00:09, 245.14it/s]
Adding requests:  46%|████▌     | 1878/4096 [00:08<00:11, 192.67it/s]
Adding requests:  46%|████▋     | 1900/4096 [00:08<00:12, 174.98it/s]
Adding requests:  47%|████▋     | 1920/4096 [00:08<00:12, 177.97it/s]
Adding requests:  47%|████▋     | 1944/4096 [00:08<00:11, 192.45it/s]
Adding requests:  48%|████▊     | 1974/4096 [00:09<00:09, 217.74it/s]
Adding requests:  49%|████▉     | 2003/4096 [00:09<00:08, 234.89it/s]
Adding requests:  50%|████▉     | 2031/4096 [00:09<00:08, 245.59it/s]
Adding requests:  50%|█████     | 2058/4096 [00:09<00:08, 249.95it/s]
Adding requests:  51%|█████     | 2084/4096 [00:09<00:08, 248.16it/s]
Adding requests:  52%|█████▏    | 2115/4096 [00:09<00:07, 265.02it/s]
Adding requests:  52%|█████▏    | 2144/4096 [00:09<00:07, 269.85it/s]
Adding requests:  53%|█████▎    | 2172/4096 [00:09<00:07, 258.74it/s]
Adding requests:  54%|█████▎    | 2199/4096 [00:09<00:07, 254.69it/s]
Adding requests:  54%|█████▍    | 2226/4096 [00:09<00:07, 256.88it/s]
Adding requests:  55%|█████▌    | 2254/4096 [00:10<00:07, 262.92it/s]
Adding requests:  56%|█████▌    | 2282/4096 [00:10<00:06, 267.46it/s]
Adding requests:  56%|█████▋    | 2309/4096 [00:10<00:06, 267.21it/s]
Adding requests:  57%|█████▋    | 2341/4096 [00:10<00:06, 279.82it/s]
Adding requests:  58%|█████▊    | 2370/4096 [00:10<00:06, 252.18it/s]
Adding requests:  58%|█████▊    | 2396/4096 [00:10<00:07, 238.00it/s]
Adding requests:  59%|█████▉    | 2424/4096 [00:10<00:06, 246.96it/s]
Adding requests:  60%|█████▉    | 2452/4096 [00:10<00:06, 254.27it/s]
Adding requests:  60%|██████    | 2478/4096 [00:10<00:06, 250.46it/s]
Adding requests:  61%|██████    | 2504/4096 [00:11<00:06, 238.73it/s]
Adding requests:  62%|██████▏   | 2533/4096 [00:11<00:06, 250.95it/s]
Adding requests:  63%|██████▎   | 2564/4096 [00:11<00:05, 265.21it/s]
Adding requests:  63%|██████▎   | 2592/4096 [00:11<00:05, 267.17it/s]
Adding requests:  64%|██████▍   | 2619/4096 [00:11<00:05, 267.52it/s]
Adding requests:  65%|██████▍   | 2646/4096 [00:11<00:05, 263.39it/s]
Adding requests:  65%|██████▌   | 2673/4096 [00:11<00:05, 263.15it/s]
Adding requests:  66%|██████▌   | 2700/4096 [00:11<00:05, 263.82it/s]
Adding requests:  67%|██████▋   | 2727/4096 [00:11<00:05, 262.84it/s]
Adding requests:  67%|██████▋   | 2759/4096 [00:12<00:04, 277.29it/s]
Adding requests:  68%|██████▊   | 2789/4096 [00:12<00:04, 283.68it/s]
Adding requests:  69%|██████▉   | 2821/4096 [00:12<00:04, 292.56it/s]
Adding requests:  70%|██████▉   | 2852/4096 [00:12<00:04, 294.11it/s]
Adding requests:  70%|███████   | 2882/4096 [00:12<00:04, 291.69it/s]
Adding requests:  71%|███████   | 2913/4096 [00:12<00:04, 295.68it/s]
Adding requests:  72%|███████▏  | 2944/4096 [00:12<00:03, 298.12it/s]
Adding requests:  73%|███████▎  | 2974/4096 [00:12<00:05, 213.48it/s]
Adding requests:  73%|███████▎  | 2999/4096 [00:12<00:04, 220.86it/s]
Adding requests:  74%|███████▍  | 3027/4096 [00:13<00:04, 234.03it/s]
Adding requests:  75%|███████▍  | 3057/4096 [00:13<00:04, 250.77it/s]
Adding requests:  75%|███████▌  | 3088/4096 [00:13<00:03, 266.35it/s]
Adding requests:  76%|███████▌  | 3119/4096 [00:13<00:03, 278.14it/s]
Adding requests:  77%|███████▋  | 3149/4096 [00:13<00:03, 282.81it/s]
Adding requests:  78%|███████▊  | 3179/4096 [00:13<00:03, 282.70it/s]
Adding requests:  78%|███████▊  | 3208/4096 [00:13<00:03, 275.64it/s]
Adding requests:  79%|███████▉  | 3240/4096 [00:13<00:02, 285.51it/s]
Adding requests:  80%|███████▉  | 3269/4096 [00:13<00:02, 284.98it/s]
Adding requests:  81%|████████  | 3298/4096 [00:14<00:02, 271.37it/s]
Adding requests:  81%|████████  | 3326/4096 [00:14<00:02, 258.52it/s]
Adding requests:  82%|████████▏ | 3356/4096 [00:14<00:02, 268.04it/s]
Adding requests:  83%|████████▎ | 3385/4096 [00:14<00:02, 273.66it/s]
Adding requests:  83%|████████▎ | 3413/4096 [00:14<00:02, 233.56it/s]
Adding requests:  84%|████████▍ | 3438/4096 [00:14<00:03, 204.54it/s]
Adding requests:  84%|████████▍ | 3460/4096 [00:14<00:03, 208.03it/s]
Adding requests:  85%|████████▌ | 3482/4096 [00:14<00:03, 204.39it/s]
Adding requests:  86%|████████▌ | 3504/4096 [00:15<00:03, 183.46it/s]
Adding requests:  86%|████████▋ | 3533/4096 [00:15<00:02, 207.88it/s]
Adding requests:  87%|████████▋ | 3564/4096 [00:15<00:02, 232.89it/s]
Adding requests:  88%|████████▊ | 3593/4096 [00:15<00:02, 247.15it/s]
Adding requests:  88%|████████▊ | 3622/4096 [00:15<00:01, 258.41it/s]
Adding requests:  89%|████████▉ | 3653/4096 [00:15<00:01, 269.78it/s]
Adding requests:  90%|████████▉ | 3681/4096 [00:15<00:01, 266.22it/s]
Adding requests:  91%|█████████ | 3708/4096 [00:15<00:01, 266.24it/s]
Adding requests:  91%|█████████ | 3735/4096 [00:15<00:01, 220.50it/s]
Adding requests:  92%|█████████▏| 3759/4096 [00:16<00:01, 171.50it/s]
Adding requests:  92%|█████████▏| 3779/4096 [00:16<00:01, 162.84it/s]
Adding requests:  93%|█████████▎| 3797/4096 [00:16<00:01, 157.70it/s]
Adding requests:  93%|█████████▎| 3814/4096 [00:16<00:01, 153.28it/s]
Adding requests:  94%|█████████▎| 3831/4096 [00:16<00:01, 152.19it/s]
Adding requests:  94%|█████████▍| 3849/4096 [00:16<00:01, 158.22it/s]
Adding requests:  95%|█████████▍| 3879/4096 [00:16<00:01, 195.05it/s]
Adding requests:  95%|█████████▌| 3906/4096 [00:16<00:00, 213.81it/s]
Adding requests:  96%|█████████▌| 3932/4096 [00:17<00:00, 224.78it/s]
Adding requests:  97%|█████████▋| 3961/4096 [00:17<00:00, 243.15it/s]
Adding requests:  97%|█████████▋| 3990/4096 [00:17<00:00, 254.67it/s]
Adding requests:  98%|█████████▊| 4020/4096 [00:17<00:00, 264.96it/s]
Adding requests:  99%|█████████▉| 4048/4096 [00:17<00:00, 268.87it/s]
Adding requests: 100%|█████████▉| 4077/4096 [00:17<00:00, 274.37it/s]
Adding requests: 100%|██████████| 4096/4096 [00:17<00:00, 232.20it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  56%|█████▌    | 2291/4096 [00:00<00:00, 7133.19it/s, est. speed input: 7305016.88 toks/s, output: 7133.41 toks/s]
Processed prompts:  73%|███████▎  | 3005/4096 [00:05<00:02, 426.44it/s, est. speed input: 556308.65 toks/s, output: 543.27 toks/s]   
Processed prompts:  81%|████████  | 3307/4096 [00:07<00:02, 322.56it/s, est. speed input: 440195.20 toks/s, output: 429.88 toks/s]
Processed prompts:  85%|████████▍ | 3478/4096 [00:09<00:02, 269.72it/s, est. speed input: 389445.45 toks/s, output: 380.32 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:09<00:02, 251.68it/s, est. speed input: 372550.23 toks/s, output: 363.82 toks/s]
Processed prompts:  89%|████████▉ | 3661/4096 [00:10<00:01, 239.53it/s, est. speed input: 362740.03 toks/s, output: 354.24 toks/s]
Processed prompts:  91%|█████████ | 3716/4096 [00:10<00:01, 220.87it/s, est. speed input: 352178.18 toks/s, output: 343.92 toks/s]
Processed prompts:  92%|█████████▏| 3757/4096 [00:11<00:01, 216.15it/s, est. speed input: 348429.37 toks/s, output: 340.26 toks/s]
Processed prompts:  93%|█████████▎| 3791/4096 [00:11<00:01, 206.71it/s, est. speed input: 344070.31 toks/s, output: 336.01 toks/s]
Processed prompts:  93%|█████████▎| 3819/4096 [00:11<00:01, 193.16it/s, est. speed input: 339417.57 toks/s, output: 331.46 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [00:11<00:01, 175.23it/s, est. speed input: 334365.72 toks/s, output: 326.53 toks/s]
Processed prompts:  94%|█████████▍| 3861/4096 [00:12<00:01, 155.23it/s, est. speed input: 329183.45 toks/s, output: 321.47 toks/s]
Processed prompts:  95%|█████████▍| 3891/4096 [00:12<00:01, 147.97it/s, est. speed input: 325145.67 toks/s, output: 317.52 toks/s]
Processed prompts:  96%|█████████▌| 3923/4096 [00:12<00:01, 147.37it/s, est. speed input: 322033.59 toks/s, output: 314.49 toks/s]
Processed prompts:  97%|█████████▋| 3955/4096 [00:12<00:00, 143.64it/s, est. speed input: 318519.82 toks/s, output: 311.05 toks/s]
Processed prompts:  97%|█████████▋| 3987/4096 [00:12<00:00, 141.03it/s, est. speed input: 315175.78 toks/s, output: 307.79 toks/s]
Processed prompts:  98%|█████████▊| 4019/4096 [00:13<00:00, 138.34it/s, est. speed input: 311847.74 toks/s, output: 304.54 toks/s]
Processed prompts:  99%|█████████▉| 4051/4096 [00:13<00:00, 140.94it/s, est. speed input: 309270.53 toks/s, output: 302.02 toks/s]
Processed prompts: 100%|█████████▉| 4083/4096 [00:13<00:00, 163.98it/s, est. speed input: 309074.56 toks/s, output: 301.83 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:13<00:00, 163.98it/s, est. speed input: 310049.77 toks/s, output: 302.78 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:13<00:00, 302.78it/s, est. speed input: 310049.77 toks/s, output: 302.78 toks/s]
[rank0]:[W126 16:11:00.500836382 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 16:11:03
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:12:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=738393) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=738393) WARNING 01-26 16:12:32 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     def forward(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     raise e
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/tmp/torchinductor_root/77/c77zs2iegpb4fphxop7pddy5hebnskano2xyzjd5gklmhiigfqg3.py", line 1093, in call
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 8)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 327, in quant_slide_int8_triton
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) ERROR 01-26 16:12:39 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered

STDERR:
[2026-01-26 16:12:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:12:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:12:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:12:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:12:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:12:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:12:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:12:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:12:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:12:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:12:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:12:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:12:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:12:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:12:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:12:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=738393) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=738393) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=738393) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01s/it]
(EngineCore_DP0 pid=738393) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.02it/s]
(EngineCore_DP0 pid=738393) 
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=738393) [2026-01-26 16:12:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=738393) [rank0]:W0126 16:12:38.159000 738393 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=738393) [rank0]:W0126 16:12:38.246000 738393 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=738393) [rank0]:W0126 16:12:39.428000 738393 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=738393) [rank0]:W0126 16:12:39.561000 738393 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=738393) Process EngineCore_DP0:
(EngineCore_DP0 pid=738393) Traceback (most recent call last):
(EngineCore_DP0 pid=738393)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=738393)     self.run()
(EngineCore_DP0 pid=738393)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=738393)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=738393)     raise e
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=738393)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=738393)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=738393)     super().__init__(
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=738393)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=738393)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=738393)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=738393)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=738393)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=738393)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=738393)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=738393)     return func(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=738393)     return func(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=738393)     self.model_runner.profile_run()
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=738393)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=738393)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=738393)     return func(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=738393)     outputs = self.model(
(EngineCore_DP0 pid=738393)               ^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=738393)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=738393)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=738393)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=738393)     hidden_states = self.model(
(EngineCore_DP0 pid=738393)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=738393)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=738393)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=738393)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=738393)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=738393)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=738393)     def forward(
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=738393)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=738393)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=738393)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=738393)     raise e
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=738393)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=738393)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=738393)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=738393)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=738393)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=738393)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=738393)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=738393)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=738393)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=738393)     return compiled_fn(full_args)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=738393)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=738393)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=738393)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=738393)                             ^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=738393)     outs = compiled_fn(args)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=738393)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=738393)     return self.current_callable(inputs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=738393)     out = model(new_inputs)
(EngineCore_DP0 pid=738393)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/tmp/torchinductor_root/77/c77zs2iegpb4fphxop7pddy5hebnskano2xyzjd5gklmhiigfqg3.py", line 1093, in call
(EngineCore_DP0 pid=738393)     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 8)
(EngineCore_DP0 pid=738393)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=738393)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=738393)     return fn(input, L)
(EngineCore_DP0 pid=738393)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 327, in quant_slide_int8_triton
(EngineCore_DP0 pid=738393)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=738393)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=738393)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=738393)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=738393)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=738393)     self._init_handles()
(EngineCore_DP0 pid=738393)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=738393)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=738393)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=738393) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 16:12:40.046627870 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-26 17:35:04
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:35:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=848684) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=848684) WARNING 01-26 17:35:34 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=848684) WARNING 01-26 17:35:48 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 23.80 requests/s, 12210.77 total tokens/s, 23.80 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 17:35:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:35:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:35:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:35:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:35:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:35:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:35:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:35:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:35:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:35:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:35:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:35:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:35:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:35:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:35:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:35:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:35:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:35:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:35:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:35:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:35:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:35:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:35:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:35:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:35:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:35:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:35:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:35:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=848684) [2026-01-26 17:35:19] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=848684) [2026-01-26 17:35:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=848684) [2026-01-26 17:35:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=848684) [2026-01-26 17:35:19] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=848684) [2026-01-26 17:35:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=848684) [2026-01-26 17:35:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=848684) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=848684) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.44s/it]
(EngineCore_DP0 pid=848684) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.20it/s]
(EngineCore_DP0 pid=848684) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
(EngineCore_DP0 pid=848684) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
(EngineCore_DP0 pid=848684) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
(EngineCore_DP0 pid=848684) 
(EngineCore_DP0 pid=848684) [2026-01-26 17:35:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=848684) [2026-01-26 17:35:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34406400 bytes
(EngineCore_DP0 pid=848684) [2026-01-26 17:35:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=848684) [2026-01-26 17:35:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 24576000 bytes
(EngineCore_DP0 pid=848684) [2026-01-26 17:35:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=848684) [2026-01-26 17:35:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 132710400 bytes
(EngineCore_DP0 pid=848684) [2026-01-26 17:35:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=848684) [2026-01-26 17:35:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 66355200 bytes
(EngineCore_DP0 pid=848684) 2026-01-26 17:35:48,844 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=848684) 2026-01-26 17:35:48,883 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=848684) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.45it/s]
(EngineCore_DP0 pid=848684) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 10.19it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  39%|███▉      | 50/128 [00:00<00:00, 492.30it/s]
Adding requests:  85%|████████▌ | 109/128 [00:00<00:00, 546.11it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 542.66it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 34.55it/s, est. speed input: 17690.12 toks/s, output: 34.55 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:04, 27.97it/s, est. speed input: 14743.43 toks/s, output: 28.80 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:04, 26.69it/s, est. speed input: 14137.77 toks/s, output: 27.61 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:04, 26.01it/s, est. speed input: 13808.62 toks/s, output: 26.97 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:04, 25.62it/s, est. speed input: 13607.90 toks/s, output: 26.58 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:04, 25.36it/s, est. speed input: 13467.11 toks/s, output: 26.30 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:04, 25.24it/s, est. speed input: 13375.53 toks/s, output: 26.12 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:04, 25.16it/s, est. speed input: 13304.45 toks/s, output: 25.99 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:03, 25.06it/s, est. speed input: 13241.05 toks/s, output: 25.86 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:03, 25.02it/s, est. speed input: 13194.97 toks/s, output: 25.77 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:03, 25.00it/s, est. speed input: 13158.81 toks/s, output: 25.70 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:03, 24.93it/s, est. speed input: 13119.97 toks/s, output: 25.62 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:03, 24.94it/s, est. speed input: 13094.03 toks/s, output: 25.57 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:03, 24.73it/s, est. speed input: 13045.84 toks/s, output: 25.48 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:03, 24.79it/s, est. speed input: 13026.94 toks/s, output: 25.44 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:03, 24.65it/s, est. speed input: 12992.18 toks/s, output: 25.38 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:03, 24.74it/s, est. speed input: 12979.30 toks/s, output: 25.35 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:02<00:02, 24.63it/s, est. speed input: 12951.40 toks/s, output: 25.30 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:02<00:02, 24.68it/s, est. speed input: 12938.06 toks/s, output: 25.27 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:02<00:02, 24.68it/s, est. speed input: 12923.69 toks/s, output: 25.24 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:02, 24.39it/s, est. speed input: 12885.50 toks/s, output: 25.17 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:02<00:02, 24.45it/s, est. speed input: 12872.28 toks/s, output: 25.14 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:02, 24.19it/s, est. speed input: 12836.85 toks/s, output: 25.07 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:02, 24.33it/s, est. speed input: 12828.35 toks/s, output: 25.06 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:03<00:02, 24.46it/s, est. speed input: 12822.67 toks/s, output: 25.04 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:03<00:01, 24.33it/s, est. speed input: 12802.28 toks/s, output: 25.00 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:03<00:01, 24.46it/s, est. speed input: 12797.97 toks/s, output: 25.00 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:03<00:01, 24.47it/s, est. speed input: 12788.83 toks/s, output: 24.98 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:03<00:01, 24.56it/s, est. speed input: 12785.24 toks/s, output: 24.97 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:03<00:01, 24.63it/s, est. speed input: 12782.35 toks/s, output: 24.97 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:03<00:01, 24.68it/s, est. speed input: 12779.69 toks/s, output: 24.96 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:03<00:01, 24.48it/s, est. speed input: 12764.27 toks/s, output: 24.93 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:04<00:01, 24.58it/s, est. speed input: 12762.72 toks/s, output: 24.93 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:04<00:00, 24.62it/s, est. speed input: 12759.46 toks/s, output: 24.92 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:04<00:00, 24.68it/s, est. speed input: 12757.82 toks/s, output: 24.92 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:04<00:00, 24.72it/s, est. speed input: 12756.49 toks/s, output: 24.91 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:04<00:00, 24.76it/s, est. speed input: 12755.49 toks/s, output: 24.91 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:04<00:00, 24.76it/s, est. speed input: 12753.73 toks/s, output: 24.91 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:04<00:00, 24.78it/s, est. speed input: 12752.65 toks/s, output: 24.91 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:04<00:00, 24.78it/s, est. speed input: 12750.87 toks/s, output: 24.90 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:05<00:00, 24.79it/s, est. speed input: 12750.02 toks/s, output: 24.90 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 24.80it/s, est. speed input: 12748.98 toks/s, output: 24.90 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 24.80it/s, est. speed input: 12748.98 toks/s, output: 24.90 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 24.90it/s, est. speed input: 12748.98 toks/s, output: 24.90 toks/s]
[rank0]:[W126 17:35:56.204799995 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 17:35:59
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:36:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=850047) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=850047) WARNING 01-26 17:36:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=850047) WARNING 01-26 17:36:43 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 23.37 requests/s, 23952.90 total tokens/s, 23.37 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 17:36:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:36:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:36:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:36:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:36:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:36:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:36:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:36:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:36:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:36:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:36:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:36:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:36:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:36:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:36:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:36:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:36:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:36:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:36:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:36:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:36:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:36:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:36:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:36:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:36:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:36:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:36:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:36:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=850047) [2026-01-26 17:36:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=850047) [2026-01-26 17:36:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=850047) [2026-01-26 17:36:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=850047) [2026-01-26 17:36:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=850047) [2026-01-26 17:36:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=850047) [2026-01-26 17:36:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=850047) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=850047) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.28s/it]
(EngineCore_DP0 pid=850047) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.37it/s]
(EngineCore_DP0 pid=850047) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.10it/s]
(EngineCore_DP0 pid=850047) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.02s/it]
(EngineCore_DP0 pid=850047) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.02it/s]
(EngineCore_DP0 pid=850047) 
(EngineCore_DP0 pid=850047) [2026-01-26 17:36:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=850047) [2026-01-26 17:36:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34406400 bytes
(EngineCore_DP0 pid=850047) [2026-01-26 17:36:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=850047) [2026-01-26 17:36:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 24576000 bytes
(EngineCore_DP0 pid=850047) [2026-01-26 17:36:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=850047) [2026-01-26 17:36:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 132710400 bytes
(EngineCore_DP0 pid=850047) [2026-01-26 17:36:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=850047) [2026-01-26 17:36:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 66355200 bytes
(EngineCore_DP0 pid=850047) 2026-01-26 17:36:42,981 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=850047) 2026-01-26 17:36:43,025 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=850047) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.90it/s]
(EngineCore_DP0 pid=850047) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 10.07it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 273.59it/s]
Adding requests:  49%|████▉     | 63/128 [00:00<00:00, 314.16it/s]
Adding requests:  74%|███████▍  | 95/128 [00:00<00:00, 312.96it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 314.38it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:01, 91.00it/s, est. speed input: 93192.78 toks/s, output: 91.00 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 34.94it/s, est. speed input: 39615.20 toks/s, output: 38.69 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:03, 30.32it/s, est. speed input: 34788.82 toks/s, output: 33.97 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:03, 28.48it/s, est. speed input: 32977.44 toks/s, output: 32.20 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:03, 27.15it/s, est. speed input: 31706.88 toks/s, output: 30.96 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:03, 26.35it/s, est. speed input: 30967.21 toks/s, output: 30.24 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:03, 25.69it/s, est. speed input: 30359.17 toks/s, output: 29.65 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:03, 25.18it/s, est. speed input: 29856.48 toks/s, output: 29.16 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:03, 24.79it/s, est. speed input: 29429.42 toks/s, output: 28.74 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:03, 24.43it/s, est. speed input: 29042.58 toks/s, output: 28.36 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:03, 24.22it/s, est. speed input: 28723.86 toks/s, output: 28.05 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:02<00:02, 24.13it/s, est. speed input: 28459.70 toks/s, output: 27.79 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:02<00:02, 24.08it/s, est. speed input: 28228.54 toks/s, output: 27.57 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:02<00:02, 23.90it/s, est. speed input: 27992.79 toks/s, output: 27.34 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:02, 23.88it/s, est. speed input: 27803.94 toks/s, output: 27.15 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:02<00:02, 23.87it/s, est. speed input: 27634.54 toks/s, output: 26.99 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:02, 23.65it/s, est. speed input: 27443.48 toks/s, output: 26.80 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:02, 23.65it/s, est. speed input: 27295.02 toks/s, output: 26.66 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:02, 23.66it/s, est. speed input: 27162.60 toks/s, output: 26.53 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:03<00:02, 23.52it/s, est. speed input: 27016.78 toks/s, output: 26.38 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:03<00:01, 23.58it/s, est. speed input: 26907.42 toks/s, output: 26.28 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:03<00:01, 23.63it/s, est. speed input: 26808.11 toks/s, output: 26.18 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:03<00:01, 23.64it/s, est. speed input: 26712.84 toks/s, output: 26.09 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:03<00:01, 23.25it/s, est. speed input: 26569.90 toks/s, output: 25.95 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:03<00:01, 23.42it/s, est. speed input: 26495.20 toks/s, output: 25.87 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:03<00:01, 23.54it/s, est. speed input: 26425.24 toks/s, output: 25.81 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:01, 23.61it/s, est. speed input: 26358.95 toks/s, output: 25.74 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:04<00:01, 23.35it/s, est. speed input: 26259.21 toks/s, output: 25.64 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:04<00:00, 23.47it/s, est. speed input: 26201.60 toks/s, output: 25.59 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:04<00:00, 23.54it/s, est. speed input: 26144.91 toks/s, output: 25.53 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:04<00:00, 23.60it/s, est. speed input: 26092.64 toks/s, output: 25.48 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:04<00:00, 23.51it/s, est. speed input: 26029.38 toks/s, output: 25.42 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:04<00:00, 23.62it/s, est. speed input: 25986.82 toks/s, output: 25.38 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:04<00:00, 23.61it/s, est. speed input: 25939.16 toks/s, output: 25.33 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:04<00:00, 23.64it/s, est. speed input: 25896.60 toks/s, output: 25.29 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.68it/s, est. speed input: 25857.95 toks/s, output: 25.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.68it/s, est. speed input: 25857.95 toks/s, output: 25.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 25.25it/s, est. speed input: 25857.95 toks/s, output: 25.25 toks/s]
[rank0]:[W126 17:36:50.756362496 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 17:36:52
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:37:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=851339) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=851339) WARNING 01-26 17:37:22 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=851339) WARNING 01-26 17:37:37 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 45.41 requests/s, 46544.00 total tokens/s, 45.41 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 17:37:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:37:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:37:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:37:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:37:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:37:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:37:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:37:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:37:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:37:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:37:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:37:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:37:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:37:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:37:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:37:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=851339) [2026-01-26 17:37:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=851339) [2026-01-26 17:37:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=851339) [2026-01-26 17:37:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=851339) [2026-01-26 17:37:08] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=851339) [2026-01-26 17:37:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=851339) [2026-01-26 17:37:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=851339) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=851339) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.28s/it]
(EngineCore_DP0 pid=851339) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.40it/s]
(EngineCore_DP0 pid=851339) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.15it/s]
(EngineCore_DP0 pid=851339) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.01it/s]
(EngineCore_DP0 pid=851339) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.04it/s]
(EngineCore_DP0 pid=851339) 
(EngineCore_DP0 pid=851339) [2026-01-26 17:37:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=851339) [2026-01-26 17:37:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34406400 bytes
(EngineCore_DP0 pid=851339) [2026-01-26 17:37:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=851339) [2026-01-26 17:37:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 24576000 bytes
(EngineCore_DP0 pid=851339) [2026-01-26 17:37:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=851339) [2026-01-26 17:37:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 132710400 bytes
(EngineCore_DP0 pid=851339) [2026-01-26 17:37:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=851339) [2026-01-26 17:37:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 66355200 bytes
(EngineCore_DP0 pid=851339) 2026-01-26 17:37:37,202 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=851339) 2026-01-26 17:37:37,240 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=851339) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  7.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.65it/s]
(EngineCore_DP0 pid=851339) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 10.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 10.94it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:58,  4.39it/s]
Adding requests:  11%|█         | 28/256 [00:00<00:02, 106.31it/s]
Adding requests:  25%|██▍       | 63/256 [00:00<00:01, 191.72it/s]
Adding requests:  36%|███▋      | 93/256 [00:00<00:00, 227.16it/s]
Adding requests:  49%|████▉     | 126/256 [00:00<00:00, 259.30it/s]
Adding requests:  62%|██████▎   | 160/256 [00:00<00:00, 283.03it/s]
Adding requests:  75%|███████▍  | 191/256 [00:00<00:00, 286.56it/s]
Adding requests:  88%|████████▊ | 225/256 [00:00<00:00, 301.75it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 247.20it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:00<00:00, 346.44it/s, est. speed input: 354773.11 toks/s, output: 346.44 toks/s]
Processed prompts:  29%|██▊       | 73/256 [00:00<00:02, 79.95it/s, est. speed input: 93049.58 toks/s, output: 90.87 toks/s]   
Processed prompts:  36%|███▌      | 91/256 [00:01<00:02, 67.13it/s, est. speed input: 79336.75 toks/s, output: 77.48 toks/s]
Processed prompts:  40%|████      | 103/256 [00:01<00:02, 61.78it/s, est. speed input: 74073.06 toks/s, output: 72.34 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:01<00:02, 57.04it/s, est. speed input: 70219.30 toks/s, output: 68.57 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:01<00:02, 55.17it/s, est. speed input: 68343.70 toks/s, output: 66.74 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:01<00:02, 55.38it/s, est. speed input: 67648.45 toks/s, output: 66.06 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:02<00:02, 51.35it/s, est. speed input: 65520.02 toks/s, output: 63.98 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:02<00:02, 50.84it/s, est. speed input: 64674.56 toks/s, output: 63.16 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:02<00:02, 50.03it/s, est. speed input: 63811.17 toks/s, output: 62.32 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:02<00:02, 49.75it/s, est. speed input: 63129.08 toks/s, output: 61.65 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:02<00:01, 49.51it/s, est. speed input: 62506.21 toks/s, output: 61.04 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:02<00:01, 49.29it/s, est. speed input: 61932.56 toks/s, output: 60.48 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:02<00:01, 49.24it/s, est. speed input: 61430.38 toks/s, output: 59.99 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:02<00:01, 49.10it/s, est. speed input: 60951.86 toks/s, output: 59.52 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:03<00:01, 49.14it/s, est. speed input: 60535.16 toks/s, output: 59.12 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:03<00:01, 49.09it/s, est. speed input: 60137.22 toks/s, output: 58.73 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:03<00:01, 49.09it/s, est. speed input: 59774.03 toks/s, output: 58.37 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:03<00:01, 49.08it/s, est. speed input: 59434.99 toks/s, output: 58.04 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:03<00:01, 47.87it/s, est. speed input: 58949.34 toks/s, output: 57.57 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:03<00:00, 48.19it/s, est. speed input: 58658.10 toks/s, output: 57.28 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:03<00:00, 48.41it/s, est. speed input: 58383.70 toks/s, output: 57.02 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:03<00:00, 48.61it/s, est. speed input: 58131.38 toks/s, output: 56.77 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:04<00:00, 48.59it/s, est. speed input: 57876.08 toks/s, output: 56.52 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:04<00:00, 48.77it/s, est. speed input: 57658.29 toks/s, output: 56.31 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:04<00:00, 48.82it/s, est. speed input: 57443.72 toks/s, output: 56.10 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:04<00:00, 48.88it/s, est. speed input: 57243.09 toks/s, output: 55.90 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:04<00:00, 48.93it/s, est. speed input: 57054.63 toks/s, output: 55.72 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 48.93it/s, est. speed input: 56978.96 toks/s, output: 55.64 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 55.64it/s, est. speed input: 56978.96 toks/s, output: 55.64 toks/s]
[rank0]:[W126 17:37:45.577411995 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 17:37:47
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:37:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=852655) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=852655) WARNING 01-26 17:38:20 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=852655) WARNING 01-26 17:38:35 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 62.48 requests/s, 64045.56 total tokens/s, 62.48 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 17:37:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:37:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:37:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:37:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:37:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:37:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:37:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:37:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:37:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:38:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:38:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:38:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:38:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:38:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:38:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:38:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:38:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:38:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:38:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:38:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:38:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:38:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:38:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=852655) [2026-01-26 17:38:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=852655) [2026-01-26 17:38:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=852655) [2026-01-26 17:38:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=852655) [2026-01-26 17:38:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=852655) [2026-01-26 17:38:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=852655) [2026-01-26 17:38:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=852655) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=852655) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.44s/it]
(EngineCore_DP0 pid=852655) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.21it/s]
(EngineCore_DP0 pid=852655) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.02it/s]
(EngineCore_DP0 pid=852655) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.07s/it]
(EngineCore_DP0 pid=852655) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.05s/it]
(EngineCore_DP0 pid=852655) 
(EngineCore_DP0 pid=852655) [2026-01-26 17:38:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=852655) [2026-01-26 17:38:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34406400 bytes
(EngineCore_DP0 pid=852655) [2026-01-26 17:38:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=852655) [2026-01-26 17:38:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 24576000 bytes
(EngineCore_DP0 pid=852655) [2026-01-26 17:38:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=852655) [2026-01-26 17:38:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 132710400 bytes
(EngineCore_DP0 pid=852655) [2026-01-26 17:38:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=852655) [2026-01-26 17:38:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 66355200 bytes
(EngineCore_DP0 pid=852655) 2026-01-26 17:38:35,604 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=852655) 2026-01-26 17:38:35,643 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=852655) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  9.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.89it/s]
(EngineCore_DP0 pid=852655) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.91it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▍         | 25/512 [00:00<00:02, 240.68it/s]
Adding requests:  11%|█▏        | 58/512 [00:00<00:01, 290.07it/s]
Adding requests:  18%|█▊        | 91/512 [00:00<00:01, 303.53it/s]
Adding requests:  24%|██▍       | 125/512 [00:00<00:01, 314.90it/s]
Adding requests:  31%|███       | 159/512 [00:00<00:01, 323.61it/s]
Adding requests:  38%|███▊      | 194/512 [00:00<00:00, 329.63it/s]
Adding requests:  45%|████▍     | 230/512 [00:00<00:00, 338.16it/s]
Adding requests:  52%|█████▏    | 264/512 [00:00<00:00, 336.49it/s]
Adding requests:  59%|█████▊    | 300/512 [00:00<00:00, 343.51it/s]
Adding requests:  66%|██████▌   | 338/512 [00:01<00:00, 351.10it/s]
Adding requests:  73%|███████▎  | 375/512 [00:01<00:00, 353.68it/s]
Adding requests:  81%|████████  | 413/512 [00:01<00:00, 358.85it/s]
Adding requests:  88%|████████▊ | 449/512 [00:01<00:00, 349.98it/s]
Adding requests:  95%|█████████▍| 485/512 [00:01<00:00, 348.20it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 337.44it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:00<00:00, 656.72it/s, est. speed input: 672513.66 toks/s, output: 656.73 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:01<00:02, 119.85it/s, est. speed input: 143801.61 toks/s, output: 140.43 toks/s]
Processed prompts:  38%|███▊      | 195/512 [00:01<00:03, 96.68it/s, est. speed input: 119078.52 toks/s, output: 116.29 toks/s] 
Processed prompts:  42%|████▏     | 215/512 [00:01<00:03, 88.22it/s, est. speed input: 110603.55 toks/s, output: 108.01 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:02<00:03, 81.48it/s, est. speed input: 104895.77 toks/s, output: 102.44 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:02<00:03, 77.78it/s, est. speed input: 101708.19 toks/s, output: 99.32 toks/s] 
Processed prompts:  49%|████▉     | 252/512 [00:02<00:03, 77.88it/s, est. speed input: 100643.14 toks/s, output: 98.28 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:02<00:03, 71.43it/s, est. speed input: 97388.14 toks/s, output: 95.11 toks/s] 
Processed prompts:  53%|█████▎    | 270/512 [00:02<00:03, 69.69it/s, est. speed input: 95906.71 toks/s, output: 93.66 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:03<00:03, 68.34it/s, est. speed input: 94608.30 toks/s, output: 92.39 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:03<00:03, 67.08it/s, est. speed input: 93390.14 toks/s, output: 91.20 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:03<00:03, 65.93it/s, est. speed input: 92240.03 toks/s, output: 90.08 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:03<00:03, 65.08it/s, est. speed input: 91187.64 toks/s, output: 89.05 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:03<00:03, 66.17it/s, est. speed input: 90524.34 toks/s, output: 88.40 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:03<00:02, 65.38it/s, est. speed input: 89634.12 toks/s, output: 87.53 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:03<00:02, 64.57it/s, est. speed input: 88767.66 toks/s, output: 86.69 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:03<00:02, 64.08it/s, est. speed input: 87970.78 toks/s, output: 85.91 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:04<00:02, 63.54it/s, est. speed input: 87195.39 toks/s, output: 85.15 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:04<00:02, 63.17it/s, est. speed input: 86470.64 toks/s, output: 84.44 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:04<00:02, 63.11it/s, est. speed input: 85817.10 toks/s, output: 83.81 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:04<00:02, 62.98it/s, est. speed input: 85189.39 toks/s, output: 83.19 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:04<00:02, 62.99it/s, est. speed input: 84609.34 toks/s, output: 82.63 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:04<00:02, 63.01it/s, est. speed input: 84063.35 toks/s, output: 82.09 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:04<00:01, 62.94it/s, est. speed input: 83535.80 toks/s, output: 81.58 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:04<00:01, 62.93it/s, est. speed input: 83040.53 toks/s, output: 81.09 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:05<00:01, 62.92it/s, est. speed input: 82569.93 toks/s, output: 80.63 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:05<00:01, 62.92it/s, est. speed input: 82122.80 toks/s, output: 80.20 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:05<00:01, 62.96it/s, est. speed input: 81701.70 toks/s, output: 79.79 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:05<00:01, 62.81it/s, est. speed input: 81282.82 toks/s, output: 79.38 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:05<00:01, 64.51it/s, est. speed input: 81056.28 toks/s, output: 79.16 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:05<00:01, 63.93it/s, est. speed input: 80673.72 toks/s, output: 78.78 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:05<00:00, 63.53it/s, est. speed input: 80308.32 toks/s, output: 78.43 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:05<00:00, 63.43it/s, est. speed input: 79974.60 toks/s, output: 78.10 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:06<00:00, 63.15it/s, est. speed input: 79636.11 toks/s, output: 77.77 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:06<00:00, 63.11it/s, est. speed input: 79325.16 toks/s, output: 77.47 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:06<00:00, 62.94it/s, est. speed input: 79015.52 toks/s, output: 77.16 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:06<00:00, 62.89it/s, est. speed input: 78722.78 toks/s, output: 76.88 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:06<00:00, 62.83it/s, est. speed input: 78440.13 toks/s, output: 76.60 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:06<00:00, 63.65it/s, est. speed input: 78235.50 toks/s, output: 76.40 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 63.65it/s, est. speed input: 78540.98 toks/s, output: 76.70 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 76.70it/s, est. speed input: 78540.98 toks/s, output: 76.70 toks/s]
[rank0]:[W126 17:38:46.020493209 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 17:38:48
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:39:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=854087) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=854087) WARNING 01-26 17:39:24 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=854087) WARNING 01-26 17:39:40 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 68.09 requests/s, 69795.01 total tokens/s, 68.09 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 17:39:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:39:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:39:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:39:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:39:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:39:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:39:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:39:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:39:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:39:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:39:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:39:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:39:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:39:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:39:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:39:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:39:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:39:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:39:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:39:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:39:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:39:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:39:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:39:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:39:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:39:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:39:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:39:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=854087) [2026-01-26 17:39:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=854087) [2026-01-26 17:39:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=854087) [2026-01-26 17:39:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=854087) [2026-01-26 17:39:10] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=854087) [2026-01-26 17:39:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=854087) [2026-01-26 17:39:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=854087) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=854087) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.23s/it]
(EngineCore_DP0 pid=854087) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.41it/s]
(EngineCore_DP0 pid=854087) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.14it/s]
(EngineCore_DP0 pid=854087) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.01it/s]
(EngineCore_DP0 pid=854087) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.05it/s]
(EngineCore_DP0 pid=854087) 
(EngineCore_DP0 pid=854087) [2026-01-26 17:39:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=854087) [2026-01-26 17:39:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34406400 bytes
(EngineCore_DP0 pid=854087) [2026-01-26 17:39:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=854087) [2026-01-26 17:39:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 24576000 bytes
(EngineCore_DP0 pid=854087) [2026-01-26 17:39:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=854087) [2026-01-26 17:39:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 132710400 bytes
(EngineCore_DP0 pid=854087) [2026-01-26 17:39:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=854087) [2026-01-26 17:39:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 66355200 bytes
(EngineCore_DP0 pid=854087) 2026-01-26 17:39:40,122 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=854087) 2026-01-26 17:39:40,162 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=854087) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  8.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  7.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  9.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.90it/s]
(EngineCore_DP0 pid=854087) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 11.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 11.18it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 11.16it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 276.61it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 317.32it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 314.42it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 319.93it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 328.92it/s]
Adding requests:  19%|█▉        | 199/1024 [00:00<00:02, 338.87it/s]
Adding requests:  23%|██▎       | 235/1024 [00:00<00:02, 344.20it/s]
Adding requests:  26%|██▋       | 270/1024 [00:00<00:02, 342.41it/s]
Adding requests:  30%|██▉       | 306/1024 [00:00<00:02, 347.29it/s]
Adding requests:  33%|███▎      | 343/1024 [00:01<00:01, 353.40it/s]
Adding requests:  37%|███▋      | 380/1024 [00:01<00:01, 356.10it/s]
Adding requests:  41%|████      | 417/1024 [00:01<00:01, 357.98it/s]
Adding requests:  44%|████▍     | 453/1024 [00:01<00:01, 344.92it/s]
Adding requests:  48%|████▊     | 490/1024 [00:01<00:01, 351.46it/s]
Adding requests:  51%|█████▏    | 527/1024 [00:01<00:01, 356.84it/s]
Adding requests:  55%|█████▍    | 563/1024 [00:01<00:01, 351.03it/s]
Adding requests:  58%|█████▊    | 599/1024 [00:01<00:01, 339.66it/s]
Adding requests:  62%|██████▏   | 634/1024 [00:01<00:01, 336.17it/s]
Adding requests:  65%|██████▌   | 668/1024 [00:01<00:01, 329.33it/s]
Adding requests:  69%|██████▉   | 704/1024 [00:02<00:00, 335.01it/s]
Adding requests:  72%|███████▏  | 738/1024 [00:02<00:00, 329.94it/s]
Adding requests:  75%|███████▌  | 772/1024 [00:02<00:00, 331.32it/s]
Adding requests:  79%|███████▊  | 806/1024 [00:02<00:00, 330.68it/s]
Adding requests:  82%|████████▏ | 842/1024 [00:02<00:00, 336.24it/s]
Adding requests:  86%|████████▌ | 876/1024 [00:02<00:00, 337.30it/s]
Adding requests:  89%|████████▉ | 911/1024 [00:02<00:00, 338.21it/s]
Adding requests:  92%|█████████▏| 945/1024 [00:02<00:00, 333.02it/s]
Adding requests:  96%|█████████▌| 979/1024 [00:02<00:00, 334.61it/s]
Adding requests:  99%|█████████▉| 1013/1024 [00:03<00:00, 328.79it/s]
Adding requests: 100%|██████████| 1024/1024 [00:03<00:00, 337.38it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:00<00:00, 1110.77it/s, est. speed input: 1137472.80 toks/s, output: 1110.78 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:01<00:04, 147.57it/s, est. speed input: 181984.68 toks/s, output: 177.72 toks/s]   
Processed prompts:  36%|███▋      | 372/1024 [00:02<00:05, 120.08it/s, est. speed input: 151800.45 toks/s, output: 148.24 toks/s]
Processed prompts:  39%|███▉      | 403/1024 [00:02<00:05, 106.45it/s, est. speed input: 138794.75 toks/s, output: 135.54 toks/s]
Processed prompts:  41%|████▏     | 424/1024 [00:03<00:05, 103.97it/s, est. speed input: 135442.92 toks/s, output: 132.27 toks/s]
Processed prompts:  43%|████▎     | 441/1024 [00:03<00:05, 98.61it/s, est. speed input: 131461.94 toks/s, output: 128.38 toks/s] 
Processed prompts:  44%|████▍     | 455/1024 [00:03<00:06, 90.13it/s, est. speed input: 126791.43 toks/s, output: 123.82 toks/s]
Processed prompts:  46%|████▌     | 467/1024 [00:03<00:06, 81.45it/s, est. speed input: 122404.68 toks/s, output: 119.54 toks/s]
Processed prompts:  47%|████▋     | 477/1024 [00:04<00:06, 82.14it/s, est. speed input: 121431.18 toks/s, output: 118.58 toks/s]
Processed prompts:  47%|████▋     | 486/1024 [00:04<00:06, 81.41it/s, est. speed input: 120258.97 toks/s, output: 117.44 toks/s]
Processed prompts:  48%|████▊     | 495/1024 [00:04<00:06, 80.62it/s, est. speed input: 119133.76 toks/s, output: 116.34 toks/s]
Processed prompts:  49%|████▉     | 504/1024 [00:04<00:06, 80.12it/s, est. speed input: 118108.91 toks/s, output: 115.34 toks/s]
Processed prompts:  50%|█████     | 513/1024 [00:04<00:06, 79.52it/s, est. speed input: 117108.92 toks/s, output: 114.36 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:04<00:07, 62.99it/s, est. speed input: 113266.26 toks/s, output: 110.61 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:04<00:07, 64.19it/s, est. speed input: 112218.73 toks/s, output: 109.59 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:04<00:07, 64.71it/s, est. speed input: 111139.14 toks/s, output: 108.53 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:05<00:07, 65.52it/s, est. speed input: 110170.81 toks/s, output: 107.59 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:05<00:07, 66.48it/s, est. speed input: 109293.83 toks/s, output: 106.73 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:05<00:06, 67.20it/s, est. speed input: 108452.32 toks/s, output: 105.91 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:05<00:06, 67.61it/s, est. speed input: 107633.06 toks/s, output: 105.11 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:05<00:06, 67.59it/s, est. speed input: 106810.23 toks/s, output: 104.31 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:05<00:06, 67.77it/s, est. speed input: 106044.13 toks/s, output: 103.56 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:05<00:06, 68.32it/s, est. speed input: 105354.20 toks/s, output: 102.88 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:05<00:06, 68.57it/s, est. speed input: 104676.29 toks/s, output: 102.22 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:06<00:06, 68.51it/s, est. speed input: 104000.62 toks/s, output: 101.56 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:06<00:05, 68.38it/s, est. speed input: 103342.27 toks/s, output: 100.92 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:06<00:05, 68.00it/s, est. speed input: 102682.24 toks/s, output: 100.28 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:06<00:05, 68.03it/s, est. speed input: 102073.64 toks/s, output: 99.68 toks/s] 
Processed prompts:  63%|██████▎   | 642/1024 [00:06<00:05, 67.04it/s, est. speed input: 101394.79 toks/s, output: 99.02 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:06<00:05, 66.51it/s, est. speed input: 100754.56 toks/s, output: 98.39 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:06<00:05, 67.14it/s, est. speed input: 100227.00 toks/s, output: 97.88 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:06<00:05, 67.60it/s, est. speed input: 99718.39 toks/s, output: 97.38 toks/s] 
Processed prompts:  66%|██████▌   | 674/1024 [00:06<00:05, 67.77it/s, est. speed input: 99213.67 toks/s, output: 96.89 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:07<00:05, 68.16it/s, est. speed input: 98747.29 toks/s, output: 96.43 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:07<00:04, 68.04it/s, est. speed input: 98265.09 toks/s, output: 95.96 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:07<00:04, 68.56it/s, est. speed input: 97844.70 toks/s, output: 95.55 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:07<00:04, 68.64it/s, est. speed input: 97416.11 toks/s, output: 95.13 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:07<00:04, 67.95it/s, est. speed input: 96946.20 toks/s, output: 94.67 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:07<00:04, 67.60it/s, est. speed input: 96499.96 toks/s, output: 94.24 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:07<00:04, 67.51it/s, est. speed input: 96078.73 toks/s, output: 93.83 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:07<00:04, 67.95it/s, est. speed input: 95705.07 toks/s, output: 93.46 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:08<00:04, 68.33it/s, est. speed input: 95347.04 toks/s, output: 93.11 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:08<00:03, 68.49it/s, est. speed input: 94992.47 toks/s, output: 92.77 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:08<00:03, 68.61it/s, est. speed input: 94647.89 toks/s, output: 92.43 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:08<00:03, 68.10it/s, est. speed input: 94275.13 toks/s, output: 92.07 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:08<00:03, 67.99it/s, est. speed input: 93928.11 toks/s, output: 91.73 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:08<00:03, 69.63it/s, est. speed input: 93696.12 toks/s, output: 91.50 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:08<00:03, 69.23it/s, est. speed input: 93376.73 toks/s, output: 91.19 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:08<00:03, 68.96it/s, est. speed input: 93066.19 toks/s, output: 90.88 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:08<00:03, 68.31it/s, est. speed input: 92736.69 toks/s, output: 90.56 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:09<00:03, 68.61it/s, est. speed input: 92459.57 toks/s, output: 90.29 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:09<00:02, 68.46it/s, est. speed input: 92169.11 toks/s, output: 90.01 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:09<00:02, 68.30it/s, est. speed input: 91882.30 toks/s, output: 89.73 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:09<00:02, 68.06it/s, est. speed input: 91596.15 toks/s, output: 89.45 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:09<00:02, 67.40it/s, est. speed input: 91289.18 toks/s, output: 89.15 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:09<00:02, 67.80it/s, est. speed input: 91037.32 toks/s, output: 88.90 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:09<00:02, 68.00it/s, est. speed input: 90786.94 toks/s, output: 88.66 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:09<00:02, 68.32it/s, est. speed input: 90552.13 toks/s, output: 88.43 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:10<00:02, 67.54it/s, est. speed input: 90270.39 toks/s, output: 88.15 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:10<00:01, 67.24it/s, est. speed input: 90007.73 toks/s, output: 87.90 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:10<00:01, 67.48it/s, est. speed input: 89774.43 toks/s, output: 87.67 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:10<00:01, 67.11it/s, est. speed input: 89518.85 toks/s, output: 87.42 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:10<00:01, 67.63it/s, est. speed input: 89308.53 toks/s, output: 87.22 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:10<00:01, 68.02it/s, est. speed input: 89103.83 toks/s, output: 87.02 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:10<00:01, 68.15it/s, est. speed input: 88896.19 toks/s, output: 86.81 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:10<00:01, 68.13it/s, est. speed input: 88688.02 toks/s, output: 86.61 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:10<00:01, 67.92it/s, est. speed input: 88475.23 toks/s, output: 86.40 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:11<00:01, 67.52it/s, est. speed input: 88255.20 toks/s, output: 86.19 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:11<00:00, 67.23it/s, est. speed input: 88039.64 toks/s, output: 85.98 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:11<00:00, 67.55it/s, est. speed input: 87852.33 toks/s, output: 85.79 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:11<00:00, 67.82it/s, est. speed input: 87670.62 toks/s, output: 85.62 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:11<00:00, 67.81it/s, est. speed input: 87483.68 toks/s, output: 85.43 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:11<00:00, 67.98it/s, est. speed input: 87308.47 toks/s, output: 85.26 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:11<00:00, 68.38it/s, est. speed input: 87148.49 toks/s, output: 85.11 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:11<00:00, 68.29it/s, est. speed input: 86976.52 toks/s, output: 84.94 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:12<00:00, 69.42it/s, est. speed input: 86856.07 toks/s, output: 84.82 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 69.42it/s, est. speed input: 87367.17 toks/s, output: 85.32 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 85.32it/s, est. speed input: 87367.17 toks/s, output: 85.32 toks/s]
[rank0]:[W126 17:39:58.250419625 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 17:40:00
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:40:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=855706) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=855706) WARNING 01-26 17:40:41 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=855706) WARNING 01-26 17:40:59 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 69.37 requests/s, 71106.52 total tokens/s, 69.37 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 17:40:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:40:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:40:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:40:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:40:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:40:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:40:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:40:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:40:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:40:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:40:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:40:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:40:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:40:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:40:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:40:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:40:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:40:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:40:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:40:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:40:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:40:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:40:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:40:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:40:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:40:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:40:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:40:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=855706) [2026-01-26 17:40:27] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=855706) [2026-01-26 17:40:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=855706) [2026-01-26 17:40:27] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=855706) [2026-01-26 17:40:27] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=855706) [2026-01-26 17:40:27] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=855706) [2026-01-26 17:40:27] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=855706) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=855706) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.15s/it]
(EngineCore_DP0 pid=855706) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.51it/s]
(EngineCore_DP0 pid=855706) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.20it/s]
(EngineCore_DP0 pid=855706) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.03it/s]
(EngineCore_DP0 pid=855706) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.08it/s]
(EngineCore_DP0 pid=855706) 
(EngineCore_DP0 pid=855706) [2026-01-26 17:40:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=855706) [2026-01-26 17:40:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34406400 bytes
(EngineCore_DP0 pid=855706) [2026-01-26 17:40:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=855706) [2026-01-26 17:40:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 24576000 bytes
(EngineCore_DP0 pid=855706) [2026-01-26 17:40:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=855706) [2026-01-26 17:40:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 132710400 bytes
(EngineCore_DP0 pid=855706) [2026-01-26 17:40:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=855706) [2026-01-26 17:40:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 66355200 bytes
(EngineCore_DP0 pid=855706) [rank0]:W0126 17:40:49.453000 855706 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=855706) [rank0]:W0126 17:40:49.523000 855706 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=855706) [rank0]:W0126 17:40:50.349000 855706 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=855706) [rank0]:W0126 17:40:50.457000 855706 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=855706) 2026-01-26 17:40:59,460 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=855706) 2026-01-26 17:40:59,518 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=855706) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  9.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  9.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 10.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 10.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.88it/s]
(EngineCore_DP0 pid=855706) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 10.23it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 10.65it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 10.43it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 26/2048 [00:00<00:07, 259.08it/s]
Adding requests:   3%|▎         | 60/2048 [00:00<00:06, 305.14it/s]
Adding requests:   4%|▍         | 92/2048 [00:00<00:06, 308.71it/s]
Adding requests:   6%|▌         | 126/2048 [00:00<00:06, 319.58it/s]
Adding requests:   8%|▊         | 161/2048 [00:00<00:05, 328.68it/s]
Adding requests:  10%|▉         | 197/2048 [00:00<00:05, 338.89it/s]
Adding requests:  11%|█▏        | 233/2048 [00:00<00:05, 344.24it/s]
Adding requests:  13%|█▎        | 268/2048 [00:00<00:05, 343.53it/s]
Adding requests:  15%|█▍        | 304/2048 [00:00<00:05, 347.27it/s]
Adding requests:  17%|█▋        | 342/2048 [00:01<00:04, 354.77it/s]
Adding requests:  18%|█▊        | 378/2048 [00:01<00:04, 356.00it/s]
Adding requests:  20%|██        | 417/2048 [00:01<00:04, 364.29it/s]
Adding requests:  22%|██▏       | 454/2048 [00:01<00:04, 360.14it/s]
Adding requests:  24%|██▍       | 494/2048 [00:01<00:04, 371.83it/s]
Adding requests:  26%|██▌       | 533/2048 [00:01<00:04, 375.17it/s]
Adding requests:  28%|██▊       | 571/2048 [00:01<00:03, 371.51it/s]
Adding requests:  30%|██▉       | 609/2048 [00:01<00:04, 359.37it/s]
Adding requests:  32%|███▏      | 646/2048 [00:01<00:03, 351.88it/s]
Adding requests:  33%|███▎      | 682/2048 [00:01<00:03, 353.66it/s]
Adding requests:  35%|███▌      | 718/2048 [00:02<00:03, 355.31it/s]
Adding requests:  37%|███▋      | 754/2048 [00:02<00:03, 351.56it/s]
Adding requests:  39%|███▊      | 790/2048 [00:02<00:03, 353.25it/s]
Adding requests:  40%|████      | 826/2048 [00:02<00:03, 344.50it/s]
Adding requests:  42%|████▏     | 863/2048 [00:02<00:03, 349.37it/s]
Adding requests:  44%|████▍     | 900/2048 [00:02<00:03, 354.77it/s]
Adding requests:  46%|████▌     | 936/2048 [00:02<00:03, 343.64it/s]
Adding requests:  48%|████▊     | 973/2048 [00:02<00:03, 350.69it/s]
Adding requests:  49%|████▉     | 1009/2048 [00:02<00:02, 346.80it/s]
Adding requests:  51%|█████     | 1045/2048 [00:02<00:02, 347.79it/s]
Adding requests:  53%|█████▎    | 1081/2048 [00:03<00:02, 349.35it/s]
Adding requests:  54%|█████▍    | 1116/2048 [00:03<00:02, 347.77it/s]
Adding requests:  56%|█████▋    | 1152/2048 [00:03<00:02, 348.90it/s]
Adding requests:  58%|█████▊    | 1187/2048 [00:03<00:02, 348.98it/s]
Adding requests:  60%|█████▉    | 1225/2048 [00:03<00:02, 355.77it/s]
Adding requests:  62%|██████▏   | 1261/2048 [00:03<00:02, 345.83it/s]
Adding requests:  63%|██████▎   | 1296/2048 [00:03<00:02, 343.14it/s]
Adding requests:  65%|██████▌   | 1332/2048 [00:03<00:02, 347.40it/s]
Adding requests:  67%|██████▋   | 1369/2048 [00:03<00:01, 352.60it/s]
Adding requests:  69%|██████▊   | 1405/2048 [00:04<00:01, 351.30it/s]
Adding requests:  70%|███████   | 1441/2048 [00:04<00:01, 352.85it/s]
Adding requests:  72%|███████▏  | 1477/2048 [00:04<00:01, 353.63it/s]
Adding requests:  74%|███████▍  | 1515/2048 [00:04<00:01, 358.18it/s]
Adding requests:  76%|███████▌  | 1551/2048 [00:04<00:01, 356.21it/s]
Adding requests:  77%|███████▋  | 1587/2048 [00:04<00:01, 349.28it/s]
Adding requests:  79%|███████▉  | 1622/2048 [00:04<00:01, 344.27it/s]
Adding requests:  81%|████████  | 1657/2048 [00:04<00:01, 340.50it/s]
Adding requests:  83%|████████▎ | 1693/2048 [00:04<00:01, 341.77it/s]
Adding requests:  84%|████████▍ | 1730/2048 [00:04<00:00, 348.37it/s]
Adding requests:  86%|████████▋ | 1768/2048 [00:05<00:00, 355.93it/s]
Adding requests:  88%|████████▊ | 1804/2048 [00:05<00:00, 338.72it/s]
Adding requests:  90%|████████▉ | 1840/2048 [00:05<00:00, 344.21it/s]
Adding requests:  92%|█████████▏| 1875/2048 [00:05<00:00, 345.86it/s]
Adding requests:  93%|█████████▎| 1912/2048 [00:05<00:00, 352.03it/s]
Adding requests:  95%|█████████▌| 1950/2048 [00:05<00:00, 359.47it/s]
Adding requests:  97%|█████████▋| 1987/2048 [00:05<00:00, 355.99it/s]
Adding requests:  99%|█████████▉| 2023/2048 [00:05<00:00, 347.58it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 349.24it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:00<00:00, 2181.76it/s, est. speed input: 2234226.66 toks/s, output: 2181.79 toks/s]
Processed prompts:  30%|███       | 621/2048 [00:03<00:08, 160.60it/s, est. speed input: 200533.71 toks/s, output: 195.83 toks/s]   
Processed prompts:  35%|███▍      | 716/2048 [00:04<00:10, 124.54it/s, est. speed input: 160595.36 toks/s, output: 156.83 toks/s]
Processed prompts:  38%|███▊      | 771/2048 [00:05<00:11, 106.93it/s, est. speed input: 143759.07 toks/s, output: 140.39 toks/s]
Processed prompts:  39%|███▉      | 807/2048 [00:05<00:12, 102.66it/s, est. speed input: 139065.14 toks/s, output: 135.81 toks/s]
Processed prompts:  41%|████      | 833/2048 [00:06<00:11, 103.67it/s, est. speed input: 138163.22 toks/s, output: 134.92 toks/s]
Processed prompts:  42%|████▏     | 854/2048 [00:06<00:13, 90.07it/s, est. speed input: 131654.62 toks/s, output: 128.57 toks/s] 
Processed prompts:  42%|████▏     | 870/2048 [00:06<00:13, 87.35it/s, est. speed input: 129683.14 toks/s, output: 126.64 toks/s]
Processed prompts:  43%|████▎     | 883/2048 [00:07<00:14, 82.29it/s, est. speed input: 127377.11 toks/s, output: 124.39 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:07<00:14, 79.04it/s, est. speed input: 125484.07 toks/s, output: 122.54 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:07<00:14, 76.57it/s, est. speed input: 123735.98 toks/s, output: 120.84 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:07<00:14, 74.95it/s, est. speed input: 122189.66 toks/s, output: 119.33 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:08<00:15, 73.34it/s, est. speed input: 120680.77 toks/s, output: 117.85 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:08<00:15, 72.20it/s, est. speed input: 119277.58 toks/s, output: 116.48 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:08<00:15, 71.17it/s, est. speed input: 117924.67 toks/s, output: 115.16 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:08<00:14, 70.68it/s, est. speed input: 116687.32 toks/s, output: 113.95 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:08<00:14, 70.36it/s, est. speed input: 115519.46 toks/s, output: 112.81 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:09<00:14, 69.68it/s, est. speed input: 114345.83 toks/s, output: 111.67 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:09<00:14, 69.36it/s, est. speed input: 113254.36 toks/s, output: 110.60 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:09<00:14, 69.06it/s, est. speed input: 112204.84 toks/s, output: 109.57 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:09<00:14, 69.30it/s, est. speed input: 111263.63 toks/s, output: 108.66 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:10<00:13, 69.31it/s, est. speed input: 110345.50 toks/s, output: 107.76 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:10<00:13, 69.35it/s, est. speed input: 109471.81 toks/s, output: 106.91 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:10<00:13, 69.35it/s, est. speed input: 108633.02 toks/s, output: 106.09 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:10<00:13, 69.09it/s, est. speed input: 107801.02 toks/s, output: 105.27 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:11<00:12, 69.19it/s, est. speed input: 107034.11 toks/s, output: 104.53 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:11<00:12, 68.98it/s, est. speed input: 106269.35 toks/s, output: 103.78 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:11<00:12, 68.93it/s, est. speed input: 105546.38 toks/s, output: 103.07 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:11<00:12, 69.96it/s, est. speed input: 104957.21 toks/s, output: 102.50 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:11<00:11, 69.76it/s, est. speed input: 104300.48 toks/s, output: 101.86 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:12<00:11, 69.44it/s, est. speed input: 103652.50 toks/s, output: 101.22 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:12<00:11, 69.21it/s, est. speed input: 103027.37 toks/s, output: 100.61 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:12<00:11, 69.23it/s, est. speed input: 102441.70 toks/s, output: 100.04 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:12<00:11, 69.24it/s, est. speed input: 101876.50 toks/s, output: 99.49 toks/s] 
Processed prompts:  63%|██████▎   | 1298/2048 [00:13<00:10, 69.35it/s, est. speed input: 101340.02 toks/s, output: 98.96 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:13<00:10, 69.41it/s, est. speed input: 100820.80 toks/s, output: 98.46 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:13<00:10, 69.40it/s, est. speed input: 100315.43 toks/s, output: 97.96 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:13<00:10, 69.37it/s, est. speed input: 99824.28 toks/s, output: 97.48 toks/s] 
Processed prompts:  67%|██████▋   | 1362/2048 [00:14<00:09, 69.26it/s, est. speed input: 99342.95 toks/s, output: 97.01 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:14<00:09, 69.12it/s, est. speed input: 98871.52 toks/s, output: 96.55 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:14<00:09, 68.93it/s, est. speed input: 98408.84 toks/s, output: 96.10 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:14<00:09, 69.16it/s, est. speed input: 97988.11 toks/s, output: 95.69 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:14<00:09, 69.06it/s, est. speed input: 97560.47 toks/s, output: 95.27 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:15<00:08, 69.16it/s, est. speed input: 97158.88 toks/s, output: 94.88 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:15<00:08, 70.29it/s, est. speed input: 96841.83 toks/s, output: 94.57 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:15<00:08, 70.11it/s, est. speed input: 96467.95 toks/s, output: 94.21 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:15<00:07, 69.94it/s, est. speed input: 96101.97 toks/s, output: 93.85 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:16<00:07, 69.73it/s, est. speed input: 95740.28 toks/s, output: 93.50 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:16<00:07, 70.86it/s, est. speed input: 95469.90 toks/s, output: 93.23 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:16<00:07, 70.33it/s, est. speed input: 95124.78 toks/s, output: 92.90 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:16<00:06, 71.12it/s, est. speed input: 94858.94 toks/s, output: 92.64 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:17<00:06, 70.55it/s, est. speed input: 94533.68 toks/s, output: 92.32 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:17<00:06, 69.96it/s, est. speed input: 94205.86 toks/s, output: 92.00 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:17<00:06, 69.71it/s, est. speed input: 93895.61 toks/s, output: 91.69 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:17<00:06, 70.76it/s, est. speed input: 93663.49 toks/s, output: 91.47 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:17<00:05, 70.38it/s, est. speed input: 93375.05 toks/s, output: 91.19 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:18<00:05, 69.72it/s, est. speed input: 93071.30 toks/s, output: 90.89 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:18<00:05, 69.62it/s, est. speed input: 92794.99 toks/s, output: 90.62 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:18<00:05, 69.17it/s, est. speed input: 92504.52 toks/s, output: 90.34 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:18<00:05, 69.10it/s, est. speed input: 92234.96 toks/s, output: 90.07 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:19<00:04, 68.99it/s, est. speed input: 91968.51 toks/s, output: 89.81 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:19<00:04, 70.19it/s, est. speed input: 91775.38 toks/s, output: 89.62 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:19<00:04, 71.23it/s, est. speed input: 91594.95 toks/s, output: 89.45 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:19<00:04, 70.57it/s, est. speed input: 91350.39 toks/s, output: 89.21 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:19<00:03, 69.93it/s, est. speed input: 91102.32 toks/s, output: 88.97 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:20<00:03, 69.71it/s, est. speed input: 90870.55 toks/s, output: 88.74 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:20<00:03, 69.63it/s, est. speed input: 90648.20 toks/s, output: 88.52 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:20<00:03, 69.52it/s, est. speed input: 90428.01 toks/s, output: 88.31 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:20<00:02, 69.22it/s, est. speed input: 90201.66 toks/s, output: 88.09 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:21<00:02, 69.24it/s, est. speed input: 89991.45 toks/s, output: 87.88 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:21<00:02, 69.22it/s, est. speed input: 89783.90 toks/s, output: 87.68 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:21<00:02, 70.22it/s, est. speed input: 89627.15 toks/s, output: 87.53 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:21<00:02, 69.58it/s, est. speed input: 89413.70 toks/s, output: 87.32 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:22<00:01, 69.46it/s, est. speed input: 89219.07 toks/s, output: 87.13 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:22<00:01, 69.47it/s, est. speed input: 89032.62 toks/s, output: 86.95 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:22<00:01, 69.69it/s, est. speed input: 88858.98 toks/s, output: 86.78 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:22<00:01, 69.50it/s, est. speed input: 88674.45 toks/s, output: 86.60 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:22<00:00, 70.66it/s, est. speed input: 88547.46 toks/s, output: 86.47 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:23<00:00, 69.82it/s, est. speed input: 88354.94 toks/s, output: 86.28 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:23<00:00, 69.60it/s, est. speed input: 88180.77 toks/s, output: 86.11 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:23<00:00, 70.33it/s, est. speed input: 88046.14 toks/s, output: 85.98 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:23<00:00, 70.33it/s, est. speed input: 88651.32 toks/s, output: 86.57 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:23<00:00, 86.57it/s, est. speed input: 88651.32 toks/s, output: 86.57 toks/s]
[rank0]:[W126 17:41:32.437731621 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 17:41:34
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:42:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=857686) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=857686) WARNING 01-26 17:42:27 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=857686) WARNING 01-26 17:42:46 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 71.25 requests/s, 73028.35 total tokens/s, 71.25 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 17:42:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:42:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:42:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:42:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:42:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:42:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:42:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:42:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:42:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:42:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:42:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:42:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:42:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:42:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:42:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:42:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:42:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:42:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:42:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:42:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:42:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:42:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:42:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:42:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:42:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:42:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:42:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:42:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=857686) [2026-01-26 17:42:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=857686) [2026-01-26 17:42:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=857686) [2026-01-26 17:42:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=857686) [2026-01-26 17:42:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=857686) [2026-01-26 17:42:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=857686) [2026-01-26 17:42:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=857686) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=857686) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.14s/it]
(EngineCore_DP0 pid=857686) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.52it/s]
(EngineCore_DP0 pid=857686) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.20it/s]
(EngineCore_DP0 pid=857686) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.03it/s]
(EngineCore_DP0 pid=857686) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.09it/s]
(EngineCore_DP0 pid=857686) 
(EngineCore_DP0 pid=857686) [2026-01-26 17:42:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=857686) [2026-01-26 17:42:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34406400 bytes
(EngineCore_DP0 pid=857686) [2026-01-26 17:42:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=857686) [2026-01-26 17:42:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 24576000 bytes
(EngineCore_DP0 pid=857686) [2026-01-26 17:42:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=857686) [2026-01-26 17:42:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 132710400 bytes
(EngineCore_DP0 pid=857686) [2026-01-26 17:42:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=857686) [2026-01-26 17:42:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 66355200 bytes
(EngineCore_DP0 pid=857686) [rank0]:W0126 17:42:35.143000 857686 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=857686) [rank0]:W0126 17:42:35.213000 857686 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=857686) [rank0]:W0126 17:42:36.155000 857686 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=857686) [rank0]:W0126 17:42:36.266000 857686 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=857686) 2026-01-26 17:42:45,740 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=857686) 2026-01-26 17:42:45,852 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=857686) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  6.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  5.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:01,  6.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  7.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  8.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  9.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.44it/s]
(EngineCore_DP0 pid=857686) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  9.52it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 10.09it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00, 10.56it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 10.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 10.62it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 272.46it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 313.69it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 310.36it/s]
Adding requests:   3%|▎         | 128/4096 [00:00<00:12, 316.10it/s]
Adding requests:   4%|▍         | 163/4096 [00:00<00:12, 325.83it/s]
Adding requests:   5%|▍         | 199/4096 [00:00<00:11, 335.36it/s]
Adding requests:   6%|▌         | 234/4096 [00:00<00:11, 339.22it/s]
Adding requests:   7%|▋         | 268/4096 [00:00<00:11, 339.45it/s]
Adding requests:   7%|▋         | 304/4096 [00:00<00:11, 342.56it/s]
Adding requests:   8%|▊         | 341/4096 [00:01<00:10, 349.67it/s]
Adding requests:   9%|▉         | 377/4096 [00:01<00:10, 352.30it/s]
Adding requests:  10%|█         | 415/4096 [00:01<00:10, 357.50it/s]
Adding requests:  11%|█         | 451/4096 [00:01<00:10, 354.25it/s]
Adding requests:  12%|█▏        | 491/4096 [00:01<00:09, 366.50it/s]
Adding requests:  13%|█▎        | 530/4096 [00:01<00:09, 373.11it/s]
Adding requests:  14%|█▍        | 568/4096 [00:01<00:09, 371.23it/s]
Adding requests:  15%|█▍        | 606/4096 [00:01<00:09, 357.46it/s]
Adding requests:  16%|█▌        | 642/4096 [00:01<00:09, 355.04it/s]
Adding requests:  17%|█▋        | 678/4096 [00:01<00:09, 351.68it/s]
Adding requests:  17%|█▋        | 715/4096 [00:02<00:09, 355.70it/s]
Adding requests:  18%|█▊        | 751/4096 [00:02<00:09, 349.23it/s]
Adding requests:  19%|█▉        | 788/4096 [00:02<00:09, 351.83it/s]
Adding requests:  20%|██        | 824/4096 [00:02<00:09, 351.36it/s]
Adding requests:  21%|██        | 862/4096 [00:02<00:09, 357.14it/s]
Adding requests:  22%|██▏       | 899/4096 [00:02<00:08, 359.11it/s]
Adding requests:  23%|██▎       | 935/4096 [00:02<00:08, 351.67it/s]
Adding requests:  24%|██▎       | 972/4096 [00:02<00:08, 355.52it/s]
Adding requests:  25%|██▍       | 1008/4096 [00:02<00:09, 342.98it/s]
Adding requests:  25%|██▌       | 1044/4096 [00:02<00:08, 345.09it/s]
Adding requests:  26%|██▋       | 1079/4096 [00:03<00:08, 344.87it/s]
Adding requests:  27%|██▋       | 1114/4096 [00:03<00:08, 345.31it/s]
Adding requests:  28%|██▊       | 1150/4096 [00:03<00:08, 347.11it/s]
Adding requests:  29%|██▉       | 1185/4096 [00:03<00:08, 347.49it/s]
Adding requests:  30%|██▉       | 1223/4096 [00:03<00:08, 355.74it/s]
Adding requests:  31%|███       | 1259/4096 [00:03<00:08, 351.36it/s]
Adding requests:  32%|███▏      | 1295/4096 [00:03<00:08, 346.89it/s]
Adding requests:  32%|███▏      | 1331/4096 [00:03<00:07, 350.37it/s]
Adding requests:  33%|███▎      | 1368/4096 [00:03<00:07, 354.57it/s]
Adding requests:  34%|███▍      | 1404/4096 [00:04<00:07, 352.79it/s]
Adding requests:  35%|███▌      | 1440/4096 [00:04<00:07, 354.70it/s]
Adding requests:  36%|███▌      | 1476/4096 [00:04<00:07, 351.52it/s]
Adding requests:  37%|███▋      | 1514/4096 [00:04<00:07, 357.84it/s]
Adding requests:  38%|███▊      | 1550/4096 [00:04<00:07, 355.70it/s]
Adding requests:  39%|███▊      | 1586/4096 [00:04<00:07, 347.16it/s]
Adding requests:  40%|███▉      | 1621/4096 [00:04<00:07, 343.38it/s]
Adding requests:  40%|████      | 1656/4096 [00:04<00:07, 338.94it/s]
Adding requests:  41%|████▏     | 1691/4096 [00:04<00:07, 340.73it/s]
Adding requests:  42%|████▏     | 1728/4096 [00:04<00:06, 347.14it/s]
Adding requests:  43%|████▎     | 1764/4096 [00:05<00:06, 349.96it/s]
Adding requests:  44%|████▍     | 1800/4096 [00:05<00:06, 350.19it/s]
Adding requests:  45%|████▍     | 1836/4096 [00:05<00:06, 348.70it/s]
Adding requests:  46%|████▌     | 1873/4096 [00:05<00:06, 352.13it/s]
Adding requests:  47%|████▋     | 1909/4096 [00:05<00:06, 353.63it/s]
Adding requests:  48%|████▊     | 1947/4096 [00:05<00:05, 359.69it/s]
Adding requests:  48%|████▊     | 1984/4096 [00:05<00:05, 359.47it/s]
Adding requests:  49%|████▉     | 2020/4096 [00:05<00:05, 347.67it/s]
Adding requests:  50%|█████     | 2055/4096 [00:05<00:05, 347.49it/s]
Adding requests:  51%|█████     | 2090/4096 [00:05<00:05, 339.86it/s]
Adding requests:  52%|█████▏    | 2128/4096 [00:06<00:05, 348.41it/s]
Adding requests:  53%|█████▎    | 2163/4096 [00:06<00:05, 336.59it/s]
Adding requests:  54%|█████▎    | 2197/4096 [00:06<00:05, 333.77it/s]
Adding requests:  55%|█████▍    | 2233/4096 [00:06<00:05, 338.88it/s]
Adding requests:  55%|█████▌    | 2270/4096 [00:06<00:05, 346.97it/s]
Adding requests:  56%|█████▋    | 2306/4096 [00:06<00:05, 348.97it/s]
Adding requests:  57%|█████▋    | 2343/4096 [00:06<00:04, 354.57it/s]
Adding requests:  58%|█████▊    | 2379/4096 [00:06<00:04, 356.00it/s]
Adding requests:  59%|█████▉    | 2417/4096 [00:06<00:04, 361.71it/s]
Adding requests:  60%|█████▉    | 2454/4096 [00:07<00:04, 359.45it/s]
Adding requests:  61%|██████    | 2490/4096 [00:07<00:04, 359.42it/s]
Adding requests:  62%|██████▏   | 2527/4096 [00:07<00:04, 361.20it/s]
Adding requests:  63%|██████▎   | 2568/4096 [00:07<00:04, 373.97it/s]
Adding requests:  64%|██████▎   | 2606/4096 [00:07<00:04, 369.97it/s]
Adding requests:  65%|██████▍   | 2644/4096 [00:07<00:04, 359.15it/s]
Adding requests:  65%|██████▌   | 2680/4096 [00:07<00:03, 356.96it/s]
Adding requests:  66%|██████▋   | 2716/4096 [00:07<00:03, 351.93it/s]
Adding requests:  67%|██████▋   | 2752/4096 [00:07<00:03, 349.25it/s]
Adding requests:  68%|██████▊   | 2791/4096 [00:07<00:03, 358.42it/s]
Adding requests:  69%|██████▉   | 2827/4096 [00:08<00:03, 358.35it/s]
Adding requests:  70%|██████▉   | 2863/4096 [00:08<00:03, 358.32it/s]
Adding requests:  71%|███████   | 2899/4096 [00:08<00:03, 357.80it/s]
Adding requests:  72%|███████▏  | 2936/4096 [00:08<00:03, 361.27it/s]
Adding requests:  73%|███████▎  | 2973/4096 [00:08<00:03, 358.14it/s]
Adding requests:  74%|███████▎  | 3011/4096 [00:08<00:02, 363.80it/s]
Adding requests:  74%|███████▍  | 3048/4096 [00:08<00:02, 361.53it/s]
Adding requests:  75%|███████▌  | 3086/4096 [00:08<00:02, 365.65it/s]
Adding requests:  76%|███████▋  | 3124/4096 [00:08<00:02, 368.64it/s]
Adding requests:  77%|███████▋  | 3161/4096 [00:08<00:02, 364.41it/s]
Adding requests:  78%|███████▊  | 3198/4096 [00:09<00:02, 358.72it/s]
Adding requests:  79%|███████▉  | 3236/4096 [00:09<00:02, 361.47it/s]
Adding requests:  80%|███████▉  | 3273/4096 [00:09<00:02, 356.81it/s]
Adding requests:  81%|████████  | 3309/4096 [00:09<00:02, 346.02it/s]
Adding requests:  82%|████████▏ | 3346/4096 [00:09<00:02, 350.95it/s]
Adding requests:  83%|████████▎ | 3382/4096 [00:09<00:02, 352.79it/s]
Adding requests:  83%|████████▎ | 3418/4096 [00:09<00:01, 354.85it/s]
Adding requests:  84%|████████▍ | 3455/4096 [00:09<00:01, 357.00it/s]
Adding requests:  85%|████████▌ | 3491/4096 [00:09<00:01, 343.77it/s]
Adding requests:  86%|████████▌ | 3530/4096 [00:10<00:01, 354.25it/s]
Adding requests:  87%|████████▋ | 3568/4096 [00:10<00:01, 360.52it/s]
Adding requests:  88%|████████▊ | 3605/4096 [00:10<00:01, 356.23it/s]
Adding requests:  89%|████████▉ | 3643/4096 [00:10<00:01, 361.49it/s]
Adding requests:  90%|████████▉ | 3680/4096 [00:10<00:01, 353.20it/s]
Adding requests:  91%|█████████ | 3716/4096 [00:10<00:01, 354.62it/s]
Adding requests:  92%|█████████▏| 3752/4096 [00:10<00:00, 348.40it/s]
Adding requests:  92%|█████████▏| 3787/4096 [00:10<00:00, 337.58it/s]
Adding requests:  93%|█████████▎| 3821/4096 [00:10<00:00, 329.13it/s]
Adding requests:  94%|█████████▍| 3854/4096 [00:10<00:00, 328.03it/s]
Adding requests:  95%|█████████▍| 3888/4096 [00:11<00:00, 329.91it/s]
Adding requests:  96%|█████████▌| 3922/4096 [00:11<00:00, 326.47it/s]
Adding requests:  97%|█████████▋| 3957/4096 [00:11<00:00, 332.83it/s]
Adding requests:  97%|█████████▋| 3991/4096 [00:11<00:00, 333.10it/s]
Adding requests:  98%|█████████▊| 4026/4096 [00:11<00:00, 337.73it/s]
Adding requests:  99%|█████████▉| 4061/4096 [00:11<00:00, 338.10it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 339.93it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 350.12it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 819/4096 [00:00<00:01, 2784.42it/s, est. speed input: 2851328.29 toks/s, output: 2784.44 toks/s]
Processed prompts:  27%|██▋       | 1098/4096 [00:03<00:13, 223.78it/s, est. speed input: 288528.90 toks/s, output: 281.77 toks/s]  
Processed prompts:  30%|██▉       | 1218/4096 [00:05<00:17, 160.88it/s, est. speed input: 219385.27 toks/s, output: 214.24 toks/s]
Processed prompts:  31%|███▏      | 1286/4096 [00:06<00:19, 141.81it/s, est. speed input: 200008.67 toks/s, output: 195.32 toks/s]
Processed prompts:  32%|███▏      | 1331/4096 [00:07<00:23, 119.45it/s, est. speed input: 182038.89 toks/s, output: 177.77 toks/s]
Processed prompts:  33%|███▎      | 1363/4096 [00:07<00:24, 112.29it/s, est. speed input: 175829.75 toks/s, output: 171.71 toks/s]
Processed prompts:  34%|███▍      | 1395/4096 [00:08<00:25, 105.17it/s, est. speed input: 170328.22 toks/s, output: 166.34 toks/s]
Processed prompts:  35%|███▍      | 1427/4096 [00:08<00:27, 98.41it/s, est. speed input: 165372.90 toks/s, output: 161.50 toks/s] 
Processed prompts:  36%|███▌      | 1459/4096 [00:09<00:28, 92.98it/s, est. speed input: 161120.52 toks/s, output: 157.34 toks/s]
Processed prompts:  36%|███▋      | 1491/4096 [00:09<00:29, 87.69it/s, est. speed input: 157033.69 toks/s, output: 153.35 toks/s]
Processed prompts:  37%|███▋      | 1523/4096 [00:10<00:30, 83.94it/s, est. speed input: 153464.55 toks/s, output: 149.87 toks/s]
Processed prompts:  38%|███▊      | 1555/4096 [00:10<00:31, 81.08it/s, est. speed input: 150220.86 toks/s, output: 146.70 toks/s]
Processed prompts:  39%|███▊      | 1587/4096 [00:11<00:32, 78.23it/s, est. speed input: 147049.50 toks/s, output: 143.60 toks/s]
Processed prompts:  40%|███▉      | 1619/4096 [00:11<00:32, 76.85it/s, est. speed input: 144312.70 toks/s, output: 140.93 toks/s]
Processed prompts:  40%|████      | 1651/4096 [00:11<00:32, 75.17it/s, est. speed input: 141616.31 toks/s, output: 138.30 toks/s]
Processed prompts:  41%|████      | 1683/4096 [00:12<00:32, 73.89it/s, est. speed input: 139095.51 toks/s, output: 135.84 toks/s]
Processed prompts:  42%|████▏     | 1715/4096 [00:12<00:32, 73.55it/s, est. speed input: 136878.65 toks/s, output: 133.67 toks/s]
Processed prompts:  43%|████▎     | 1747/4096 [00:13<00:31, 73.46it/s, est. speed input: 134841.47 toks/s, output: 131.68 toks/s]
Processed prompts:  43%|████▎     | 1779/4096 [00:13<00:31, 72.85it/s, est. speed input: 132826.33 toks/s, output: 129.71 toks/s]
Processed prompts:  44%|████▍     | 1811/4096 [00:14<00:31, 72.34it/s, est. speed input: 130922.85 toks/s, output: 127.85 toks/s]
Processed prompts:  45%|████▍     | 1843/4096 [00:14<00:31, 71.96it/s, est. speed input: 129130.58 toks/s, output: 126.10 toks/s]
Processed prompts:  46%|████▌     | 1875/4096 [00:15<00:30, 72.38it/s, est. speed input: 127566.52 toks/s, output: 124.58 toks/s]
Processed prompts:  47%|████▋     | 1907/4096 [00:15<00:30, 71.82it/s, est. speed input: 125947.09 toks/s, output: 123.00 toks/s]
Processed prompts:  47%|████▋     | 1939/4096 [00:15<00:30, 71.61it/s, est. speed input: 124449.05 toks/s, output: 121.53 toks/s]
Processed prompts:  48%|████▊     | 1971/4096 [00:16<00:29, 72.09it/s, est. speed input: 123131.17 toks/s, output: 120.25 toks/s]
Processed prompts:  49%|████▉     | 2003/4096 [00:16<00:29, 71.80it/s, est. speed input: 121787.44 toks/s, output: 118.93 toks/s]
Processed prompts:  50%|████▉     | 2035/4096 [00:17<00:28, 71.50it/s, est. speed input: 120499.45 toks/s, output: 117.68 toks/s]
Processed prompts:  50%|█████     | 2067/4096 [00:17<00:28, 71.80it/s, est. speed input: 119348.75 toks/s, output: 116.55 toks/s]
Processed prompts:  51%|█████     | 2099/4096 [00:18<00:27, 71.67it/s, est. speed input: 118207.55 toks/s, output: 115.44 toks/s]
Processed prompts:  52%|█████▏    | 2131/4096 [00:18<00:27, 71.52it/s, est. speed input: 117113.43 toks/s, output: 114.37 toks/s]
Processed prompts:  53%|█████▎    | 2163/4096 [00:19<00:27, 71.38it/s, est. speed input: 116067.43 toks/s, output: 113.35 toks/s]
Processed prompts:  54%|█████▎    | 2195/4096 [00:19<00:26, 71.71it/s, est. speed input: 115120.90 toks/s, output: 112.42 toks/s]
Processed prompts:  54%|█████▍    | 2227/4096 [00:19<00:26, 71.49it/s, est. speed input: 114163.39 toks/s, output: 111.49 toks/s]
Processed prompts:  55%|█████▌    | 2259/4096 [00:20<00:25, 71.47it/s, est. speed input: 113263.18 toks/s, output: 110.61 toks/s]
Processed prompts:  56%|█████▌    | 2291/4096 [00:20<00:25, 71.31it/s, est. speed input: 112385.19 toks/s, output: 109.75 toks/s]
Processed prompts:  57%|█████▋    | 2323/4096 [00:21<00:24, 71.36it/s, est. speed input: 111562.07 toks/s, output: 108.95 toks/s]
Processed prompts:  57%|█████▋    | 2355/4096 [00:21<00:24, 71.17it/s, est. speed input: 110749.54 toks/s, output: 108.15 toks/s]
Processed prompts:  58%|█████▊    | 2387/4096 [00:22<00:24, 71.07it/s, est. speed input: 109972.22 toks/s, output: 107.39 toks/s]
Processed prompts:  59%|█████▉    | 2419/4096 [00:22<00:23, 71.03it/s, est. speed input: 109230.38 toks/s, output: 106.67 toks/s]
Processed prompts:  60%|█████▉    | 2451/4096 [00:23<00:23, 70.90it/s, est. speed input: 108506.37 toks/s, output: 105.96 toks/s]
Processed prompts:  61%|██████    | 2483/4096 [00:23<00:22, 71.01it/s, est. speed input: 107829.36 toks/s, output: 105.30 toks/s]
Processed prompts:  61%|██████▏   | 2515/4096 [00:24<00:22, 71.50it/s, est. speed input: 107217.18 toks/s, output: 104.70 toks/s]
Processed prompts:  62%|██████▏   | 2547/4096 [00:24<00:21, 71.39it/s, est. speed input: 106585.11 toks/s, output: 104.09 toks/s]
Processed prompts:  63%|██████▎   | 2579/4096 [00:24<00:21, 71.86it/s, est. speed input: 106024.03 toks/s, output: 103.54 toks/s]
Processed prompts:  64%|██████▎   | 2611/4096 [00:25<00:20, 71.55it/s, est. speed input: 105427.70 toks/s, output: 102.96 toks/s]
Processed prompts:  65%|██████▍   | 2643/4096 [00:25<00:20, 71.40it/s, est. speed input: 104857.64 toks/s, output: 102.40 toks/s]
Processed prompts:  65%|██████▌   | 2675/4096 [00:26<00:19, 71.36it/s, est. speed input: 104312.61 toks/s, output: 101.87 toks/s]
Processed prompts:  66%|██████▌   | 2707/4096 [00:26<00:19, 71.17it/s, est. speed input: 103772.96 toks/s, output: 101.34 toks/s]
Processed prompts:  67%|██████▋   | 2739/4096 [00:27<00:18, 71.68it/s, est. speed input: 103301.77 toks/s, output: 100.88 toks/s]
Processed prompts:  68%|██████▊   | 2771/4096 [00:27<00:18, 71.26it/s, est. speed input: 102785.82 toks/s, output: 100.38 toks/s]
Processed prompts:  68%|██████▊   | 2803/4096 [00:28<00:18, 71.17it/s, est. speed input: 102301.39 toks/s, output: 99.90 toks/s] 
Processed prompts:  69%|██████▉   | 2835/4096 [00:28<00:17, 71.22it/s, est. speed input: 101841.04 toks/s, output: 99.45 toks/s]
Processed prompts:  70%|██████▉   | 2867/4096 [00:28<00:17, 71.12it/s, est. speed input: 101385.08 toks/s, output: 99.01 toks/s]
Processed prompts:  71%|███████   | 2899/4096 [00:29<00:16, 71.96it/s, est. speed input: 101007.89 toks/s, output: 98.64 toks/s]
Processed prompts:  72%|███████▏  | 2931/4096 [00:29<00:16, 71.74it/s, est. speed input: 100585.48 toks/s, output: 98.23 toks/s]
Processed prompts:  72%|███████▏  | 2963/4096 [00:30<00:15, 71.48it/s, est. speed input: 100168.14 toks/s, output: 97.82 toks/s]
Processed prompts:  73%|███████▎  | 2995/4096 [00:30<00:15, 71.40it/s, est. speed input: 99769.97 toks/s, output: 97.43 toks/s] 
Processed prompts:  74%|███████▍  | 3027/4096 [00:31<00:15, 71.21it/s, est. speed input: 99373.99 toks/s, output: 97.04 toks/s]
Processed prompts:  75%|███████▍  | 3059/4096 [00:31<00:14, 71.21it/s, est. speed input: 98998.51 toks/s, output: 96.68 toks/s]
Processed prompts:  75%|███████▌  | 3091/4096 [00:32<00:14, 71.14it/s, est. speed input: 98628.37 toks/s, output: 96.32 toks/s]
Processed prompts:  76%|███████▌  | 3123/4096 [00:32<00:13, 70.95it/s, est. speed input: 98260.28 toks/s, output: 95.96 toks/s]
Processed prompts:  77%|███████▋  | 3155/4096 [00:32<00:13, 70.98it/s, est. speed input: 97912.00 toks/s, output: 95.62 toks/s]
Processed prompts:  78%|███████▊  | 3187/4096 [00:33<00:12, 71.03it/s, est. speed input: 97575.05 toks/s, output: 95.29 toks/s]
Processed prompts:  79%|███████▊  | 3219/4096 [00:33<00:12, 71.00it/s, est. speed input: 97243.27 toks/s, output: 94.96 toks/s]
Processed prompts:  79%|███████▉  | 3251/4096 [00:34<00:11, 71.10it/s, est. speed input: 96927.02 toks/s, output: 94.66 toks/s]
Processed prompts:  80%|████████  | 3283/4096 [00:34<00:11, 71.05it/s, est. speed input: 96612.34 toks/s, output: 94.35 toks/s]
Processed prompts:  81%|████████  | 3315/4096 [00:35<00:11, 70.97it/s, est. speed input: 96302.76 toks/s, output: 94.05 toks/s]
Processed prompts:  82%|████████▏ | 3347/4096 [00:35<00:10, 70.96it/s, est. speed input: 96003.54 toks/s, output: 93.75 toks/s]
Processed prompts:  82%|████████▏ | 3379/4096 [00:36<00:10, 71.04it/s, est. speed input: 95716.89 toks/s, output: 93.47 toks/s]
Processed prompts:  83%|████████▎ | 3411/4096 [00:36<00:09, 70.98it/s, est. speed input: 95430.88 toks/s, output: 93.19 toks/s]
Processed prompts:  84%|████████▍ | 3443/4096 [00:37<00:09, 71.03it/s, est. speed input: 95156.93 toks/s, output: 92.93 toks/s]
Processed prompts:  85%|████████▍ | 3475/4096 [00:37<00:08, 70.89it/s, est. speed input: 94880.23 toks/s, output: 92.66 toks/s]
Processed prompts:  86%|████████▌ | 3507/4096 [00:37<00:08, 70.90it/s, est. speed input: 94615.64 toks/s, output: 92.40 toks/s]
Processed prompts:  86%|████████▋ | 3539/4096 [00:38<00:07, 71.56it/s, est. speed input: 94391.05 toks/s, output: 92.18 toks/s]
Processed prompts:  87%|████████▋ | 3571/4096 [00:38<00:07, 71.48it/s, est. speed input: 94143.83 toks/s, output: 91.94 toks/s]
Processed prompts:  88%|████████▊ | 3603/4096 [00:39<00:06, 71.32it/s, est. speed input: 93897.12 toks/s, output: 91.70 toks/s]
Processed prompts:  89%|████████▊ | 3635/4096 [00:39<00:06, 71.14it/s, est. speed input: 93652.88 toks/s, output: 91.46 toks/s]
Processed prompts:  90%|████████▉ | 3667/4096 [00:40<00:05, 71.67it/s, est. speed input: 93445.75 toks/s, output: 91.26 toks/s]
Processed prompts:  90%|█████████ | 3699/4096 [00:40<00:05, 71.50it/s, est. speed input: 93217.14 toks/s, output: 91.03 toks/s]
Processed prompts:  91%|█████████ | 3731/4096 [00:41<00:05, 71.20it/s, est. speed input: 92984.87 toks/s, output: 90.81 toks/s]
Processed prompts:  92%|█████████▏| 3763/4096 [00:41<00:04, 71.00it/s, est. speed input: 92758.42 toks/s, output: 90.58 toks/s]
Processed prompts:  93%|█████████▎| 3795/4096 [00:41<00:04, 70.83it/s, est. speed input: 92535.26 toks/s, output: 90.37 toks/s]
Processed prompts:  93%|█████████▎| 3827/4096 [00:42<00:03, 70.96it/s, est. speed input: 92328.24 toks/s, output: 90.16 toks/s]
Processed prompts:  94%|█████████▍| 3859/4096 [00:42<00:03, 71.00it/s, est. speed input: 92123.16 toks/s, output: 89.96 toks/s]
Processed prompts:  95%|█████████▍| 3891/4096 [00:43<00:02, 71.07it/s, est. speed input: 91924.36 toks/s, output: 89.77 toks/s]
Processed prompts:  96%|█████████▌| 3923/4096 [00:43<00:02, 71.61it/s, est. speed input: 91751.11 toks/s, output: 89.60 toks/s]
Processed prompts:  97%|█████████▋| 3955/4096 [00:44<00:01, 71.29it/s, est. speed input: 91551.39 toks/s, output: 89.41 toks/s]
Processed prompts:  97%|█████████▋| 3987/4096 [00:44<00:01, 71.76it/s, est. speed input: 91385.10 toks/s, output: 89.24 toks/s]
Processed prompts:  98%|█████████▊| 4019/4096 [00:45<00:01, 71.63it/s, est. speed input: 91202.65 toks/s, output: 89.07 toks/s]
Processed prompts:  99%|█████████▉| 4051/4096 [00:45<00:00, 71.43it/s, est. speed input: 91019.32 toks/s, output: 88.89 toks/s]
Processed prompts: 100%|█████████▉| 4083/4096 [00:45<00:00, 84.73it/s, est. speed input: 91310.24 toks/s, output: 89.17 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:45<00:00, 84.73it/s, est. speed input: 91600.57 toks/s, output: 89.45 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:45<00:00, 89.45it/s, est. speed input: 91600.57 toks/s, output: 89.45 toks/s]
[rank0]:[W126 17:43:47.823881229 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 17:43:49
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:44:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=860417) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=860417) WARNING 01-26 17:45:07 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=860417) WARNING 01-26 17:45:26 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 72.04 requests/s, 73838.07 total tokens/s, 72.04 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 17:44:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:44:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:44:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:44:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:44:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:44:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:44:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:44:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:44:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:44:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:44:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:44:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:44:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:44:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:44:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:44:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:44:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:44:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:44:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:44:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:44:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:44:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:44:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:44:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:44:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:44:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:44:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:44:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=860417) [2026-01-26 17:44:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=860417) [2026-01-26 17:44:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=860417) [2026-01-26 17:44:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=860417) [2026-01-26 17:44:53] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=860417) [2026-01-26 17:44:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=860417) [2026-01-26 17:44:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=860417) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=860417) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.14s/it]
(EngineCore_DP0 pid=860417) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.52it/s]
(EngineCore_DP0 pid=860417) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.20it/s]
(EngineCore_DP0 pid=860417) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.03it/s]
(EngineCore_DP0 pid=860417) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.09it/s]
(EngineCore_DP0 pid=860417) 
(EngineCore_DP0 pid=860417) [2026-01-26 17:44:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=860417) [2026-01-26 17:44:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34406400 bytes
(EngineCore_DP0 pid=860417) [2026-01-26 17:44:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=860417) [2026-01-26 17:44:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 24576000 bytes
(EngineCore_DP0 pid=860417) [2026-01-26 17:44:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=860417) [2026-01-26 17:44:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 132710400 bytes
(EngineCore_DP0 pid=860417) [2026-01-26 17:44:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=860417) [2026-01-26 17:44:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 66355200 bytes
(EngineCore_DP0 pid=860417) [rank0]:W0126 17:45:14.763000 860417 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=860417) [rank0]:W0126 17:45:14.832000 860417 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=860417) [rank0]:W0126 17:45:15.621000 860417 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=860417) [rank0]:W0126 17:45:15.734000 860417 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=860417) 2026-01-26 17:45:25,593 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=860417) 2026-01-26 17:45:25,805 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=860417) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:02,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:01,  9.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  9.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  8.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:01,  9.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:01,  9.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  9.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:01<00:00,  9.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00,  9.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:01<00:00,  9.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.20it/s]
(EngineCore_DP0 pid=860417) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 10.86it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 11.07it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 11.11it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 11.12it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 10.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00, 10.71it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 27/8192 [00:00<00:30, 269.33it/s]
Adding requests:   1%|          | 62/8192 [00:00<00:25, 312.96it/s]
Adding requests:   1%|          | 94/8192 [00:00<00:26, 311.26it/s]
Adding requests:   2%|▏         | 128/8192 [00:00<00:25, 318.44it/s]
Adding requests:   2%|▏         | 163/8192 [00:00<00:24, 327.98it/s]
Adding requests:   2%|▏         | 197/8192 [00:00<00:24, 331.59it/s]
Adding requests:   3%|▎         | 232/8192 [00:00<00:23, 336.18it/s]
Adding requests:   3%|▎         | 266/8192 [00:00<00:23, 337.23it/s]
Adding requests:   4%|▎         | 301/8192 [00:00<00:23, 340.25it/s]
Adding requests:   4%|▍         | 338/8192 [00:01<00:22, 346.97it/s]
Adding requests:   5%|▍         | 375/8192 [00:01<00:22, 350.78it/s]
Adding requests:   5%|▌         | 412/8192 [00:01<00:21, 355.82it/s]
Adding requests:   5%|▌         | 448/8192 [00:01<00:21, 353.33it/s]
Adding requests:   6%|▌         | 488/8192 [00:01<00:21, 365.97it/s]
Adding requests:   6%|▋         | 527/8192 [00:01<00:20, 372.52it/s]
Adding requests:   7%|▋         | 565/8192 [00:01<00:20, 371.08it/s]
Adding requests:   7%|▋         | 603/8192 [00:01<00:21, 357.16it/s]
Adding requests:   8%|▊         | 639/8192 [00:01<00:21, 356.28it/s]
Adding requests:   8%|▊         | 675/8192 [00:01<00:21, 350.27it/s]
Adding requests:   9%|▊         | 712/8192 [00:02<00:21, 354.91it/s]
Adding requests:   9%|▉         | 748/8192 [00:02<00:21, 349.54it/s]
Adding requests:  10%|▉         | 784/8192 [00:02<00:21, 352.36it/s]
Adding requests:  10%|█         | 820/8192 [00:02<00:21, 350.14it/s]
Adding requests:  10%|█         | 859/8192 [00:02<00:20, 359.14it/s]
Adding requests:  11%|█         | 896/8192 [00:02<00:20, 360.54it/s]
Adding requests:  11%|█▏        | 933/8192 [00:02<00:20, 353.45it/s]
Adding requests:  12%|█▏        | 970/8192 [00:02<00:20, 355.68it/s]
Adding requests:  12%|█▏        | 1006/8192 [00:02<00:20, 348.65it/s]
Adding requests:  13%|█▎        | 1042/8192 [00:02<00:20, 350.95it/s]
Adding requests:  13%|█▎        | 1078/8192 [00:03<00:20, 347.88it/s]
Adding requests:  14%|█▎        | 1114/8192 [00:03<00:20, 348.61it/s]
Adding requests:  14%|█▍        | 1149/8192 [00:03<00:20, 341.06it/s]
Adding requests:  14%|█▍        | 1184/8192 [00:03<00:20, 341.00it/s]
Adding requests:  15%|█▍        | 1222/8192 [00:03<00:19, 348.98it/s]
Adding requests:  15%|█▌        | 1257/8192 [00:03<00:19, 348.98it/s]
Adding requests:  16%|█▌        | 1292/8192 [00:03<00:20, 338.66it/s]
Adding requests:  16%|█▌        | 1329/8192 [00:03<00:19, 346.18it/s]
Adding requests:  17%|█▋        | 1366/8192 [00:03<00:19, 352.41it/s]
Adding requests:  17%|█▋        | 1402/8192 [00:04<00:19, 350.25it/s]
Adding requests:  18%|█▊        | 1438/8192 [00:04<00:19, 352.44it/s]
Adding requests:  18%|█▊        | 1474/8192 [00:04<00:19, 350.43it/s]
Adding requests:  18%|█▊        | 1512/8192 [00:04<00:18, 358.07it/s]
Adding requests:  19%|█▉        | 1548/8192 [00:04<00:18, 355.80it/s]
Adding requests:  19%|█▉        | 1584/8192 [00:04<00:19, 347.61it/s]
Adding requests:  20%|█▉        | 1619/8192 [00:04<00:19, 342.26it/s]
Adding requests:  20%|██        | 1654/8192 [00:04<00:19, 330.91it/s]
Adding requests:  21%|██        | 1689/8192 [00:04<00:19, 333.83it/s]
Adding requests:  21%|██        | 1726/8192 [00:04<00:18, 343.00it/s]
Adding requests:  21%|██▏       | 1761/8192 [00:05<00:18, 344.74it/s]
Adding requests:  22%|██▏       | 1797/8192 [00:05<00:18, 347.33it/s]
Adding requests:  22%|██▏       | 1833/8192 [00:05<00:18, 350.43it/s]
Adding requests:  23%|██▎       | 1869/8192 [00:05<00:18, 349.06it/s]
Adding requests:  23%|██▎       | 1906/8192 [00:05<00:17, 353.08it/s]
Adding requests:  24%|██▎       | 1944/8192 [00:05<00:17, 359.03it/s]
Adding requests:  24%|██▍       | 1981/8192 [00:05<00:17, 358.28it/s]
Adding requests:  25%|██▍       | 2017/8192 [00:05<00:17, 349.95it/s]
Adding requests:  25%|██▌       | 2053/8192 [00:05<00:17, 347.46it/s]
Adding requests:  25%|██▌       | 2088/8192 [00:06<00:17, 339.95it/s]
Adding requests:  26%|██▌       | 2126/8192 [00:06<00:17, 349.61it/s]
Adding requests:  26%|██▋       | 2162/8192 [00:06<00:17, 344.83it/s]
Adding requests:  27%|██▋       | 2197/8192 [00:06<00:17, 339.84it/s]
Adding requests:  27%|██▋       | 2233/8192 [00:06<00:17, 344.29it/s]
Adding requests:  28%|██▊       | 2269/8192 [00:06<00:17, 348.14it/s]
Adding requests:  28%|██▊       | 2305/8192 [00:06<00:16, 351.15it/s]
Adding requests:  29%|██▊       | 2342/8192 [00:06<00:16, 356.23it/s]
Adding requests:  29%|██▉       | 2378/8192 [00:06<00:16, 355.32it/s]
Adding requests:  30%|██▉       | 2417/8192 [00:06<00:15, 362.01it/s]
Adding requests:  30%|██▉       | 2454/8192 [00:07<00:15, 359.85it/s]
Adding requests:  30%|███       | 2491/8192 [00:07<00:15, 361.60it/s]
Adding requests:  31%|███       | 2528/8192 [00:07<00:15, 363.15it/s]
Adding requests:  31%|███▏      | 2568/8192 [00:07<00:15, 372.61it/s]
Adding requests:  32%|███▏      | 2606/8192 [00:07<00:15, 370.57it/s]
Adding requests:  32%|███▏      | 2644/8192 [00:07<00:15, 358.43it/s]
Adding requests:  33%|███▎      | 2680/8192 [00:07<00:15, 355.66it/s]
Adding requests:  33%|███▎      | 2716/8192 [00:07<00:15, 350.57it/s]
Adding requests:  34%|███▎      | 2753/8192 [00:07<00:15, 355.13it/s]
Adding requests:  34%|███▍      | 2791/8192 [00:07<00:14, 360.97it/s]
Adding requests:  35%|███▍      | 2828/8192 [00:08<00:14, 361.27it/s]
Adding requests:  35%|███▍      | 2865/8192 [00:08<00:14, 359.38it/s]
Adding requests:  35%|███▌      | 2901/8192 [00:08<00:14, 357.95it/s]
Adding requests:  36%|███▌      | 2937/8192 [00:08<00:14, 351.64it/s]
Adding requests:  36%|███▋      | 2973/8192 [00:08<00:14, 350.67it/s]
Adding requests:  37%|███▋      | 3012/8192 [00:08<00:14, 358.89it/s]
Adding requests:  37%|███▋      | 3048/8192 [00:08<00:14, 358.55it/s]
Adding requests:  38%|███▊      | 3086/8192 [00:08<00:14, 363.54it/s]
Adding requests:  38%|███▊      | 3124/8192 [00:08<00:13, 366.98it/s]
Adding requests:  39%|███▊      | 3161/8192 [00:08<00:13, 363.51it/s]
Adding requests:  39%|███▉      | 3198/8192 [00:09<00:13, 358.16it/s]
Adding requests:  40%|███▉      | 3236/8192 [00:09<00:13, 361.15it/s]
Adding requests:  40%|███▉      | 3273/8192 [00:09<00:13, 356.92it/s]
Adding requests:  40%|████      | 3309/8192 [00:09<00:14, 346.65it/s]
Adding requests:  41%|████      | 3346/8192 [00:09<00:13, 351.91it/s]
Adding requests:  41%|████▏     | 3382/8192 [00:09<00:13, 353.95it/s]
Adding requests:  42%|████▏     | 3419/8192 [00:09<00:13, 356.14it/s]
Adding requests:  42%|████▏     | 3457/8192 [00:09<00:13, 361.69it/s]
Adding requests:  43%|████▎     | 3494/8192 [00:09<00:13, 355.04it/s]
Adding requests:  43%|████▎     | 3534/8192 [00:10<00:12, 364.61it/s]
Adding requests:  44%|████▎     | 3572/8192 [00:10<00:12, 366.96it/s]
Adding requests:  44%|████▍     | 3609/8192 [00:10<00:12, 364.83it/s]
Adding requests:  45%|████▍     | 3647/8192 [00:10<00:12, 367.16it/s]
Adding requests:  45%|████▍     | 3684/8192 [00:10<00:12, 355.90it/s]
Adding requests:  45%|████▌     | 3721/8192 [00:10<00:12, 357.14it/s]
Adding requests:  46%|████▌     | 3757/8192 [00:10<00:12, 349.14it/s]
Adding requests:  46%|████▋     | 3792/8192 [00:10<00:12, 340.45it/s]
Adding requests:  47%|████▋     | 3827/8192 [00:10<00:12, 337.91it/s]
Adding requests:  47%|████▋     | 3863/8192 [00:10<00:12, 342.37it/s]
Adding requests:  48%|████▊     | 3898/8192 [00:11<00:12, 342.59it/s]
Adding requests:  48%|████▊     | 3933/8192 [00:11<00:12, 337.85it/s]
Adding requests:  48%|████▊     | 3969/8192 [00:11<00:12, 341.66it/s]
Adding requests:  49%|████▉     | 4004/8192 [00:11<00:12, 342.80it/s]
Adding requests:  49%|████▉     | 4039/8192 [00:11<00:12, 343.91it/s]
Adding requests:  50%|████▉     | 4074/8192 [00:11<00:12, 341.48it/s]
Adding requests:  50%|█████     | 4111/8192 [00:11<00:11, 347.38it/s]
Adding requests:  51%|█████     | 4146/8192 [00:11<00:11, 344.20it/s]
Adding requests:  51%|█████     | 4182/8192 [00:11<00:11, 348.67it/s]
Adding requests:  51%|█████▏    | 4217/8192 [00:12<00:11, 344.30it/s]
Adding requests:  52%|█████▏    | 4252/8192 [00:12<00:11, 341.39it/s]
Adding requests:  52%|█████▏    | 4287/8192 [00:12<00:11, 326.08it/s]
Adding requests:  53%|█████▎    | 4323/8192 [00:12<00:11, 334.63it/s]
Adding requests:  53%|█████▎    | 4358/8192 [00:12<00:11, 338.93it/s]
Adding requests:  54%|█████▎    | 4393/8192 [00:12<00:11, 339.34it/s]
Adding requests:  54%|█████▍    | 4429/8192 [00:12<00:10, 343.37it/s]
Adding requests:  54%|█████▍    | 4464/8192 [00:12<00:10, 345.29it/s]
Adding requests:  55%|█████▍    | 4500/8192 [00:12<00:10, 347.39it/s]
Adding requests:  55%|█████▌    | 4537/8192 [00:12<00:10, 353.12it/s]
Adding requests:  56%|█████▌    | 4573/8192 [00:13<00:10, 348.91it/s]
Adding requests:  56%|█████▋    | 4609/8192 [00:13<00:10, 351.50it/s]
Adding requests:  57%|█████▋    | 4645/8192 [00:13<00:10, 348.40it/s]
Adding requests:  57%|█████▋    | 4680/8192 [00:13<00:10, 342.95it/s]
Adding requests:  58%|█████▊    | 4715/8192 [00:13<00:10, 341.96it/s]
Adding requests:  58%|█████▊    | 4754/8192 [00:13<00:09, 354.43it/s]
Adding requests:  58%|█████▊    | 4790/8192 [00:13<00:09, 347.79it/s]
Adding requests:  59%|█████▉    | 4825/8192 [00:13<00:09, 346.80it/s]
Adding requests:  59%|█████▉    | 4860/8192 [00:13<00:09, 345.44it/s]
Adding requests:  60%|█████▉    | 4895/8192 [00:13<00:09, 346.03it/s]
Adding requests:  60%|██████    | 4932/8192 [00:14<00:09, 352.23it/s]
Adding requests:  61%|██████    | 4968/8192 [00:14<00:09, 353.15it/s]
Adding requests:  61%|██████    | 5004/8192 [00:14<00:09, 352.04it/s]
Adding requests:  62%|██████▏   | 5042/8192 [00:14<00:08, 358.89it/s]
Adding requests:  62%|██████▏   | 5078/8192 [00:14<00:08, 358.91it/s]
Adding requests:  62%|██████▏   | 5114/8192 [00:14<00:08, 356.43it/s]
Adding requests:  63%|██████▎   | 5150/8192 [00:14<00:08, 356.19it/s]
Adding requests:  63%|██████▎   | 5186/8192 [00:14<00:08, 352.96it/s]
Adding requests:  64%|██████▎   | 5222/8192 [00:14<00:08, 351.69it/s]
Adding requests:  64%|██████▍   | 5258/8192 [00:15<00:08, 347.77it/s]
Adding requests:  65%|██████▍   | 5294/8192 [00:15<00:08, 349.75it/s]
Adding requests:  65%|██████▌   | 5330/8192 [00:15<00:08, 350.35it/s]
Adding requests:  66%|██████▌   | 5366/8192 [00:15<00:08, 352.59it/s]
Adding requests:  66%|██████▌   | 5402/8192 [00:15<00:08, 346.72it/s]
Adding requests:  66%|██████▋   | 5437/8192 [00:15<00:08, 344.30it/s]
Adding requests:  67%|██████▋   | 5474/8192 [00:15<00:07, 349.72it/s]
Adding requests:  67%|██████▋   | 5511/8192 [00:15<00:07, 354.33it/s]
Adding requests:  68%|██████▊   | 5547/8192 [00:15<00:07, 352.15it/s]
Adding requests:  68%|██████▊   | 5583/8192 [00:15<00:07, 341.68it/s]
Adding requests:  69%|██████▊   | 5618/8192 [00:16<00:07, 326.38it/s]
Adding requests:  69%|██████▉   | 5655/8192 [00:16<00:07, 337.25it/s]
Adding requests:  69%|██████▉   | 5689/8192 [00:16<00:07, 337.91it/s]
Adding requests:  70%|██████▉   | 5725/8192 [00:16<00:07, 342.77it/s]
Adding requests:  70%|███████   | 5761/8192 [00:16<00:07, 346.03it/s]
Adding requests:  71%|███████   | 5798/8192 [00:16<00:06, 351.49it/s]
Adding requests:  71%|███████   | 5834/8192 [00:16<00:06, 347.69it/s]
Adding requests:  72%|███████▏  | 5872/8192 [00:16<00:06, 353.74it/s]
Adding requests:  72%|███████▏  | 5908/8192 [00:16<00:06, 354.25it/s]
Adding requests:  73%|███████▎  | 5945/8192 [00:16<00:06, 357.79it/s]
Adding requests:  73%|███████▎  | 5982/8192 [00:17<00:06, 358.75it/s]
Adding requests:  73%|███████▎  | 6019/8192 [00:17<00:06, 359.89it/s]
Adding requests:  74%|███████▍  | 6056/8192 [00:17<00:06, 355.46it/s]
Adding requests:  74%|███████▍  | 6092/8192 [00:17<00:05, 354.44it/s]
Adding requests:  75%|███████▍  | 6128/8192 [00:17<00:05, 356.02it/s]
Adding requests:  75%|███████▌  | 6165/8192 [00:17<00:05, 359.50it/s]
Adding requests:  76%|███████▌  | 6201/8192 [00:17<00:05, 349.66it/s]
Adding requests:  76%|███████▌  | 6237/8192 [00:17<00:05, 350.85it/s]
Adding requests:  77%|███████▋  | 6274/8192 [00:17<00:05, 353.64it/s]
Adding requests:  77%|███████▋  | 6310/8192 [00:18<00:05, 355.00it/s]
Adding requests:  78%|███████▊  | 6349/8192 [00:18<00:05, 362.39it/s]
Adding requests:  78%|███████▊  | 6386/8192 [00:18<00:05, 360.16it/s]
Adding requests:  78%|███████▊  | 6423/8192 [00:18<00:05, 351.86it/s]
Adding requests:  79%|███████▉  | 6459/8192 [00:18<00:04, 346.90it/s]
Adding requests:  79%|███████▉  | 6494/8192 [00:18<00:04, 344.12it/s]
Adding requests:  80%|███████▉  | 6531/8192 [00:18<00:04, 348.89it/s]
Adding requests:  80%|████████  | 6567/8192 [00:18<00:04, 350.16it/s]
Adding requests:  81%|████████  | 6603/8192 [00:18<00:04, 347.57it/s]
Adding requests:  81%|████████  | 6640/8192 [00:18<00:04, 353.73it/s]
Adding requests:  81%|████████▏ | 6676/8192 [00:19<00:04, 352.59it/s]
Adding requests:  82%|████████▏ | 6712/8192 [00:19<00:04, 349.69it/s]
Adding requests:  82%|████████▏ | 6749/8192 [00:19<00:04, 354.32it/s]
Adding requests:  83%|████████▎ | 6786/8192 [00:19<00:03, 357.23it/s]
Adding requests:  83%|████████▎ | 6822/8192 [00:19<00:03, 347.66it/s]
Adding requests:  84%|████████▎ | 6858/8192 [00:19<00:03, 350.92it/s]
Adding requests:  84%|████████▍ | 6894/8192 [00:19<00:03, 353.32it/s]
Adding requests:  85%|████████▍ | 6930/8192 [00:19<00:03, 345.15it/s]
Adding requests:  85%|████████▌ | 6965/8192 [00:19<00:03, 343.87it/s]
Adding requests:  85%|████████▌ | 7001/8192 [00:19<00:03, 348.02it/s]
Adding requests:  86%|████████▌ | 7037/8192 [00:20<00:03, 350.68it/s]
Adding requests:  86%|████████▋ | 7073/8192 [00:20<00:03, 347.71it/s]
Adding requests:  87%|████████▋ | 7108/8192 [00:20<00:03, 346.24it/s]
Adding requests:  87%|████████▋ | 7145/8192 [00:20<00:02, 352.49it/s]
Adding requests:  88%|████████▊ | 7181/8192 [00:20<00:02, 352.91it/s]
Adding requests:  88%|████████▊ | 7219/8192 [00:20<00:02, 360.45it/s]
Adding requests:  89%|████████▊ | 7256/8192 [00:20<00:02, 359.93it/s]
Adding requests:  89%|████████▉ | 7293/8192 [00:20<00:02, 362.32it/s]
Adding requests:  89%|████████▉ | 7330/8192 [00:20<00:02, 355.63it/s]
Adding requests:  90%|████████▉ | 7367/8192 [00:21<00:02, 357.90it/s]
Adding requests:  90%|█████████ | 7403/8192 [00:21<00:02, 353.39it/s]
Adding requests:  91%|█████████ | 7439/8192 [00:21<00:02, 353.58it/s]
Adding requests:  91%|█████████ | 7475/8192 [00:21<00:02, 349.76it/s]
Adding requests:  92%|█████████▏| 7512/8192 [00:21<00:01, 354.50it/s]
Adding requests:  92%|█████████▏| 7548/8192 [00:21<00:01, 349.79it/s]
Adding requests:  93%|█████████▎| 7584/8192 [00:21<00:01, 349.28it/s]
Adding requests:  93%|█████████▎| 7620/8192 [00:21<00:01, 351.75it/s]
Adding requests:  93%|█████████▎| 7657/8192 [00:21<00:01, 355.11it/s]
Adding requests:  94%|█████████▍| 7696/8192 [00:21<00:01, 362.37it/s]
Adding requests:  94%|█████████▍| 7733/8192 [00:22<00:01, 359.52it/s]
Adding requests:  95%|█████████▍| 7769/8192 [00:22<00:01, 355.46it/s]
Adding requests:  95%|█████████▌| 7805/8192 [00:22<00:01, 356.40it/s]
Adding requests:  96%|█████████▌| 7841/8192 [00:22<00:01, 349.18it/s]
Adding requests:  96%|█████████▌| 7876/8192 [00:22<00:00, 348.65it/s]
Adding requests:  97%|█████████▋| 7912/8192 [00:22<00:00, 351.12it/s]
Adding requests:  97%|█████████▋| 7951/8192 [00:22<00:00, 361.00it/s]
Adding requests:  98%|█████████▊| 7990/8192 [00:22<00:00, 366.07it/s]
Adding requests:  98%|█████████▊| 8027/8192 [00:22<00:00, 358.94it/s]
Adding requests:  98%|█████████▊| 8065/8192 [00:22<00:00, 363.92it/s]
Adding requests:  99%|█████████▉| 8102/8192 [00:23<00:00, 358.98it/s]
Adding requests:  99%|█████████▉| 8138/8192 [00:23<00:00, 354.11it/s]
Adding requests: 100%|█████████▉| 8174/8192 [00:23<00:00, 349.12it/s]
Adding requests: 100%|██████████| 8192/8192 [00:23<00:00, 350.92it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  21%|██        | 1684/8192 [00:00<00:03, 1782.30it/s, est. speed input: 1825115.11 toks/s, output: 1782.31 toks/s]
Processed prompts:  23%|██▎       | 1863/8192 [00:02<00:11, 557.57it/s, est. speed input: 701707.36 toks/s, output: 685.26 toks/s]   
Processed prompts:  24%|██▎       | 1943/8192 [00:04<00:21, 297.08it/s, est. speed input: 443857.52 toks/s, output: 433.45 toks/s]
Processed prompts:  24%|██▍       | 2004/8192 [00:05<00:26, 235.53it/s, est. speed input: 381707.88 toks/s, output: 372.76 toks/s]
Processed prompts:  25%|██▌       | 2068/8192 [00:06<00:32, 190.77it/s, est. speed input: 338438.53 toks/s, output: 330.51 toks/s]
Processed prompts:  26%|██▌       | 2132/8192 [00:07<00:38, 156.65it/s, est. speed input: 305149.10 toks/s, output: 298.00 toks/s]
Processed prompts:  27%|██▋       | 2196/8192 [00:08<00:45, 132.68it/s, est. speed input: 279891.07 toks/s, output: 273.33 toks/s]
Processed prompts:  28%|██▊       | 2260/8192 [00:08<00:51, 115.08it/s, est. speed input: 259448.70 toks/s, output: 253.37 toks/s]
Processed prompts:  28%|██▊       | 2324/8192 [00:09<00:57, 102.37it/s, est. speed input: 242599.87 toks/s, output: 236.91 toks/s]
Processed prompts:  29%|██▉       | 2388/8192 [00:10<01:02, 93.35it/s, est. speed input: 228552.54 toks/s, output: 223.20 toks/s] 
Processed prompts:  30%|██▉       | 2452/8192 [00:11<01:05, 87.12it/s, est. speed input: 216750.06 toks/s, output: 211.67 toks/s]
Processed prompts:  31%|███       | 2516/8192 [00:12<01:08, 82.90it/s, est. speed input: 206731.33 toks/s, output: 201.89 toks/s]
Processed prompts:  31%|███▏      | 2580/8192 [00:13<01:10, 79.84it/s, est. speed input: 197991.40 toks/s, output: 193.35 toks/s]
Processed prompts:  32%|███▏      | 2644/8192 [00:14<01:11, 77.53it/s, est. speed input: 190251.29 toks/s, output: 185.79 toks/s]
Processed prompts:  33%|███▎      | 2708/8192 [00:15<01:11, 76.17it/s, est. speed input: 183545.28 toks/s, output: 179.24 toks/s]
Processed prompts:  34%|███▍      | 2772/8192 [00:15<01:12, 74.90it/s, est. speed input: 177436.94 toks/s, output: 173.28 toks/s]
Processed prompts:  35%|███▍      | 2836/8192 [00:16<01:12, 74.00it/s, est. speed input: 171963.86 toks/s, output: 167.93 toks/s]
Processed prompts:  35%|███▌      | 2900/8192 [00:17<01:11, 73.83it/s, est. speed input: 167216.85 toks/s, output: 163.30 toks/s]
Processed prompts:  36%|███▌      | 2964/8192 [00:18<01:11, 73.12it/s, est. speed input: 162703.59 toks/s, output: 158.89 toks/s]
Processed prompts:  37%|███▋      | 3028/8192 [00:19<01:10, 72.84it/s, est. speed input: 158676.61 toks/s, output: 154.96 toks/s]
Processed prompts:  38%|███▊      | 3092/8192 [00:20<01:10, 72.49it/s, est. speed input: 154949.98 toks/s, output: 151.32 toks/s]
Processed prompts:  39%|███▊      | 3156/8192 [00:21<01:09, 72.33it/s, est. speed input: 151559.58 toks/s, output: 148.01 toks/s]
Processed prompts:  39%|███▉      | 3220/8192 [00:22<01:08, 72.12it/s, est. speed input: 148415.07 toks/s, output: 144.94 toks/s]
Processed prompts:  40%|████      | 3284/8192 [00:23<01:08, 72.05it/s, est. speed input: 145532.46 toks/s, output: 142.12 toks/s]
Processed prompts:  41%|████      | 3348/8192 [00:23<01:07, 71.99it/s, est. speed input: 142861.84 toks/s, output: 139.51 toks/s]
Processed prompts:  42%|████▏     | 3412/8192 [00:24<01:06, 71.98it/s, est. speed input: 140389.55 toks/s, output: 137.10 toks/s]
Processed prompts:  42%|████▏     | 3476/8192 [00:25<01:05, 71.99it/s, est. speed input: 138092.33 toks/s, output: 134.86 toks/s]
Processed prompts:  43%|████▎     | 3540/8192 [00:26<01:04, 71.99it/s, est. speed input: 135944.85 toks/s, output: 132.76 toks/s]
Processed prompts:  44%|████▍     | 3604/8192 [00:27<01:03, 71.95it/s, est. speed input: 133930.13 toks/s, output: 130.79 toks/s]
Processed prompts:  45%|████▍     | 3668/8192 [00:28<01:02, 72.16it/s, est. speed input: 132085.81 toks/s, output: 128.99 toks/s]
Processed prompts:  46%|████▌     | 3732/8192 [00:29<01:01, 72.12it/s, est. speed input: 130318.29 toks/s, output: 127.26 toks/s]
Processed prompts:  46%|████▋     | 3796/8192 [00:30<01:01, 71.96it/s, est. speed input: 128632.30 toks/s, output: 125.62 toks/s]
Processed prompts:  47%|████▋     | 3860/8192 [00:31<01:00, 71.99it/s, est. speed input: 127065.27 toks/s, output: 124.09 toks/s]
Processed prompts:  48%|████▊     | 3924/8192 [00:31<00:59, 72.15it/s, est. speed input: 125609.47 toks/s, output: 122.67 toks/s]
Processed prompts:  49%|████▊     | 3988/8192 [00:32<00:58, 72.35it/s, est. speed input: 124244.95 toks/s, output: 121.33 toks/s]
Processed prompts:  49%|████▉     | 4052/8192 [00:33<00:57, 72.45it/s, est. speed input: 122945.57 toks/s, output: 120.06 toks/s]
Processed prompts:  50%|█████     | 4116/8192 [00:34<00:56, 72.22it/s, est. speed input: 121669.05 toks/s, output: 118.82 toks/s]
Processed prompts:  51%|█████     | 4180/8192 [00:35<00:55, 72.07it/s, est. speed input: 120457.48 toks/s, output: 117.63 toks/s]
Processed prompts:  52%|█████▏    | 4244/8192 [00:36<00:54, 72.03it/s, est. speed input: 119314.04 toks/s, output: 116.52 toks/s]
Processed prompts:  53%|█████▎    | 4308/8192 [00:37<00:54, 71.90it/s, est. speed input: 118213.14 toks/s, output: 115.44 toks/s]
Processed prompts:  53%|█████▎    | 4372/8192 [00:38<00:53, 71.82it/s, est. speed input: 117163.37 toks/s, output: 114.42 toks/s]
Processed prompts:  54%|█████▍    | 4436/8192 [00:39<00:52, 72.05it/s, est. speed input: 116198.36 toks/s, output: 113.47 toks/s]
Processed prompts:  55%|█████▍    | 4500/8192 [00:39<00:51, 72.26it/s, est. speed input: 115280.25 toks/s, output: 112.58 toks/s]
Processed prompts:  56%|█████▌    | 4564/8192 [00:40<00:50, 72.38it/s, est. speed input: 114398.95 toks/s, output: 111.72 toks/s]
Processed prompts:  56%|█████▋    | 4628/8192 [00:41<00:49, 72.27it/s, est. speed input: 113533.50 toks/s, output: 110.87 toks/s]
Processed prompts:  57%|█████▋    | 4692/8192 [00:42<00:48, 72.14it/s, est. speed input: 112697.70 toks/s, output: 110.06 toks/s]
Processed prompts:  58%|█████▊    | 4756/8192 [00:43<00:47, 72.14it/s, est. speed input: 111906.45 toks/s, output: 109.28 toks/s]
Processed prompts:  59%|█████▉    | 4820/8192 [00:44<00:46, 72.05it/s, est. speed input: 111137.26 toks/s, output: 108.53 toks/s]
Processed prompts:  60%|█████▉    | 4884/8192 [00:45<00:45, 72.15it/s, est. speed input: 110414.85 toks/s, output: 107.83 toks/s]
Processed prompts:  60%|██████    | 4948/8192 [00:46<00:44, 72.18it/s, est. speed input: 109716.38 toks/s, output: 107.14 toks/s]
Processed prompts:  61%|██████    | 5012/8192 [00:47<00:44, 72.06it/s, est. speed input: 109030.30 toks/s, output: 106.47 toks/s]
Processed prompts:  62%|██████▏   | 5076/8192 [00:47<00:43, 72.02it/s, est. speed input: 108374.35 toks/s, output: 105.83 toks/s]
Processed prompts:  63%|██████▎   | 5140/8192 [00:48<00:42, 71.89it/s, est. speed input: 107732.51 toks/s, output: 105.21 toks/s]
Processed prompts:  64%|██████▎   | 5204/8192 [00:49<00:41, 72.06it/s, est. speed input: 107137.33 toks/s, output: 104.63 toks/s]
Processed prompts:  64%|██████▍   | 5268/8192 [00:50<00:40, 72.17it/s, est. speed input: 106561.47 toks/s, output: 104.06 toks/s]
Processed prompts:  65%|██████▌   | 5332/8192 [00:51<00:39, 72.03it/s, est. speed input: 105987.80 toks/s, output: 103.50 toks/s]
Processed prompts:  66%|██████▌   | 5396/8192 [00:52<00:38, 71.99it/s, est. speed input: 105437.43 toks/s, output: 102.97 toks/s]
Processed prompts:  67%|██████▋   | 5460/8192 [00:53<00:37, 72.27it/s, est. speed input: 104931.14 toks/s, output: 102.47 toks/s]
Processed prompts:  67%|██████▋   | 5524/8192 [00:54<00:36, 72.40it/s, est. speed input: 104435.75 toks/s, output: 101.99 toks/s]
Processed prompts:  68%|██████▊   | 5588/8192 [00:55<00:36, 72.20it/s, est. speed input: 103934.16 toks/s, output: 101.50 toks/s]
Processed prompts:  69%|██████▉   | 5652/8192 [00:55<00:35, 72.16it/s, est. speed input: 103455.62 toks/s, output: 101.03 toks/s]
Processed prompts:  70%|██████▉   | 5716/8192 [00:56<00:34, 72.05it/s, est. speed input: 102985.76 toks/s, output: 100.57 toks/s]
Processed prompts:  71%|███████   | 5780/8192 [00:57<00:33, 71.98it/s, est. speed input: 102531.09 toks/s, output: 100.13 toks/s]
Processed prompts:  71%|███████▏  | 5844/8192 [00:58<00:32, 71.96it/s, est. speed input: 102092.53 toks/s, output: 99.70 toks/s] 
Processed prompts:  72%|███████▏  | 5908/8192 [00:59<00:31, 72.15it/s, est. speed input: 101681.57 toks/s, output: 99.30 toks/s]
Processed prompts:  73%|███████▎  | 5972/8192 [01:00<00:30, 72.30it/s, est. speed input: 101283.36 toks/s, output: 98.91 toks/s]
Processed prompts:  74%|███████▎  | 6036/8192 [01:01<00:29, 72.12it/s, est. speed input: 100877.26 toks/s, output: 98.51 toks/s]
Processed prompts:  74%|███████▍  | 6100/8192 [01:02<00:29, 72.09it/s, est. speed input: 100489.59 toks/s, output: 98.13 toks/s]
Processed prompts:  75%|███████▌  | 6164/8192 [01:03<00:28, 71.92it/s, est. speed input: 100103.15 toks/s, output: 97.76 toks/s]
Processed prompts:  76%|███████▌  | 6228/8192 [01:03<00:27, 72.15it/s, est. speed input: 99749.58 toks/s, output: 97.41 toks/s] 
Processed prompts:  77%|███████▋  | 6292/8192 [01:04<00:26, 72.05it/s, est. speed input: 99389.27 toks/s, output: 97.06 toks/s]
Processed prompts:  78%|███████▊  | 6356/8192 [01:05<00:25, 71.90it/s, est. speed input: 99033.66 toks/s, output: 96.71 toks/s]
Processed prompts:  78%|███████▊  | 6420/8192 [01:06<00:24, 71.87it/s, est. speed input: 98692.44 toks/s, output: 96.38 toks/s]
Processed prompts:  79%|███████▉  | 6484/8192 [01:07<00:23, 71.80it/s, est. speed input: 98356.98 toks/s, output: 96.05 toks/s]
Processed prompts:  80%|███████▉  | 6548/8192 [01:08<00:22, 71.76it/s, est. speed input: 98031.04 toks/s, output: 95.73 toks/s]
Processed prompts:  81%|████████  | 6612/8192 [01:09<00:22, 71.77it/s, est. speed input: 97715.62 toks/s, output: 95.43 toks/s]
Processed prompts:  81%|████████▏ | 6676/8192 [01:10<00:21, 71.77it/s, est. speed input: 97407.82 toks/s, output: 95.12 toks/s]
Processed prompts:  82%|████████▏ | 6740/8192 [01:11<00:20, 71.82it/s, est. speed input: 97110.81 toks/s, output: 94.83 toks/s]
Processed prompts:  83%|████████▎ | 6804/8192 [01:11<00:19, 71.84it/s, est. speed input: 96819.79 toks/s, output: 94.55 toks/s]
Processed prompts:  84%|████████▍ | 6868/8192 [01:12<00:18, 71.85it/s, est. speed input: 96536.16 toks/s, output: 94.27 toks/s]
Processed prompts:  85%|████████▍ | 6932/8192 [01:13<00:17, 71.80it/s, est. speed input: 96255.88 toks/s, output: 94.00 toks/s]
Processed prompts:  85%|████████▌ | 6996/8192 [01:14<00:16, 71.80it/s, est. speed input: 95984.53 toks/s, output: 93.73 toks/s]
Processed prompts:  86%|████████▌ | 7060/8192 [01:15<00:15, 72.05it/s, est. speed input: 95732.70 toks/s, output: 93.49 toks/s]
Processed prompts:  87%|████████▋ | 7124/8192 [01:16<00:14, 72.29it/s, est. speed input: 95489.68 toks/s, output: 93.25 toks/s]
Processed prompts:  88%|████████▊ | 7188/8192 [01:17<00:13, 71.95it/s, est. speed input: 95226.45 toks/s, output: 92.99 toks/s]
Processed prompts:  89%|████████▊ | 7252/8192 [01:18<00:13, 72.18it/s, est. speed input: 94993.06 toks/s, output: 92.77 toks/s]
Processed prompts:  89%|████████▉ | 7316/8192 [01:19<00:12, 72.07it/s, est. speed input: 94750.99 toks/s, output: 92.53 toks/s]
Processed prompts:  90%|█████████ | 7380/8192 [01:19<00:11, 71.91it/s, est. speed input: 94510.72 toks/s, output: 92.30 toks/s]
Processed prompts:  91%|█████████ | 7444/8192 [01:20<00:10, 72.20it/s, est. speed input: 94294.87 toks/s, output: 92.08 toks/s]
Processed prompts:  92%|█████████▏| 7508/8192 [01:21<00:09, 72.08it/s, est. speed input: 94068.28 toks/s, output: 91.86 toks/s]
Processed prompts:  92%|█████████▏| 7572/8192 [01:22<00:08, 71.97it/s, est. speed input: 93845.61 toks/s, output: 91.65 toks/s]
Processed prompts:  93%|█████████▎| 7636/8192 [01:23<00:07, 72.21it/s, est. speed input: 93641.94 toks/s, output: 91.45 toks/s]
Processed prompts:  94%|█████████▍| 7700/8192 [01:24<00:06, 72.34it/s, est. speed input: 93441.13 toks/s, output: 91.25 toks/s]
Processed prompts:  95%|█████████▍| 7764/8192 [01:25<00:05, 72.05it/s, est. speed input: 93227.02 toks/s, output: 91.04 toks/s]
Processed prompts:  96%|█████████▌| 7828/8192 [01:26<00:05, 71.91it/s, est. speed input: 93020.58 toks/s, output: 90.84 toks/s]
Processed prompts:  96%|█████████▋| 7892/8192 [01:27<00:04, 71.88it/s, est. speed input: 92820.80 toks/s, output: 90.65 toks/s]
Processed prompts:  97%|█████████▋| 7956/8192 [01:27<00:03, 71.79it/s, est. speed input: 92622.36 toks/s, output: 90.45 toks/s]
Processed prompts:  98%|█████████▊| 8020/8192 [01:28<00:02, 71.81it/s, est. speed input: 92431.55 toks/s, output: 90.27 toks/s]
Processed prompts:  99%|█████████▊| 8084/8192 [01:29<00:01, 71.75it/s, est. speed input: 92241.47 toks/s, output: 90.08 toks/s]
Processed prompts:  99%|█████████▉| 8148/8192 [01:30<00:00, 78.74it/s, est. speed input: 92325.60 toks/s, output: 90.16 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:30<00:00, 78.74it/s, est. speed input: 92823.58 toks/s, output: 90.65 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:30<00:00, 90.65it/s, est. speed input: 92823.58 toks/s, output: 90.65 toks/s]
[rank0]:[W126 17:47:25.582072372 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

