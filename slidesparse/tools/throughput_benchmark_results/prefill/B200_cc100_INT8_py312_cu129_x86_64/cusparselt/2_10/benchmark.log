
========== M=16 ==========
Time: 2026-01-26 13:06:35
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:06:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=454695) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=454695) WARNING 01-26 13:06:54 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=454695) WARNING 01-26 13:07:01 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 43.65 requests/s, 742.06 total tokens/s, 43.65 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-26 13:06:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:06:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:06:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:06:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:06:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:06:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:06:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:06:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:06:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:06:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:06:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:06:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:06:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:06:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:06:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:06:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:06:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:06:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:06:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:06:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:06:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:06:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:06:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:06:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:06:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:06:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:06:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:06:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=454695) [2026-01-26 13:06:49] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=454695) [2026-01-26 13:06:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=454695) [2026-01-26 13:06:49] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=454695) [2026-01-26 13:06:49] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=454695) [2026-01-26 13:06:49] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=454695) [2026-01-26 13:06:49] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=454695) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=454695) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.49it/s]
(EngineCore_DP0 pid=454695) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.49it/s]
(EngineCore_DP0 pid=454695) 
(EngineCore_DP0 pid=454695) [2026-01-26 13:06:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=454695) [2026-01-26 13:06:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=454695) [2026-01-26 13:06:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=454695) [2026-01-26 13:06:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=454695) [2026-01-26 13:06:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=454695) [2026-01-26 13:06:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=454695) [2026-01-26 13:06:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=454695) [2026-01-26 13:06:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=454695) 2026-01-26 13:07:01,707 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=454695) 2026-01-26 13:07:01,729 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=454695) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.78it/s]
(EngineCore_DP0 pid=454695) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 21.42it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 463.91it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 548.38it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:00, 125.58it/s, est. speed input: 2009.38 toks/s, output: 125.58 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:01, 61.98it/s, est. speed input: 1079.49 toks/s, output: 67.47 toks/s]  
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 54.69it/s, est. speed input: 965.42 toks/s, output: 60.34 toks/s] 
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 51.05it/s, est. speed input: 909.98 toks/s, output: 56.87 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 48.94it/s, est. speed input: 877.99 toks/s, output: 54.87 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 47.36it/s, est. speed input: 853.89 toks/s, output: 53.37 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 46.51it/s, est. speed input: 839.18 toks/s, output: 52.45 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 45.86it/s, est. speed input: 827.11 toks/s, output: 51.69 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 45.30it/s, est. speed input: 816.69 toks/s, output: 51.04 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 44.93it/s, est. speed input: 808.05 toks/s, output: 50.50 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 44.72it/s, est. speed input: 800.92 toks/s, output: 50.06 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 44.53it/s, est. speed input: 794.55 toks/s, output: 49.66 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 44.29it/s, est. speed input: 788.60 toks/s, output: 49.29 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 43.81it/s, est. speed input: 782.25 toks/s, output: 48.89 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 43.80it/s, est. speed input: 777.73 toks/s, output: 48.61 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 43.83it/s, est. speed input: 773.76 toks/s, output: 48.36 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 43.92it/s, est. speed input: 770.41 toks/s, output: 48.15 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 43.92it/s, est. speed input: 767.20 toks/s, output: 47.95 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 43.54it/s, est. speed input: 763.28 toks/s, output: 47.70 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 43.62it/s, est. speed input: 760.56 toks/s, output: 47.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.62it/s, est. speed input: 759.10 toks/s, output: 47.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 47.44it/s, est. speed input: 759.10 toks/s, output: 47.44 toks/s]
[rank0]:[W126 13:07:07.230872565 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 13:07:08
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:07:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=455765) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=455765) WARNING 01-26 13:07:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=455765) WARNING 01-26 13:07:35 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 41.31 requests/s, 5328.38 total tokens/s, 41.31 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-26 13:07:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:07:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:07:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:07:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:07:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:07:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:07:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:07:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:07:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:07:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:07:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:07:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:07:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:07:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:07:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:07:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:07:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:07:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=455765) [2026-01-26 13:07:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=455765) [2026-01-26 13:07:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=455765) [2026-01-26 13:07:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=455765) [2026-01-26 13:07:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=455765) [2026-01-26 13:07:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=455765) [2026-01-26 13:07:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=455765) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=455765) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.43it/s]
(EngineCore_DP0 pid=455765) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.43it/s]
(EngineCore_DP0 pid=455765) 
(EngineCore_DP0 pid=455765) [2026-01-26 13:07:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=455765) [2026-01-26 13:07:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=455765) [2026-01-26 13:07:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=455765) [2026-01-26 13:07:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=455765) [2026-01-26 13:07:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=455765) [2026-01-26 13:07:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=455765) [2026-01-26 13:07:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=455765) [2026-01-26 13:07:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=455765) 2026-01-26 13:07:35,039 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=455765) 2026-01-26 13:07:35,063 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=455765) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.89it/s]
(EngineCore_DP0 pid=455765) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.82it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 424.92it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 491.43it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:01, 82.52it/s, est. speed input: 10563.20 toks/s, output: 82.52 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 54.07it/s, est. speed input: 7298.56 toks/s, output: 57.02 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 49.86it/s, est. speed input: 6785.28 toks/s, output: 53.01 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 47.55it/s, est. speed input: 6505.22 toks/s, output: 50.82 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:02, 46.38it/s, est. speed input: 6357.48 toks/s, output: 49.67 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 45.53it/s, est. speed input: 6248.69 toks/s, output: 48.82 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 44.87it/s, est. speed input: 6162.46 toks/s, output: 48.14 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:01, 44.46it/s, est. speed input: 6097.36 toks/s, output: 47.64 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 44.25it/s, est. speed input: 6048.89 toks/s, output: 47.26 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 44.07it/s, est. speed input: 6007.30 toks/s, output: 46.93 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 43.93it/s, est. speed input: 5972.28 toks/s, output: 46.66 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 43.88it/s, est. speed input: 5944.36 toks/s, output: 46.44 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 43.84it/s, est. speed input: 5920.02 toks/s, output: 46.25 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 43.83it/s, est. speed input: 5899.35 toks/s, output: 46.09 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 43.37it/s, est. speed input: 5868.78 toks/s, output: 45.85 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 43.54it/s, est. speed input: 5854.66 toks/s, output: 45.74 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 43.21it/s, est. speed input: 5830.85 toks/s, output: 45.55 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 43.44it/s, est. speed input: 5820.51 toks/s, output: 45.47 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 43.65it/s, est. speed input: 5812.20 toks/s, output: 45.41 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 43.70it/s, est. speed input: 5802.56 toks/s, output: 45.33 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 43.75it/s, est. speed input: 5794.17 toks/s, output: 45.27 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 43.78it/s, est. speed input: 5786.45 toks/s, output: 45.21 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 43.73it/s, est. speed input: 5778.04 toks/s, output: 45.14 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.73it/s, est. speed input: 5774.33 toks/s, output: 45.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.11it/s, est. speed input: 5774.33 toks/s, output: 45.11 toks/s]
[rank0]:[W126 13:07:39.158188964 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 13:07:41
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:07:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=456792) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=456792) WARNING 01-26 13:08:01 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=456792) WARNING 01-26 13:08:07 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 37.91 requests/s, 9743.76 total tokens/s, 37.91 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-26 13:07:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:07:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:07:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:07:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:07:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:07:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:07:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:07:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:07:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:07:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:07:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:07:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:07:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:07:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:07:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:07:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:07:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:07:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:07:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=456792) [2026-01-26 13:07:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=456792) [2026-01-26 13:07:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=456792) [2026-01-26 13:07:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=456792) [2026-01-26 13:07:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=456792) [2026-01-26 13:07:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=456792) [2026-01-26 13:07:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=456792) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=456792) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.40it/s]
(EngineCore_DP0 pid=456792) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.40it/s]
(EngineCore_DP0 pid=456792) 
(EngineCore_DP0 pid=456792) [2026-01-26 13:07:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=456792) [2026-01-26 13:07:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=456792) [2026-01-26 13:07:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=456792) [2026-01-26 13:07:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=456792) [2026-01-26 13:07:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=456792) [2026-01-26 13:07:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=456792) [2026-01-26 13:07:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=456792) [2026-01-26 13:07:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=456792) 2026-01-26 13:08:07,939 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=456792) 2026-01-26 13:08:07,962 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=456792) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 21.04it/s]
(EngineCore_DP0 pid=456792) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.64it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  98%|█████████▊| 126/128 [00:00<00:00, 1251.52it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1252.30it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:28,  4.38it/s, est. speed input: 1121.94 toks/s, output: 4.38 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:05, 20.35it/s, est. speed input: 4406.88 toks/s, output: 17.21 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:04, 28.45it/s, est. speed input: 6011.62 toks/s, output: 23.48 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 33.13it/s, est. speed input: 6964.03 toks/s, output: 27.20 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 36.10it/s, est. speed input: 7603.60 toks/s, output: 29.70 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 38.01it/s, est. speed input: 8058.42 toks/s, output: 31.48 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 39.35it/s, est. speed input: 8404.74 toks/s, output: 32.83 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:02, 40.23it/s, est. speed input: 8671.97 toks/s, output: 33.87 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 40.79it/s, est. speed input: 8882.27 toks/s, output: 34.70 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:01, 41.12it/s, est. speed input: 9050.62 toks/s, output: 35.35 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 41.34it/s, est. speed input: 9190.34 toks/s, output: 35.90 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 41.64it/s, est. speed input: 9317.17 toks/s, output: 36.40 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 41.66it/s, est. speed input: 9415.33 toks/s, output: 36.78 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 41.64it/s, est. speed input: 9498.76 toks/s, output: 37.10 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 41.71it/s, est. speed input: 9575.69 toks/s, output: 37.40 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 41.09it/s, est. speed input: 9612.53 toks/s, output: 37.55 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 41.29it/s, est. speed input: 9672.64 toks/s, output: 37.78 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 41.44it/s, est. speed input: 9727.03 toks/s, output: 38.00 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 41.19it/s, est. speed input: 9761.65 toks/s, output: 38.13 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 41.40it/s, est. speed input: 9807.43 toks/s, output: 38.31 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 41.51it/s, est. speed input: 9847.81 toks/s, output: 38.47 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 41.58it/s, est. speed input: 9884.34 toks/s, output: 38.61 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 41.72it/s, est. speed input: 9921.02 toks/s, output: 38.75 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 41.86it/s, est. speed input: 9956.03 toks/s, output: 38.89 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 41.39it/s, est. speed input: 9970.79 toks/s, output: 38.95 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 41.60it/s, est. speed input: 10000.33 toks/s, output: 39.06 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.60it/s, est. speed input: 10012.38 toks/s, output: 39.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.11it/s, est. speed input: 10012.38 toks/s, output: 39.11 toks/s]
[rank0]:[W126 13:08:13.378413173 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 13:50:28
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:50:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=530253) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=530253) WARNING 01-26 13:50:48 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=530253) WARNING 01-26 13:50:55 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 42.84 requests/s, 21976.05 total tokens/s, 42.84 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 13:50:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:50:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:50:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:50:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:50:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:50:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:50:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:50:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:50:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:50:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:50:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:50:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:50:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:50:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:50:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:50:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:50:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:50:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=530253) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=530253) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.43it/s]
(EngineCore_DP0 pid=530253) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.43it/s]
(EngineCore_DP0 pid=530253) 
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=530253) 2026-01-26 13:50:55,318 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=530253) 2026-01-26 13:50:55,339 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=530253) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.63it/s]
(EngineCore_DP0 pid=530253) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.52it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  58%|█████▊    | 74/128 [00:00<00:00, 735.37it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 737.84it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:01, 95.87it/s, est. speed input: 49090.84 toks/s, output: 95.88 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:01, 56.17it/s, est. speed input: 30665.44 toks/s, output: 59.89 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:01, 50.77it/s, est. speed input: 27966.42 toks/s, output: 54.62 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 48.21it/s, est. speed input: 26706.79 toks/s, output: 52.16 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 46.90it/s, est. speed input: 25986.18 toks/s, output: 50.75 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:00<00:01, 45.65it/s, est. speed input: 25430.68 toks/s, output: 49.67 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:01, 45.06it/s, est. speed input: 25074.39 toks/s, output: 48.97 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 44.66it/s, est. speed input: 24795.16 toks/s, output: 48.43 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 44.37it/s, est. speed input: 24568.54 toks/s, output: 47.99 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 44.17it/s, est. speed input: 24381.58 toks/s, output: 47.62 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 44.02it/s, est. speed input: 24223.03 toks/s, output: 47.31 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 43.96it/s, est. speed input: 24093.53 toks/s, output: 47.06 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:01, 43.91it/s, est. speed input: 23979.76 toks/s, output: 46.84 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:01, 43.49it/s, est. speed input: 23836.03 toks/s, output: 46.55 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 43.55it/s, est. speed input: 23749.22 toks/s, output: 46.39 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 43.61it/s, est. speed input: 23673.58 toks/s, output: 46.24 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 43.26it/s, est. speed input: 23567.57 toks/s, output: 46.03 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 43.43it/s, est. speed input: 23510.66 toks/s, output: 45.92 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 43.52it/s, est. speed input: 23457.15 toks/s, output: 45.81 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 43.53it/s, est. speed input: 23403.37 toks/s, output: 45.71 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 43.58it/s, est. speed input: 23358.38 toks/s, output: 45.62 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:02<00:00, 43.63it/s, est. speed input: 23318.05 toks/s, output: 45.54 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.63it/s, est. speed input: 23294.65 toks/s, output: 45.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.50it/s, est. speed input: 23294.65 toks/s, output: 45.50 toks/s]
[rank0]:[W126 13:51:00.370429565 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 13:51:02
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:51:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=531281) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=531281) WARNING 01-26 13:51:21 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=531281) WARNING 01-26 13:51:28 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 41.06 requests/s, 42085.19 total tokens/s, 41.06 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 13:51:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:51:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:51:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:51:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:51:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:51:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:51:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:51:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:51:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:51:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:51:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:51:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:51:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:51:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=531281) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=531281) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.62it/s]
(EngineCore_DP0 pid=531281) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.62it/s]
(EngineCore_DP0 pid=531281) 
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=531281) 2026-01-26 13:51:28,720 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=531281) 2026-01-26 13:51:28,742 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=531281) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.48it/s]
(EngineCore_DP0 pid=531281) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.12it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:00, 399.01it/s]
Adding requests:  66%|██████▋   | 85/128 [00:00<00:00, 425.00it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 425.27it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 422.46it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:01, 86.88it/s, est. speed input: 88972.45 toks/s, output: 86.88 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:01, 55.57it/s, est. speed input: 60342.20 toks/s, output: 58.93 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 50.30it/s, est. speed input: 55154.87 toks/s, output: 53.86 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 47.96it/s, est. speed input: 52838.27 toks/s, output: 51.60 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 46.86it/s, est. speed input: 51667.07 toks/s, output: 50.46 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 46.01it/s, est. speed input: 50777.67 toks/s, output: 49.59 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 45.39it/s, est. speed input: 50090.98 toks/s, output: 48.92 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 44.98it/s, est. speed input: 49560.39 toks/s, output: 48.40 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 44.66it/s, est. speed input: 49121.30 toks/s, output: 47.97 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 44.47it/s, est. speed input: 48766.23 toks/s, output: 47.62 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 44.21it/s, est. speed input: 48435.47 toks/s, output: 47.30 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 44.13it/s, est. speed input: 48179.59 toks/s, output: 47.05 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 43.72it/s, est. speed input: 47868.60 toks/s, output: 46.75 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 43.78it/s, est. speed input: 47682.52 toks/s, output: 46.56 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 43.77it/s, est. speed input: 47505.97 toks/s, output: 46.39 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 43.61it/s, est. speed input: 47318.10 toks/s, output: 46.21 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 43.70it/s, est. speed input: 47190.20 toks/s, output: 46.08 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 43.74it/s, est. speed input: 47072.06 toks/s, output: 45.97 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 43.70it/s, est. speed input: 46953.98 toks/s, output: 45.85 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 43.72it/s, est. speed input: 46854.54 toks/s, output: 45.76 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 43.78it/s, est. speed input: 46770.72 toks/s, output: 45.67 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 43.78it/s, est. speed input: 46687.57 toks/s, output: 45.59 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 43.69it/s, est. speed input: 46598.59 toks/s, output: 45.51 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.69it/s, est. speed input: 46587.79 toks/s, output: 45.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.49it/s, est. speed input: 46587.79 toks/s, output: 45.50 toks/s]
[rank0]:[W126 13:51:33.954347312 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 13:51:35
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:51:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=532322) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=532322) WARNING 01-26 13:51:55 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=532322) WARNING 01-26 13:52:02 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 77.23 requests/s, 79164.19 total tokens/s, 77.23 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 13:51:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:51:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:51:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:51:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:51:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:51:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:51:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:51:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:51:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:50] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:51:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:51:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:51:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:51:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:51:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=532322) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=532322) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=532322) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=532322) 
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=532322) 2026-01-26 13:52:02,698 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=532322) 2026-01-26 13:52:02,721 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=532322) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.68it/s]
(EngineCore_DP0 pid=532322) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 22.50it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:51,  4.91it/s]
Adding requests:  16%|█▋        | 42/256 [00:00<00:01, 170.65it/s]
Adding requests:  34%|███▎      | 86/256 [00:00<00:00, 269.10it/s]
Adding requests:  51%|█████     | 130/256 [00:00<00:00, 327.35it/s]
Adding requests:  68%|██████▊   | 174/256 [00:00<00:00, 362.92it/s]
Adding requests:  84%|████████▎ | 214/256 [00:00<00:00, 370.67it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 316.48it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:00<00:00, 460.10it/s, est. speed input: 471177.05 toks/s, output: 460.11 toks/s]
Processed prompts:  38%|███▊      | 97/256 [00:00<00:01, 134.18it/s, est. speed input: 154301.25 toks/s, output: 150.68 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:00<00:01, 112.90it/s, est. speed input: 132091.87 toks/s, output: 128.99 toks/s]
Processed prompts:  54%|█████▍    | 139/256 [00:01<00:01, 107.05it/s, est. speed input: 125680.92 toks/s, output: 122.73 toks/s]
Processed prompts:  60%|█████▉    | 153/256 [00:01<00:01, 101.73it/s, est. speed input: 120919.48 toks/s, output: 118.08 toks/s]
Processed prompts:  64%|██████▍   | 165/256 [00:01<00:00, 98.17it/s, est. speed input: 117788.84 toks/s, output: 115.03 toks/s] 
Processed prompts:  69%|██████▉   | 176/256 [00:01<00:00, 93.30it/s, est. speed input: 114496.41 toks/s, output: 111.81 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:01<00:00, 91.68it/s, est. speed input: 112697.66 toks/s, output: 110.06 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:01<00:00, 90.20it/s, est. speed input: 111087.76 toks/s, output: 108.48 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:01<00:00, 89.01it/s, est. speed input: 109668.88 toks/s, output: 107.10 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:02<00:00, 88.24it/s, est. speed input: 108449.69 toks/s, output: 105.91 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:02<00:00, 87.45it/s, est. speed input: 107312.18 toks/s, output: 104.80 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 87.03it/s, est. speed input: 106326.32 toks/s, output: 103.83 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:02<00:00, 86.91it/s, est. speed input: 105472.83 toks/s, output: 103.00 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 86.65it/s, est. speed input: 104666.52 toks/s, output: 102.21 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 86.65it/s, est. speed input: 104666.52 toks/s, output: 102.21 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 102.21it/s, est. speed input: 104666.52 toks/s, output: 102.21 toks/s]
[rank0]:[W126 13:52:07.203815120 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 13:52:09
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:52:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=533362) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=533362) WARNING 01-26 13:52:31 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=533362) WARNING 01-26 13:52:38 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 161.32 requests/s, 165356.57 total tokens/s, 161.32 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 13:52:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:52:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:52:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:52:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:52:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:52:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:52:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:52:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:52:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:52:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:52:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:52:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:52:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:52:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:52:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:52:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:52:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:52:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=533362) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=533362) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.01it/s]
(EngineCore_DP0 pid=533362) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.01it/s]
(EngineCore_DP0 pid=533362) 
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=533362) 2026-01-26 13:52:37,980 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=533362) 2026-01-26 13:52:38,002 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=533362) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 22.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 19.81it/s]
(EngineCore_DP0 pid=533362) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 22.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 22.87it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|▋         | 33/512 [00:00<00:01, 329.61it/s]
Adding requests:  14%|█▍        | 74/512 [00:00<00:01, 373.23it/s]
Adding requests:  23%|██▎       | 117/512 [00:00<00:00, 396.79it/s]
Adding requests:  31%|███       | 157/512 [00:00<00:01, 346.89it/s]
Adding requests:  39%|███▉      | 200/512 [00:00<00:00, 373.08it/s]
Adding requests:  48%|████▊     | 247/512 [00:00<00:00, 401.43it/s]
Adding requests:  57%|█████▋    | 292/512 [00:00<00:00, 415.92it/s]
Adding requests:  66%|██████▌   | 337/512 [00:00<00:00, 423.68it/s]
Adding requests:  75%|███████▌  | 384/512 [00:00<00:00, 436.09it/s]
Adding requests:  84%|████████▍ | 432/512 [00:01<00:00, 447.47it/s]
Adding requests:  94%|█████████▎| 479/512 [00:01<00:00, 452.11it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 418.40it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:00<00:00, 1655.86it/s, est. speed input: 1695696.22 toks/s, output: 1655.88 toks/s]
Processed prompts:  71%|███████   | 364/512 [00:01<00:00, 292.80it/s, est. speed input: 346356.97 toks/s, output: 338.24 toks/s]   
Processed prompts:  87%|████████▋ | 443/512 [00:01<00:00, 245.67it/s, est. speed input: 295056.16 toks/s, output: 288.14 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:01<00:00, 223.49it/s, est. speed input: 273691.09 toks/s, output: 267.27 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 223.49it/s, est. speed input: 269047.78 toks/s, output: 262.74 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 262.73it/s, est. speed input: 269047.78 toks/s, output: 262.74 toks/s]
[rank0]:[W126 13:52:43.773965901 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 13:52:45
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:52:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=534442) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=534442) WARNING 01-26 13:53:09 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=534442) WARNING 01-26 13:53:16 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 302.61 requests/s, 310175.13 total tokens/s, 302.61 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 13:52:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:52:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:52:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:52:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:52:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:52:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:52:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:52:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:52:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:53:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:53:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:53:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:53:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:53:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:53:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:53:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:53:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:53:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:04] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=534442) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=534442) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.01it/s]
(EngineCore_DP0 pid=534442) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=534442) 
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=534442) 2026-01-26 13:53:15,989 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=534442) 2026-01-26 13:53:16,012 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=534442) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 17.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 17.55it/s]
(EngineCore_DP0 pid=534442) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 23.75it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 395.30it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 421.64it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 431.18it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:01, 432.68it/s]
Adding requests:  21%|██▏       | 220/1024 [00:00<00:01, 439.64it/s]
Adding requests:  26%|██▌       | 268/1024 [00:00<00:01, 451.39it/s]
Adding requests:  31%|███       | 314/1024 [00:00<00:01, 449.50it/s]
Adding requests:  35%|███▌      | 360/1024 [00:00<00:01, 452.31it/s]
Adding requests:  40%|███▉      | 407/1024 [00:00<00:01, 455.42it/s]
Adding requests:  44%|████▍     | 454/1024 [00:01<00:01, 458.24it/s]
Adding requests:  49%|████▉     | 500/1024 [00:01<00:01, 457.43it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 453.96it/s]
Adding requests:  58%|█████▊    | 595/1024 [00:01<00:00, 463.03it/s]
Adding requests:  63%|██████▎   | 643/1024 [00:01<00:00, 465.82it/s]
Adding requests:  68%|██████▊   | 692/1024 [00:01<00:00, 472.87it/s]
Adding requests:  72%|███████▏  | 740/1024 [00:01<00:00, 473.47it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:01<00:00, 470.52it/s]
Adding requests:  82%|████████▏ | 836/1024 [00:01<00:00, 458.38it/s]
Adding requests:  86%|████████▋ | 885/1024 [00:01<00:00, 465.93it/s]
Adding requests:  91%|█████████ | 934/1024 [00:02<00:00, 470.47it/s]
Adding requests:  96%|█████████▌| 982/1024 [00:02<00:00, 470.50it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 459.10it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:00<00:00, 5431.46it/s, est. speed input: 5562135.69 toks/s, output: 5431.54 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 5431.46it/s, est. speed input: 910086.08 toks/s, output: 888.75 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 888.67it/s, est. speed input: 910086.08 toks/s, output: 888.75 toks/s] 
[rank0]:[W126 13:53:22.228984171 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 13:53:23
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:53:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=535587) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=535587) WARNING 01-26 13:53:51 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=535587) WARNING 01-26 13:53:58 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 436.34 requests/s, 447248.66 total tokens/s, 436.34 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 13:53:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:53:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:53:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:53:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:53:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:53:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:53:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:53:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:53:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:53:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:53:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:53:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:53:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:53:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:53:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:53:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:53:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:53:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=535587) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=535587) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.04it/s]
(EngineCore_DP0 pid=535587) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.04it/s]
(EngineCore_DP0 pid=535587) 
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=535587) 2026-01-26 13:53:58,455 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=535587) 2026-01-26 13:53:58,479 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=535587) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 21.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 22.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 20.92it/s]
(EngineCore_DP0 pid=535587) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 20.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.12it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 41/2048 [00:00<00:04, 402.42it/s]
Adding requests:   4%|▍         | 86/2048 [00:00<00:04, 428.25it/s]
Adding requests:   6%|▋         | 131/2048 [00:00<00:04, 435.99it/s]
Adding requests:   9%|▊         | 175/2048 [00:00<00:04, 436.89it/s]
Adding requests:  11%|█         | 222/2048 [00:00<00:04, 446.43it/s]
Adding requests:  13%|█▎        | 270/2048 [00:00<00:03, 455.90it/s]
Adding requests:  15%|█▌        | 316/2048 [00:00<00:03, 451.84it/s]
Adding requests:  18%|█▊        | 364/2048 [00:00<00:03, 459.13it/s]
Adding requests:  20%|██        | 411/2048 [00:00<00:03, 461.41it/s]
Adding requests:  22%|██▏       | 459/2048 [00:01<00:03, 465.59it/s]
Adding requests:  25%|██▍       | 506/2048 [00:01<00:03, 461.24it/s]
Adding requests:  27%|██▋       | 553/2048 [00:01<00:03, 458.02it/s]
Adding requests:  29%|██▉       | 601/2048 [00:01<00:03, 462.89it/s]
Adding requests:  32%|███▏      | 650/2048 [00:01<00:02, 468.05it/s]
Adding requests:  34%|███▍      | 700/2048 [00:01<00:02, 476.05it/s]
Adding requests:  37%|███▋      | 748/2048 [00:01<00:02, 473.66it/s]
Adding requests:  39%|███▉      | 796/2048 [00:01<00:02, 473.84it/s]
Adding requests:  41%|████      | 844/2048 [00:01<00:02, 462.31it/s]
Adding requests:  44%|████▎     | 894/2048 [00:01<00:02, 472.90it/s]
Adding requests:  46%|████▌     | 942/2048 [00:02<00:02, 474.07it/s]
Adding requests:  48%|████▊     | 991/2048 [00:02<00:02, 477.78it/s]
Adding requests:  51%|█████     | 1040/2048 [00:02<00:02, 479.58it/s]
Adding requests:  53%|█████▎    | 1088/2048 [00:02<00:02, 475.46it/s]
Adding requests:  55%|█████▌    | 1136/2048 [00:02<00:01, 472.44it/s]
Adding requests:  58%|█████▊    | 1184/2048 [00:02<00:01, 472.44it/s]
Adding requests:  60%|██████    | 1233/2048 [00:02<00:01, 477.39it/s]
Adding requests:  63%|██████▎   | 1281/2048 [00:02<00:01, 473.23it/s]
Adding requests:  65%|██████▍   | 1330/2048 [00:02<00:01, 477.82it/s]
Adding requests:  67%|██████▋   | 1379/2048 [00:02<00:01, 479.55it/s]
Adding requests:  70%|██████▉   | 1427/2048 [00:03<00:01, 464.74it/s]
Adding requests:  72%|███████▏  | 1476/2048 [00:03<00:01, 469.44it/s]
Adding requests:  74%|███████▍  | 1524/2048 [00:03<00:01, 461.11it/s]
Adding requests:  77%|███████▋  | 1573/2048 [00:03<00:01, 467.04it/s]
Adding requests:  79%|███████▉  | 1622/2048 [00:03<00:00, 473.22it/s]
Adding requests:  82%|████████▏ | 1670/2048 [00:03<00:00, 472.09it/s]
Adding requests:  84%|████████▍ | 1719/2048 [00:03<00:00, 474.80it/s]
Adding requests:  86%|████████▋ | 1767/2048 [00:03<00:00, 473.26it/s]
Adding requests:  89%|████████▊ | 1815/2048 [00:03<00:00, 474.05it/s]
Adding requests:  91%|█████████ | 1863/2048 [00:03<00:00, 471.38it/s]
Adding requests:  93%|█████████▎| 1912/2048 [00:04<00:00, 474.60it/s]
Adding requests:  96%|█████████▌| 1961/2048 [00:04<00:00, 476.41it/s]
Adding requests:  98%|█████████▊| 2010/2048 [00:04<00:00, 480.10it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 468.34it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:00<00:00, 14838.46it/s, est. speed input: 15195385.99 toks/s, output: 14838.68 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 14838.46it/s, est. speed input: 6572734.22 toks/s, output: 6418.59 toks/s]  
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 6416.34it/s, est. speed input: 6572734.22 toks/s, output: 6418.59 toks/s] 
[rank0]:[W126 13:54:05.768474129 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 13:54:07
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:54:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=536868) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=536868) WARNING 01-26 13:54:45 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=536868) WARNING 01-26 13:54:51 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 461.06 requests/s, 472586.65 total tokens/s, 461.06 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 13:54:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:54:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:54:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:54:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:54:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:54:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:54:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:54:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:54:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:54:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:54:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:54:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:54:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:54:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:54:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:54:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:54:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:54:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:40] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=536868) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=536868) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=536868) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=536868) 
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=536868) [rank0]:W0126 13:54:48.588000 536868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=536868) [rank0]:W0126 13:54:48.658000 536868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=536868) [rank0]:W0126 13:54:49.467000 536868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=536868) [rank0]:W0126 13:54:49.566000 536868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=536868) 2026-01-26 13:54:51,885 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=536868) 2026-01-26 13:54:51,910 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=536868) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 19.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 20.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 21.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 18.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 19.07it/s]
(EngineCore_DP0 pid=536868) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 16.75it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 20.49it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 19.71it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 40/4096 [00:00<00:10, 396.91it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 419.24it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:09, 428.49it/s]
Adding requests:   4%|▍         | 173/4096 [00:00<00:09, 431.72it/s]
Adding requests:   5%|▌         | 218/4096 [00:00<00:08, 436.90it/s]
Adding requests:   6%|▋         | 266/4096 [00:00<00:08, 449.45it/s]
Adding requests:   8%|▊         | 311/4096 [00:00<00:08, 447.84it/s]
Adding requests:   9%|▊         | 357/4096 [00:00<00:08, 451.40it/s]
Adding requests:  10%|▉         | 404/4096 [00:00<00:08, 454.96it/s]
Adding requests:  11%|█         | 451/4096 [00:01<00:07, 456.70it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:07, 458.91it/s]
Adding requests:  13%|█▎        | 544/4096 [00:01<00:07, 451.72it/s]
Adding requests:  14%|█▍        | 593/4096 [00:01<00:07, 461.74it/s]
Adding requests:  16%|█▌        | 641/4096 [00:01<00:07, 464.88it/s]
Adding requests:  17%|█▋        | 690/4096 [00:01<00:07, 471.64it/s]
Adding requests:  18%|█▊        | 739/4096 [00:01<00:07, 474.88it/s]
Adding requests:  19%|█▉        | 787/4096 [00:01<00:07, 471.52it/s]
Adding requests:  20%|██        | 835/4096 [00:01<00:07, 459.08it/s]
Adding requests:  22%|██▏       | 881/4096 [00:01<00:07, 449.93it/s]
Adding requests:  23%|██▎       | 929/4096 [00:02<00:06, 456.99it/s]
Adding requests:  24%|██▍       | 977/4096 [00:02<00:06, 462.09it/s]
Adding requests:  25%|██▌       | 1025/4096 [00:02<00:06, 466.99it/s]
Adding requests:  26%|██▌       | 1072/4096 [00:02<00:06, 462.91it/s]
Adding requests:  27%|██▋       | 1119/4096 [00:02<00:06, 463.32it/s]
Adding requests:  29%|██▊       | 1169/4096 [00:02<00:06, 470.80it/s]
Adding requests:  30%|██▉       | 1217/4096 [00:02<00:06, 472.07it/s]
Adding requests:  31%|███       | 1265/4096 [00:02<00:06, 467.25it/s]
Adding requests:  32%|███▏      | 1313/4096 [00:02<00:05, 469.49it/s]
Adding requests:  33%|███▎      | 1362/4096 [00:02<00:05, 474.04it/s]
Adding requests:  34%|███▍      | 1412/4096 [00:03<00:05, 479.82it/s]
Adding requests:  36%|███▌      | 1460/4096 [00:03<00:05, 477.59it/s]
Adding requests:  37%|███▋      | 1509/4096 [00:03<00:05, 479.10it/s]
Adding requests:  38%|███▊      | 1557/4096 [00:03<00:05, 476.76it/s]
Adding requests:  39%|███▉      | 1607/4096 [00:03<00:05, 481.34it/s]
Adding requests:  40%|████      | 1656/4096 [00:03<00:05, 477.05it/s]
Adding requests:  42%|████▏     | 1704/4096 [00:03<00:05, 473.91it/s]
Adding requests:  43%|████▎     | 1752/4096 [00:03<00:04, 473.28it/s]
Adding requests:  44%|████▍     | 1800/4096 [00:03<00:04, 473.08it/s]
Adding requests:  45%|████▌     | 1848/4096 [00:03<00:04, 472.76it/s]
Adding requests:  46%|████▋     | 1896/4096 [00:04<00:04, 453.88it/s]
Adding requests:  47%|████▋     | 1942/4096 [00:04<00:04, 444.77it/s]
Adding requests:  49%|████▊     | 1989/4096 [00:04<00:04, 451.15it/s]
Adding requests:  50%|████▉     | 2037/4096 [00:04<00:04, 457.64it/s]
Adding requests:  51%|█████     | 2085/4096 [00:04<00:04, 463.55it/s]
Adding requests:  52%|█████▏    | 2132/4096 [00:04<00:04, 460.63it/s]
Adding requests:  53%|█████▎    | 2179/4096 [00:04<00:04, 457.43it/s]
Adding requests:  54%|█████▍    | 2226/4096 [00:04<00:04, 460.99it/s]
Adding requests:  55%|█████▌    | 2273/4096 [00:04<00:03, 463.48it/s]
Adding requests:  57%|█████▋    | 2321/4096 [00:05<00:03, 467.57it/s]
Adding requests:  58%|█████▊    | 2368/4096 [00:05<00:03, 466.87it/s]
Adding requests:  59%|█████▉    | 2415/4096 [00:05<00:03, 467.75it/s]
Adding requests:  60%|██████    | 2463/4096 [00:05<00:03, 469.11it/s]
Adding requests:  61%|██████▏   | 2510/4096 [00:05<00:03, 469.02it/s]
Adding requests:  62%|██████▏   | 2559/4096 [00:05<00:03, 474.18it/s]
Adding requests:  64%|██████▎   | 2607/4096 [00:05<00:03, 472.90it/s]
Adding requests:  65%|██████▍   | 2656/4096 [00:05<00:03, 477.25it/s]
Adding requests:  66%|██████▌   | 2704/4096 [00:05<00:02, 470.02it/s]
Adding requests:  67%|██████▋   | 2752/4096 [00:05<00:02, 470.78it/s]
Adding requests:  68%|██████▊   | 2800/4096 [00:06<00:02, 467.23it/s]
Adding requests:  70%|██████▉   | 2847/4096 [00:06<00:02, 466.00it/s]
Adding requests:  71%|███████   | 2895/4096 [00:06<00:02, 468.19it/s]
Adding requests:  72%|███████▏  | 2942/4096 [00:06<00:02, 465.64it/s]
Adding requests:  73%|███████▎  | 2991/4096 [00:06<00:02, 470.92it/s]
Adding requests:  74%|███████▍  | 3039/4096 [00:06<00:02, 470.04it/s]
Adding requests:  75%|███████▌  | 3087/4096 [00:06<00:02, 459.36it/s]
Adding requests:  77%|███████▋  | 3135/4096 [00:06<00:02, 463.84it/s]
Adding requests:  78%|███████▊  | 3183/4096 [00:06<00:01, 464.06it/s]
Adding requests:  79%|███████▉  | 3231/4096 [00:06<00:01, 467.01it/s]
Adding requests:  80%|████████  | 3279/4096 [00:07<00:01, 469.04it/s]
Adding requests:  81%|████████  | 3326/4096 [00:07<00:01, 468.66it/s]
Adding requests:  82%|████████▏ | 3374/4096 [00:07<00:01, 471.14it/s]
Adding requests:  84%|████████▎ | 3423/4096 [00:07<00:01, 475.37it/s]
Adding requests:  85%|████████▍ | 3471/4096 [00:07<00:01, 464.06it/s]
Adding requests:  86%|████████▌ | 3519/4096 [00:07<00:01, 468.21it/s]
Adding requests:  87%|████████▋ | 3566/4096 [00:07<00:01, 462.57it/s]
Adding requests:  88%|████████▊ | 3613/4096 [00:07<00:01, 463.49it/s]
Adding requests:  89%|████████▉ | 3660/4096 [00:07<00:00, 461.22it/s]
Adding requests:  91%|█████████ | 3709/4096 [00:07<00:00, 467.67it/s]
Adding requests:  92%|█████████▏| 3756/4096 [00:08<00:00, 467.99it/s]
Adding requests:  93%|█████████▎| 3805/4096 [00:08<00:00, 474.49it/s]
Adding requests:  94%|█████████▍| 3854/4096 [00:08<00:00, 477.26it/s]
Adding requests:  95%|█████████▌| 3902/4096 [00:08<00:00, 477.81it/s]
Adding requests:  96%|█████████▋| 3950/4096 [00:08<00:00, 462.63it/s]
Adding requests:  98%|█████████▊| 3997/4096 [00:08<00:00, 462.33it/s]
Adding requests:  99%|█████████▊| 4044/4096 [00:08<00:00, 463.40it/s]
Adding requests: 100%|█████████▉| 4092/4096 [00:08<00:00, 468.10it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 464.89it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 57493.71it/s, est. speed input: 58885781.28 toks/s, output: 57501.80 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 57408.99it/s, est. speed input: 58885781.28 toks/s, output: 57501.80 toks/s]
[rank0]:[W126 13:55:03.877534291 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 13:55:05
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-1B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:55:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=538464) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=538464) WARNING 01-26 13:56:01 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=538464) WARNING 01-26 13:56:08 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 474.51 requests/s, 486369.11 total tokens/s, 474.51 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 13:55:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:55:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:55:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:55:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:55:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:55:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:55:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:55:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:55:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:55:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:55:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:55:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:55:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:55:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:55:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:55:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:55:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:55:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=538464) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=538464) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.03it/s]
(EngineCore_DP0 pid=538464) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.03it/s]
(EngineCore_DP0 pid=538464) 
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=538464) [rank0]:W0126 13:56:04.765000 538464 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=538464) [rank0]:W0126 13:56:04.835000 538464 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=538464) [rank0]:W0126 13:56:05.646000 538464 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=538464) [rank0]:W0126 13:56:05.745000 538464 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=538464) 2026-01-26 13:56:08,109 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=538464) 2026-01-26 13:56:08,134 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=538464) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:00, 17.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:00, 19.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 20.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 20.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 21.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:00<00:00, 21.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 20.25it/s]
(EngineCore_DP0 pid=538464) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 18.02it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 17.93it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 20.66it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 21.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 21.18it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 40/8192 [00:00<00:20, 399.04it/s]
Adding requests:   1%|          | 85/8192 [00:00<00:18, 427.23it/s]
Adding requests:   2%|▏         | 129/8192 [00:00<00:18, 432.49it/s]
Adding requests:   2%|▏         | 173/8192 [00:00<00:18, 434.70it/s]
Adding requests:   3%|▎         | 218/8192 [00:00<00:18, 439.49it/s]
Adding requests:   3%|▎         | 266/8192 [00:00<00:17, 453.21it/s]
Adding requests:   4%|▍         | 312/8192 [00:00<00:17, 452.03it/s]
Adding requests:   4%|▍         | 359/8192 [00:00<00:17, 456.03it/s]
Adding requests:   5%|▍         | 407/8192 [00:00<00:16, 461.51it/s]
Adding requests:   6%|▌         | 454/8192 [00:01<00:16, 460.07it/s]
Adding requests:   6%|▌         | 501/8192 [00:01<00:16, 461.74it/s]
Adding requests:   7%|▋         | 548/8192 [00:01<00:16, 457.52it/s]
Adding requests:   7%|▋         | 597/8192 [00:01<00:16, 466.30it/s]
Adding requests:   8%|▊         | 646/8192 [00:01<00:16, 470.31it/s]
Adding requests:   8%|▊         | 696/8192 [00:01<00:15, 476.82it/s]
Adding requests:   9%|▉         | 744/8192 [00:01<00:15, 476.12it/s]
Adding requests:  10%|▉         | 792/8192 [00:01<00:15, 474.86it/s]
Adding requests:  10%|█         | 840/8192 [00:01<00:15, 465.23it/s]
Adding requests:  11%|█         | 890/8192 [00:01<00:15, 473.02it/s]
Adding requests:  11%|█▏        | 939/8192 [00:02<00:15, 475.45it/s]
Adding requests:  12%|█▏        | 988/8192 [00:02<00:15, 477.36it/s]
Adding requests:  13%|█▎        | 1037/8192 [00:02<00:14, 479.26it/s]
Adding requests:  13%|█▎        | 1085/8192 [00:02<00:14, 475.20it/s]
Adding requests:  14%|█▍        | 1133/8192 [00:02<00:14, 472.70it/s]
Adding requests:  14%|█▍        | 1183/8192 [00:02<00:14, 479.91it/s]
Adding requests:  15%|█▌        | 1233/8192 [00:02<00:14, 483.44it/s]
Adding requests:  16%|█▌        | 1282/8192 [00:02<00:14, 478.35it/s]
Adding requests:  16%|█▌        | 1331/8192 [00:02<00:14, 480.94it/s]
Adding requests:  17%|█▋        | 1380/8192 [00:02<00:14, 482.17it/s]
Adding requests:  17%|█▋        | 1429/8192 [00:03<00:14, 483.05it/s]
Adding requests:  18%|█▊        | 1478/8192 [00:03<00:13, 484.56it/s]
Adding requests:  19%|█▊        | 1527/8192 [00:03<00:14, 473.89it/s]
Adding requests:  19%|█▉        | 1575/8192 [00:03<00:14, 468.02it/s]
Adding requests:  20%|█▉        | 1624/8192 [00:03<00:13, 473.03it/s]
Adding requests:  20%|██        | 1672/8192 [00:03<00:13, 470.50it/s]
Adding requests:  21%|██        | 1721/8192 [00:03<00:13, 475.76it/s]
Adding requests:  22%|██▏       | 1769/8192 [00:03<00:13, 474.38it/s]
Adding requests:  22%|██▏       | 1818/8192 [00:03<00:13, 477.70it/s]
Adding requests:  23%|██▎       | 1866/8192 [00:03<00:13, 475.14it/s]
Adding requests:  23%|██▎       | 1914/8192 [00:04<00:13, 476.34it/s]
Adding requests:  24%|██▍       | 1963/8192 [00:04<00:13, 478.19it/s]
Adding requests:  25%|██▍       | 2012/8192 [00:04<00:12, 481.43it/s]
Adding requests:  25%|██▌       | 2061/8192 [00:04<00:12, 483.25it/s]
Adding requests:  26%|██▌       | 2110/8192 [00:04<00:12, 485.21it/s]
Adding requests:  26%|██▋       | 2159/8192 [00:04<00:12, 475.08it/s]
Adding requests:  27%|██▋       | 2207/8192 [00:04<00:12, 472.99it/s]
Adding requests:  28%|██▊       | 2257/8192 [00:04<00:12, 480.27it/s]
Adding requests:  28%|██▊       | 2306/8192 [00:04<00:12, 479.53it/s]
Adding requests:  29%|██▊       | 2354/8192 [00:04<00:12, 479.28it/s]
Adding requests:  29%|██▉       | 2402/8192 [00:05<00:12, 477.78it/s]
Adding requests:  30%|██▉       | 2451/8192 [00:05<00:11, 479.72it/s]
Adding requests:  31%|███       | 2500/8192 [00:05<00:11, 481.27it/s]
Adding requests:  31%|███       | 2549/8192 [00:05<00:11, 481.97it/s]
Adding requests:  32%|███▏      | 2598/8192 [00:05<00:11, 472.79it/s]
Adding requests:  32%|███▏      | 2647/8192 [00:05<00:11, 477.03it/s]
Adding requests:  33%|███▎      | 2695/8192 [00:05<00:11, 476.71it/s]
Adding requests:  33%|███▎      | 2743/8192 [00:05<00:11, 476.42it/s]
Adding requests:  34%|███▍      | 2791/8192 [00:05<00:11, 473.58it/s]
Adding requests:  35%|███▍      | 2839/8192 [00:06<00:11, 464.55it/s]
Adding requests:  35%|███▌      | 2886/8192 [00:06<00:11, 462.05it/s]
Adding requests:  36%|███▌      | 2933/8192 [00:06<00:11, 461.95it/s]
Adding requests:  36%|███▋      | 2982/8192 [00:06<00:11, 467.02it/s]
Adding requests:  37%|███▋      | 3030/8192 [00:06<00:11, 469.19it/s]
Adding requests:  38%|███▊      | 3077/8192 [00:06<00:10, 468.75it/s]
Adding requests:  38%|███▊      | 3126/8192 [00:06<00:10, 474.56it/s]
Adding requests:  39%|███▊      | 3174/8192 [00:06<00:10, 472.59it/s]
Adding requests:  39%|███▉      | 3222/8192 [00:06<00:10, 472.22it/s]
Adding requests:  40%|███▉      | 3271/8192 [00:06<00:10, 476.80it/s]
Adding requests:  41%|████      | 3320/8192 [00:07<00:10, 477.94it/s]
Adding requests:  41%|████      | 3370/8192 [00:07<00:09, 482.48it/s]
Adding requests:  42%|████▏     | 3419/8192 [00:07<00:09, 482.60it/s]
Adding requests:  42%|████▏     | 3468/8192 [00:07<00:09, 476.90it/s]
Adding requests:  43%|████▎     | 3516/8192 [00:07<00:09, 476.64it/s]
Adding requests:  44%|████▎     | 3564/8192 [00:07<00:09, 475.01it/s]
Adding requests:  44%|████▍     | 3612/8192 [00:07<00:09, 475.94it/s]
Adding requests:  45%|████▍     | 3660/8192 [00:07<00:09, 473.38it/s]
Adding requests:  45%|████▌     | 3709/8192 [00:07<00:09, 477.80it/s]
Adding requests:  46%|████▌     | 3757/8192 [00:07<00:09, 476.56it/s]
Adding requests:  46%|████▋     | 3805/8192 [00:08<00:09, 470.16it/s]
Adding requests:  47%|████▋     | 3855/8192 [00:08<00:09, 476.53it/s]
Adding requests:  48%|████▊     | 3903/8192 [00:08<00:08, 476.83it/s]
Adding requests:  48%|████▊     | 3952/8192 [00:08<00:08, 477.80it/s]
Adding requests:  49%|████▉     | 4000/8192 [00:08<00:08, 475.67it/s]
Adding requests:  49%|████▉     | 4048/8192 [00:08<00:08, 475.27it/s]
Adding requests:  50%|█████     | 4097/8192 [00:08<00:08, 476.83it/s]
Adding requests:  51%|█████     | 4145/8192 [00:08<00:08, 471.43it/s]
Adding requests:  51%|█████     | 4195/8192 [00:08<00:08, 477.68it/s]
Adding requests:  52%|█████▏    | 4244/8192 [00:08<00:08, 480.14it/s]
Adding requests:  52%|█████▏    | 4293/8192 [00:09<00:08, 477.06it/s]
Adding requests:  53%|█████▎    | 4344/8192 [00:09<00:07, 484.80it/s]
Adding requests:  54%|█████▎    | 4394/8192 [00:09<00:07, 488.36it/s]
Adding requests:  54%|█████▍    | 4443/8192 [00:09<00:07, 488.20it/s]
Adding requests:  55%|█████▍    | 4492/8192 [00:09<00:07, 481.87it/s]
Adding requests:  55%|█████▌    | 4541/8192 [00:09<00:07, 481.53it/s]
Adding requests:  56%|█████▌    | 4590/8192 [00:09<00:07, 483.02it/s]
Adding requests:  57%|█████▋    | 4640/8192 [00:09<00:07, 487.11it/s]
Adding requests:  57%|█████▋    | 4689/8192 [00:09<00:07, 480.09it/s]
Adding requests:  58%|█████▊    | 4740/8192 [00:09<00:07, 485.56it/s]
Adding requests:  58%|█████▊    | 4789/8192 [00:10<00:07, 484.06it/s]
Adding requests:  59%|█████▉    | 4838/8192 [00:10<00:06, 484.22it/s]
Adding requests:  60%|█████▉    | 4887/8192 [00:10<00:06, 478.95it/s]
Adding requests:  60%|██████    | 4936/8192 [00:10<00:06, 480.73it/s]
Adding requests:  61%|██████    | 4985/8192 [00:10<00:06, 466.76it/s]
Adding requests:  61%|██████▏   | 5035/8192 [00:10<00:06, 474.52it/s]
Adding requests:  62%|██████▏   | 5084/8192 [00:10<00:06, 478.93it/s]
Adding requests:  63%|██████▎   | 5134/8192 [00:10<00:06, 484.19it/s]
Adding requests:  63%|██████▎   | 5183/8192 [00:10<00:06, 481.46it/s]
Adding requests:  64%|██████▍   | 5232/8192 [00:11<00:06, 481.77it/s]
Adding requests:  64%|██████▍   | 5281/8192 [00:11<00:06, 479.25it/s]
Adding requests:  65%|██████▌   | 5332/8192 [00:11<00:05, 485.46it/s]
Adding requests:  66%|██████▌   | 5381/8192 [00:11<00:05, 484.81it/s]
Adding requests:  66%|██████▋   | 5431/8192 [00:11<00:05, 486.75it/s]
Adding requests:  67%|██████▋   | 5480/8192 [00:11<00:05, 481.60it/s]
Adding requests:  67%|██████▋   | 5529/8192 [00:11<00:05, 479.62it/s]
Adding requests:  68%|██████▊   | 5578/8192 [00:11<00:05, 481.25it/s]
Adding requests:  69%|██████▊   | 5627/8192 [00:11<00:05, 477.24it/s]
Adding requests:  69%|██████▉   | 5675/8192 [00:11<00:05, 475.35it/s]
Adding requests:  70%|██████▉   | 5725/8192 [00:12<00:05, 480.55it/s]
Adding requests:  70%|███████   | 5774/8192 [00:12<00:05, 482.10it/s]
Adding requests:  71%|███████   | 5823/8192 [00:12<00:05, 473.46it/s]
Adding requests:  72%|███████▏  | 5872/8192 [00:12<00:04, 476.87it/s]
Adding requests:  72%|███████▏  | 5922/8192 [00:12<00:04, 480.67it/s]
Adding requests:  73%|███████▎  | 5971/8192 [00:12<00:04, 480.67it/s]
Adding requests:  73%|███████▎  | 6021/8192 [00:12<00:04, 484.08it/s]
Adding requests:  74%|███████▍  | 6071/8192 [00:12<00:04, 488.74it/s]
Adding requests:  75%|███████▍  | 6120/8192 [00:12<00:04, 472.58it/s]
Adding requests:  75%|███████▌  | 6168/8192 [00:12<00:04, 473.71it/s]
Adding requests:  76%|███████▌  | 6219/8192 [00:13<00:04, 483.57it/s]
Adding requests:  77%|███████▋  | 6270/8192 [00:13<00:03, 488.46it/s]
Adding requests:  77%|███████▋  | 6320/8192 [00:13<00:03, 491.37it/s]
Adding requests:  78%|███████▊  | 6370/8192 [00:13<00:03, 492.31it/s]
Adding requests:  78%|███████▊  | 6420/8192 [00:13<00:03, 491.43it/s]
Adding requests:  79%|███████▉  | 6470/8192 [00:13<00:03, 493.64it/s]
Adding requests:  80%|███████▉  | 6521/8192 [00:13<00:03, 498.40it/s]
Adding requests:  80%|████████  | 6571/8192 [00:13<00:03, 496.93it/s]
Adding requests:  81%|████████  | 6621/8192 [00:13<00:03, 490.34it/s]
Adding requests:  81%|████████▏ | 6671/8192 [00:13<00:03, 488.34it/s]
Adding requests:  82%|████████▏ | 6721/8192 [00:14<00:03, 488.93it/s]
Adding requests:  83%|████████▎ | 6770/8192 [00:14<00:02, 487.26it/s]
Adding requests:  83%|████████▎ | 6821/8192 [00:14<00:02, 492.18it/s]
Adding requests:  84%|████████▍ | 6872/8192 [00:14<00:02, 496.63it/s]
Adding requests:  84%|████████▍ | 6922/8192 [00:14<00:02, 494.44it/s]
Adding requests:  85%|████████▌ | 6973/8192 [00:14<00:02, 498.05it/s]
Adding requests:  86%|████████▌ | 7023/8192 [00:14<00:02, 489.42it/s]
Adding requests:  86%|████████▋ | 7072/8192 [00:14<00:02, 487.89it/s]
Adding requests:  87%|████████▋ | 7122/8192 [00:14<00:02, 491.36it/s]
Adding requests:  88%|████████▊ | 7172/8192 [00:15<00:02, 486.97it/s]
Adding requests:  88%|████████▊ | 7221/8192 [00:15<00:01, 486.61it/s]
Adding requests:  89%|████████▉ | 7272/8192 [00:15<00:01, 490.50it/s]
Adding requests:  89%|████████▉ | 7322/8192 [00:15<00:01, 479.53it/s]
Adding requests:  90%|████████▉ | 7371/8192 [00:15<00:01, 479.46it/s]
Adding requests:  91%|█████████ | 7422/8192 [00:15<00:01, 486.02it/s]
Adding requests:  91%|█████████ | 7472/8192 [00:15<00:01, 489.68it/s]
Adding requests:  92%|█████████▏| 7522/8192 [00:15<00:01, 487.62it/s]
Adding requests:  92%|█████████▏| 7571/8192 [00:15<00:01, 487.30it/s]
Adding requests:  93%|█████████▎| 7620/8192 [00:15<00:01, 483.32it/s]
Adding requests:  94%|█████████▎| 7671/8192 [00:16<00:01, 488.96it/s]
Adding requests:  94%|█████████▍| 7720/8192 [00:16<00:00, 488.25it/s]
Adding requests:  95%|█████████▍| 7769/8192 [00:16<00:00, 484.55it/s]
Adding requests:  95%|█████████▌| 7818/8192 [00:16<00:00, 482.57it/s]
Adding requests:  96%|█████████▌| 7867/8192 [00:16<00:00, 483.92it/s]
Adding requests:  97%|█████████▋| 7916/8192 [00:16<00:00, 479.48it/s]
Adding requests:  97%|█████████▋| 7964/8192 [00:16<00:00, 479.00it/s]
Adding requests:  98%|█████████▊| 8012/8192 [00:16<00:00, 476.26it/s]
Adding requests:  98%|█████████▊| 8060/8192 [00:16<00:00, 476.65it/s]
Adding requests:  99%|█████████▉| 8110/8192 [00:16<00:00, 481.51it/s]
Adding requests: 100%|█████████▉| 8159/8192 [00:17<00:00, 481.82it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 478.41it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  71%|███████▏  | 5842/8192 [00:00<00:00, 58384.70it/s, est. speed input: 59790490.56 toks/s, output: 58385.81 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 58384.70it/s, est. speed input: 60871562.93 toks/s, output: 59442.73 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 59395.46it/s, est. speed input: 60871562.93 toks/s, output: 59442.73 toks/s]
[rank0]:[W126 13:56:28.174789904 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 14:56:27
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:56:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=630582) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=630582) WARNING 01-26 14:56:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=630582) WARNING 01-26 14:56:59 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 32.56 requests/s, 16703.40 total tokens/s, 32.56 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 14:56:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:56:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:56:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:56:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:56:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:56:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:56:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:56:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:56:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:56:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:56:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:56:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:56:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:56:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:56:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:56:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:56:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:56:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:56:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:56:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:56:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:56:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:56:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:56:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:56:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:56:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:56:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:56:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=630582) [2026-01-26 14:56:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=630582) [2026-01-26 14:56:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=630582) [2026-01-26 14:56:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=630582) [2026-01-26 14:56:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=630582) [2026-01-26 14:56:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=630582) [2026-01-26 14:56:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=630582) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=630582) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.01s/it]
(EngineCore_DP0 pid=630582) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.01s/it]
(EngineCore_DP0 pid=630582) 
(EngineCore_DP0 pid=630582) [2026-01-26 14:56:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=630582) [2026-01-26 14:56:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15810560 bytes
(EngineCore_DP0 pid=630582) [2026-01-26 14:56:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=630582) [2026-01-26 14:56:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9486336 bytes
(EngineCore_DP0 pid=630582) [2026-01-26 14:56:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=630582) [2026-01-26 14:56:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50593792 bytes
(EngineCore_DP0 pid=630582) [2026-01-26 14:56:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=630582) [2026-01-26 14:56:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25214976 bytes
(EngineCore_DP0 pid=630582) 2026-01-26 14:56:59,905 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=630582) 2026-01-26 14:56:59,929 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=630582) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.02it/s]
(EngineCore_DP0 pid=630582) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.31it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  59%|█████▊    | 75/128 [00:00<00:00, 744.55it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 756.56it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:22,  5.57it/s, est. speed input: 2854.24 toks/s, output: 5.57 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:06, 19.67it/s, est. speed input: 8744.83 toks/s, output: 17.08 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 26.11it/s, est. speed input: 11385.00 toks/s, output: 22.24 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 29.57it/s, est. speed input: 12871.38 toks/s, output: 25.14 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 31.67it/s, est. speed input: 13834.23 toks/s, output: 27.02 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 32.85it/s, est. speed input: 14476.70 toks/s, output: 28.27 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 33.66it/s, est. speed input: 14958.43 toks/s, output: 29.22 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 34.20it/s, est. speed input: 15326.08 toks/s, output: 29.93 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 34.56it/s, est. speed input: 15617.51 toks/s, output: 30.50 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 34.82it/s, est. speed input: 15854.52 toks/s, output: 30.97 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 34.96it/s, est. speed input: 16045.39 toks/s, output: 31.34 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 35.08it/s, est. speed input: 16208.91 toks/s, output: 31.66 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 35.16it/s, est. speed input: 16348.99 toks/s, output: 31.93 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 35.26it/s, est. speed input: 16473.03 toks/s, output: 32.17 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 35.40it/s, est. speed input: 16588.92 toks/s, output: 32.40 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 35.42it/s, est. speed input: 16683.34 toks/s, output: 32.58 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 35.40it/s, est. speed input: 16764.59 toks/s, output: 32.74 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 35.46it/s, est. speed input: 16842.34 toks/s, output: 32.90 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 35.46it/s, est. speed input: 16909.43 toks/s, output: 33.03 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 35.48it/s, est. speed input: 16971.52 toks/s, output: 33.15 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 35.48it/s, est. speed input: 17027.05 toks/s, output: 33.26 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 35.47it/s, est. speed input: 17076.54 toks/s, output: 33.35 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 35.46it/s, est. speed input: 17122.14 toks/s, output: 33.44 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 35.49it/s, est. speed input: 17165.59 toks/s, output: 33.53 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 35.50it/s, est. speed input: 17205.66 toks/s, output: 33.60 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 35.54it/s, est. speed input: 17244.54 toks/s, output: 33.68 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 35.54it/s, est. speed input: 17279.25 toks/s, output: 33.75 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 35.54it/s, est. speed input: 17310.82 toks/s, output: 33.81 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 35.50it/s, est. speed input: 17338.75 toks/s, output: 33.86 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 35.43it/s, est. speed input: 17362.24 toks/s, output: 33.91 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 35.39it/s, est. speed input: 17384.78 toks/s, output: 33.95 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 35.41it/s, est. speed input: 17408.67 toks/s, output: 34.00 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.41it/s, est. speed input: 17425.77 toks/s, output: 34.03 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.03it/s, est. speed input: 17425.77 toks/s, output: 34.03 toks/s]
[rank0]:[W126 14:57:06.523936408 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 14:57:08
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:57:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=631770) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=631770) WARNING 01-26 14:57:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=631770) WARNING 01-26 14:57:40 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 34.66 requests/s, 35521.95 total tokens/s, 34.66 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 14:57:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:57:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:57:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:57:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:57:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:57:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:57:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:57:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:57:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:57:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:57:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:57:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:57:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:57:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:57:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:57:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:57:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:57:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:57:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:57:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:57:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:57:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:57:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:57:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:57:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:57:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:57:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:57:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=631770) [2026-01-26 14:57:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=631770) [2026-01-26 14:57:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=631770) [2026-01-26 14:57:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=631770) [2026-01-26 14:57:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=631770) [2026-01-26 14:57:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=631770) [2026-01-26 14:57:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=631770) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=631770) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.02s/it]
(EngineCore_DP0 pid=631770) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.02s/it]
(EngineCore_DP0 pid=631770) 
(EngineCore_DP0 pid=631770) [2026-01-26 14:57:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=631770) [2026-01-26 14:57:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15810560 bytes
(EngineCore_DP0 pid=631770) [2026-01-26 14:57:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=631770) [2026-01-26 14:57:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9486336 bytes
(EngineCore_DP0 pid=631770) [2026-01-26 14:57:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=631770) [2026-01-26 14:57:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50593792 bytes
(EngineCore_DP0 pid=631770) [2026-01-26 14:57:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=631770) [2026-01-26 14:57:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25214976 bytes
(EngineCore_DP0 pid=631770) 2026-01-26 14:57:40,557 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=631770) 2026-01-26 14:57:40,581 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=631770) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.95it/s]
(EngineCore_DP0 pid=631770) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.45it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:00, 392.34it/s]
Adding requests:  65%|██████▍   | 83/128 [00:00<00:00, 412.20it/s]
Adding requests:  99%|█████████▉| 127/128 [00:00<00:00, 420.52it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 416.11it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:01, 73.15it/s, est. speed input: 74906.78 toks/s, output: 73.15 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:02, 46.05it/s, est. speed input: 49926.46 toks/s, output: 48.76 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 41.84it/s, est. speed input: 45760.05 toks/s, output: 44.69 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 40.00it/s, est. speed input: 43934.42 toks/s, output: 42.90 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 38.91it/s, est. speed input: 42803.53 toks/s, output: 41.80 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 38.20it/s, est. speed input: 42098.25 toks/s, output: 41.11 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:02, 37.67it/s, est. speed input: 41549.92 toks/s, output: 40.58 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 37.35it/s, est. speed input: 41139.57 toks/s, output: 40.18 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 37.14it/s, est. speed input: 40808.74 toks/s, output: 39.85 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 37.01it/s, est. speed input: 40539.76 toks/s, output: 39.59 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 36.91it/s, est. speed input: 40311.68 toks/s, output: 39.37 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 36.86it/s, est. speed input: 40120.15 toks/s, output: 39.18 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 36.81it/s, est. speed input: 39950.98 toks/s, output: 39.01 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 36.80it/s, est. speed input: 39809.34 toks/s, output: 38.88 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 36.73it/s, est. speed input: 39669.27 toks/s, output: 38.74 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 36.71it/s, est. speed input: 39551.56 toks/s, output: 38.62 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 36.71it/s, est. speed input: 39448.38 toks/s, output: 38.52 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 36.69it/s, est. speed input: 39352.42 toks/s, output: 38.43 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 36.69it/s, est. speed input: 39267.04 toks/s, output: 38.35 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 36.66it/s, est. speed input: 39185.06 toks/s, output: 38.27 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 36.72it/s, est. speed input: 39123.66 toks/s, output: 38.21 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 36.72it/s, est. speed input: 39060.82 toks/s, output: 38.15 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 36.70it/s, est. speed input: 38998.78 toks/s, output: 38.08 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 36.68it/s, est. speed input: 38942.26 toks/s, output: 38.03 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 36.69it/s, est. speed input: 38892.26 toks/s, output: 37.98 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 36.70it/s, est. speed input: 38846.75 toks/s, output: 37.94 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 36.74it/s, est. speed input: 38808.19 toks/s, output: 37.90 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 36.71it/s, est. speed input: 38765.37 toks/s, output: 37.86 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.69it/s, est. speed input: 38725.47 toks/s, output: 37.82 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.69it/s, est. speed input: 38725.47 toks/s, output: 37.82 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.82it/s, est. speed input: 38725.47 toks/s, output: 37.82 toks/s]
[rank0]:[W126 14:57:45.156325946 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 14:57:47
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:57:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=632902) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=632902) WARNING 01-26 14:58:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=632902) WARNING 01-26 14:58:20 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 68.19 requests/s, 69896.50 total tokens/s, 68.19 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 14:57:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:57:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:57:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:57:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:57:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:57:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:57:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:57:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:57:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:57:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:57:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:57:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:57:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:57:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:58:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:58:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:58:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:58:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:58:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:58:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:58:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:58:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:58:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:58:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:58:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:58:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:58:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:58:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=632902) [2026-01-26 14:58:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=632902) [2026-01-26 14:58:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=632902) [2026-01-26 14:58:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=632902) [2026-01-26 14:58:03] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=632902) [2026-01-26 14:58:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=632902) [2026-01-26 14:58:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=632902) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=632902) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.02s/it]
(EngineCore_DP0 pid=632902) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.02s/it]
(EngineCore_DP0 pid=632902) 
(EngineCore_DP0 pid=632902) [2026-01-26 14:58:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=632902) [2026-01-26 14:58:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15810560 bytes
(EngineCore_DP0 pid=632902) [2026-01-26 14:58:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=632902) [2026-01-26 14:58:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9486336 bytes
(EngineCore_DP0 pid=632902) [2026-01-26 14:58:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=632902) [2026-01-26 14:58:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50593792 bytes
(EngineCore_DP0 pid=632902) [2026-01-26 14:58:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=632902) [2026-01-26 14:58:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25214976 bytes
(EngineCore_DP0 pid=632902) 2026-01-26 14:58:20,635 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=632902) 2026-01-26 14:58:20,658 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=632902) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.54it/s]
(EngineCore_DP0 pid=632902) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 12.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 12.94it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:47,  5.34it/s]
Adding requests:  16%|█▌        | 41/256 [00:00<00:01, 175.39it/s]
Adding requests:  33%|███▎      | 85/256 [00:00<00:00, 276.10it/s]
Adding requests:  50%|█████     | 129/256 [00:00<00:00, 333.58it/s]
Adding requests:  68%|██████▊   | 173/256 [00:00<00:00, 367.49it/s]
Adding requests:  85%|████████▍ | 217/256 [00:00<00:00, 388.79it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 331.03it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:00<00:00, 364.02it/s, est. speed input: 372774.07 toks/s, output: 364.02 toks/s]
Processed prompts:  32%|███▏      | 81/256 [00:00<00:01, 118.71it/s, est. speed input: 136553.79 toks/s, output: 133.35 toks/s]
Processed prompts:  39%|███▉      | 101/256 [00:00<00:01, 100.56it/s, est. speed input: 117736.10 toks/s, output: 114.98 toks/s]
Processed prompts:  45%|████▍     | 115/256 [00:01<00:01, 92.99it/s, est. speed input: 110338.57 toks/s, output: 107.75 toks/s] 
Processed prompts:  50%|████▉     | 127/256 [00:01<00:01, 88.10it/s, est. speed input: 105815.63 toks/s, output: 103.34 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:01<00:01, 84.89it/s, est. speed input: 102902.92 toks/s, output: 100.49 toks/s]
Processed prompts:  57%|█████▋    | 147/256 [00:01<00:01, 82.29it/s, est. speed input: 100522.05 toks/s, output: 98.17 toks/s] 
Processed prompts:  61%|██████    | 156/256 [00:01<00:01, 78.21it/s, est. speed input: 97883.48 toks/s, output: 95.59 toks/s] 
Processed prompts:  64%|██████▍   | 164/256 [00:01<00:01, 77.35it/s, est. speed input: 96548.65 toks/s, output: 94.29 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:01, 76.51it/s, est. speed input: 95332.93 toks/s, output: 93.10 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:01<00:01, 75.98it/s, est. speed input: 94285.48 toks/s, output: 92.08 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:02<00:00, 75.39it/s, est. speed input: 93305.49 toks/s, output: 91.12 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:02<00:00, 75.11it/s, est. speed input: 92456.22 toks/s, output: 90.29 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:02<00:00, 74.64it/s, est. speed input: 91634.79 toks/s, output: 89.49 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:02<00:00, 74.61it/s, est. speed input: 90945.92 toks/s, output: 88.81 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:02<00:00, 74.38it/s, est. speed input: 90279.30 toks/s, output: 88.16 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 74.63it/s, est. speed input: 89737.54 toks/s, output: 87.63 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 74.36it/s, est. speed input: 89167.39 toks/s, output: 87.08 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:02<00:00, 74.16it/s, est. speed input: 88640.00 toks/s, output: 86.56 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:02<00:00, 74.23it/s, est. speed input: 88182.24 toks/s, output: 86.12 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 74.23it/s, est. speed input: 87978.31 toks/s, output: 85.92 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 85.91it/s, est. speed input: 87978.31 toks/s, output: 85.92 toks/s]
[rank0]:[W126 14:58:26.382669505 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 14:58:28
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:58:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=634028) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=634028) WARNING 01-26 14:58:52 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=634028) WARNING 01-26 14:59:01 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 138.42 requests/s, 141885.55 total tokens/s, 138.42 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 14:58:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:58:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:58:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:58:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:58:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:58:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:58:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:58:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:58:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:58:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:58:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:58:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:58:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:58:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:58:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:58:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:58:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:58:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:58:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:58:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:58:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:58:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:58:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:58:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:58:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:58:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:58:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:58:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=634028) [2026-01-26 14:58:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=634028) [2026-01-26 14:58:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=634028) [2026-01-26 14:58:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=634028) [2026-01-26 14:58:44] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=634028) [2026-01-26 14:58:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=634028) [2026-01-26 14:58:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=634028) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=634028) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.07it/s]
(EngineCore_DP0 pid=634028) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.07it/s]
(EngineCore_DP0 pid=634028) 
(EngineCore_DP0 pid=634028) [2026-01-26 14:58:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=634028) [2026-01-26 14:58:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15810560 bytes
(EngineCore_DP0 pid=634028) [2026-01-26 14:58:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=634028) [2026-01-26 14:58:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9486336 bytes
(EngineCore_DP0 pid=634028) [2026-01-26 14:58:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=634028) [2026-01-26 14:58:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50593792 bytes
(EngineCore_DP0 pid=634028) [2026-01-26 14:58:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=634028) [2026-01-26 14:58:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25214976 bytes
(EngineCore_DP0 pid=634028) 2026-01-26 14:59:01,962 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=634028) 2026-01-26 14:59:01,987 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=634028) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.46it/s]
(EngineCore_DP0 pid=634028) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 16.50it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 17.09it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 38/512 [00:00<00:01, 374.69it/s]
Adding requests:  16%|█▌        | 82/512 [00:00<00:01, 410.85it/s]
Adding requests:  24%|██▍       | 124/512 [00:00<00:00, 395.95it/s]
Adding requests:  33%|███▎      | 167/512 [00:00<00:00, 406.93it/s]
Adding requests:  41%|████▏     | 212/512 [00:00<00:00, 420.47it/s]
Adding requests:  51%|█████     | 260/512 [00:00<00:00, 438.05it/s]
Adding requests:  60%|█████▉    | 305/512 [00:00<00:00, 439.82it/s]
Adding requests:  69%|██████▉   | 352/512 [00:00<00:00, 446.99it/s]
Adding requests:  78%|███████▊  | 400/512 [00:00<00:00, 454.03it/s]
Adding requests:  87%|████████▋ | 446/512 [00:01<00:00, 455.75it/s]
Adding requests:  96%|█████████▋| 493/512 [00:01<00:00, 460.02it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 439.69it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  31%|███       | 158/512 [00:00<00:00, 1369.53it/s, est. speed input: 1402484.68 toks/s, output: 1369.55 toks/s]
Processed prompts:  58%|█████▊    | 295/512 [00:01<00:00, 246.61it/s, est. speed input: 290853.08 toks/s, output: 284.03 toks/s]   
Processed prompts:  71%|███████   | 361/512 [00:01<00:00, 208.42it/s, est. speed input: 249090.05 toks/s, output: 243.25 toks/s]
Processed prompts:  79%|███████▉  | 404/512 [00:01<00:00, 191.09it/s, est. speed input: 232156.98 toks/s, output: 226.72 toks/s]
Processed prompts:  85%|████████▌ | 436/512 [00:01<00:00, 181.59it/s, est. speed input: 223392.56 toks/s, output: 218.16 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:02<00:00, 169.93it/s, est. speed input: 215138.01 toks/s, output: 210.10 toks/s]
Processed prompts:  95%|█████████▍| 484/512 [00:02<00:00, 168.52it/s, est. speed input: 212277.81 toks/s, output: 207.30 toks/s]
Processed prompts:  98%|█████████▊| 504/512 [00:02<00:00, 164.17it/s, est. speed input: 208886.59 toks/s, output: 203.99 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 164.17it/s, est. speed input: 206972.66 toks/s, output: 202.12 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 202.11it/s, est. speed input: 206972.66 toks/s, output: 202.12 toks/s]
[rank0]:[W126 14:59:07.110089764 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 14:59:09
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:59:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=635201) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=635201) WARNING 01-26 14:59:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=635201) WARNING 01-26 14:59:45 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 201.69 requests/s, 206733.83 total tokens/s, 201.69 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 14:59:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:59:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:59:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:59:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:59:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:59:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:59:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:59:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:59:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:59:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:59:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:59:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:59:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:59:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:59:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:59:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:59:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:59:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:59:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:59:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:59:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:59:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:59:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:59:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:59:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:59:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:59:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:59:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=635201) [2026-01-26 14:59:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=635201) [2026-01-26 14:59:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=635201) [2026-01-26 14:59:28] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=635201) [2026-01-26 14:59:28] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=635201) [2026-01-26 14:59:28] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=635201) [2026-01-26 14:59:28] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=635201) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=635201) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]
(EngineCore_DP0 pid=635201) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]
(EngineCore_DP0 pid=635201) 
(EngineCore_DP0 pid=635201) [2026-01-26 14:59:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=635201) [2026-01-26 14:59:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15810560 bytes
(EngineCore_DP0 pid=635201) [2026-01-26 14:59:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=635201) [2026-01-26 14:59:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9486336 bytes
(EngineCore_DP0 pid=635201) [2026-01-26 14:59:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=635201) [2026-01-26 14:59:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50593792 bytes
(EngineCore_DP0 pid=635201) [2026-01-26 14:59:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=635201) [2026-01-26 14:59:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25214976 bytes
(EngineCore_DP0 pid=635201) 2026-01-26 14:59:45,435 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=635201) 2026-01-26 14:59:45,459 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=635201) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 16.04it/s]
(EngineCore_DP0 pid=635201) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 15.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.41it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.19it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 396.50it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 422.54it/s]
Adding requests:  13%|█▎        | 129/1024 [00:00<00:02, 430.27it/s]
Adding requests:  17%|█▋        | 173/1024 [00:00<00:01, 432.62it/s]
Adding requests:  21%|██▏       | 218/1024 [00:00<00:01, 438.39it/s]
Adding requests:  26%|██▌       | 265/1024 [00:00<00:01, 448.39it/s]
Adding requests:  30%|███       | 310/1024 [00:00<00:01, 447.07it/s]
Adding requests:  35%|███▍      | 357/1024 [00:00<00:01, 451.54it/s]
Adding requests:  39%|███▉      | 403/1024 [00:00<00:01, 452.60it/s]
Adding requests:  44%|████▍     | 450/1024 [00:01<00:01, 455.67it/s]
Adding requests:  48%|████▊     | 496/1024 [00:01<00:01, 456.96it/s]
Adding requests:  53%|█████▎    | 542/1024 [00:01<00:01, 449.86it/s]
Adding requests:  58%|█████▊    | 591/1024 [00:01<00:00, 460.61it/s]
Adding requests:  62%|██████▏   | 638/1024 [00:01<00:00, 462.01it/s]
Adding requests:  67%|██████▋   | 687/1024 [00:01<00:00, 467.79it/s]
Adding requests:  72%|███████▏  | 736/1024 [00:01<00:00, 473.99it/s]
Adding requests:  77%|███████▋  | 784/1024 [00:01<00:00, 467.95it/s]
Adding requests:  81%|████████  | 831/1024 [00:01<00:00, 459.30it/s]
Adding requests:  86%|████████▌ | 879/1024 [00:01<00:00, 463.71it/s]
Adding requests:  91%|█████████ | 928/1024 [00:02<00:00, 469.39it/s]
Adding requests:  95%|█████████▌| 976/1024 [00:02<00:00, 471.56it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 457.91it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:00<00:00, 3765.70it/s, est. speed input: 3856300.49 toks/s, output: 3765.76 toks/s]
Processed prompts:  82%|████████▏ | 835/1024 [00:01<00:00, 365.06it/s, est. speed input: 439065.46 toks/s, output: 428.77 toks/s]   
Processed prompts:  98%|█████████▊| 1003/1024 [00:02<00:00, 305.54it/s, est. speed input: 372421.23 toks/s, output: 363.69 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 305.54it/s, est. speed input: 369278.82 toks/s, output: 360.62 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 360.61it/s, est. speed input: 369278.82 toks/s, output: 360.62 toks/s]
[rank0]:[W126 14:59:52.120952329 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 14:59:54
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:00:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=636442) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=636442) WARNING 01-26 15:00:26 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=636442) WARNING 01-26 15:00:35 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 219.20 requests/s, 224683.97 total tokens/s, 219.20 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 15:00:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:00:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 15:00:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 15:00:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:00:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:00:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:00:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:00:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:00:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 15:00:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:00:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:00:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:00:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:00:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:00:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:00:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 15:00:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 15:00:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:00:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:00:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:00:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:00:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:00:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 15:00:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:00:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:00:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:00:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:00:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=636442) [2026-01-26 15:00:18] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=636442) [2026-01-26 15:00:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=636442) [2026-01-26 15:00:18] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=636442) [2026-01-26 15:00:18] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=636442) [2026-01-26 15:00:18] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=636442) [2026-01-26 15:00:18] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=636442) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=636442) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]
(EngineCore_DP0 pid=636442) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]
(EngineCore_DP0 pid=636442) 
(EngineCore_DP0 pid=636442) [2026-01-26 15:00:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=636442) [2026-01-26 15:00:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15810560 bytes
(EngineCore_DP0 pid=636442) [2026-01-26 15:00:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=636442) [2026-01-26 15:00:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9486336 bytes
(EngineCore_DP0 pid=636442) [2026-01-26 15:00:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=636442) [2026-01-26 15:00:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50593792 bytes
(EngineCore_DP0 pid=636442) [2026-01-26 15:00:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=636442) [2026-01-26 15:00:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25214976 bytes
(EngineCore_DP0 pid=636442) 2026-01-26 15:00:35,847 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=636442) 2026-01-26 15:00:35,871 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=636442) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  9.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 12.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 15.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.20it/s]
(EngineCore_DP0 pid=636442) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 18.23it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 18.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.25it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 40/2048 [00:00<00:05, 395.73it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 421.20it/s]
Adding requests:   6%|▋         | 128/2048 [00:00<00:04, 428.53it/s]
Adding requests:   8%|▊         | 172/2048 [00:00<00:04, 431.52it/s]
Adding requests:  11%|█         | 217/2048 [00:00<00:04, 437.77it/s]
Adding requests:  13%|█▎        | 265/2048 [00:00<00:03, 449.79it/s]
Adding requests:  15%|█▌        | 311/2048 [00:00<00:03, 450.19it/s]
Adding requests:  17%|█▋        | 357/2048 [00:00<00:03, 452.10it/s]
Adding requests:  20%|█▉        | 404/2048 [00:00<00:03, 457.05it/s]
Adding requests:  22%|██▏       | 451/2048 [00:01<00:03, 458.38it/s]
Adding requests:  24%|██▍       | 498/2048 [00:01<00:03, 460.19it/s]
Adding requests:  27%|██▋       | 545/2048 [00:01<00:03, 453.11it/s]
Adding requests:  29%|██▉       | 594/2048 [00:01<00:03, 463.44it/s]
Adding requests:  31%|███▏      | 641/2048 [00:01<00:03, 460.74it/s]
Adding requests:  34%|███▎      | 690/2048 [00:01<00:02, 467.80it/s]
Adding requests:  36%|███▌      | 739/2048 [00:01<00:02, 471.60it/s]
Adding requests:  38%|███▊      | 787/2048 [00:01<00:02, 468.72it/s]
Adding requests:  41%|████      | 834/2048 [00:01<00:02, 457.28it/s]
Adding requests:  43%|████▎     | 883/2048 [00:01<00:02, 464.70it/s]
Adding requests:  45%|████▌     | 931/2048 [00:02<00:02, 468.97it/s]
Adding requests:  48%|████▊     | 979/2048 [00:02<00:02, 470.26it/s]
Adding requests:  50%|█████     | 1028/2048 [00:02<00:02, 473.19it/s]
Adding requests:  53%|█████▎    | 1076/2048 [00:02<00:02, 462.99it/s]
Adding requests:  55%|█████▍    | 1123/2048 [00:02<00:01, 464.32it/s]
Adding requests:  57%|█████▋    | 1172/2048 [00:02<00:01, 471.02it/s]
Adding requests:  60%|█████▉    | 1222/2048 [00:02<00:01, 478.07it/s]
Adding requests:  62%|██████▏   | 1270/2048 [00:02<00:01, 472.65it/s]
Adding requests:  64%|██████▍   | 1319/2048 [00:02<00:01, 475.15it/s]
Adding requests:  67%|██████▋   | 1367/2048 [00:02<00:01, 476.35it/s]
Adding requests:  69%|██████▉   | 1416/2048 [00:03<00:01, 477.85it/s]
Adding requests:  72%|███████▏  | 1465/2048 [00:03<00:01, 479.14it/s]
Adding requests:  74%|███████▍  | 1513/2048 [00:03<00:01, 473.97it/s]
Adding requests:  76%|███████▌  | 1561/2048 [00:03<00:01, 471.67it/s]
Adding requests:  79%|███████▊  | 1611/2048 [00:03<00:00, 476.33it/s]
Adding requests:  81%|████████  | 1659/2048 [00:03<00:00, 475.64it/s]
Adding requests:  83%|████████▎ | 1707/2048 [00:03<00:00, 475.18it/s]
Adding requests:  86%|████████▌ | 1755/2048 [00:03<00:00, 463.25it/s]
Adding requests:  88%|████████▊ | 1802/2048 [00:03<00:00, 464.03it/s]
Adding requests:  90%|█████████ | 1850/2048 [00:03<00:00, 466.15it/s]
Adding requests:  93%|█████████▎| 1898/2048 [00:04<00:00, 468.31it/s]
Adding requests:  95%|█████████▌| 1946/2048 [00:04<00:00, 468.63it/s]
Adding requests:  97%|█████████▋| 1993/2048 [00:04<00:00, 468.93it/s]
Adding requests: 100%|█████████▉| 2041/2048 [00:04<00:00, 471.93it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 464.77it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:00<00:00, 9174.11it/s, est. speed input: 9394929.67 toks/s, output: 9174.26 toks/s]
Processed prompts:  92%|█████████▏| 1880/2048 [00:04<00:00, 381.11it/s, est. speed input: 457580.36 toks/s, output: 446.86 toks/s]  
Processed prompts: 100%|██████████| 2048/2048 [00:04<00:00, 381.11it/s, est. speed input: 424984.22 toks/s, output: 415.02 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:04<00:00, 415.01it/s, est. speed input: 424984.22 toks/s, output: 415.02 toks/s]
[rank0]:[W126 15:00:47.831604324 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 15:00:49
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:01:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=637874) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=637874) WARNING 01-26 15:01:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=637874) WARNING 01-26 15:01:40 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 228.15 requests/s, 233853.73 total tokens/s, 228.15 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 15:01:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:01:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 15:01:14] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 15:01:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:01:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:01:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:01:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:01:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:01:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 15:01:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:01:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:01:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:01:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:01:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:01:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:01:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 15:01:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 15:01:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:01:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:01:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:01:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:01:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:01:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 15:01:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:01:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:01:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:01:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:01:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=637874) [2026-01-26 15:01:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=637874) [2026-01-26 15:01:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=637874) [2026-01-26 15:01:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=637874) [2026-01-26 15:01:22] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=637874) [2026-01-26 15:01:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=637874) [2026-01-26 15:01:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=637874) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=637874) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.21it/s]
(EngineCore_DP0 pid=637874) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.21it/s]
(EngineCore_DP0 pid=637874) 
(EngineCore_DP0 pid=637874) [2026-01-26 15:01:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=637874) [2026-01-26 15:01:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15810560 bytes
(EngineCore_DP0 pid=637874) [2026-01-26 15:01:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=637874) [2026-01-26 15:01:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9486336 bytes
(EngineCore_DP0 pid=637874) [2026-01-26 15:01:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=637874) [2026-01-26 15:01:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50593792 bytes
(EngineCore_DP0 pid=637874) [2026-01-26 15:01:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=637874) [2026-01-26 15:01:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25214976 bytes
(EngineCore_DP0 pid=637874) [rank0]:W0126 15:01:35.290000 637874 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=637874) [rank0]:W0126 15:01:35.377000 637874 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=637874) [rank0]:W0126 15:01:36.315000 637874 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=637874) [rank0]:W0126 15:01:36.441000 637874 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=637874) 2026-01-26 15:01:39,890 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=637874) 2026-01-26 15:01:39,916 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=637874) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 15.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 15.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 15.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 15.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 15.26it/s]
(EngineCore_DP0 pid=637874) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.55it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.61it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 17.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 17.88it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 41/4096 [00:00<00:10, 404.76it/s]
Adding requests:   2%|▏         | 86/4096 [00:00<00:09, 426.25it/s]
Adding requests:   3%|▎         | 131/4096 [00:00<00:09, 436.50it/s]
Adding requests:   4%|▍         | 176/4096 [00:00<00:08, 439.71it/s]
Adding requests:   5%|▌         | 222/4096 [00:00<00:08, 445.06it/s]
Adding requests:   7%|▋         | 270/4096 [00:00<00:08, 456.44it/s]
Adding requests:   8%|▊         | 316/4096 [00:00<00:08, 453.27it/s]
Adding requests:   9%|▉         | 364/4096 [00:00<00:08, 460.35it/s]
Adding requests:  10%|█         | 411/4096 [00:00<00:08, 453.59it/s]
Adding requests:  11%|█         | 458/4096 [00:01<00:07, 458.20it/s]
Adding requests:  12%|█▏        | 505/4096 [00:01<00:07, 458.73it/s]
Adding requests:  13%|█▎        | 551/4096 [00:01<00:07, 455.89it/s]
Adding requests:  15%|█▍        | 600/4096 [00:01<00:07, 461.46it/s]
Adding requests:  16%|█▌        | 650/4096 [00:01<00:07, 470.07it/s]
Adding requests:  17%|█▋        | 700/4096 [00:01<00:07, 477.63it/s]
Adding requests:  18%|█▊        | 748/4096 [00:01<00:07, 475.34it/s]
Adding requests:  19%|█▉        | 796/4096 [00:01<00:06, 473.35it/s]
Adding requests:  21%|██        | 844/4096 [00:01<00:07, 464.08it/s]
Adding requests:  22%|██▏       | 891/4096 [00:01<00:06, 463.89it/s]
Adding requests:  23%|██▎       | 939/4096 [00:02<00:06, 467.53it/s]
Adding requests:  24%|██▍       | 987/4096 [00:02<00:06, 470.30it/s]
Adding requests:  25%|██▌       | 1036/4096 [00:02<00:06, 473.15it/s]
Adding requests:  26%|██▋       | 1084/4096 [00:02<00:06, 472.10it/s]
Adding requests:  28%|██▊       | 1132/4096 [00:02<00:06, 470.42it/s]
Adding requests:  29%|██▉       | 1182/4096 [00:02<00:06, 478.59it/s]
Adding requests:  30%|███       | 1232/4096 [00:02<00:05, 483.80it/s]
Adding requests:  31%|███▏      | 1281/4096 [00:02<00:05, 478.17it/s]
Adding requests:  32%|███▏      | 1331/4096 [00:02<00:05, 482.13it/s]
Adding requests:  34%|███▎      | 1380/4096 [00:02<00:05, 483.94it/s]
Adding requests:  35%|███▍      | 1429/4096 [00:03<00:05, 484.90it/s]
Adding requests:  36%|███▌      | 1478/4096 [00:03<00:05, 486.38it/s]
Adding requests:  37%|███▋      | 1528/4096 [00:03<00:05, 487.49it/s]
Adding requests:  39%|███▊      | 1578/4096 [00:03<00:05, 488.15it/s]
Adding requests:  40%|███▉      | 1629/4096 [00:03<00:05, 493.17it/s]
Adding requests:  41%|████      | 1679/4096 [00:03<00:04, 486.68it/s]
Adding requests:  42%|████▏     | 1728/4096 [00:03<00:05, 469.71it/s]
Adding requests:  43%|████▎     | 1776/4096 [00:03<00:04, 468.72it/s]
Adding requests:  45%|████▍     | 1826/4096 [00:03<00:04, 474.47it/s]
Adding requests:  46%|████▌     | 1874/4096 [00:03<00:04, 475.99it/s]
Adding requests:  47%|████▋     | 1923/4096 [00:04<00:04, 478.63it/s]
Adding requests:  48%|████▊     | 1971/4096 [00:04<00:04, 465.04it/s]
Adding requests:  49%|████▉     | 2020/4096 [00:04<00:04, 472.06it/s]
Adding requests:  51%|█████     | 2069/4096 [00:04<00:04, 477.01it/s]
Adding requests:  52%|█████▏    | 2118/4096 [00:04<00:04, 480.00it/s]
Adding requests:  53%|█████▎    | 2167/4096 [00:04<00:04, 472.61it/s]
Adding requests:  54%|█████▍    | 2215/4096 [00:04<00:03, 472.54it/s]
Adding requests:  55%|█████▌    | 2264/4096 [00:04<00:03, 475.89it/s]
Adding requests:  56%|█████▋    | 2314/4096 [00:04<00:03, 481.80it/s]
Adding requests:  58%|█████▊    | 2363/4096 [00:05<00:03, 478.50it/s]
Adding requests:  59%|█████▉    | 2411/4096 [00:05<00:03, 478.55it/s]
Adding requests:  60%|██████    | 2461/4096 [00:05<00:03, 482.92it/s]
Adding requests:  61%|██████▏   | 2510/4096 [00:05<00:03, 480.29it/s]
Adding requests:  62%|██████▎   | 2560/4096 [00:05<00:03, 485.45it/s]
Adding requests:  64%|██████▎   | 2609/4096 [00:05<00:03, 483.70it/s]
Adding requests:  65%|██████▍   | 2659/4096 [00:05<00:02, 487.21it/s]
Adding requests:  66%|██████▌   | 2708/4096 [00:05<00:02, 480.91it/s]
Adding requests:  67%|██████▋   | 2757/4096 [00:05<00:02, 481.18it/s]
Adding requests:  69%|██████▊   | 2806/4096 [00:05<00:02, 478.60it/s]
Adding requests:  70%|██████▉   | 2855/4096 [00:06<00:02, 479.90it/s]
Adding requests:  71%|███████   | 2905/4096 [00:06<00:02, 483.10it/s]
Adding requests:  72%|███████▏  | 2954/4096 [00:06<00:02, 477.92it/s]
Adding requests:  73%|███████▎  | 3003/4096 [00:06<00:02, 479.21it/s]
Adding requests:  74%|███████▍  | 3051/4096 [00:06<00:02, 473.66it/s]
Adding requests:  76%|███████▌  | 3099/4096 [00:06<00:02, 473.58it/s]
Adding requests:  77%|███████▋  | 3147/4096 [00:06<00:02, 472.70it/s]
Adding requests:  78%|███████▊  | 3196/4096 [00:06<00:01, 476.08it/s]
Adding requests:  79%|███████▉  | 3244/4096 [00:06<00:01, 466.92it/s]
Adding requests:  80%|████████  | 3293/4096 [00:06<00:01, 471.46it/s]
Adding requests:  82%|████████▏ | 3341/4096 [00:07<00:01, 473.85it/s]
Adding requests:  83%|████████▎ | 3390/4096 [00:07<00:01, 474.63it/s]
Adding requests:  84%|████████▍ | 3440/4096 [00:07<00:01, 481.63it/s]
Adding requests:  85%|████████▌ | 3489/4096 [00:07<00:01, 472.10it/s]
Adding requests:  86%|████████▋ | 3537/4096 [00:07<00:01, 474.20it/s]
Adding requests:  88%|████████▊ | 3585/4096 [00:07<00:01, 469.86it/s]
Adding requests:  89%|████████▊ | 3633/4096 [00:07<00:00, 471.54it/s]
Adding requests:  90%|████████▉ | 3681/4096 [00:07<00:00, 473.43it/s]
Adding requests:  91%|█████████ | 3729/4096 [00:07<00:00, 475.18it/s]
Adding requests:  92%|█████████▏| 3779/4096 [00:07<00:00, 481.56it/s]
Adding requests:  93%|█████████▎| 3828/4096 [00:08<00:00, 482.23it/s]
Adding requests:  95%|█████████▍| 3879/4096 [00:08<00:00, 487.98it/s]
Adding requests:  96%|█████████▌| 3928/4096 [00:08<00:00, 486.64it/s]
Adding requests:  97%|█████████▋| 3977/4096 [00:08<00:00, 484.87it/s]
Adding requests:  98%|█████████▊| 4026/4096 [00:08<00:00, 481.56it/s]
Adding requests:  99%|█████████▉| 4075/4096 [00:08<00:00, 473.56it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 473.98it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  48%|████▊     | 1973/4096 [00:00<00:00, 11118.27it/s, est. speed input: 11385613.39 toks/s, output: 11118.39 toks/s]
Processed prompts:  75%|███████▌  | 3085/4096 [00:04<00:01, 511.97it/s, est. speed input: 641713.48 toks/s, output: 626.67 toks/s]      
Processed prompts:  87%|████████▋ | 3557/4096 [00:07<00:01, 404.05it/s, est. speed input: 520224.55 toks/s, output: 508.03 toks/s]
Processed prompts:  93%|█████████▎| 3824/4096 [00:08<00:00, 366.55it/s, est. speed input: 482064.33 toks/s, output: 470.77 toks/s]
Processed prompts:  98%|█████████▊| 3995/4096 [00:08<00:00, 335.16it/s, est. speed input: 456648.27 toks/s, output: 445.95 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:09<00:00, 335.16it/s, est. speed input: 450536.68 toks/s, output: 439.98 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:09<00:00, 439.97it/s, est. speed input: 450536.68 toks/s, output: 439.98 toks/s]
[rank0]:[W126 15:02:00.137139275 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 15:02:02
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Llama3.2-3B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:02:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=639666) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=639666) WARNING 01-26 15:03:02 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=639666) WARNING 01-26 15:03:12 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 232.66 requests/s, 238472.37 total tokens/s, 232.66 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 15:02:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:02:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 15:02:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 15:02:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:02:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:02:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:02:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:02:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:02:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 15:02:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:02:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:02:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:02:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:02:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:02:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:02:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 15:02:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 15:02:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:02:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:02:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:02:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:02:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 15:02:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 15:02:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:02:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:02:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:02:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:02:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=639666) [2026-01-26 15:02:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=639666) [2026-01-26 15:02:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=639666) [2026-01-26 15:02:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=639666) [2026-01-26 15:02:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=639666) [2026-01-26 15:02:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=639666) [2026-01-26 15:02:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=639666) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=639666) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.01s/it]
(EngineCore_DP0 pid=639666) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.01s/it]
(EngineCore_DP0 pid=639666) 
(EngineCore_DP0 pid=639666) [2026-01-26 15:02:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=639666) [2026-01-26 15:02:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15810560 bytes
(EngineCore_DP0 pid=639666) [2026-01-26 15:02:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=639666) [2026-01-26 15:02:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9486336 bytes
(EngineCore_DP0 pid=639666) [2026-01-26 15:02:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=639666) [2026-01-26 15:02:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50593792 bytes
(EngineCore_DP0 pid=639666) [2026-01-26 15:02:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=639666) [2026-01-26 15:02:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25214976 bytes
(EngineCore_DP0 pid=639666) [rank0]:W0126 15:03:07.211000 639666 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=639666) [rank0]:W0126 15:03:07.296000 639666 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=639666) [rank0]:W0126 15:03:08.245000 639666 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=639666) [rank0]:W0126 15:03:08.368000 639666 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=639666) 2026-01-26 15:03:11,821 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=639666) 2026-01-26 15:03:11,847 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=639666) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:06,  2.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:06,  2.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:02,  5.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:01,  7.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:00,  9.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00, 11.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00, 12.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00, 14.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:01<00:00, 15.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00,  9.93it/s]
(EngineCore_DP0 pid=639666) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 17.66it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 18.13it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 18.20it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 18.37it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 16.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 17.00it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 40/8192 [00:00<00:20, 395.87it/s]
Adding requests:   1%|          | 85/8192 [00:00<00:19, 424.28it/s]
Adding requests:   2%|▏         | 129/8192 [00:00<00:18, 429.69it/s]
Adding requests:   2%|▏         | 174/8192 [00:00<00:18, 433.95it/s]
Adding requests:   3%|▎         | 220/8192 [00:00<00:18, 440.84it/s]
Adding requests:   3%|▎         | 268/8192 [00:00<00:17, 452.48it/s]
Adding requests:   4%|▍         | 314/8192 [00:00<00:17, 448.55it/s]
Adding requests:   4%|▍         | 360/8192 [00:00<00:17, 451.36it/s]
Adding requests:   5%|▍         | 407/8192 [00:00<00:17, 455.61it/s]
Adding requests:   6%|▌         | 454/8192 [00:01<00:16, 458.49it/s]
Adding requests:   6%|▌         | 500/8192 [00:01<00:16, 453.43it/s]
Adding requests:   7%|▋         | 546/8192 [00:01<00:16, 450.19it/s]
Adding requests:   7%|▋         | 595/8192 [00:01<00:16, 460.17it/s]
Adding requests:   8%|▊         | 643/8192 [00:01<00:16, 463.35it/s]
Adding requests:   8%|▊         | 690/8192 [00:01<00:16, 460.41it/s]
Adding requests:   9%|▉         | 737/8192 [00:01<00:16, 459.24it/s]
Adding requests:  10%|▉         | 783/8192 [00:01<00:16, 449.89it/s]
Adding requests:  10%|█         | 829/8192 [00:01<00:16, 439.55it/s]
Adding requests:  11%|█         | 874/8192 [00:01<00:16, 440.36it/s]
Adding requests:  11%|█         | 921/8192 [00:02<00:16, 446.53it/s]
Adding requests:  12%|█▏        | 969/8192 [00:02<00:15, 453.97it/s]
Adding requests:  12%|█▏        | 1018/8192 [00:02<00:15, 462.58it/s]
Adding requests:  13%|█▎        | 1066/8192 [00:02<00:15, 466.15it/s]
Adding requests:  14%|█▎        | 1113/8192 [00:02<00:15, 464.75it/s]
Adding requests:  14%|█▍        | 1161/8192 [00:02<00:15, 467.89it/s]
Adding requests:  15%|█▍        | 1212/8192 [00:02<00:14, 477.34it/s]
Adding requests:  15%|█▌        | 1260/8192 [00:02<00:14, 472.18it/s]
Adding requests:  16%|█▌        | 1308/8192 [00:02<00:14, 464.73it/s]
Adding requests:  17%|█▋        | 1355/8192 [00:02<00:14, 456.52it/s]
Adding requests:  17%|█▋        | 1403/8192 [00:03<00:14, 462.19it/s]
Adding requests:  18%|█▊        | 1451/8192 [00:03<00:14, 464.73it/s]
Adding requests:  18%|█▊        | 1500/8192 [00:03<00:14, 470.18it/s]
Adding requests:  19%|█▉        | 1548/8192 [00:03<00:14, 471.93it/s]
Adding requests:  19%|█▉        | 1596/8192 [00:03<00:14, 468.50it/s]
Adding requests:  20%|██        | 1645/8192 [00:03<00:13, 472.62it/s]
Adding requests:  21%|██        | 1693/8192 [00:03<00:13, 469.62it/s]
Adding requests:  21%|██▏       | 1742/8192 [00:03<00:13, 474.15it/s]
Adding requests:  22%|██▏       | 1790/8192 [00:03<00:13, 470.66it/s]
Adding requests:  22%|██▏       | 1838/8192 [00:04<00:13, 473.04it/s]
Adding requests:  23%|██▎       | 1886/8192 [00:04<00:13, 473.40it/s]
Adding requests:  24%|██▎       | 1934/8192 [00:04<00:13, 473.58it/s]
Adding requests:  24%|██▍       | 1982/8192 [00:04<00:13, 473.85it/s]
Adding requests:  25%|██▍       | 2031/8192 [00:04<00:12, 477.72it/s]
Adding requests:  25%|██▌       | 2080/8192 [00:04<00:12, 479.20it/s]
Adding requests:  26%|██▌       | 2128/8192 [00:04<00:12, 474.33it/s]
Adding requests:  27%|██▋       | 2176/8192 [00:04<00:12, 469.61it/s]
Adding requests:  27%|██▋       | 2224/8192 [00:04<00:12, 470.88it/s]
Adding requests:  28%|██▊       | 2272/8192 [00:04<00:12, 470.36it/s]
Adding requests:  28%|██▊       | 2321/8192 [00:05<00:12, 474.54it/s]
Adding requests:  29%|██▉       | 2369/8192 [00:05<00:12, 472.80it/s]
Adding requests:  30%|██▉       | 2417/8192 [00:05<00:12, 472.91it/s]
Adding requests:  30%|███       | 2465/8192 [00:05<00:12, 456.73it/s]
Adding requests:  31%|███       | 2512/8192 [00:05<00:12, 459.96it/s]
Adding requests:  31%|███▏      | 2561/8192 [00:05<00:12, 467.20it/s]
Adding requests:  32%|███▏      | 2608/8192 [00:05<00:12, 462.74it/s]
Adding requests:  32%|███▏      | 2658/8192 [00:05<00:11, 470.93it/s]
Adding requests:  33%|███▎      | 2706/8192 [00:05<00:11, 465.16it/s]
Adding requests:  34%|███▎      | 2754/8192 [00:05<00:11, 466.94it/s]
Adding requests:  34%|███▍      | 2801/8192 [00:06<00:11, 451.42it/s]
Adding requests:  35%|███▍      | 2849/8192 [00:06<00:11, 457.51it/s]
Adding requests:  35%|███▌      | 2897/8192 [00:06<00:11, 462.26it/s]
Adding requests:  36%|███▌      | 2944/8192 [00:06<00:11, 459.33it/s]
Adding requests:  37%|███▋      | 2993/8192 [00:06<00:11, 466.21it/s]
Adding requests:  37%|███▋      | 3040/8192 [00:06<00:11, 464.10it/s]
Adding requests:  38%|███▊      | 3087/8192 [00:06<00:10, 465.15it/s]
Adding requests:  38%|███▊      | 3134/8192 [00:06<00:10, 466.13it/s]
Adding requests:  39%|███▉      | 3182/8192 [00:06<00:10, 467.55it/s]
Adding requests:  39%|███▉      | 3229/8192 [00:06<00:10, 465.06it/s]
Adding requests:  40%|████      | 3277/8192 [00:07<00:10, 468.30it/s]
Adding requests:  41%|████      | 3324/8192 [00:07<00:10, 467.07it/s]
Adding requests:  41%|████      | 3374/8192 [00:07<00:10, 474.09it/s]
Adding requests:  42%|████▏     | 3423/8192 [00:07<00:09, 477.19it/s]
Adding requests:  42%|████▏     | 3471/8192 [00:07<00:10, 464.36it/s]
Adding requests:  43%|████▎     | 3519/8192 [00:07<00:09, 468.49it/s]
Adding requests:  44%|████▎     | 3566/8192 [00:07<00:09, 466.02it/s]
Adding requests:  44%|████▍     | 3614/8192 [00:07<00:09, 467.97it/s]
Adding requests:  45%|████▍     | 3661/8192 [00:07<00:09, 464.38it/s]
Adding requests:  45%|████▌     | 3710/8192 [00:08<00:09, 470.95it/s]
Adding requests:  46%|████▌     | 3758/8192 [00:08<00:09, 468.87it/s]
Adding requests:  46%|████▋     | 3807/8192 [00:08<00:09, 474.46it/s]
Adding requests:  47%|████▋     | 3855/8192 [00:08<00:09, 475.27it/s]
Adding requests:  48%|████▊     | 3903/8192 [00:08<00:09, 472.28it/s]
Adding requests:  48%|████▊     | 3951/8192 [00:08<00:08, 473.44it/s]
Adding requests:  49%|████▉     | 3999/8192 [00:08<00:09, 457.64it/s]
Adding requests:  49%|████▉     | 4046/8192 [00:08<00:09, 459.42it/s]
Adding requests:  50%|████▉     | 4093/8192 [00:08<00:09, 446.00it/s]
Adding requests:  51%|█████     | 4141/8192 [00:08<00:08, 452.81it/s]
Adding requests:  51%|█████     | 4190/8192 [00:09<00:08, 461.37it/s]
Adding requests:  52%|█████▏    | 4238/8192 [00:09<00:08, 464.81it/s]
Adding requests:  52%|█████▏    | 4285/8192 [00:09<00:08, 465.36it/s]
Adding requests:  53%|█████▎    | 4335/8192 [00:09<00:08, 473.22it/s]
Adding requests:  54%|█████▎    | 4385/8192 [00:09<00:07, 478.14it/s]
Adding requests:  54%|█████▍    | 4433/8192 [00:09<00:07, 477.17it/s]
Adding requests:  55%|█████▍    | 4481/8192 [00:09<00:07, 476.29it/s]
Adding requests:  55%|█████▌    | 4529/8192 [00:09<00:07, 467.42it/s]
Adding requests:  56%|█████▌    | 4578/8192 [00:09<00:07, 471.06it/s]
Adding requests:  56%|█████▋    | 4627/8192 [00:09<00:07, 474.14it/s]
Adding requests:  57%|█████▋    | 4675/8192 [00:10<00:07, 475.18it/s]
Adding requests:  58%|█████▊    | 4723/8192 [00:10<00:07, 476.24it/s]
Adding requests:  58%|█████▊    | 4772/8192 [00:10<00:07, 477.54it/s]
Adding requests:  59%|█████▉    | 4820/8192 [00:10<00:07, 476.53it/s]
Adding requests:  59%|█████▉    | 4868/8192 [00:10<00:07, 474.34it/s]
Adding requests:  60%|██████    | 4916/8192 [00:10<00:06, 472.75it/s]
Adding requests:  61%|██████    | 4965/8192 [00:10<00:06, 475.78it/s]
Adding requests:  61%|██████    | 5014/8192 [00:10<00:06, 479.94it/s]
Adding requests:  62%|██████▏   | 5063/8192 [00:10<00:06, 481.27it/s]
Adding requests:  62%|██████▏   | 5114/8192 [00:10<00:06, 487.48it/s]
Adding requests:  63%|██████▎   | 5163/8192 [00:11<00:06, 487.07it/s]
Adding requests:  64%|██████▎   | 5212/8192 [00:11<00:06, 486.43it/s]
Adding requests:  64%|██████▍   | 5261/8192 [00:11<00:06, 481.33it/s]
Adding requests:  65%|██████▍   | 5311/8192 [00:11<00:05, 486.69it/s]
Adding requests:  65%|██████▌   | 5360/8192 [00:11<00:05, 473.95it/s]
Adding requests:  66%|██████▌   | 5409/8192 [00:11<00:05, 476.78it/s]
Adding requests:  67%|██████▋   | 5457/8192 [00:11<00:05, 475.69it/s]
Adding requests:  67%|██████▋   | 5505/8192 [00:11<00:05, 472.23it/s]
Adding requests:  68%|██████▊   | 5554/8192 [00:11<00:05, 473.84it/s]
Adding requests:  68%|██████▊   | 5602/8192 [00:12<00:05, 474.60it/s]
Adding requests:  69%|██████▉   | 5650/8192 [00:12<00:05, 458.20it/s]
Adding requests:  70%|██████▉   | 5697/8192 [00:12<00:05, 461.44it/s]
Adding requests:  70%|███████   | 5745/8192 [00:12<00:05, 466.80it/s]
Adding requests:  71%|███████   | 5794/8192 [00:12<00:05, 470.57it/s]
Adding requests:  71%|███████▏  | 5842/8192 [00:12<00:05, 466.85it/s]
Adding requests:  72%|███████▏  | 5891/8192 [00:12<00:04, 471.64it/s]
Adding requests:  73%|███████▎  | 5940/8192 [00:12<00:04, 474.25it/s]
Adding requests:  73%|███████▎  | 5989/8192 [00:12<00:04, 478.39it/s]
Adding requests:  74%|███████▎  | 6037/8192 [00:12<00:04, 466.08it/s]
Adding requests:  74%|███████▍  | 6086/8192 [00:13<00:04, 469.64it/s]
Adding requests:  75%|███████▍  | 6135/8192 [00:13<00:04, 473.88it/s]
Adding requests:  75%|███████▌  | 6184/8192 [00:13<00:04, 475.76it/s]
Adding requests:  76%|███████▌  | 6235/8192 [00:13<00:04, 482.49it/s]
Adding requests:  77%|███████▋  | 6285/8192 [00:13<00:03, 487.06it/s]
Adding requests:  77%|███████▋  | 6334/8192 [00:13<00:03, 484.00it/s]
Adding requests:  78%|███████▊  | 6383/8192 [00:13<00:03, 485.03it/s]
Adding requests:  79%|███████▊  | 6434/8192 [00:13<00:03, 491.01it/s]
Adding requests:  79%|███████▉  | 6484/8192 [00:13<00:03, 479.07it/s]
Adding requests:  80%|███████▉  | 6534/8192 [00:13<00:03, 483.01it/s]
Adding requests:  80%|████████  | 6583/8192 [00:14<00:03, 483.04it/s]
Adding requests:  81%|████████  | 6632/8192 [00:14<00:03, 466.74it/s]
Adding requests:  82%|████████▏ | 6681/8192 [00:14<00:03, 471.97it/s]
Adding requests:  82%|████████▏ | 6731/8192 [00:14<00:03, 478.31it/s]
Adding requests:  83%|████████▎ | 6780/8192 [00:14<00:02, 479.76it/s]
Adding requests:  83%|████████▎ | 6830/8192 [00:14<00:02, 484.84it/s]
Adding requests:  84%|████████▍ | 6880/8192 [00:14<00:02, 487.43it/s]
Adding requests:  85%|████████▍ | 6929/8192 [00:14<00:02, 487.44it/s]
Adding requests:  85%|████████▌ | 6978/8192 [00:14<00:02, 486.11it/s]
Adding requests:  86%|████████▌ | 7027/8192 [00:14<00:02, 480.98it/s]
Adding requests:  86%|████████▋ | 7076/8192 [00:15<00:02, 467.23it/s]
Adding requests:  87%|████████▋ | 7126/8192 [00:15<00:02, 473.94it/s]
Adding requests:  88%|████████▊ | 7174/8192 [00:15<00:02, 471.13it/s]
Adding requests:  88%|████████▊ | 7224/8192 [00:15<00:02, 477.48it/s]
Adding requests:  89%|████████▉ | 7273/8192 [00:15<00:01, 479.69it/s]
Adding requests:  89%|████████▉ | 7323/8192 [00:15<00:01, 484.62it/s]
Adding requests:  90%|████████▉ | 7372/8192 [00:15<00:01, 481.93it/s]
Adding requests:  91%|█████████ | 7423/8192 [00:15<00:01, 489.55it/s]
Adding requests:  91%|█████████ | 7474/8192 [00:15<00:01, 493.52it/s]
Adding requests:  92%|█████████▏| 7524/8192 [00:16<00:01, 489.79it/s]
Adding requests:  92%|█████████▏| 7573/8192 [00:16<00:01, 489.59it/s]
Adding requests:  93%|█████████▎| 7622/8192 [00:16<00:01, 483.33it/s]
Adding requests:  94%|█████████▎| 7672/8192 [00:16<00:01, 487.11it/s]
Adding requests:  94%|█████████▍| 7721/8192 [00:16<00:00, 487.37it/s]
Adding requests:  95%|█████████▍| 7770/8192 [00:16<00:00, 484.22it/s]
Adding requests:  95%|█████████▌| 7819/8192 [00:16<00:00, 482.94it/s]
Adding requests:  96%|█████████▌| 7868/8192 [00:16<00:00, 471.41it/s]
Adding requests:  97%|█████████▋| 7916/8192 [00:16<00:00, 454.04it/s]
Adding requests:  97%|█████████▋| 7964/8192 [00:16<00:00, 460.52it/s]
Adding requests:  98%|█████████▊| 8012/8192 [00:17<00:00, 464.31it/s]
Adding requests:  98%|█████████▊| 8060/8192 [00:17<00:00, 467.02it/s]
Adding requests:  99%|█████████▉| 8110/8192 [00:17<00:00, 474.95it/s]
Adding requests: 100%|█████████▉| 8158/8192 [00:17<00:00, 476.15it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 470.15it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  49%|████▉     | 4023/8192 [00:00<00:00, 22121.97it/s, est. speed input: 22654349.28 toks/s, output: 22122.31 toks/s]
Processed prompts:  76%|███████▌  | 6236/8192 [00:09<00:03, 534.12it/s, est. speed input: 674287.49 toks/s, output: 658.48 toks/s]      
Processed prompts:  87%|████████▋ | 7161/8192 [00:13<00:02, 415.26it/s, est. speed input: 539807.78 toks/s, output: 527.16 toks/s]
Processed prompts:  94%|█████████▎| 7674/8192 [00:15<00:01, 374.36it/s, est. speed input: 498194.02 toks/s, output: 486.52 toks/s]
Processed prompts:  98%|█████████▊| 7994/8192 [00:17<00:00, 350.64it/s, est. speed input: 477461.96 toks/s, output: 466.27 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:17<00:00, 350.64it/s, est. speed input: 471717.03 toks/s, output: 460.66 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:17<00:00, 460.66it/s, est. speed input: 471717.03 toks/s, output: 460.66 toks/s]
[rank0]:[W126 15:03:51.002660191 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 16:12:44
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:12:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=739510) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=739510) WARNING 01-26 16:13:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=739510) WARNING 01-26 16:13:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 38.30 requests/s, 19646.34 total tokens/s, 38.30 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 16:12:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:12:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:12:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:12:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:12:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:12:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:12:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:12:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:12:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:12:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:13:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:13:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:13:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:13:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:13:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:13:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:13:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=739510) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=739510) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.01it/s]
(EngineCore_DP0 pid=739510) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.27s/it]
(EngineCore_DP0 pid=739510) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.23s/it]
(EngineCore_DP0 pid=739510) 
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=739510) [2026-01-26 16:13:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=739510) 2026-01-26 16:13:21,052 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=739510) 2026-01-26 16:13:21,075 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=739510) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.54it/s]
(EngineCore_DP0 pid=739510) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.19it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 517.40it/s]
Adding requests:  88%|████████▊ | 112/128 [00:00<00:00, 561.55it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 559.69it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 52.95it/s, est. speed input: 27111.67 toks/s, output: 52.95 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:02, 44.98it/s, est. speed input: 23562.95 toks/s, output: 46.02 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 43.21it/s, est. speed input: 22726.36 toks/s, output: 44.39 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 42.37it/s, est. speed input: 22309.79 toks/s, output: 43.57 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 41.78it/s, est. speed input: 22022.42 toks/s, output: 43.01 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 41.42it/s, est. speed input: 21829.13 toks/s, output: 42.63 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 41.18it/s, est. speed input: 21688.85 toks/s, output: 42.36 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:02, 40.98it/s, est. speed input: 21573.38 toks/s, output: 42.14 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:01, 40.81it/s, est. speed input: 21478.72 toks/s, output: 41.95 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 40.84it/s, est. speed input: 21426.64 toks/s, output: 41.85 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 40.86it/s, est. speed input: 21382.27 toks/s, output: 41.76 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 40.87it/s, est. speed input: 21345.43 toks/s, output: 41.69 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 40.86it/s, est. speed input: 21312.01 toks/s, output: 41.62 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 40.77it/s, est. speed input: 21274.17 toks/s, output: 41.55 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 40.77it/s, est. speed input: 21246.95 toks/s, output: 41.50 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 40.75it/s, est. speed input: 21221.83 toks/s, output: 41.45 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 40.71it/s, est. speed input: 21197.37 toks/s, output: 41.40 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 40.73it/s, est. speed input: 21179.45 toks/s, output: 41.37 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 40.68it/s, est. speed input: 21158.24 toks/s, output: 41.32 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 40.62it/s, est. speed input: 21136.20 toks/s, output: 41.28 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 40.65it/s, est. speed input: 21122.77 toks/s, output: 41.26 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 40.54it/s, est. speed input: 21100.02 toks/s, output: 41.21 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 40.59it/s, est. speed input: 21089.06 toks/s, output: 41.19 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 40.56it/s, est. speed input: 21073.88 toks/s, output: 41.16 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 40.51it/s, est. speed input: 21058.08 toks/s, output: 41.13 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.51it/s, est. speed input: 21056.66 toks/s, output: 41.13 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.12it/s, est. speed input: 21056.66 toks/s, output: 41.13 toks/s]
[rank0]:[W126 16:13:27.415505137 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 16:13:29
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:13:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=740752) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=740752) WARNING 01-26 16:13:55 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=740752) WARNING 01-26 16:14:05 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 39.46 requests/s, 40448.83 total tokens/s, 39.46 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 16:13:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:13:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:13:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:13:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:13:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:13:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:13:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:13:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:13:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:13:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:13:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:13:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:13:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:13:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:13:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:13:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:13:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=740752) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=740752) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.02it/s]
(EngineCore_DP0 pid=740752) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]
(EngineCore_DP0 pid=740752) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.21s/it]
(EngineCore_DP0 pid=740752) 
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=740752) [2026-01-26 16:13:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=740752) 2026-01-26 16:14:05,492 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=740752) 2026-01-26 16:14:05,515 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=740752) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.88it/s]
(EngineCore_DP0 pid=740752) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.14it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  20%|██        | 26/128 [00:00<00:00, 254.56it/s]
Adding requests:  45%|████▌     | 58/128 [00:00<00:00, 289.48it/s]
Adding requests:  68%|██████▊   | 87/128 [00:00<00:00, 256.58it/s]
Adding requests:  91%|█████████▏| 117/128 [00:00<00:00, 272.10it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 273.02it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:00, 183.76it/s, est. speed input: 188180.43 toks/s, output: 183.76 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 60.93it/s, est. speed input: 69731.39 toks/s, output: 68.10 toks/s]   
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 52.91it/s, est. speed input: 61314.82 toks/s, output: 59.88 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 49.02it/s, est. speed input: 57468.45 toks/s, output: 56.12 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 46.81it/s, est. speed input: 55402.57 toks/s, output: 54.10 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 45.10it/s, est. speed input: 53820.50 toks/s, output: 52.56 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 43.87it/s, est. speed input: 52709.58 toks/s, output: 51.47 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 42.96it/s, est. speed input: 51809.40 toks/s, output: 50.59 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:01, 42.24it/s, est. speed input: 51038.81 toks/s, output: 49.84 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 41.65it/s, est. speed input: 50360.01 toks/s, output: 49.18 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 41.32it/s, est. speed input: 49795.38 toks/s, output: 48.63 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 41.02it/s, est. speed input: 49284.09 toks/s, output: 48.13 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 40.89it/s, est. speed input: 48851.35 toks/s, output: 47.71 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 40.58it/s, est. speed input: 48417.89 toks/s, output: 47.28 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 40.46it/s, est. speed input: 48046.50 toks/s, output: 46.92 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 40.40it/s, est. speed input: 47718.65 toks/s, output: 46.60 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 40.34it/s, est. speed input: 47416.00 toks/s, output: 46.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 40.34it/s, est. speed input: 47263.83 toks/s, output: 46.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 46.15it/s, est. speed input: 47263.83 toks/s, output: 46.16 toks/s]
[rank0]:[W126 16:14:10.006281229 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 16:14:13
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:14:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=741907) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=741907) WARNING 01-26 16:14:39 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=741907) WARNING 01-26 16:14:49 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 76.88 requests/s, 78802.15 total tokens/s, 76.88 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 16:14:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:14:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:14:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:14:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:14:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:14:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:14:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:14:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:14:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:14:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:14:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:14:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:14:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:14:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:14:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:14:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:14:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:29] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:29] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:29] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:29] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:29] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=741907) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=741907) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.02s/it]
(EngineCore_DP0 pid=741907) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.29s/it]
(EngineCore_DP0 pid=741907) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]
(EngineCore_DP0 pid=741907) 
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=741907) [2026-01-26 16:14:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=741907) 2026-01-26 16:14:49,218 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=741907) 2026-01-26 16:14:49,241 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=741907) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 13.26it/s]
(EngineCore_DP0 pid=741907) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.51it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.49it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 20/256 [00:00<00:01, 196.92it/s]
Adding requests:  20%|█▉        | 51/256 [00:00<00:00, 262.27it/s]
Adding requests:  33%|███▎      | 84/256 [00:00<00:00, 288.74it/s]
Adding requests:  44%|████▍     | 113/256 [00:00<00:01, 124.91it/s]
Adding requests:  56%|█████▋    | 144/256 [00:00<00:00, 160.58it/s]
Adding requests:  69%|██████▉   | 177/256 [00:00<00:00, 196.90it/s]
Adding requests:  83%|████████▎ | 212/256 [00:01<00:00, 232.74it/s]
Adding requests:  96%|█████████▌| 246/256 [00:01<00:00, 259.00it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 217.20it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:00<00:00, 918.72it/s, est. speed input: 940840.44 toks/s, output: 918.74 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:01<00:00, 128.92it/s, est. speed input: 151555.00 toks/s, output: 148.00 toks/s]
Processed prompts:  89%|████████▊ | 227/256 [00:01<00:00, 110.33it/s, est. speed input: 130882.79 toks/s, output: 127.81 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:02<00:00, 101.62it/s, est. speed input: 122416.27 toks/s, output: 119.55 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 101.62it/s, est. speed input: 121964.97 toks/s, output: 119.11 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 119.10it/s, est. speed input: 121964.97 toks/s, output: 119.11 toks/s]
[rank0]:[W126 16:14:54.881211953 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 16:14:56
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:15:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=743076) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=743076) WARNING 01-26 16:15:23 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=743076) WARNING 01-26 16:15:33 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 111.13 requests/s, 113905.16 total tokens/s, 111.13 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 16:15:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:15:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:15:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:15:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:15:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:15:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:15:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:15:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:15:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:15:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:15:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:15:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:15:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:15:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:15:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:15:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=743076) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=743076) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.10it/s]
(EngineCore_DP0 pid=743076) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.16s/it]
(EngineCore_DP0 pid=743076) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.12s/it]
(EngineCore_DP0 pid=743076) 
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=743076) [2026-01-26 16:15:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=743076) 2026-01-26 16:15:33,554 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=743076) 2026-01-26 16:15:33,577 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=743076) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.76it/s]
(EngineCore_DP0 pid=743076) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 17.30it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 17.49it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 23/512 [00:00<00:02, 226.72it/s]
Adding requests:  11%|█         | 54/512 [00:00<00:01, 274.73it/s]
Adding requests:  17%|█▋        | 87/512 [00:00<00:01, 298.60it/s]
Adding requests:  23%|██▎       | 120/512 [00:00<00:01, 308.87it/s]
Adding requests:  30%|██▉       | 153/512 [00:00<00:01, 314.44it/s]
Adding requests:  37%|███▋      | 188/512 [00:00<00:00, 326.08it/s]
Adding requests:  44%|████▎     | 223/512 [00:00<00:00, 333.81it/s]
Adding requests:  50%|█████     | 257/512 [00:00<00:00, 331.60it/s]
Adding requests:  57%|█████▋    | 292/512 [00:00<00:00, 335.84it/s]
Adding requests:  64%|██████▍   | 327/512 [00:01<00:00, 338.61it/s]
Adding requests:  71%|███████▏  | 365/512 [00:01<00:00, 347.56it/s]
Adding requests:  79%|███████▊  | 402/512 [00:01<00:00, 353.05it/s]
Adding requests:  86%|████████▌ | 438/512 [00:01<00:00, 354.24it/s]
Adding requests:  93%|█████████▎| 474/512 [00:01<00:00, 352.04it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 359.98it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 336.14it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:00<00:00, 1330.62it/s, est. speed input: 1362638.39 toks/s, output: 1330.64 toks/s]
Processed prompts:  61%|██████    | 312/512 [00:01<00:00, 204.54it/s, est. speed input: 244931.64 toks/s, output: 239.19 toks/s]   
Processed prompts:  73%|███████▎  | 374/512 [00:01<00:00, 167.35it/s, est. speed input: 204627.67 toks/s, output: 199.83 toks/s]
Processed prompts:  81%|████████  | 413/512 [00:02<00:00, 156.75it/s, est. speed input: 193185.54 toks/s, output: 188.66 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:02<00:00, 144.42it/s, est. speed input: 183268.80 toks/s, output: 178.97 toks/s]
Processed prompts:  91%|█████████ | 464/512 [00:02<00:00, 140.59it/s, est. speed input: 179342.66 toks/s, output: 175.14 toks/s]
Processed prompts:  94%|█████████▍| 483/512 [00:02<00:00, 133.66it/s, est. speed input: 174838.98 toks/s, output: 170.74 toks/s]
Processed prompts:  97%|█████████▋| 499/512 [00:02<00:00, 129.19it/s, est. speed input: 171785.60 toks/s, output: 167.76 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 129.19it/s, est. speed input: 170073.81 toks/s, output: 166.09 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:03<00:00, 166.08it/s, est. speed input: 170073.81 toks/s, output: 166.09 toks/s]
[rank0]:[W126 16:15:40.604741239 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 16:15:42
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:15:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=744321) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=744321) WARNING 01-26 16:16:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=744321) WARNING 01-26 16:16:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 118.74 requests/s, 121707.27 total tokens/s, 118.74 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 16:15:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:15:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:15:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:15:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:15:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:15:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:15:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:15:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:15:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:16:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:16:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:16:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:16:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:16:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:16:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:16:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:16:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:03] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=744321) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=744321) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.25it/s]
(EngineCore_DP0 pid=744321) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03s/it]
(EngineCore_DP0 pid=744321) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
(EngineCore_DP0 pid=744321) 
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=744321) [2026-01-26 16:16:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=744321) 2026-01-26 16:16:21,877 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=744321) 2026-01-26 16:16:21,902 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=744321) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 14.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 12.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 12.28it/s]
(EngineCore_DP0 pid=744321) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 17.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.11it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 276.92it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 317.11it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 314.16it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 318.71it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 328.69it/s]
Adding requests:  20%|█▉        | 200/1024 [00:00<00:02, 339.89it/s]
Adding requests:  23%|██▎       | 236/1024 [00:00<00:02, 344.48it/s]
Adding requests:  26%|██▋       | 271/1024 [00:00<00:02, 342.74it/s]
Adding requests:  30%|██▉       | 307/1024 [00:00<00:02, 346.89it/s]
Adding requests:  34%|███▎      | 344/1024 [00:01<00:01, 353.14it/s]
Adding requests:  37%|███▋      | 380/1024 [00:01<00:01, 355.14it/s]
Adding requests:  41%|████      | 419/1024 [00:01<00:01, 363.60it/s]
Adding requests:  45%|████▍     | 456/1024 [00:01<00:01, 360.79it/s]
Adding requests:  48%|████▊     | 496/1024 [00:01<00:01, 370.10it/s]
Adding requests:  52%|█████▏    | 535/1024 [00:01<00:01, 374.04it/s]
Adding requests:  56%|█████▌    | 573/1024 [00:01<00:01, 371.35it/s]
Adding requests:  60%|█████▉    | 611/1024 [00:01<00:01, 357.35it/s]
Adding requests:  63%|██████▎   | 647/1024 [00:01<00:01, 354.26it/s]
Adding requests:  67%|██████▋   | 684/1024 [00:01<00:00, 356.71it/s]
Adding requests:  70%|███████   | 720/1024 [00:02<00:00, 354.90it/s]
Adding requests:  74%|███████▍  | 756/1024 [00:02<00:00, 351.58it/s]
Adding requests:  77%|███████▋  | 792/1024 [00:02<00:00, 351.39it/s]
Adding requests:  81%|████████  | 830/1024 [00:02<00:00, 358.84it/s]
Adding requests:  85%|████████▍ | 866/1024 [00:02<00:00, 358.31it/s]
Adding requests:  88%|████████▊ | 904/1024 [00:02<00:00, 363.06it/s]
Adding requests:  92%|█████████▏| 941/1024 [00:02<00:00, 355.91it/s]
Adding requests:  95%|█████████▌| 977/1024 [00:02<00:00, 354.36it/s]
Adding requests:  99%|█████████▉| 1013/1024 [00:02<00:00, 350.11it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 351.47it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:00<00:00, 2296.95it/s, est. speed input: 2352194.32 toks/s, output: 2296.99 toks/s]
Processed prompts:  57%|█████▋    | 584/1024 [00:02<00:01, 241.20it/s, est. speed input: 295007.40 toks/s, output: 288.09 toks/s]   
Processed prompts:  67%|██████▋   | 686/1024 [00:02<00:01, 193.10it/s, est. speed input: 242164.57 toks/s, output: 236.49 toks/s]
Processed prompts:  73%|███████▎  | 747/1024 [00:03<00:01, 173.04it/s, est. speed input: 222578.53 toks/s, output: 217.36 toks/s]
Processed prompts:  77%|███████▋  | 789/1024 [00:03<00:01, 164.61it/s, est. speed input: 214430.70 toks/s, output: 209.40 toks/s]
Processed prompts:  80%|████████  | 820/1024 [00:04<00:01, 156.28it/s, est. speed input: 208159.36 toks/s, output: 203.28 toks/s]
Processed prompts:  83%|████████▎ | 845/1024 [00:04<00:01, 151.33it/s, est. speed input: 204355.01 toks/s, output: 199.57 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:04<00:01, 142.89it/s, est. speed input: 200001.62 toks/s, output: 195.31 toks/s]
Processed prompts:  86%|████████▋ | 884/1024 [00:04<00:00, 141.46it/s, est. speed input: 198128.02 toks/s, output: 193.48 toks/s]
Processed prompts:  88%|████████▊ | 901/1024 [00:04<00:00, 138.79it/s, est. speed input: 196158.73 toks/s, output: 191.56 toks/s]
Processed prompts:  90%|████████▉ | 917/1024 [00:04<00:00, 134.94it/s, est. speed input: 194118.76 toks/s, output: 189.57 toks/s]
Processed prompts:  91%|█████████ | 932/1024 [00:04<00:00, 129.84it/s, est. speed input: 191978.20 toks/s, output: 187.48 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:05<00:00, 123.93it/s, est. speed input: 189782.51 toks/s, output: 185.33 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:05<00:00, 122.89it/s, est. speed input: 188074.37 toks/s, output: 183.67 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:05<00:00, 121.64it/s, est. speed input: 186387.78 toks/s, output: 182.02 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:05<00:00, 121.10it/s, est. speed input: 184837.85 toks/s, output: 180.51 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:05<00:00, 120.71it/s, est. speed input: 183362.74 toks/s, output: 179.06 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 120.71it/s, est. speed input: 183660.64 toks/s, output: 179.36 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:05<00:00, 179.35it/s, est. speed input: 183660.64 toks/s, output: 179.36 toks/s]
[rank0]:[W126 16:16:33.251868985 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 16:16:34
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:16:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=745670) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=745670) WARNING 01-26 16:17:10 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=745670) WARNING 01-26 16:17:20 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 126.50 requests/s, 129661.82 total tokens/s, 126.50 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 16:16:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:16:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:16:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:16:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:16:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:16:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:16:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:16:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:16:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:17:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:17:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:17:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:17:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:17:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:17:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:17:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:17:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:17:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:17:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:17:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:17:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:17:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:17:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=745670) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=745670) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=745670) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03s/it]
(EngineCore_DP0 pid=745670) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.00it/s]
(EngineCore_DP0 pid=745670) 
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=745670) [2026-01-26 16:17:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=745670) [rank0]:W0126 16:17:15.361000 745670 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=745670) [rank0]:W0126 16:17:15.443000 745670 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=745670) [rank0]:W0126 16:17:16.416000 745670 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=745670) [rank0]:W0126 16:17:16.546000 745670 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=745670) 2026-01-26 16:17:19,951 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=745670) 2026-01-26 16:17:19,977 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=745670) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 16.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 15.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.56it/s]
(EngineCore_DP0 pid=745670) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 16.13it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 17.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 17.19it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 278.53it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 318.87it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:06, 316.45it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:05, 323.39it/s]
Adding requests:   8%|▊         | 164/2048 [00:00<00:05, 332.81it/s]
Adding requests:  10%|▉         | 200/2048 [00:00<00:05, 341.35it/s]
Adding requests:  11%|█▏        | 235/2048 [00:00<00:05, 339.58it/s]
Adding requests:  13%|█▎        | 269/2048 [00:00<00:05, 333.66it/s]
Adding requests:  15%|█▍        | 304/2048 [00:00<00:05, 335.06it/s]
Adding requests:  17%|█▋        | 340/2048 [00:01<00:05, 340.10it/s]
Adding requests:  18%|█▊        | 375/2048 [00:01<00:04, 340.47it/s]
Adding requests:  20%|██        | 411/2048 [00:01<00:04, 344.68it/s]
Adding requests:  22%|██▏       | 446/2048 [00:01<00:04, 341.64it/s]
Adding requests:  24%|██▎       | 483/2048 [00:01<00:04, 347.54it/s]
Adding requests:  25%|██▌       | 522/2048 [00:01<00:04, 359.01it/s]
Adding requests:  27%|██▋       | 558/2048 [00:01<00:04, 354.31it/s]
Adding requests:  29%|██▉       | 594/2048 [00:01<00:04, 343.32it/s]
Adding requests:  31%|███       | 629/2048 [00:01<00:04, 341.71it/s]
Adding requests:  32%|███▏      | 664/2048 [00:01<00:04, 339.92it/s]
Adding requests:  34%|███▍      | 701/2048 [00:02<00:03, 346.82it/s]
Adding requests:  36%|███▌      | 736/2048 [00:02<00:03, 342.36it/s]
Adding requests:  38%|███▊      | 772/2048 [00:02<00:03, 346.44it/s]
Adding requests:  39%|███▉      | 808/2048 [00:02<00:03, 348.30it/s]
Adding requests:  41%|████      | 844/2048 [00:02<00:03, 349.39it/s]
Adding requests:  43%|████▎     | 881/2048 [00:02<00:03, 354.42it/s]
Adding requests:  45%|████▍     | 917/2048 [00:02<00:03, 351.55it/s]
Adding requests:  47%|████▋     | 953/2048 [00:02<00:03, 352.49it/s]
Adding requests:  48%|████▊     | 989/2048 [00:02<00:02, 353.19it/s]
Adding requests:  50%|█████     | 1025/2048 [00:02<00:03, 337.05it/s]
Adding requests:  52%|█████▏    | 1060/2048 [00:03<00:02, 339.21it/s]
Adding requests:  53%|█████▎    | 1095/2048 [00:03<00:02, 339.12it/s]
Adding requests:  55%|█████▌    | 1133/2048 [00:03<00:02, 350.10it/s]
Adding requests:  57%|█████▋    | 1169/2048 [00:03<00:02, 347.05it/s]
Adding requests:  59%|█████▉    | 1206/2048 [00:03<00:02, 351.90it/s]
Adding requests:  61%|██████    | 1243/2048 [00:03<00:02, 355.08it/s]
Adding requests:  62%|██████▏   | 1279/2048 [00:03<00:02, 350.03it/s]
Adding requests:  64%|██████▍   | 1315/2048 [00:03<00:02, 350.19it/s]
Adding requests:  66%|██████▌   | 1352/2048 [00:03<00:01, 355.74it/s]
Adding requests:  68%|██████▊   | 1388/2048 [00:04<00:01, 354.43it/s]
Adding requests:  70%|██████▉   | 1424/2048 [00:04<00:01, 351.55it/s]
Adding requests:  71%|███████▏  | 1460/2048 [00:04<00:01, 353.50it/s]
Adding requests:  73%|███████▎  | 1498/2048 [00:04<00:01, 358.46it/s]
Adding requests:  75%|███████▍  | 1534/2048 [00:04<00:01, 356.65it/s]
Adding requests:  77%|███████▋  | 1570/2048 [00:04<00:01, 350.18it/s]
Adding requests:  78%|███████▊  | 1606/2048 [00:04<00:01, 349.07it/s]
Adding requests:  80%|████████  | 1641/2048 [00:04<00:01, 342.72it/s]
Adding requests:  82%|████████▏ | 1676/2048 [00:04<00:01, 338.39it/s]
Adding requests:  84%|████████▎ | 1713/2048 [00:04<00:00, 346.50it/s]
Adding requests:  85%|████████▌ | 1748/2048 [00:05<00:00, 340.80it/s]
Adding requests:  87%|████████▋ | 1784/2048 [00:05<00:00, 344.88it/s]
Adding requests:  89%|████████▉ | 1819/2048 [00:05<00:00, 343.05it/s]
Adding requests:  91%|█████████ | 1857/2048 [00:05<00:00, 352.06it/s]
Adding requests:  92%|█████████▏| 1893/2048 [00:05<00:00, 351.73it/s]
Adding requests:  94%|█████████▍| 1931/2048 [00:05<00:00, 357.19it/s]
Adding requests:  96%|█████████▌| 1968/2048 [00:05<00:00, 357.93it/s]
Adding requests:  98%|█████████▊| 2004/2048 [00:05<00:00, 351.81it/s]
Adding requests: 100%|█████████▉| 2040/2048 [00:05<00:00, 348.36it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 346.11it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  37%|███▋      | 749/2048 [00:00<00:00, 4074.57it/s, est. speed input: 4172542.99 toks/s, output: 4074.62 toks/s]
Processed prompts:  56%|█████▋    | 1157/2048 [00:03<00:03, 284.39it/s, est. speed input: 355430.26 toks/s, output: 347.10 toks/s]  
Processed prompts:  65%|██████▌   | 1333/2048 [00:04<00:03, 225.74it/s, est. speed input: 289856.65 toks/s, output: 283.06 toks/s]
Processed prompts:  70%|███████   | 1435/2048 [00:05<00:02, 205.14it/s, est. speed input: 268910.93 toks/s, output: 262.61 toks/s]
Processed prompts:  73%|███████▎  | 1502/2048 [00:06<00:02, 184.62it/s, est. speed input: 252719.11 toks/s, output: 246.80 toks/s]
Processed prompts:  76%|███████▌  | 1549/2048 [00:06<00:02, 175.47it/s, est. speed input: 245480.90 toks/s, output: 239.73 toks/s]
Processed prompts:  77%|███████▋  | 1584/2048 [00:06<00:02, 171.02it/s, est. speed input: 241709.53 toks/s, output: 236.04 toks/s]
Processed prompts:  79%|███████▉  | 1613/2048 [00:06<00:02, 162.64it/s, est. speed input: 237331.75 toks/s, output: 231.77 toks/s]
Processed prompts:  80%|███████▉  | 1636/2048 [00:07<00:02, 164.47it/s, est. speed input: 236395.42 toks/s, output: 230.85 toks/s]
Processed prompts:  81%|████████  | 1658/2048 [00:07<00:02, 165.83it/s, est. speed input: 235407.53 toks/s, output: 229.89 toks/s]
Processed prompts:  82%|████████▏ | 1679/2048 [00:07<00:02, 143.87it/s, est. speed input: 230289.68 toks/s, output: 224.89 toks/s]
Processed prompts:  83%|████████▎ | 1696/2048 [00:07<00:02, 141.94it/s, est. speed input: 228673.79 toks/s, output: 223.31 toks/s]
Processed prompts:  84%|████████▎ | 1712/2048 [00:07<00:02, 139.05it/s, est. speed input: 227043.27 toks/s, output: 221.72 toks/s]
Processed prompts:  84%|████████▍ | 1727/2048 [00:07<00:02, 134.70it/s, est. speed input: 225329.44 toks/s, output: 220.05 toks/s]
Processed prompts:  85%|████████▌ | 1741/2048 [00:07<00:02, 130.31it/s, est. speed input: 223685.27 toks/s, output: 218.44 toks/s]
Processed prompts:  86%|████████▌ | 1757/2048 [00:08<00:02, 130.62it/s, est. speed input: 222348.81 toks/s, output: 217.14 toks/s]
Processed prompts:  87%|████████▋ | 1773/2048 [00:08<00:02, 129.76it/s, est. speed input: 220943.74 toks/s, output: 215.76 toks/s]
Processed prompts:  87%|████████▋ | 1789/2048 [00:08<00:01, 129.97it/s, est. speed input: 219660.68 toks/s, output: 214.51 toks/s]
Processed prompts:  88%|████████▊ | 1805/2048 [00:08<00:01, 129.29it/s, est. speed input: 218341.20 toks/s, output: 213.22 toks/s]
Processed prompts:  89%|████████▉ | 1821/2048 [00:08<00:01, 128.71it/s, est. speed input: 217053.29 toks/s, output: 211.97 toks/s]
Processed prompts:  90%|████████▉ | 1837/2048 [00:08<00:01, 128.32it/s, est. speed input: 215805.02 toks/s, output: 210.75 toks/s]
Processed prompts:  90%|█████████ | 1853/2048 [00:08<00:01, 127.06it/s, est. speed input: 214511.52 toks/s, output: 209.48 toks/s]
Processed prompts:  91%|█████████▏| 1869/2048 [00:08<00:01, 127.13it/s, est. speed input: 213332.54 toks/s, output: 208.33 toks/s]
Processed prompts:  92%|█████████▏| 1885/2048 [00:09<00:01, 128.46it/s, est. speed input: 212285.20 toks/s, output: 207.31 toks/s]
Processed prompts:  93%|█████████▎| 1901/2048 [00:09<00:01, 128.28it/s, est. speed input: 211180.08 toks/s, output: 206.23 toks/s]
Processed prompts:  94%|█████████▎| 1917/2048 [00:09<00:01, 126.82it/s, est. speed input: 210006.42 toks/s, output: 205.08 toks/s]
Processed prompts:  94%|█████████▍| 1933/2048 [00:09<00:00, 128.41it/s, est. speed input: 209054.57 toks/s, output: 204.15 toks/s]
Processed prompts:  95%|█████████▌| 1949/2048 [00:09<00:00, 127.99it/s, est. speed input: 208017.18 toks/s, output: 203.14 toks/s]
Processed prompts:  96%|█████████▌| 1965/2048 [00:09<00:00, 127.67it/s, est. speed input: 207005.17 toks/s, output: 202.15 toks/s]
Processed prompts:  97%|█████████▋| 1981/2048 [00:09<00:00, 128.74it/s, est. speed input: 206107.52 toks/s, output: 201.28 toks/s]
Processed prompts:  98%|█████████▊| 1997/2048 [00:09<00:00, 127.75it/s, est. speed input: 205114.71 toks/s, output: 200.31 toks/s]
Processed prompts:  98%|█████████▊| 2013/2048 [00:10<00:00, 127.41it/s, est. speed input: 204170.60 toks/s, output: 199.39 toks/s]
Processed prompts:  99%|█████████▉| 2029/2048 [00:10<00:00, 127.68it/s, est. speed input: 203282.51 toks/s, output: 198.52 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:10<00:00, 127.68it/s, est. speed input: 204182.27 toks/s, output: 199.40 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:10<00:00, 199.39it/s, est. speed input: 204182.27 toks/s, output: 199.40 toks/s]
[rank0]:[W126 16:17:38.930659967 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 16:17:40
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:18:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=747285) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=747285) WARNING 01-26 16:18:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=747285) WARNING 01-26 16:18:38 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 129.17 requests/s, 132396.13 total tokens/s, 129.17 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 16:18:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:18:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:18:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:18:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:18:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:18:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:18:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:18:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:18:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:18:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:18:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:18:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:18:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:18:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:18:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:18:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:18:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:19] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:19] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=747285) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=747285) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=747285) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03s/it]
(EngineCore_DP0 pid=747285) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
(EngineCore_DP0 pid=747285) 
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=747285) [2026-01-26 16:18:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=747285) [rank0]:W0126 16:18:33.101000 747285 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=747285) [rank0]:W0126 16:18:33.184000 747285 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=747285) [rank0]:W0126 16:18:34.153000 747285 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=747285) [rank0]:W0126 16:18:34.286000 747285 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=747285) 2026-01-26 16:18:37,951 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=747285) 2026-01-26 16:18:37,978 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=747285) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 13.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 15.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 15.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 16.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 17.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 15.69it/s]
(EngineCore_DP0 pid=747285) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.72it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.88it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 17.41it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 16.47it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 277.35it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 319.39it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 316.61it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:12, 322.02it/s]
Adding requests:   4%|▍         | 165/4096 [00:00<00:11, 332.08it/s]
Adding requests:   5%|▍         | 202/4096 [00:00<00:11, 342.38it/s]
Adding requests:   6%|▌         | 238/4096 [00:00<00:11, 347.07it/s]
Adding requests:   7%|▋         | 273/4096 [00:00<00:11, 344.43it/s]
Adding requests:   8%|▊         | 310/4096 [00:00<00:10, 349.91it/s]
Adding requests:   8%|▊         | 346/4096 [00:01<00:10, 351.02it/s]
Adding requests:   9%|▉         | 383/4096 [00:01<00:10, 355.82it/s]
Adding requests:  10%|█         | 421/4096 [00:01<00:10, 361.99it/s]
Adding requests:  11%|█         | 458/4096 [00:01<00:10, 360.17it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:09, 368.33it/s]
Adding requests:  13%|█▎        | 537/4096 [00:01<00:09, 373.97it/s]
Adding requests:  14%|█▍        | 575/4096 [00:01<00:09, 373.09it/s]
Adding requests:  15%|█▍        | 613/4096 [00:01<00:09, 363.10it/s]
Adding requests:  16%|█▌        | 650/4096 [00:01<00:09, 356.75it/s]
Adding requests:  17%|█▋        | 687/4096 [00:01<00:09, 359.07it/s]
Adding requests:  18%|█▊        | 723/4096 [00:02<00:09, 356.09it/s]
Adding requests:  19%|█▊        | 759/4096 [00:02<00:09, 354.35it/s]
Adding requests:  19%|█▉        | 795/4096 [00:02<00:09, 354.07it/s]
Adding requests:  20%|██        | 833/4096 [00:02<00:09, 358.76it/s]
Adding requests:  21%|██        | 870/4096 [00:02<00:08, 361.97it/s]
Adding requests:  22%|██▏       | 907/4096 [00:02<00:08, 357.71it/s]
Adding requests:  23%|██▎       | 943/4096 [00:02<00:08, 352.46it/s]
Adding requests:  24%|██▍       | 979/4096 [00:02<00:08, 353.42it/s]
Adding requests:  25%|██▍       | 1015/4096 [00:02<00:08, 348.90it/s]
Adding requests:  26%|██▌       | 1051/4096 [00:02<00:08, 350.84it/s]
Adding requests:  27%|██▋       | 1087/4096 [00:03<00:08, 350.65it/s]
Adding requests:  27%|██▋       | 1123/4096 [00:03<00:08, 353.13it/s]
Adding requests:  28%|██▊       | 1159/4096 [00:03<00:08, 349.88it/s]
Adding requests:  29%|██▉       | 1196/4096 [00:03<00:08, 352.63it/s]
Adding requests:  30%|███       | 1234/4096 [00:03<00:07, 360.37it/s]
Adding requests:  31%|███       | 1271/4096 [00:03<00:07, 354.28it/s]
Adding requests:  32%|███▏      | 1307/4096 [00:03<00:08, 344.41it/s]
Adding requests:  33%|███▎      | 1342/4096 [00:03<00:07, 344.93it/s]
Adding requests:  34%|███▎      | 1379/4096 [00:03<00:07, 350.53it/s]
Adding requests:  35%|███▍      | 1415/4096 [00:04<00:07, 349.70it/s]
Adding requests:  35%|███▌      | 1451/4096 [00:04<00:07, 351.35it/s]
Adding requests:  36%|███▋      | 1488/4096 [00:04<00:07, 356.53it/s]
Adding requests:  37%|███▋      | 1525/4096 [00:04<00:07, 359.11it/s]
Adding requests:  38%|███▊      | 1561/4096 [00:04<00:07, 354.20it/s]
Adding requests:  39%|███▉      | 1597/4096 [00:04<00:07, 350.69it/s]
Adding requests:  40%|███▉      | 1633/4096 [00:04<00:07, 345.59it/s]
Adding requests:  41%|████      | 1668/4096 [00:04<00:07, 339.52it/s]
Adding requests:  42%|████▏     | 1705/4096 [00:04<00:06, 346.57it/s]
Adding requests:  43%|████▎     | 1741/4096 [00:04<00:06, 349.72it/s]
Adding requests:  43%|████▎     | 1779/4096 [00:05<00:06, 356.05it/s]
Adding requests:  44%|████▍     | 1815/4096 [00:05<00:06, 352.47it/s]
Adding requests:  45%|████▌     | 1851/4096 [00:05<00:06, 354.45it/s]
Adding requests:  46%|████▌     | 1888/4096 [00:05<00:06, 358.15it/s]
Adding requests:  47%|████▋     | 1925/4096 [00:05<00:06, 359.13it/s]
Adding requests:  48%|████▊     | 1963/4096 [00:05<00:05, 362.95it/s]
Adding requests:  49%|████▉     | 2000/4096 [00:05<00:06, 348.06it/s]
Adding requests:  50%|████▉     | 2035/4096 [00:05<00:05, 344.53it/s]
Adding requests:  51%|█████     | 2070/4096 [00:05<00:05, 340.64it/s]
Adding requests:  51%|█████▏    | 2106/4096 [00:05<00:05, 345.73it/s]
Adding requests:  52%|█████▏    | 2142/4096 [00:06<00:05, 347.59it/s]
Adding requests:  53%|█████▎    | 2177/4096 [00:06<00:05, 342.15it/s]
Adding requests:  54%|█████▍    | 2212/4096 [00:06<00:05, 341.48it/s]
Adding requests:  55%|█████▍    | 2248/4096 [00:06<00:05, 346.47it/s]
Adding requests:  56%|█████▌    | 2284/4096 [00:06<00:05, 348.56it/s]
Adding requests:  57%|█████▋    | 2320/4096 [00:06<00:05, 351.69it/s]
Adding requests:  58%|█████▊    | 2358/4096 [00:06<00:04, 357.47it/s]
Adding requests:  58%|█████▊    | 2396/4096 [00:06<00:04, 363.62it/s]
Adding requests:  59%|█████▉    | 2433/4096 [00:06<00:04, 365.39it/s]
Adding requests:  60%|██████    | 2470/4096 [00:07<00:04, 361.14it/s]
Adding requests:  61%|██████▏   | 2509/4096 [00:07<00:04, 367.03it/s]
Adding requests:  62%|██████▏   | 2548/4096 [00:07<00:04, 371.64it/s]
Adding requests:  63%|██████▎   | 2587/4096 [00:07<00:04, 374.98it/s]
Adding requests:  64%|██████▍   | 2625/4096 [00:07<00:03, 368.74it/s]
Adding requests:  65%|██████▍   | 2662/4096 [00:07<00:03, 360.41it/s]
Adding requests:  66%|██████▌   | 2699/4096 [00:07<00:03, 356.92it/s]
Adding requests:  67%|██████▋   | 2735/4096 [00:07<00:03, 356.45it/s]
Adding requests:  68%|██████▊   | 2773/4096 [00:07<00:03, 363.12it/s]
Adding requests:  69%|██████▊   | 2811/4096 [00:07<00:03, 368.06it/s]
Adding requests:  70%|██████▉   | 2849/4096 [00:08<00:03, 368.59it/s]
Adding requests:  70%|███████   | 2886/4096 [00:08<00:03, 362.13it/s]
Adding requests:  71%|███████▏  | 2923/4096 [00:08<00:03, 362.69it/s]
Adding requests:  72%|███████▏  | 2962/4096 [00:08<00:03, 368.47it/s]
Adding requests:  73%|███████▎  | 2999/4096 [00:08<00:02, 366.39it/s]
Adding requests:  74%|███████▍  | 3037/4096 [00:08<00:02, 369.66it/s]
Adding requests:  75%|███████▌  | 3075/4096 [00:08<00:02, 372.49it/s]
Adding requests:  76%|███████▌  | 3113/4096 [00:08<00:02, 369.74it/s]
Adding requests:  77%|███████▋  | 3151/4096 [00:08<00:02, 370.30it/s]
Adding requests:  78%|███████▊  | 3189/4096 [00:08<00:02, 365.93it/s]
Adding requests:  79%|███████▉  | 3226/4096 [00:09<00:02, 363.32it/s]
Adding requests:  80%|███████▉  | 3263/4096 [00:09<00:02, 363.32it/s]
Adding requests:  81%|████████  | 3300/4096 [00:09<00:02, 344.41it/s]
Adding requests:  81%|████████▏ | 3335/4096 [00:09<00:02, 345.89it/s]
Adding requests:  82%|████████▏ | 3373/4096 [00:09<00:02, 354.32it/s]
Adding requests:  83%|████████▎ | 3410/4096 [00:09<00:01, 356.80it/s]
Adding requests:  84%|████████▍ | 3447/4096 [00:09<00:01, 359.06it/s]
Adding requests:  85%|████████▌ | 3483/4096 [00:09<00:01, 358.84it/s]
Adding requests:  86%|████████▌ | 3521/4096 [00:09<00:01, 362.56it/s]
Adding requests:  87%|████████▋ | 3561/4096 [00:10<00:01, 372.09it/s]
Adding requests:  88%|████████▊ | 3599/4096 [00:10<00:01, 364.64it/s]
Adding requests:  89%|████████▉ | 3636/4096 [00:10<00:01, 366.11it/s]
Adding requests:  90%|████████▉ | 3673/4096 [00:10<00:01, 358.55it/s]
Adding requests:  91%|█████████ | 3709/4096 [00:10<00:01, 356.85it/s]
Adding requests:  91%|█████████▏| 3745/4096 [00:10<00:00, 357.10it/s]
Adding requests:  92%|█████████▏| 3781/4096 [00:10<00:00, 346.94it/s]
Adding requests:  93%|█████████▎| 3816/4096 [00:10<00:00, 338.51it/s]
Adding requests:  94%|█████████▍| 3852/4096 [00:10<00:00, 342.49it/s]
Adding requests:  95%|█████████▍| 3888/4096 [00:10<00:00, 346.66it/s]
Adding requests:  96%|█████████▌| 3923/4096 [00:11<00:00, 342.52it/s]
Adding requests:  97%|█████████▋| 3959/4096 [00:11<00:00, 346.06it/s]
Adding requests:  98%|█████████▊| 3994/4096 [00:11<00:00, 345.55it/s]
Adding requests:  98%|█████████▊| 4029/4096 [00:11<00:00, 345.54it/s]
Adding requests:  99%|█████████▉| 4064/4096 [00:11<00:00, 344.86it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 354.45it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▋      | 1489/4096 [00:00<00:00, 4696.85it/s, est. speed input: 4809702.35 toks/s, output: 4696.89 toks/s]
Processed prompts:  48%|████▊     | 1959/4096 [00:03<00:05, 412.31it/s, est. speed input: 533088.71 toks/s, output: 520.59 toks/s]   
Processed prompts:  53%|█████▎    | 2161/4096 [00:05<00:06, 294.19it/s, est. speed input: 403768.90 toks/s, output: 394.30 toks/s]
Processed prompts:  56%|█████▌    | 2276/4096 [00:06<00:06, 266.56it/s, est. speed input: 374526.26 toks/s, output: 365.75 toks/s]
Processed prompts:  57%|█████▋    | 2352/4096 [00:06<00:07, 248.90it/s, est. speed input: 358717.94 toks/s, output: 350.31 toks/s]
Processed prompts:  59%|█████▊    | 2406/4096 [00:07<00:07, 224.03it/s, est. speed input: 342340.12 toks/s, output: 334.32 toks/s]
Processed prompts:  60%|█████▉    | 2446/4096 [00:07<00:07, 216.64it/s, est. speed input: 336430.58 toks/s, output: 328.55 toks/s]
Processed prompts:  60%|██████    | 2478/4096 [00:07<00:07, 203.67it/s, est. speed input: 329708.26 toks/s, output: 321.98 toks/s]
Processed prompts:  61%|██████    | 2504/4096 [00:07<00:08, 188.07it/s, est. speed input: 323149.36 toks/s, output: 315.58 toks/s]
Processed prompts:  62%|██████▏   | 2526/4096 [00:08<00:09, 168.30it/s, est. speed input: 316038.96 toks/s, output: 308.63 toks/s]
Processed prompts:  62%|██████▏   | 2545/4096 [00:08<00:10, 148.19it/s, est. speed input: 309041.32 toks/s, output: 301.80 toks/s]
Processed prompts:  63%|██████▎   | 2577/4096 [00:08<00:10, 144.48it/s, est. speed input: 304203.91 toks/s, output: 297.07 toks/s]
Processed prompts:  64%|██████▎   | 2609/4096 [00:08<00:10, 139.75it/s, est. speed input: 299236.69 toks/s, output: 292.22 toks/s]
Processed prompts:  64%|██████▍   | 2641/4096 [00:09<00:10, 137.50it/s, est. speed input: 294851.12 toks/s, output: 287.94 toks/s]
Processed prompts:  65%|██████▌   | 2673/4096 [00:09<00:10, 134.97it/s, est. speed input: 290527.09 toks/s, output: 283.72 toks/s]
Processed prompts:  66%|██████▌   | 2705/4096 [00:09<00:10, 132.65it/s, est. speed input: 286339.48 toks/s, output: 279.63 toks/s]
Processed prompts:  67%|██████▋   | 2737/4096 [00:09<00:10, 132.09it/s, est. speed input: 282573.19 toks/s, output: 275.95 toks/s]
Processed prompts:  68%|██████▊   | 2769/4096 [00:10<00:10, 130.86it/s, est. speed input: 278842.86 toks/s, output: 272.31 toks/s]
Processed prompts:  68%|██████▊   | 2801/4096 [00:10<00:09, 129.84it/s, est. speed input: 275267.33 toks/s, output: 268.82 toks/s]
Processed prompts:  69%|██████▉   | 2833/4096 [00:10<00:09, 129.11it/s, est. speed input: 271857.56 toks/s, output: 265.49 toks/s]
Processed prompts:  70%|██████▉   | 2865/4096 [00:10<00:09, 130.01it/s, est. speed input: 268831.44 toks/s, output: 262.53 toks/s]
Processed prompts:  71%|███████   | 2897/4096 [00:11<00:09, 131.50it/s, est. speed input: 266062.09 toks/s, output: 259.83 toks/s]
Processed prompts:  72%|███████▏  | 2929/4096 [00:11<00:08, 130.76it/s, est. speed input: 263148.72 toks/s, output: 256.98 toks/s]
Processed prompts:  72%|███████▏  | 2961/4096 [00:11<00:08, 131.07it/s, est. speed input: 260474.86 toks/s, output: 254.37 toks/s]
Processed prompts:  73%|███████▎  | 2993/4096 [00:11<00:08, 131.32it/s, est. speed input: 257914.78 toks/s, output: 251.87 toks/s]
Processed prompts:  74%|███████▍  | 3025/4096 [00:12<00:08, 130.58it/s, est. speed input: 255337.56 toks/s, output: 249.35 toks/s]
Processed prompts:  75%|███████▍  | 3057/4096 [00:12<00:07, 129.90it/s, est. speed input: 252841.38 toks/s, output: 246.92 toks/s]
Processed prompts:  75%|███████▌  | 3089/4096 [00:12<00:07, 129.79it/s, est. speed input: 250490.22 toks/s, output: 244.62 toks/s]
Processed prompts:  76%|███████▌  | 3121/4096 [00:12<00:07, 129.32it/s, est. speed input: 248180.07 toks/s, output: 242.36 toks/s]
Processed prompts:  77%|███████▋  | 3153/4096 [00:13<00:07, 130.16it/s, est. speed input: 246098.04 toks/s, output: 240.33 toks/s]
Processed prompts:  78%|███████▊  | 3185/4096 [00:13<00:07, 129.55it/s, est. speed input: 243952.48 toks/s, output: 238.23 toks/s]
Processed prompts:  79%|███████▊  | 3217/4096 [00:13<00:06, 129.26it/s, est. speed input: 241900.78 toks/s, output: 236.23 toks/s]
Processed prompts:  79%|███████▉  | 3249/4096 [00:13<00:06, 128.80it/s, est. speed input: 239894.19 toks/s, output: 234.27 toks/s]
Processed prompts:  80%|████████  | 3281/4096 [00:14<00:06, 128.05it/s, est. speed input: 237911.95 toks/s, output: 232.34 toks/s]
Processed prompts:  81%|████████  | 3313/4096 [00:14<00:06, 128.47it/s, est. speed input: 236098.94 toks/s, output: 230.57 toks/s]
Processed prompts:  82%|████████▏ | 3345/4096 [00:14<00:05, 128.37it/s, est. speed input: 234307.08 toks/s, output: 228.82 toks/s]
Processed prompts:  82%|████████▏ | 3377/4096 [00:14<00:05, 128.14it/s, est. speed input: 232559.07 toks/s, output: 227.11 toks/s]
Processed prompts:  83%|████████▎ | 3409/4096 [00:15<00:05, 129.17it/s, est. speed input: 230987.30 toks/s, output: 225.57 toks/s]
Processed prompts:  84%|████████▍ | 3441/4096 [00:15<00:05, 128.33it/s, est. speed input: 229314.44 toks/s, output: 223.94 toks/s]
Processed prompts:  85%|████████▍ | 3473/4096 [00:15<00:04, 128.47it/s, est. speed input: 227763.70 toks/s, output: 222.43 toks/s]
Processed prompts:  86%|████████▌ | 3505/4096 [00:15<00:04, 127.97it/s, est. speed input: 226206.97 toks/s, output: 220.91 toks/s]
Processed prompts:  86%|████████▋ | 3537/4096 [00:16<00:04, 128.89it/s, est. speed input: 224813.44 toks/s, output: 219.54 toks/s]
Processed prompts:  87%|████████▋ | 3569/4096 [00:16<00:04, 129.54it/s, est. speed input: 223460.82 toks/s, output: 218.22 toks/s]
Processed prompts:  88%|████████▊ | 3601/4096 [00:16<00:03, 128.98it/s, est. speed input: 222062.41 toks/s, output: 216.86 toks/s]
Processed prompts:  89%|████████▊ | 3633/4096 [00:16<00:03, 129.94it/s, est. speed input: 220817.59 toks/s, output: 215.64 toks/s]
Processed prompts:  89%|████████▉ | 3665/4096 [00:17<00:03, 130.85it/s, est. speed input: 219626.51 toks/s, output: 214.48 toks/s]
Processed prompts:  90%|█████████ | 3697/4096 [00:17<00:03, 129.85it/s, est. speed input: 218339.17 toks/s, output: 213.22 toks/s]
Processed prompts:  91%|█████████ | 3729/4096 [00:17<00:02, 130.06it/s, est. speed input: 217159.09 toks/s, output: 212.07 toks/s]
Processed prompts:  92%|█████████▏| 3761/4096 [00:17<00:02, 129.86it/s, est. speed input: 215984.69 toks/s, output: 210.92 toks/s]
Processed prompts:  93%|█████████▎| 3793/4096 [00:18<00:02, 129.39it/s, est. speed input: 214818.11 toks/s, output: 209.78 toks/s]
Processed prompts:  93%|█████████▎| 3825/4096 [00:18<00:02, 129.53it/s, est. speed input: 213717.17 toks/s, output: 208.71 toks/s]
Processed prompts:  94%|█████████▍| 3857/4096 [00:18<00:01, 128.93it/s, est. speed input: 212594.83 toks/s, output: 207.61 toks/s]
Processed prompts:  95%|█████████▍| 3889/4096 [00:18<00:01, 128.45it/s, est. speed input: 211497.96 toks/s, output: 206.54 toks/s]
Processed prompts:  96%|█████████▌| 3921/4096 [00:19<00:01, 131.36it/s, est. speed input: 210657.19 toks/s, output: 205.72 toks/s]
Processed prompts:  97%|█████████▋| 3953/4096 [00:19<00:01, 130.61it/s, est. speed input: 209645.16 toks/s, output: 204.73 toks/s]
Processed prompts:  97%|█████████▋| 3985/4096 [00:19<00:00, 130.50it/s, est. speed input: 208687.20 toks/s, output: 203.80 toks/s]
Processed prompts:  98%|█████████▊| 4017/4096 [00:19<00:00, 129.72it/s, est. speed input: 207705.74 toks/s, output: 202.84 toks/s]
Processed prompts:  99%|█████████▉| 4049/4096 [00:20<00:00, 133.81it/s, est. speed input: 207044.12 toks/s, output: 202.19 toks/s]
Processed prompts: 100%|█████████▉| 4081/4096 [00:20<00:00, 155.70it/s, est. speed input: 207364.74 toks/s, output: 202.50 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:20<00:00, 155.70it/s, est. speed input: 208124.52 toks/s, output: 203.25 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:20<00:00, 203.25it/s, est. speed input: 208124.52 toks/s, output: 203.25 toks/s]
[rank0]:[W126 16:19:12.054547988 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 16:19:14
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 16:20:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=749465) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=749465) WARNING 01-26 16:20:27 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     def forward(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     raise e
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/tmp/torchinductor_root/pd/cpdfnyfxaigvavknvje6smbtj4tkg266uncqqyt44bgx5tueerkj.py", line 1093, in call
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 10)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 327, in quant_slide_int8_triton
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) ERROR 01-26 16:20:33 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered

STDERR:
[2026-01-26 16:20:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:20:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:20:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:20:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:20:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:20:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:20:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:20:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 16:20:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 16:20:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:20:16] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 16:20:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 16:20:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 16:20:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 16:20:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 16:20:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 16:20:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:18] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:18] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:18] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:18] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:18] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=749465) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=749465) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
(EngineCore_DP0 pid=749465) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03s/it]
(EngineCore_DP0 pid=749465) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.00it/s]
(EngineCore_DP0 pid=749465) 
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=749465) [2026-01-26 16:20:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=749465) [rank0]:W0126 16:20:32.038000 749465 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=749465) [rank0]:W0126 16:20:32.121000 749465 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=749465) [rank0]:W0126 16:20:33.229000 749465 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=749465) [rank0]:W0126 16:20:33.354000 749465 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=749465) Process EngineCore_DP0:
(EngineCore_DP0 pid=749465) Traceback (most recent call last):
(EngineCore_DP0 pid=749465)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=749465)     self.run()
(EngineCore_DP0 pid=749465)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=749465)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=749465)     raise e
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=749465)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=749465)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=749465)     super().__init__(
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=749465)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=749465)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=749465)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=749465)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=749465)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=749465)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=749465)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=749465)     return func(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=749465)     return func(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=749465)     self.model_runner.profile_run()
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=749465)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=749465)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=749465)     return func(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=749465)     outputs = self.model(
(EngineCore_DP0 pid=749465)               ^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=749465)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=749465)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=749465)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=749465)     hidden_states = self.model(
(EngineCore_DP0 pid=749465)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=749465)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=749465)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=749465)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=749465)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=749465)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=749465)     def forward(
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=749465)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=749465)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=749465)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=749465)     raise e
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=749465)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=749465)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=749465)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=749465)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=749465)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=749465)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=749465)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=749465)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=749465)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=749465)     return compiled_fn(full_args)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=749465)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=749465)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=749465)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=749465)                             ^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=749465)     outs = compiled_fn(args)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=749465)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=749465)     return self.current_callable(inputs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=749465)     out = model(new_inputs)
(EngineCore_DP0 pid=749465)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/tmp/torchinductor_root/pd/cpdfnyfxaigvavknvje6smbtj4tkg266uncqqyt44bgx5tueerkj.py", line 1093, in call
(EngineCore_DP0 pid=749465)     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 10)
(EngineCore_DP0 pid=749465)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=749465)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=749465)     return fn(input, L)
(EngineCore_DP0 pid=749465)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/B200_cc100_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 327, in quant_slide_int8_triton
(EngineCore_DP0 pid=749465)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=749465)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=749465)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=749465)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=749465)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=749465)     self._init_handles()
(EngineCore_DP0 pid=749465)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=749465)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=749465)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=749465) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 16:20:34.387107116 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-26 17:47:27
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:47:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=863258) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=863258) WARNING 01-26 17:47:57 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=863258) WARNING 01-26 17:48:12 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 22.43 requests/s, 11505.41 total tokens/s, 22.43 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 17:47:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:47:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:47:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:47:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:47:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:47:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:47:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:47:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:47:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:47:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:47:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:47:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:47:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:47:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:47:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:47:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:47:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:47:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:47:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:47:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:47:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:47:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:47:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:47:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:47:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:47:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:47:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:47:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=863258) [2026-01-26 17:47:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=863258) [2026-01-26 17:47:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=863258) [2026-01-26 17:47:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=863258) [2026-01-26 17:47:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=863258) [2026-01-26 17:47:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=863258) [2026-01-26 17:47:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=863258) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=863258) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.43s/it]
(EngineCore_DP0 pid=863258) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.19it/s]
(EngineCore_DP0 pid=863258) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
(EngineCore_DP0 pid=863258) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.25s/it]
(EngineCore_DP0 pid=863258) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.17s/it]
(EngineCore_DP0 pid=863258) 
(EngineCore_DP0 pid=863258) [2026-01-26 17:47:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=863258) [2026-01-26 17:47:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36700160 bytes
(EngineCore_DP0 pid=863258) [2026-01-26 17:47:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=863258) [2026-01-26 17:47:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26214400 bytes
(EngineCore_DP0 pid=863258) [2026-01-26 17:47:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=863258) [2026-01-26 17:47:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 141557760 bytes
(EngineCore_DP0 pid=863258) [2026-01-26 17:47:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=863258) [2026-01-26 17:47:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70860800 bytes
(EngineCore_DP0 pid=863258) 2026-01-26 17:48:12,249 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=863258) 2026-01-26 17:48:12,289 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=863258) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.39it/s]
(EngineCore_DP0 pid=863258) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 10.11it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 518.14it/s]
Adding requests:  88%|████████▊ | 112/128 [00:00<00:00, 565.99it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 563.16it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:18,  6.76it/s, est. speed input: 3459.62 toks/s, output: 6.76 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:07, 15.98it/s, est. speed input: 7420.69 toks/s, output: 14.49 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:06, 19.41it/s, est. speed input: 8918.79 toks/s, output: 17.42 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:05, 21.12it/s, est. speed input: 9703.66 toks/s, output: 18.95 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:05, 22.08it/s, est. speed input: 10185.30 toks/s, output: 19.89 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:04, 22.67it/s, est. speed input: 10511.37 toks/s, output: 20.53 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:04, 23.07it/s, est. speed input: 10750.33 toks/s, output: 21.00 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:04, 23.32it/s, est. speed input: 10929.57 toks/s, output: 21.35 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:04, 23.51it/s, est. speed input: 11072.80 toks/s, output: 21.63 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:04, 23.65it/s, est. speed input: 11190.58 toks/s, output: 21.86 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:04, 23.75it/s, est. speed input: 11286.31 toks/s, output: 22.04 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:03, 23.77it/s, est. speed input: 11360.94 toks/s, output: 22.19 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:03, 23.79it/s, est. speed input: 11425.47 toks/s, output: 22.32 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:03, 23.84it/s, est. speed input: 11483.72 toks/s, output: 22.43 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:03, 23.53it/s, est. speed input: 11498.43 toks/s, output: 22.46 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:03, 23.58it/s, est. speed input: 11537.98 toks/s, output: 22.54 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:03, 23.64it/s, est. speed input: 11574.91 toks/s, output: 22.61 toks/s]
Processed prompts:  41%|████      | 52/128 [00:02<00:03, 23.61it/s, est. speed input: 11601.39 toks/s, output: 22.66 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:02<00:03, 23.66it/s, est. speed input: 11631.27 toks/s, output: 22.72 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:02<00:02, 23.69it/s, est. speed input: 11657.53 toks/s, output: 22.77 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:02, 23.74it/s, est. speed input: 11683.81 toks/s, output: 22.82 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:02<00:02, 23.79it/s, est. speed input: 11709.13 toks/s, output: 22.87 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:02<00:02, 23.81it/s, est. speed input: 11730.36 toks/s, output: 22.91 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:03<00:02, 23.81it/s, est. speed input: 11749.26 toks/s, output: 22.95 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:03<00:02, 23.80it/s, est. speed input: 11766.60 toks/s, output: 22.98 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:03<00:02, 23.80it/s, est. speed input: 11782.66 toks/s, output: 23.01 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:03<00:02, 23.84it/s, est. speed input: 11799.76 toks/s, output: 23.05 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:03<00:01, 23.88it/s, est. speed input: 11816.39 toks/s, output: 23.08 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:03<00:01, 23.87it/s, est. speed input: 11829.97 toks/s, output: 23.11 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:03<00:01, 23.89it/s, est. speed input: 11844.01 toks/s, output: 23.13 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:03<00:01, 23.87it/s, est. speed input: 11855.49 toks/s, output: 23.16 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:04<00:01, 23.87it/s, est. speed input: 11866.69 toks/s, output: 23.18 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:04<00:01, 23.88it/s, est. speed input: 11877.98 toks/s, output: 23.20 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:04<00:01, 23.89it/s, est. speed input: 11888.67 toks/s, output: 23.22 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:04<00:01, 23.82it/s, est. speed input: 11895.12 toks/s, output: 23.23 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:04<00:00, 23.87it/s, est. speed input: 11905.82 toks/s, output: 23.25 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:04<00:00, 23.88it/s, est. speed input: 11914.47 toks/s, output: 23.27 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:04<00:00, 23.89it/s, est. speed input: 11923.20 toks/s, output: 23.29 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:04<00:00, 23.90it/s, est. speed input: 11931.56 toks/s, output: 23.30 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:05<00:00, 23.92it/s, est. speed input: 11939.68 toks/s, output: 23.32 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:05<00:00, 23.89it/s, est. speed input: 11946.16 toks/s, output: 23.33 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:05<00:00, 23.87it/s, est. speed input: 11952.20 toks/s, output: 23.34 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:05<00:00, 23.90it/s, est. speed input: 11959.41 toks/s, output: 23.36 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.90it/s, est. speed input: 11961.67 toks/s, output: 23.36 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.36it/s, est. speed input: 11961.67 toks/s, output: 23.36 toks/s]
[rank0]:[W126 17:48:20.970739314 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 17:48:22
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:48:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=864640) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=864640) WARNING 01-26 17:48:52 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=864640) WARNING 01-26 17:49:07 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 23.45 requests/s, 24034.18 total tokens/s, 23.45 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 17:48:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:48:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:48:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:48:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:48:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:48:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:48:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:48:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:48:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:48:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:48:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:48:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:48:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:48:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:48:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:48:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:48:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:48:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:48:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:48:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:48:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:48:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:48:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:48:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:48:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:48:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:48:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:48:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=864640) [2026-01-26 17:48:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=864640) [2026-01-26 17:48:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=864640) [2026-01-26 17:48:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=864640) [2026-01-26 17:48:38] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=864640) [2026-01-26 17:48:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=864640) [2026-01-26 17:48:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=864640) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=864640) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.48s/it]
(EngineCore_DP0 pid=864640) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.20it/s]
(EngineCore_DP0 pid=864640) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.05it/s]
(EngineCore_DP0 pid=864640) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.18s/it]
(EngineCore_DP0 pid=864640) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
(EngineCore_DP0 pid=864640) 
(EngineCore_DP0 pid=864640) [2026-01-26 17:48:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=864640) [2026-01-26 17:48:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36700160 bytes
(EngineCore_DP0 pid=864640) [2026-01-26 17:48:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=864640) [2026-01-26 17:48:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26214400 bytes
(EngineCore_DP0 pid=864640) [2026-01-26 17:48:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=864640) [2026-01-26 17:48:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 141557760 bytes
(EngineCore_DP0 pid=864640) [2026-01-26 17:48:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=864640) [2026-01-26 17:48:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70860800 bytes
(EngineCore_DP0 pid=864640) 2026-01-26 17:49:07,433 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=864640) 2026-01-26 17:49:07,471 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=864640) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.46it/s]
(EngineCore_DP0 pid=864640) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.46it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 276.68it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 320.65it/s]
Adding requests:  76%|███████▌  | 97/128 [00:00<00:00, 319.16it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 203.62it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:00, 137.66it/s, est. speed input: 140976.62 toks/s, output: 137.67 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 37.73it/s, est. speed input: 43716.03 toks/s, output: 42.69 toks/s]   
Processed prompts:  30%|██▉       | 38/128 [00:01<00:02, 31.94it/s, est. speed input: 37492.50 toks/s, output: 36.61 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 29.74it/s, est. speed input: 35291.05 toks/s, output: 34.46 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 28.33it/s, est. speed input: 33992.47 toks/s, output: 33.20 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 27.03it/s, est. speed input: 32892.82 toks/s, output: 32.12 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:02, 26.19it/s, est. speed input: 32084.16 toks/s, output: 31.33 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:02, 25.66it/s, est. speed input: 31566.96 toks/s, output: 30.83 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:02, 25.16it/s, est. speed input: 31096.55 toks/s, output: 30.37 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:02<00:02, 24.83it/s, est. speed input: 30705.13 toks/s, output: 29.99 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:02<00:02, 24.56it/s, est. speed input: 30352.93 toks/s, output: 29.64 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:02<00:02, 24.34it/s, est. speed input: 30034.99 toks/s, output: 29.33 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:02, 24.21it/s, est. speed input: 29755.70 toks/s, output: 29.06 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:02, 23.74it/s, est. speed input: 29425.37 toks/s, output: 28.74 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:02, 23.79it/s, est. speed input: 29202.31 toks/s, output: 28.52 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 23.80it/s, est. speed input: 28993.71 toks/s, output: 28.31 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:03<00:01, 23.67it/s, est. speed input: 28778.16 toks/s, output: 28.10 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:03<00:01, 23.71it/s, est. speed input: 28602.43 toks/s, output: 27.93 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:03<00:01, 23.71it/s, est. speed input: 28436.17 toks/s, output: 27.77 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:03<00:01, 23.74it/s, est. speed input: 28285.82 toks/s, output: 27.62 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:01, 23.75it/s, est. speed input: 28145.35 toks/s, output: 27.49 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:03<00:01, 23.81it/s, est. speed input: 28021.15 toks/s, output: 27.36 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:03<00:01, 23.84it/s, est. speed input: 27903.76 toks/s, output: 27.25 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:03<00:00, 23.84it/s, est. speed input: 27791.50 toks/s, output: 27.14 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:04<00:00, 23.81it/s, est. speed input: 27682.66 toks/s, output: 27.03 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:04<00:00, 23.80it/s, est. speed input: 27580.78 toks/s, output: 26.93 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:04<00:00, 23.83it/s, est. speed input: 27489.94 toks/s, output: 26.85 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:04<00:00, 23.83it/s, est. speed input: 27401.30 toks/s, output: 26.76 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:04<00:00, 23.84it/s, est. speed input: 27319.19 toks/s, output: 26.68 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:04<00:00, 23.84it/s, est. speed input: 27240.87 toks/s, output: 26.60 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:04<00:00, 23.85it/s, est. speed input: 27167.04 toks/s, output: 26.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 23.85it/s, est. speed input: 27143.59 toks/s, output: 26.51 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 26.51it/s, est. speed input: 27143.59 toks/s, output: 26.51 toks/s]
[rank0]:[W126 17:49:14.162539528 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 17:49:16
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:49:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=865952) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=865952) WARNING 01-26 17:49:47 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=865952) WARNING 01-26 17:50:02 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 44.23 requests/s, 45333.11 total tokens/s, 44.23 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 17:49:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:49:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:49:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:49:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:49:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:49:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:49:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:49:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:49:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:49:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:49:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:49:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:49:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:49:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:49:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:49:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:49:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:49:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:49:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:49:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:49:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:49:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:49:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:49:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:49:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:49:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:49:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:49:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=865952) [2026-01-26 17:49:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=865952) [2026-01-26 17:49:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=865952) [2026-01-26 17:49:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=865952) [2026-01-26 17:49:32] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=865952) [2026-01-26 17:49:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=865952) [2026-01-26 17:49:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=865952) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=865952) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.43s/it]
(EngineCore_DP0 pid=865952) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.23it/s]
(EngineCore_DP0 pid=865952) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.06it/s]
(EngineCore_DP0 pid=865952) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.18s/it]
(EngineCore_DP0 pid=865952) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
(EngineCore_DP0 pid=865952) 
(EngineCore_DP0 pid=865952) [2026-01-26 17:49:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=865952) [2026-01-26 17:49:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36700160 bytes
(EngineCore_DP0 pid=865952) [2026-01-26 17:49:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=865952) [2026-01-26 17:49:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26214400 bytes
(EngineCore_DP0 pid=865952) [2026-01-26 17:49:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=865952) [2026-01-26 17:49:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 141557760 bytes
(EngineCore_DP0 pid=865952) [2026-01-26 17:49:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=865952) [2026-01-26 17:49:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70860800 bytes
(EngineCore_DP0 pid=865952) 2026-01-26 17:50:02,186 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=865952) 2026-01-26 17:50:02,225 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=865952) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  9.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.26it/s]
(EngineCore_DP0 pid=865952) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 10.64it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 10.63it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:53,  4.78it/s]
Adding requests:  11%|█▏        | 29/256 [00:00<00:01, 115.67it/s]
Adding requests:  25%|██▌       | 64/256 [00:00<00:00, 201.27it/s]
Adding requests:  37%|███▋      | 95/256 [00:00<00:00, 238.49it/s]
Adding requests:  50%|█████     | 129/256 [00:00<00:00, 269.36it/s]
Adding requests:  64%|██████▍   | 164/256 [00:00<00:00, 294.43it/s]
Adding requests:  78%|███████▊  | 199/256 [00:00<00:00, 311.09it/s]
Adding requests:  92%|█████████▏| 235/256 [00:00<00:00, 324.58it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 261.24it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:00<00:00, 270.16it/s, est. speed input: 276663.80 toks/s, output: 270.17 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:00<00:02, 77.13it/s, est. speed input: 89815.67 toks/s, output: 87.71 toks/s]   
Processed prompts:  30%|███       | 78/256 [00:01<00:02, 65.79it/s, est. speed input: 77872.31 toks/s, output: 76.05 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:01<00:02, 60.59it/s, est. speed input: 72814.18 toks/s, output: 71.11 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:01<00:02, 57.32it/s, est. speed input: 69849.96 toks/s, output: 68.21 toks/s]
Processed prompts:  40%|████      | 103/256 [00:01<00:02, 56.91it/s, est. speed input: 68761.41 toks/s, output: 67.15 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:01<00:02, 52.50it/s, est. speed input: 66124.66 toks/s, output: 64.57 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:01<00:02, 51.25it/s, est. speed input: 64880.32 toks/s, output: 63.36 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:01<00:02, 50.30it/s, est. speed input: 63826.90 toks/s, output: 62.33 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:02<00:02, 49.47it/s, est. speed input: 62876.07 toks/s, output: 61.40 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:02<00:02, 48.85it/s, est. speed input: 62040.77 toks/s, output: 60.59 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:02<00:02, 48.35it/s, est. speed input: 61285.78 toks/s, output: 59.85 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:02<00:02, 48.02it/s, est. speed input: 60618.80 toks/s, output: 59.20 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:02<00:02, 47.79it/s, est. speed input: 60018.11 toks/s, output: 58.61 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:02<00:02, 47.66it/s, est. speed input: 59479.99 toks/s, output: 58.09 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:02<00:01, 47.54it/s, est. speed input: 58984.98 toks/s, output: 57.60 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:02<00:01, 47.44it/s, est. speed input: 58529.34 toks/s, output: 57.16 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:03<00:01, 47.35it/s, est. speed input: 58109.55 toks/s, output: 56.75 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:03<00:01, 47.29it/s, est. speed input: 57721.74 toks/s, output: 56.37 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:03<00:01, 47.32it/s, est. speed input: 57374.62 toks/s, output: 56.03 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:03<00:01, 47.25it/s, est. speed input: 57040.11 toks/s, output: 55.70 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:03<00:01, 47.26it/s, est. speed input: 56736.41 toks/s, output: 55.41 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:03<00:01, 46.32it/s, est. speed input: 56323.91 toks/s, output: 55.00 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:03<00:00, 46.57it/s, est. speed input: 56060.13 toks/s, output: 54.75 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:03<00:00, 46.79it/s, est. speed input: 55818.24 toks/s, output: 54.51 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:04<00:00, 46.84it/s, est. speed input: 55579.59 toks/s, output: 54.28 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:04<00:00, 46.95it/s, est. speed input: 55362.39 toks/s, output: 54.06 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:04<00:00, 46.97it/s, est. speed input: 55152.92 toks/s, output: 53.86 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:04<00:00, 47.00it/s, est. speed input: 54955.76 toks/s, output: 53.67 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:04<00:00, 47.04it/s, est. speed input: 54772.19 toks/s, output: 53.49 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:04<00:00, 47.09it/s, est. speed input: 54600.16 toks/s, output: 53.32 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 47.09it/s, est. speed input: 54531.56 toks/s, output: 53.25 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 53.25it/s, est. speed input: 54531.56 toks/s, output: 53.25 toks/s]
[rank0]:[W126 17:50:10.603475267 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 17:50:12
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:50:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=867283) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=867283) WARNING 01-26 17:50:44 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=867283) WARNING 01-26 17:50:59 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 61.31 requests/s, 62844.49 total tokens/s, 61.31 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 17:50:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:50:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:50:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:50:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:50:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:50:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:50:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:50:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:50:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:50:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:50:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:50:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:50:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:50:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:50:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:50:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:50:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:50:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:50:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:50:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:50:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:50:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:50:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:50:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:50:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:50:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:50:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:50:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=867283) [2026-01-26 17:50:29] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=867283) [2026-01-26 17:50:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=867283) [2026-01-26 17:50:29] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=867283) [2026-01-26 17:50:29] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=867283) [2026-01-26 17:50:29] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=867283) [2026-01-26 17:50:29] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=867283) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=867283) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.47s/it]
(EngineCore_DP0 pid=867283) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.21it/s]
(EngineCore_DP0 pid=867283) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.05it/s]
(EngineCore_DP0 pid=867283) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.17s/it]
(EngineCore_DP0 pid=867283) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
(EngineCore_DP0 pid=867283) 
(EngineCore_DP0 pid=867283) [2026-01-26 17:50:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=867283) [2026-01-26 17:50:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36700160 bytes
(EngineCore_DP0 pid=867283) [2026-01-26 17:50:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=867283) [2026-01-26 17:50:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26214400 bytes
(EngineCore_DP0 pid=867283) [2026-01-26 17:50:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=867283) [2026-01-26 17:50:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 141557760 bytes
(EngineCore_DP0 pid=867283) [2026-01-26 17:50:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=867283) [2026-01-26 17:50:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70860800 bytes
(EngineCore_DP0 pid=867283) 2026-01-26 17:50:59,408 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=867283) 2026-01-26 17:50:59,446 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=867283) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  9.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.09it/s]
(EngineCore_DP0 pid=867283) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 11.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 11.13it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 28/512 [00:00<00:01, 276.15it/s]
Adding requests:  12%|█▏        | 63/512 [00:00<00:01, 317.06it/s]
Adding requests:  19%|█▊        | 95/512 [00:00<00:01, 313.95it/s]
Adding requests:  25%|██▍       | 127/512 [00:00<00:01, 314.37it/s]
Adding requests:  31%|███▏      | 161/512 [00:00<00:01, 323.46it/s]
Adding requests:  38%|███▊      | 197/512 [00:00<00:00, 335.73it/s]
Adding requests:  46%|████▌     | 233/512 [00:00<00:00, 341.40it/s]
Adding requests:  52%|█████▏    | 268/512 [00:00<00:00, 341.31it/s]
Adding requests:  59%|█████▉    | 304/512 [00:00<00:00, 345.21it/s]
Adding requests:  67%|██████▋   | 342/512 [00:01<00:00, 353.01it/s]
Adding requests:  74%|███████▍  | 378/512 [00:01<00:00, 354.67it/s]
Adding requests:  81%|████████▏ | 417/512 [00:01<00:00, 363.14it/s]
Adding requests:  89%|████████▊ | 454/512 [00:01<00:00, 359.02it/s]
Adding requests:  96%|█████████▋| 494/512 [00:01<00:00, 370.18it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 346.70it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:00<00:00, 632.65it/s, est. speed input: 647863.13 toks/s, output: 632.66 toks/s]
Processed prompts:  31%|███       | 158/512 [00:01<00:03, 113.68it/s, est. speed input: 136377.93 toks/s, output: 133.18 toks/s]
Processed prompts:  37%|███▋      | 188/512 [00:01<00:03, 96.29it/s, est. speed input: 117226.41 toks/s, output: 114.48 toks/s] 
Processed prompts:  41%|████      | 208/512 [00:01<00:03, 87.19it/s, est. speed input: 108484.58 toks/s, output: 105.94 toks/s]
Processed prompts:  44%|████▎     | 223/512 [00:02<00:03, 80.26it/s, est. speed input: 102734.92 toks/s, output: 100.33 toks/s]
Processed prompts:  46%|████▌     | 235/512 [00:02<00:03, 76.52it/s, est. speed input: 99553.84 toks/s, output: 97.22 toks/s]  
Processed prompts:  48%|████▊     | 245/512 [00:02<00:03, 76.56it/s, est. speed input: 98481.20 toks/s, output: 96.17 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:02<00:03, 68.74it/s, est. speed input: 94838.96 toks/s, output: 92.62 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:02<00:03, 67.33it/s, est. speed input: 93402.58 toks/s, output: 91.21 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:03<00:03, 66.07it/s, est. speed input: 92088.67 toks/s, output: 89.93 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:03<00:03, 65.08it/s, est. speed input: 90904.06 toks/s, output: 88.77 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:03<00:03, 64.14it/s, est. speed input: 89783.25 toks/s, output: 87.68 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:03<00:03, 63.46it/s, est. speed input: 88761.37 toks/s, output: 86.68 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:03<00:03, 63.08it/s, est. speed input: 87837.98 toks/s, output: 85.78 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:03<00:03, 64.15it/s, est. speed input: 87212.63 toks/s, output: 85.17 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:03<00:03, 63.34it/s, est. speed input: 86370.60 toks/s, output: 84.35 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:03<00:02, 62.86it/s, est. speed input: 85600.28 toks/s, output: 83.59 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:04<00:02, 62.39it/s, est. speed input: 84859.75 toks/s, output: 82.87 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:04<00:02, 62.11it/s, est. speed input: 84173.47 toks/s, output: 82.20 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:04<00:02, 61.94it/s, est. speed input: 83532.55 toks/s, output: 81.57 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:04<00:02, 61.85it/s, est. speed input: 82932.90 toks/s, output: 80.99 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:04<00:02, 61.80it/s, est. speed input: 82368.92 toks/s, output: 80.44 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:04<00:02, 61.92it/s, est. speed input: 81855.48 toks/s, output: 79.94 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:04<00:02, 61.82it/s, est. speed input: 81348.52 toks/s, output: 79.44 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:04<00:01, 61.76it/s, est. speed input: 80868.16 toks/s, output: 78.97 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:05<00:01, 61.58it/s, est. speed input: 80397.97 toks/s, output: 78.51 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:05<00:01, 61.55it/s, est. speed input: 79961.16 toks/s, output: 78.09 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:05<00:01, 61.57it/s, est. speed input: 79550.68 toks/s, output: 77.69 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:05<00:01, 61.54it/s, est. speed input: 79154.49 toks/s, output: 77.30 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:05<00:01, 61.62it/s, est. speed input: 78787.18 toks/s, output: 76.94 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:05<00:01, 63.39it/s, est. speed input: 78596.92 toks/s, output: 76.75 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:05<00:01, 62.85it/s, est. speed input: 78252.41 toks/s, output: 76.42 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:05<00:00, 62.36it/s, est. speed input: 77912.23 toks/s, output: 76.09 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:06<00:00, 62.19it/s, est. speed input: 77601.50 toks/s, output: 75.78 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:06<00:00, 61.98it/s, est. speed input: 77296.12 toks/s, output: 75.48 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:06<00:00, 61.96it/s, est. speed input: 77013.04 toks/s, output: 75.21 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:06<00:00, 61.79it/s, est. speed input: 76729.12 toks/s, output: 74.93 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:06<00:00, 61.70it/s, est. speed input: 76458.41 toks/s, output: 74.67 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:06<00:00, 61.58it/s, est. speed input: 76193.80 toks/s, output: 74.41 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:06<00:00, 62.12it/s, est. speed input: 75987.57 toks/s, output: 74.21 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 62.12it/s, est. speed input: 76284.73 toks/s, output: 74.50 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 74.50it/s, est. speed input: 76284.73 toks/s, output: 74.50 toks/s]
[rank0]:[W126 17:51:10.672554108 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 17:51:12
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:51:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=868702) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=868702) WARNING 01-26 17:51:47 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=868702) WARNING 01-26 17:52:03 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 64.95 requests/s, 66572.51 total tokens/s, 64.95 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 17:51:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:51:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:51:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:51:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:51:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:51:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:51:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:51:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:51:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:51:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:51:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:51:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:51:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:51:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:51:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:51:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:51:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:51:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:51:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:51:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:51:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:51:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:51:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:51:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:51:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:51:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:51:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:51:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=868702) [2026-01-26 17:51:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=868702) [2026-01-26 17:51:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=868702) [2026-01-26 17:51:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=868702) [2026-01-26 17:51:32] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=868702) [2026-01-26 17:51:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=868702) [2026-01-26 17:51:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=868702) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=868702) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.46s/it]
(EngineCore_DP0 pid=868702) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.22it/s]
(EngineCore_DP0 pid=868702) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.06it/s]
(EngineCore_DP0 pid=868702) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
(EngineCore_DP0 pid=868702) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
(EngineCore_DP0 pid=868702) 
(EngineCore_DP0 pid=868702) [2026-01-26 17:51:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=868702) [2026-01-26 17:51:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36700160 bytes
(EngineCore_DP0 pid=868702) [2026-01-26 17:51:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=868702) [2026-01-26 17:51:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26214400 bytes
(EngineCore_DP0 pid=868702) [2026-01-26 17:51:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=868702) [2026-01-26 17:51:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 141557760 bytes
(EngineCore_DP0 pid=868702) [2026-01-26 17:51:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=868702) [2026-01-26 17:51:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70860800 bytes
(EngineCore_DP0 pid=868702) 2026-01-26 17:52:03,743 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=868702) 2026-01-26 17:52:03,784 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=868702) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  9.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  9.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.22it/s]
(EngineCore_DP0 pid=868702) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 10.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 11.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 11.10it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 277.94it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 320.06it/s]
Adding requests:   9%|▉         | 96/1024 [00:00<00:02, 316.74it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 320.96it/s]
Adding requests:  16%|█▌        | 166/1024 [00:00<00:02, 331.43it/s]
Adding requests:  20%|█▉        | 203/1024 [00:00<00:02, 342.30it/s]
Adding requests:  23%|██▎       | 239/1024 [00:00<00:02, 346.99it/s]
Adding requests:  27%|██▋       | 274/1024 [00:00<00:02, 345.89it/s]
Adding requests:  30%|███       | 311/1024 [00:00<00:02, 352.80it/s]
Adding requests:  34%|███▍      | 348/1024 [00:01<00:01, 355.84it/s]
Adding requests:  38%|███▊      | 386/1024 [00:01<00:01, 359.67it/s]
Adding requests:  42%|████▏     | 425/1024 [00:01<00:01, 365.92it/s]
Adding requests:  45%|████▌     | 462/1024 [00:01<00:01, 363.60it/s]
Adding requests:  49%|████▉     | 502/1024 [00:01<00:01, 371.46it/s]
Adding requests:  53%|█████▎    | 542/1024 [00:01<00:01, 377.41it/s]
Adding requests:  57%|█████▋    | 580/1024 [00:01<00:01, 372.71it/s]
Adding requests:  60%|██████    | 618/1024 [00:01<00:01, 364.39it/s]
Adding requests:  64%|██████▍   | 655/1024 [00:01<00:01, 357.55it/s]
Adding requests:  68%|██████▊   | 693/1024 [00:01<00:00, 362.45it/s]
Adding requests:  71%|███████▏  | 730/1024 [00:02<00:00, 355.19it/s]
Adding requests:  75%|███████▍  | 766/1024 [00:02<00:00, 356.07it/s]
Adding requests:  78%|███████▊  | 802/1024 [00:02<00:00, 354.82it/s]
Adding requests:  82%|████████▏ | 840/1024 [00:02<00:00, 361.21it/s]
Adding requests:  86%|████████▌ | 877/1024 [00:02<00:00, 361.27it/s]
Adding requests:  89%|████████▉ | 914/1024 [00:02<00:00, 361.47it/s]
Adding requests:  93%|█████████▎| 951/1024 [00:02<00:00, 358.03it/s]
Adding requests:  96%|█████████▋| 988/1024 [00:02<00:00, 360.31it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 354.70it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:00<00:00, 1470.49it/s, est. speed input: 1505872.62 toks/s, output: 1470.51 toks/s]
Processed prompts:  33%|███▎      | 334/1024 [00:02<00:05, 121.96it/s, est. speed input: 147486.72 toks/s, output: 144.03 toks/s]   
Processed prompts:  39%|███▉      | 399/1024 [00:03<00:06, 100.53it/s, est. speed input: 123584.31 toks/s, output: 120.69 toks/s]
Processed prompts:  43%|████▎     | 438/1024 [00:03<00:06, 91.40it/s, est. speed input: 114614.54 toks/s, output: 111.93 toks/s] 
Processed prompts:  45%|████▌     | 464/1024 [00:04<00:06, 87.66it/s, est. speed input: 110958.86 toks/s, output: 108.36 toks/s]
Processed prompts:  47%|████▋     | 483/1024 [00:04<00:06, 80.39it/s, est. speed input: 106393.05 toks/s, output: 103.90 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:04<00:06, 77.15it/s, est. speed input: 104150.57 toks/s, output: 101.71 toks/s]
Processed prompts:  50%|████▉     | 510/1024 [00:05<00:06, 79.18it/s, est. speed input: 104016.59 toks/s, output: 101.58 toks/s]
Processed prompts:  51%|█████     | 521/1024 [00:05<00:06, 80.40it/s, est. speed input: 103694.15 toks/s, output: 101.26 toks/s]
Processed prompts:  52%|█████▏    | 532/1024 [00:05<00:06, 70.47it/s, est. speed input: 100974.05 toks/s, output: 98.61 toks/s] 
Processed prompts:  53%|█████▎    | 541/1024 [00:05<00:06, 70.94it/s, est. speed input: 100401.54 toks/s, output: 98.05 toks/s]
Processed prompts:  54%|█████▎    | 550/1024 [00:05<00:06, 71.19it/s, est. speed input: 99819.10 toks/s, output: 97.48 toks/s] 
Processed prompts:  54%|█████▍    | 558/1024 [00:05<00:06, 69.82it/s, est. speed input: 99098.50 toks/s, output: 96.78 toks/s]
Processed prompts:  55%|█████▌    | 566/1024 [00:05<00:06, 68.80it/s, est. speed input: 98430.94 toks/s, output: 96.12 toks/s]
Processed prompts:  56%|█████▌    | 574/1024 [00:06<00:06, 67.90it/s, est. speed input: 97783.95 toks/s, output: 95.49 toks/s]
Processed prompts:  57%|█████▋    | 581/1024 [00:06<00:06, 64.68it/s, est. speed input: 96952.17 toks/s, output: 94.68 toks/s]
Processed prompts:  57%|█████▋    | 588/1024 [00:06<00:06, 62.49it/s, est. speed input: 96182.45 toks/s, output: 93.93 toks/s]
Processed prompts:  58%|█████▊    | 595/1024 [00:06<00:07, 61.25it/s, est. speed input: 95486.39 toks/s, output: 93.25 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:06<00:07, 59.99it/s, est. speed input: 94780.45 toks/s, output: 92.56 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:06<00:06, 61.40it/s, est. speed input: 94252.00 toks/s, output: 92.04 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:06<00:06, 62.48it/s, est. speed input: 93749.44 toks/s, output: 91.55 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:06<00:06, 63.30it/s, est. speed input: 93269.26 toks/s, output: 91.08 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:06<00:06, 64.01it/s, est. speed input: 92817.17 toks/s, output: 90.64 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:07<00:05, 64.87it/s, est. speed input: 92409.75 toks/s, output: 90.24 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:07<00:05, 64.88it/s, est. speed input: 91967.81 toks/s, output: 89.81 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:07<00:05, 65.04it/s, est. speed input: 91552.81 toks/s, output: 89.41 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:07<00:05, 65.22it/s, est. speed input: 91156.15 toks/s, output: 89.02 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:07<00:05, 64.81it/s, est. speed input: 90732.29 toks/s, output: 88.61 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:07<00:05, 64.74it/s, est. speed input: 90337.85 toks/s, output: 88.22 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:07<00:05, 64.91it/s, est. speed input: 89971.69 toks/s, output: 87.86 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:07<00:05, 64.74it/s, est. speed input: 89596.55 toks/s, output: 87.50 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:08<00:04, 65.04it/s, est. speed input: 89262.07 toks/s, output: 87.17 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:08<00:04, 65.09it/s, est. speed input: 88926.60 toks/s, output: 86.84 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:08<00:04, 64.77it/s, est. speed input: 88577.12 toks/s, output: 86.50 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:08<00:04, 64.53it/s, est. speed input: 88236.30 toks/s, output: 86.17 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:08<00:04, 64.74it/s, est. speed input: 87930.28 toks/s, output: 85.87 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:08<00:04, 64.84it/s, est. speed input: 87630.32 toks/s, output: 85.58 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:08<00:04, 65.00it/s, est. speed input: 87344.01 toks/s, output: 85.30 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:08<00:04, 64.76it/s, est. speed input: 87043.68 toks/s, output: 85.00 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:09<00:03, 65.18it/s, est. speed input: 86787.54 toks/s, output: 84.75 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:09<00:03, 65.15it/s, est. speed input: 86518.56 toks/s, output: 84.49 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:09<00:03, 66.27it/s, est. speed input: 86321.99 toks/s, output: 84.30 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:09<00:03, 65.93it/s, est. speed input: 86067.23 toks/s, output: 84.05 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:09<00:03, 65.59it/s, est. speed input: 85812.91 toks/s, output: 83.80 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:09<00:03, 65.83it/s, est. speed input: 85591.06 toks/s, output: 83.58 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:09<00:03, 65.27it/s, est. speed input: 85335.83 toks/s, output: 83.34 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:09<00:03, 65.39it/s, est. speed input: 85113.83 toks/s, output: 83.12 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:10<00:02, 65.39it/s, est. speed input: 84893.34 toks/s, output: 82.90 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:10<00:02, 65.37it/s, est. speed input: 84676.52 toks/s, output: 82.69 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:10<00:02, 65.05it/s, est. speed input: 84449.51 toks/s, output: 82.47 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:10<00:02, 65.08it/s, est. speed input: 84240.85 toks/s, output: 82.27 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:10<00:02, 65.13it/s, est. speed input: 84038.58 toks/s, output: 82.07 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:10<00:02, 65.08it/s, est. speed input: 83836.34 toks/s, output: 81.87 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:10<00:02, 65.09it/s, est. speed input: 83641.15 toks/s, output: 81.68 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:10<00:02, 64.88it/s, est. speed input: 83439.85 toks/s, output: 81.48 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:11<00:01, 65.19it/s, est. speed input: 83264.67 toks/s, output: 81.31 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:11<00:01, 65.68it/s, est. speed input: 83106.15 toks/s, output: 81.16 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:11<00:01, 65.11it/s, est. speed input: 82908.76 toks/s, output: 80.97 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:11<00:01, 65.06it/s, est. speed input: 82731.90 toks/s, output: 80.79 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:11<00:01, 65.25it/s, est. speed input: 82568.57 toks/s, output: 80.63 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:11<00:01, 65.41it/s, est. speed input: 82409.85 toks/s, output: 80.48 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:11<00:01, 65.49it/s, est. speed input: 82253.29 toks/s, output: 80.33 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:11<00:01, 65.55it/s, est. speed input: 82099.65 toks/s, output: 80.18 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:12<00:00, 65.42it/s, est. speed input: 81942.02 toks/s, output: 80.02 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:12<00:00, 65.21it/s, est. speed input: 81782.70 toks/s, output: 79.87 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:12<00:00, 65.19it/s, est. speed input: 81631.86 toks/s, output: 79.72 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:12<00:00, 64.97it/s, est. speed input: 81475.19 toks/s, output: 79.57 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:12<00:00, 64.67it/s, est. speed input: 81316.08 toks/s, output: 79.41 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:12<00:00, 64.53it/s, est. speed input: 81162.57 toks/s, output: 79.26 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:12<00:00, 65.44it/s, est. speed input: 81052.42 toks/s, output: 79.15 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:12<00:00, 66.17it/s, est. speed input: 80947.29 toks/s, output: 79.05 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 66.17it/s, est. speed input: 81423.51 toks/s, output: 79.52 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:12<00:00, 79.51it/s, est. speed input: 81423.51 toks/s, output: 79.52 toks/s]
[rank0]:[W126 17:52:22.736873525 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 17:52:24
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:52:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=870322) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=870322) WARNING 01-26 17:53:05 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=870322) WARNING 01-26 17:53:23 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 66.82 requests/s, 68488.35 total tokens/s, 66.82 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 17:52:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:52:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:52:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:52:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:52:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:52:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:52:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:52:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:52:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:52:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:52:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:52:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:52:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:52:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:52:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:52:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:52:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:52:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:52:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:52:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:52:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:52:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:52:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:52:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:52:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:52:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:52:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:52:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=870322) [2026-01-26 17:52:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=870322) [2026-01-26 17:52:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=870322) [2026-01-26 17:52:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=870322) [2026-01-26 17:52:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=870322) [2026-01-26 17:52:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=870322) [2026-01-26 17:52:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=870322) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=870322) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.47s/it]
(EngineCore_DP0 pid=870322) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.21it/s]
(EngineCore_DP0 pid=870322) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.05it/s]
(EngineCore_DP0 pid=870322) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.17s/it]
(EngineCore_DP0 pid=870322) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
(EngineCore_DP0 pid=870322) 
(EngineCore_DP0 pid=870322) [2026-01-26 17:52:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=870322) [2026-01-26 17:52:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36700160 bytes
(EngineCore_DP0 pid=870322) [2026-01-26 17:52:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=870322) [2026-01-26 17:52:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26214400 bytes
(EngineCore_DP0 pid=870322) [2026-01-26 17:52:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=870322) [2026-01-26 17:52:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 141557760 bytes
(EngineCore_DP0 pid=870322) [2026-01-26 17:52:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=870322) [2026-01-26 17:52:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70860800 bytes
(EngineCore_DP0 pid=870322) [rank0]:W0126 17:53:13.721000 870322 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=870322) [rank0]:W0126 17:53:13.791000 870322 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=870322) [rank0]:W0126 17:53:14.582000 870322 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=870322) [rank0]:W0126 17:53:14.691000 870322 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=870322) 2026-01-26 17:53:23,406 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=870322) 2026-01-26 17:53:23,471 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=870322) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  6.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  9.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  9.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.45it/s]
(EngineCore_DP0 pid=870322) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 10.93it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 11.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 11.05it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 25/2048 [00:00<00:08, 247.30it/s]
Adding requests:   3%|▎         | 58/2048 [00:00<00:06, 292.71it/s]
Adding requests:   4%|▍         | 91/2048 [00:00<00:06, 305.64it/s]
Adding requests:   6%|▌         | 125/2048 [00:00<00:06, 316.80it/s]
Adding requests:   8%|▊         | 160/2048 [00:00<00:05, 326.15it/s]
Adding requests:  10%|▉         | 197/2048 [00:00<00:05, 337.60it/s]
Adding requests:  11%|█▏        | 232/2048 [00:00<00:05, 338.70it/s]
Adding requests:  13%|█▎        | 266/2048 [00:00<00:05, 338.99it/s]
Adding requests:  15%|█▍        | 302/2048 [00:00<00:05, 343.73it/s]
Adding requests:  17%|█▋        | 339/2048 [00:01<00:04, 350.21it/s]
Adding requests:  18%|█▊        | 376/2048 [00:01<00:04, 354.36it/s]
Adding requests:  20%|██        | 413/2048 [00:01<00:04, 358.38it/s]
Adding requests:  22%|██▏       | 449/2048 [00:01<00:04, 354.91it/s]
Adding requests:  24%|██▍       | 489/2048 [00:01<00:04, 367.69it/s]
Adding requests:  26%|██▌       | 529/2048 [00:01<00:04, 375.70it/s]
Adding requests:  28%|██▊       | 567/2048 [00:01<00:03, 372.63it/s]
Adding requests:  30%|██▉       | 605/2048 [00:01<00:04, 357.26it/s]
Adding requests:  31%|███▏      | 641/2048 [00:01<00:04, 345.92it/s]
Adding requests:  33%|███▎      | 676/2048 [00:01<00:03, 343.35it/s]
Adding requests:  35%|███▍      | 713/2048 [00:02<00:03, 350.34it/s]
Adding requests:  37%|███▋      | 749/2048 [00:02<00:03, 344.26it/s]
Adding requests:  38%|███▊      | 786/2048 [00:02<00:03, 350.02it/s]
Adding requests:  40%|████      | 822/2048 [00:02<00:03, 350.11it/s]
Adding requests:  42%|████▏     | 859/2048 [00:02<00:03, 355.28it/s]
Adding requests:  44%|████▍     | 896/2048 [00:02<00:03, 358.65it/s]
Adding requests:  46%|████▌     | 932/2048 [00:02<00:03, 350.47it/s]
Adding requests:  47%|████▋     | 969/2048 [00:02<00:03, 354.82it/s]
Adding requests:  49%|████▉     | 1005/2048 [00:02<00:02, 348.34it/s]
Adding requests:  51%|█████     | 1041/2048 [00:02<00:02, 350.67it/s]
Adding requests:  53%|█████▎    | 1077/2048 [00:03<00:02, 348.20it/s]
Adding requests:  54%|█████▍    | 1112/2048 [00:03<00:02, 348.18it/s]
Adding requests:  56%|█████▌    | 1149/2048 [00:03<00:02, 351.35it/s]
Adding requests:  58%|█████▊    | 1185/2048 [00:03<00:02, 351.52it/s]
Adding requests:  60%|█████▉    | 1223/2048 [00:03<00:02, 358.07it/s]
Adding requests:  61%|██████▏   | 1259/2048 [00:03<00:02, 353.48it/s]
Adding requests:  63%|██████▎   | 1295/2048 [00:03<00:02, 349.68it/s]
Adding requests:  65%|██████▍   | 1331/2048 [00:03<00:02, 351.75it/s]
Adding requests:  67%|██████▋   | 1368/2048 [00:03<00:01, 356.64it/s]
Adding requests:  69%|██████▊   | 1404/2048 [00:04<00:01, 354.54it/s]
Adding requests:  70%|███████   | 1440/2048 [00:04<00:01, 339.61it/s]
Adding requests:  72%|███████▏  | 1475/2048 [00:04<00:01, 331.12it/s]
Adding requests:  74%|███████▍  | 1511/2048 [00:04<00:01, 337.83it/s]
Adding requests:  75%|███████▌  | 1545/2048 [00:04<00:01, 338.17it/s]
Adding requests:  77%|███████▋  | 1579/2048 [00:04<00:01, 333.46it/s]
Adding requests:  79%|███████▉  | 1614/2048 [00:04<00:01, 336.38it/s]
Adding requests:  80%|████████  | 1648/2048 [00:04<00:01, 330.42it/s]
Adding requests:  82%|████████▏ | 1682/2048 [00:04<00:01, 332.85it/s]
Adding requests:  84%|████████▍ | 1718/2048 [00:04<00:00, 339.86it/s]
Adding requests:  86%|████████▌ | 1754/2048 [00:05<00:00, 344.63it/s]
Adding requests:  87%|████████▋ | 1789/2048 [00:05<00:00, 337.02it/s]
Adding requests:  89%|████████▉ | 1825/2048 [00:05<00:00, 341.83it/s]
Adding requests:  91%|█████████ | 1861/2048 [00:05<00:00, 345.50it/s]
Adding requests:  93%|█████████▎| 1897/2048 [00:05<00:00, 346.37it/s]
Adding requests:  94%|█████████▍| 1935/2048 [00:05<00:00, 354.68it/s]
Adding requests:  96%|█████████▌| 1971/2048 [00:05<00:00, 355.75it/s]
Adding requests:  98%|█████████▊| 2007/2048 [00:05<00:00, 351.45it/s]
Adding requests: 100%|█████████▉| 2043/2048 [00:05<00:00, 347.10it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 346.11it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:00<00:00, 2555.69it/s, est. speed input: 2617175.03 toks/s, output: 2555.73 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:03<00:10, 134.22it/s, est. speed input: 165778.09 toks/s, output: 161.89 toks/s]   
Processed prompts:  37%|███▋      | 752/2048 [00:05<00:11, 114.05it/s, est. speed input: 142197.14 toks/s, output: 138.86 toks/s]
Processed prompts:  40%|███▉      | 815/2048 [00:06<00:12, 102.55it/s, est. speed input: 131243.05 toks/s, output: 128.17 toks/s]
Processed prompts:  42%|████▏     | 856/2048 [00:07<00:12, 93.07it/s, est. speed input: 123870.37 toks/s, output: 120.97 toks/s] 
Processed prompts:  43%|████▎     | 885/2048 [00:07<00:13, 87.61it/s, est. speed input: 119920.95 toks/s, output: 117.11 toks/s]
Processed prompts:  44%|████▍     | 906/2048 [00:07<00:13, 87.71it/s, est. speed input: 119030.70 toks/s, output: 116.24 toks/s]
Processed prompts:  45%|████▌     | 923/2048 [00:08<00:13, 85.57it/s, est. speed input: 117669.57 toks/s, output: 114.91 toks/s]
Processed prompts:  46%|████▌     | 937/2048 [00:08<00:13, 81.14it/s, est. speed input: 115960.15 toks/s, output: 113.24 toks/s]
Processed prompts:  46%|████▋     | 949/2048 [00:08<00:14, 75.29it/s, est. speed input: 114110.81 toks/s, output: 111.44 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:08<00:15, 70.89it/s, est. speed input: 112506.12 toks/s, output: 109.87 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:08<00:15, 70.06it/s, est. speed input: 111351.18 toks/s, output: 108.74 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:09<00:15, 68.91it/s, est. speed input: 110178.87 toks/s, output: 107.60 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:09<00:15, 68.19it/s, est. speed input: 109095.64 toks/s, output: 106.54 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:09<00:15, 67.59it/s, est. speed input: 108060.14 toks/s, output: 105.53 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:09<00:14, 67.17it/s, est. speed input: 107079.76 toks/s, output: 104.57 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:10<00:14, 67.21it/s, est. speed input: 106190.32 toks/s, output: 103.70 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:10<00:14, 66.87it/s, est. speed input: 105295.53 toks/s, output: 102.83 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:10<00:14, 66.91it/s, est. speed input: 104475.49 toks/s, output: 102.03 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:10<00:14, 66.59it/s, est. speed input: 103651.43 toks/s, output: 101.22 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:11<00:13, 66.46it/s, est. speed input: 102873.31 toks/s, output: 100.46 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:11<00:13, 66.74it/s, est. speed input: 102169.16 toks/s, output: 99.77 toks/s] 
Processed prompts:  56%|█████▋    | 1154/2048 [00:11<00:13, 66.49it/s, est. speed input: 101446.71 toks/s, output: 99.07 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:11<00:13, 66.44it/s, est. speed input: 100766.23 toks/s, output: 98.40 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:12<00:12, 66.60it/s, est. speed input: 100132.19 toks/s, output: 97.79 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:12<00:12, 67.74it/s, est. speed input: 99621.09 toks/s, output: 97.29 toks/s] 
Processed prompts:  59%|█████▉    | 1218/2048 [00:12<00:12, 67.55it/s, est. speed input: 99035.55 toks/s, output: 96.71 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:12<00:12, 67.06it/s, est. speed input: 98439.96 toks/s, output: 96.13 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:13<00:11, 67.18it/s, est. speed input: 97907.57 toks/s, output: 95.61 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:13<00:11, 67.52it/s, est. speed input: 97415.80 toks/s, output: 95.13 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:13<00:11, 67.03it/s, est. speed input: 96879.53 toks/s, output: 94.61 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:13<00:11, 66.85it/s, est. speed input: 96376.19 toks/s, output: 94.12 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:14<00:10, 66.84it/s, est. speed input: 95899.11 toks/s, output: 93.65 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:14<00:10, 66.60it/s, est. speed input: 95419.30 toks/s, output: 93.18 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:14<00:10, 66.91it/s, est. speed input: 94993.06 toks/s, output: 92.77 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:14<00:10, 66.55it/s, est. speed input: 94535.83 toks/s, output: 92.32 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:14<00:10, 66.60it/s, est. speed input: 94116.86 toks/s, output: 91.91 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:15<00:09, 66.54it/s, est. speed input: 93703.24 toks/s, output: 91.51 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:15<00:09, 66.53it/s, est. speed input: 93305.53 toks/s, output: 91.12 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:15<00:09, 66.38it/s, est. speed input: 92909.30 toks/s, output: 90.73 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:15<00:09, 66.46it/s, est. speed input: 92538.25 toks/s, output: 90.37 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:16<00:08, 67.50it/s, est. speed input: 92244.58 toks/s, output: 90.08 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:16<00:08, 67.60it/s, est. speed input: 91917.56 toks/s, output: 89.76 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:16<00:08, 67.08it/s, est. speed input: 91561.58 toks/s, output: 89.42 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:16<00:08, 66.90it/s, est. speed input: 91227.38 toks/s, output: 89.09 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:17<00:07, 67.74it/s, est. speed input: 90962.57 toks/s, output: 88.83 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:17<00:07, 67.17it/s, est. speed input: 90634.20 toks/s, output: 88.51 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:17<00:07, 68.12it/s, est. speed input: 90395.39 toks/s, output: 88.28 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:17<00:07, 67.55it/s, est. speed input: 90090.37 toks/s, output: 87.98 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:18<00:06, 67.35it/s, est. speed input: 89804.89 toks/s, output: 87.70 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:18<00:06, 66.79it/s, est. speed input: 89502.10 toks/s, output: 87.40 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:18<00:06, 68.03it/s, est. speed input: 89299.78 toks/s, output: 87.21 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:18<00:06, 67.65it/s, est. speed input: 89033.17 toks/s, output: 86.95 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:19<00:05, 67.28it/s, est. speed input: 88767.12 toks/s, output: 86.69 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:19<00:05, 67.10it/s, est. speed input: 88511.98 toks/s, output: 86.44 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:19<00:05, 67.03it/s, est. speed input: 88266.61 toks/s, output: 86.20 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:19<00:05, 67.38it/s, est. speed input: 88047.76 toks/s, output: 85.98 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:19<00:04, 67.03it/s, est. speed input: 87803.12 toks/s, output: 85.75 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:20<00:04, 68.28it/s, est. speed input: 87640.34 toks/s, output: 85.59 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:20<00:04, 68.84it/s, est. speed input: 87464.08 toks/s, output: 85.41 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:20<00:04, 67.96it/s, est. speed input: 87230.94 toks/s, output: 85.19 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:20<00:03, 67.58it/s, est. speed input: 87014.03 toks/s, output: 84.97 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:21<00:03, 67.52it/s, est. speed input: 86811.94 toks/s, output: 84.78 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:21<00:03, 67.24it/s, est. speed input: 86602.79 toks/s, output: 84.57 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:21<00:03, 66.97it/s, est. speed input: 86394.90 toks/s, output: 84.37 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:21<00:03, 66.82it/s, est. speed input: 86193.45 toks/s, output: 84.17 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:22<00:02, 66.59it/s, est. speed input: 85990.14 toks/s, output: 83.97 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:22<00:02, 66.48it/s, est. speed input: 85793.85 toks/s, output: 83.78 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:22<00:02, 67.54it/s, est. speed input: 85653.03 toks/s, output: 83.65 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:22<00:02, 67.44it/s, est. speed input: 85477.68 toks/s, output: 83.47 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:23<00:01, 67.15it/s, est. speed input: 85295.98 toks/s, output: 83.30 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:23<00:01, 67.01it/s, est. speed input: 85120.91 toks/s, output: 83.13 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:23<00:01, 66.85it/s, est. speed input: 84946.82 toks/s, output: 82.96 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:23<00:01, 67.06it/s, est. speed input: 84789.81 toks/s, output: 82.80 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:24<00:00, 68.04it/s, est. speed input: 84670.25 toks/s, output: 82.69 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:24<00:00, 67.64it/s, est. speed input: 84508.60 toks/s, output: 82.53 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:24<00:00, 66.98it/s, est. speed input: 84334.65 toks/s, output: 82.36 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:24<00:00, 67.86it/s, est. speed input: 84217.61 toks/s, output: 82.24 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:24<00:00, 67.86it/s, est. speed input: 84796.53 toks/s, output: 82.81 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:24<00:00, 82.81it/s, est. speed input: 84796.53 toks/s, output: 82.81 toks/s]
[rank0]:[W126 17:53:57.565725118 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 17:53:59
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:54:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=872334) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=872334) WARNING 01-26 17:54:52 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=872334) WARNING 01-26 17:55:11 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 68.44 requests/s, 70147.48 total tokens/s, 68.44 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 17:54:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:54:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:54:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:54:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:54:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:54:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:54:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:54:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:54:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:54:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:54:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:54:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:54:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:54:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:54:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:54:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:54:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:54:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:54:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:54:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:54:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:54:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:54:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:54:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:54:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:54:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:54:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:54:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=872334) [2026-01-26 17:54:38] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=872334) [2026-01-26 17:54:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=872334) [2026-01-26 17:54:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=872334) [2026-01-26 17:54:38] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=872334) [2026-01-26 17:54:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=872334) [2026-01-26 17:54:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=872334) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=872334) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.45s/it]
(EngineCore_DP0 pid=872334) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.22it/s]
(EngineCore_DP0 pid=872334) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.06it/s]
(EngineCore_DP0 pid=872334) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.16s/it]
(EngineCore_DP0 pid=872334) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.10s/it]
(EngineCore_DP0 pid=872334) 
(EngineCore_DP0 pid=872334) [2026-01-26 17:54:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=872334) [2026-01-26 17:54:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36700160 bytes
(EngineCore_DP0 pid=872334) [2026-01-26 17:54:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=872334) [2026-01-26 17:54:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26214400 bytes
(EngineCore_DP0 pid=872334) [2026-01-26 17:54:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=872334) [2026-01-26 17:54:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 141557760 bytes
(EngineCore_DP0 pid=872334) [2026-01-26 17:54:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=872334) [2026-01-26 17:54:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70860800 bytes
(EngineCore_DP0 pid=872334) [rank0]:W0126 17:55:00.597000 872334 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=872334) [rank0]:W0126 17:55:00.665000 872334 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=872334) [rank0]:W0126 17:55:01.594000 872334 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=872334) [rank0]:W0126 17:55:01.701000 872334 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=872334) 2026-01-26 17:55:11,103 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=872334) 2026-01-26 17:55:11,227 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=872334) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00,  9.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  9.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  9.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  9.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  9.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  9.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  9.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.43it/s]
(EngineCore_DP0 pid=872334) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 10.91it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 11.09it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 11.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 11.17it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 275.70it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 319.03it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 316.57it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:12, 321.94it/s]
Adding requests:   4%|▍         | 165/4096 [00:00<00:11, 331.96it/s]
Adding requests:   5%|▍         | 202/4096 [00:00<00:11, 341.96it/s]
Adding requests:   6%|▌         | 237/4096 [00:00<00:11, 339.64it/s]
Adding requests:   7%|▋         | 272/4096 [00:00<00:11, 339.70it/s]
Adding requests:   8%|▊         | 309/4096 [00:00<00:10, 346.59it/s]
Adding requests:   8%|▊         | 347/4096 [00:01<00:10, 353.93it/s]
Adding requests:   9%|▉         | 384/4096 [00:01<00:10, 358.03it/s]
Adding requests:  10%|█         | 423/4096 [00:01<00:10, 365.64it/s]
Adding requests:  11%|█         | 460/4096 [00:01<00:10, 362.05it/s]
Adding requests:  12%|█▏        | 500/4096 [00:01<00:09, 371.59it/s]
Adding requests:  13%|█▎        | 539/4096 [00:01<00:09, 376.43it/s]
Adding requests:  14%|█▍        | 577/4096 [00:01<00:09, 375.63it/s]
Adding requests:  15%|█▌        | 615/4096 [00:01<00:09, 364.33it/s]
Adding requests:  16%|█▌        | 652/4096 [00:01<00:09, 358.28it/s]
Adding requests:  17%|█▋        | 690/4096 [00:01<00:09, 363.18it/s]
Adding requests:  18%|█▊        | 727/4096 [00:02<00:09, 356.16it/s]
Adding requests:  19%|█▊        | 763/4096 [00:02<00:09, 355.83it/s]
Adding requests:  20%|█▉        | 799/4096 [00:02<00:09, 356.71it/s]
Adding requests:  20%|██        | 837/4096 [00:02<00:09, 360.63it/s]
Adding requests:  21%|██▏       | 875/4096 [00:02<00:08, 363.04it/s]
Adding requests:  22%|██▏       | 912/4096 [00:02<00:08, 361.97it/s]
Adding requests:  23%|██▎       | 949/4096 [00:02<00:08, 356.53it/s]
Adding requests:  24%|██▍       | 986/4096 [00:02<00:08, 359.41it/s]
Adding requests:  25%|██▍       | 1022/4096 [00:02<00:08, 349.20it/s]
Adding requests:  26%|██▌       | 1057/4096 [00:02<00:08, 348.66it/s]
Adding requests:  27%|██▋       | 1092/4096 [00:03<00:08, 348.51it/s]
Adding requests:  28%|██▊       | 1130/4096 [00:03<00:08, 355.66it/s]
Adding requests:  28%|██▊       | 1166/4096 [00:03<00:08, 353.11it/s]
Adding requests:  29%|██▉       | 1202/4096 [00:03<00:08, 352.93it/s]
Adding requests:  30%|███       | 1240/4096 [00:03<00:07, 359.08it/s]
Adding requests:  31%|███       | 1276/4096 [00:03<00:07, 354.08it/s]
Adding requests:  32%|███▏      | 1312/4096 [00:03<00:07, 352.85it/s]
Adding requests:  33%|███▎      | 1350/4096 [00:03<00:07, 359.98it/s]
Adding requests:  34%|███▍      | 1387/4096 [00:03<00:07, 358.91it/s]
Adding requests:  35%|███▍      | 1423/4096 [00:04<00:07, 355.88it/s]
Adding requests:  36%|███▌      | 1460/4096 [00:04<00:07, 356.95it/s]
Adding requests:  37%|███▋      | 1498/4096 [00:04<00:07, 362.12it/s]
Adding requests:  37%|███▋      | 1535/4096 [00:04<00:07, 361.00it/s]
Adding requests:  38%|███▊      | 1572/4096 [00:04<00:07, 353.99it/s]
Adding requests:  39%|███▉      | 1608/4096 [00:04<00:07, 355.01it/s]
Adding requests:  40%|████      | 1644/4096 [00:04<00:07, 345.80it/s]
Adding requests:  41%|████      | 1679/4096 [00:04<00:07, 344.01it/s]
Adding requests:  42%|████▏     | 1716/4096 [00:04<00:06, 350.78it/s]
Adding requests:  43%|████▎     | 1752/4096 [00:04<00:06, 342.99it/s]
Adding requests:  44%|████▎     | 1788/4096 [00:05<00:06, 345.41it/s]
Adding requests:  45%|████▍     | 1824/4096 [00:05<00:06, 348.40it/s]
Adding requests:  45%|████▌     | 1860/4096 [00:05<00:06, 351.33it/s]
Adding requests:  46%|████▋     | 1896/4096 [00:05<00:06, 352.77it/s]
Adding requests:  47%|████▋     | 1934/4096 [00:05<00:05, 360.45it/s]
Adding requests:  48%|████▊     | 1971/4096 [00:05<00:05, 360.68it/s]
Adding requests:  49%|████▉     | 2008/4096 [00:05<00:05, 358.01it/s]
Adding requests:  50%|████▉     | 2044/4096 [00:05<00:05, 353.01it/s]
Adding requests:  51%|█████     | 2080/4096 [00:05<00:05, 343.64it/s]
Adding requests:  52%|█████▏    | 2118/4096 [00:05<00:05, 352.45it/s]
Adding requests:  53%|█████▎    | 2154/4096 [00:06<00:05, 351.09it/s]
Adding requests:  53%|█████▎    | 2190/4096 [00:06<00:05, 336.47it/s]
Adding requests:  54%|█████▍    | 2225/4096 [00:06<00:05, 340.12it/s]
Adding requests:  55%|█████▌    | 2262/4096 [00:06<00:05, 345.63it/s]
Adding requests:  56%|█████▌    | 2300/4096 [00:06<00:05, 354.99it/s]
Adding requests:  57%|█████▋    | 2337/4096 [00:06<00:04, 358.22it/s]
Adding requests:  58%|█████▊    | 2374/4096 [00:06<00:04, 358.71it/s]
Adding requests:  59%|█████▉    | 2412/4096 [00:06<00:04, 364.26it/s]
Adding requests:  60%|█████▉    | 2449/4096 [00:06<00:04, 363.88it/s]
Adding requests:  61%|██████    | 2486/4096 [00:07<00:04, 364.05it/s]
Adding requests:  62%|██████▏   | 2523/4096 [00:07<00:04, 361.58it/s]
Adding requests:  63%|██████▎   | 2563/4096 [00:07<00:04, 372.67it/s]
Adding requests:  64%|██████▎   | 2601/4096 [00:07<00:03, 374.61it/s]
Adding requests:  64%|██████▍   | 2639/4096 [00:07<00:04, 361.37it/s]
Adding requests:  65%|██████▌   | 2676/4096 [00:07<00:03, 360.96it/s]
Adding requests:  66%|██████▌   | 2713/4096 [00:07<00:03, 355.84it/s]
Adding requests:  67%|██████▋   | 2750/4096 [00:07<00:03, 359.53it/s]
Adding requests:  68%|██████▊   | 2789/4096 [00:07<00:03, 366.67it/s]
Adding requests:  69%|██████▉   | 2827/4096 [00:07<00:03, 367.93it/s]
Adding requests:  70%|██████▉   | 2864/4096 [00:08<00:03, 366.54it/s]
Adding requests:  71%|███████   | 2901/4096 [00:08<00:03, 361.76it/s]
Adding requests:  72%|███████▏  | 2938/4096 [00:08<00:03, 355.70it/s]
Adding requests:  73%|███████▎  | 2974/4096 [00:08<00:03, 353.55it/s]
Adding requests:  74%|███████▎  | 3013/4096 [00:08<00:02, 361.87it/s]
Adding requests:  74%|███████▍  | 3050/4096 [00:08<00:02, 363.56it/s]
Adding requests:  75%|███████▌  | 3088/4096 [00:08<00:02, 368.33it/s]
Adding requests:  76%|███████▋  | 3127/4096 [00:08<00:02, 373.64it/s]
Adding requests:  77%|███████▋  | 3165/4096 [00:08<00:02, 369.21it/s]
Adding requests:  78%|███████▊  | 3202/4096 [00:08<00:02, 360.78it/s]
Adding requests:  79%|███████▉  | 3241/4096 [00:09<00:02, 366.13it/s]
Adding requests:  80%|████████  | 3278/4096 [00:09<00:02, 359.69it/s]
Adding requests:  81%|████████  | 3315/4096 [00:09<00:02, 353.52it/s]
Adding requests:  82%|████████▏ | 3352/4096 [00:09<00:02, 355.02it/s]
Adding requests:  83%|████████▎ | 3390/4096 [00:09<00:01, 359.27it/s]
Adding requests:  84%|████████▎ | 3426/4096 [00:09<00:01, 357.57it/s]
Adding requests:  85%|████████▍ | 3463/4096 [00:09<00:01, 360.87it/s]
Adding requests:  85%|████████▌ | 3500/4096 [00:09<00:01, 349.14it/s]
Adding requests:  86%|████████▋ | 3540/4096 [00:09<00:01, 361.41it/s]
Adding requests:  87%|████████▋ | 3577/4096 [00:10<00:01, 361.13it/s]
Adding requests:  88%|████████▊ | 3614/4096 [00:10<00:01, 362.93it/s]
Adding requests:  89%|████████▉ | 3651/4096 [00:10<00:01, 363.99it/s]
Adding requests:  90%|█████████ | 3688/4096 [00:10<00:01, 355.90it/s]
Adding requests:  91%|█████████ | 3725/4096 [00:10<00:01, 358.79it/s]
Adding requests:  92%|█████████▏| 3761/4096 [00:10<00:00, 352.41it/s]
Adding requests:  93%|█████████▎| 3797/4096 [00:10<00:00, 342.95it/s]
Adding requests:  94%|█████████▎| 3832/4096 [00:10<00:00, 341.08it/s]
Adding requests:  94%|█████████▍| 3869/4096 [00:10<00:00, 349.06it/s]
Adding requests:  95%|█████████▌| 3904/4096 [00:10<00:00, 345.78it/s]
Adding requests:  96%|█████████▌| 3939/4096 [00:11<00:00, 344.16it/s]
Adding requests:  97%|█████████▋| 3974/4096 [00:11<00:00, 344.84it/s]
Adding requests:  98%|█████████▊| 4010/4096 [00:11<00:00, 348.21it/s]
Adding requests:  99%|█████████▉| 4045/4096 [00:11<00:00, 346.39it/s]
Adding requests: 100%|█████████▉| 4080/4096 [00:11<00:00, 346.89it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 355.11it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▊        | 767/4096 [00:00<00:01, 3221.19it/s, est. speed input: 3298617.18 toks/s, output: 3221.22 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:04<00:16, 177.25it/s, est. speed input: 226735.54 toks/s, output: 221.42 toks/s]  
Processed prompts:  30%|██▉       | 1227/4096 [00:06<00:20, 140.48it/s, est. speed input: 185125.44 toks/s, output: 180.79 toks/s]
Processed prompts:  32%|███▏      | 1305/4096 [00:07<00:21, 129.00it/s, est. speed input: 173113.45 toks/s, output: 169.06 toks/s]
Processed prompts:  33%|███▎      | 1356/4096 [00:08<00:24, 112.11it/s, est. speed input: 160322.93 toks/s, output: 156.57 toks/s]
Processed prompts:  34%|███▍      | 1391/4096 [00:09<00:25, 106.99it/s, est. speed input: 156048.18 toks/s, output: 152.39 toks/s]
Processed prompts:  35%|███▍      | 1417/4096 [00:09<00:27, 98.54it/s, est. speed input: 151229.04 toks/s, output: 147.68 toks/s] 
Processed prompts:  35%|███▌      | 1439/4096 [00:10<00:29, 89.32it/s, est. speed input: 146675.01 toks/s, output: 143.24 toks/s]
Processed prompts:  36%|███▌      | 1471/4096 [00:10<00:31, 84.63it/s, est. speed input: 143200.56 toks/s, output: 139.84 toks/s]
Processed prompts:  37%|███▋      | 1503/4096 [00:10<00:31, 81.39it/s, est. speed input: 140262.97 toks/s, output: 136.98 toks/s]
Processed prompts:  37%|███▋      | 1535/4096 [00:11<00:32, 78.14it/s, est. speed input: 137400.58 toks/s, output: 134.18 toks/s]
Processed prompts:  38%|███▊      | 1567/4096 [00:11<00:33, 76.11it/s, est. speed input: 134898.45 toks/s, output: 131.74 toks/s]
Processed prompts:  39%|███▉      | 1599/4096 [00:12<00:33, 74.51it/s, est. speed input: 132571.83 toks/s, output: 129.46 toks/s]
Processed prompts:  40%|███▉      | 1631/4096 [00:12<00:33, 72.82it/s, est. speed input: 130300.19 toks/s, output: 127.25 toks/s]
Processed prompts:  41%|████      | 1663/4096 [00:13<00:33, 71.60it/s, est. speed input: 128190.91 toks/s, output: 125.19 toks/s]
Processed prompts:  41%|████▏     | 1695/4096 [00:13<00:34, 70.61it/s, est. speed input: 126200.95 toks/s, output: 123.24 toks/s]
Processed prompts:  42%|████▏     | 1727/4096 [00:14<00:33, 70.59it/s, est. speed input: 124477.67 toks/s, output: 121.56 toks/s]
Processed prompts:  43%|████▎     | 1759/4096 [00:14<00:33, 70.37it/s, est. speed input: 122823.61 toks/s, output: 119.94 toks/s]
Processed prompts:  44%|████▎     | 1791/4096 [00:15<00:33, 69.62it/s, est. speed input: 121163.50 toks/s, output: 118.32 toks/s]
Processed prompts:  45%|████▍     | 1823/4096 [00:15<00:32, 69.09it/s, est. speed input: 119602.24 toks/s, output: 116.80 toks/s]
Processed prompts:  45%|████▌     | 1855/4096 [00:16<00:32, 68.85it/s, est. speed input: 118153.39 toks/s, output: 115.38 toks/s]
Processed prompts:  46%|████▌     | 1887/4096 [00:16<00:31, 69.27it/s, est. speed input: 116881.45 toks/s, output: 114.14 toks/s]
Processed prompts:  47%|████▋     | 1919/4096 [00:17<00:31, 68.91it/s, est. speed input: 115577.30 toks/s, output: 112.87 toks/s]
Processed prompts:  48%|████▊     | 1951/4096 [00:17<00:31, 68.75it/s, est. speed input: 114357.69 toks/s, output: 111.68 toks/s]
Processed prompts:  48%|████▊     | 1983/4096 [00:17<00:30, 69.17it/s, est. speed input: 113276.46 toks/s, output: 110.62 toks/s]
Processed prompts:  49%|████▉     | 2015/4096 [00:18<00:30, 69.04it/s, est. speed input: 112190.37 toks/s, output: 109.56 toks/s]
Processed prompts:  50%|████▉     | 2047/4096 [00:18<00:29, 69.28it/s, est. speed input: 111201.77 toks/s, output: 108.60 toks/s]
Processed prompts:  51%|█████     | 2079/4096 [00:19<00:29, 69.00it/s, est. speed input: 110203.20 toks/s, output: 107.62 toks/s]
Processed prompts:  52%|█████▏    | 2111/4096 [00:19<00:28, 68.79it/s, est. speed input: 109249.99 toks/s, output: 106.69 toks/s]
Processed prompts:  52%|█████▏    | 2143/4096 [00:20<00:28, 68.52it/s, est. speed input: 108325.52 toks/s, output: 105.79 toks/s]
Processed prompts:  53%|█████▎    | 2175/4096 [00:20<00:27, 69.13it/s, est. speed input: 107536.58 toks/s, output: 105.02 toks/s]
Processed prompts:  54%|█████▍    | 2207/4096 [00:21<00:27, 68.88it/s, est. speed input: 106704.88 toks/s, output: 104.20 toks/s]
Processed prompts:  55%|█████▍    | 2239/4096 [00:21<00:26, 68.80it/s, est. speed input: 105919.75 toks/s, output: 103.44 toks/s]
Processed prompts:  55%|█████▌    | 2271/4096 [00:22<00:26, 68.83it/s, est. speed input: 105177.07 toks/s, output: 102.71 toks/s]
Processed prompts:  56%|█████▌    | 2303/4096 [00:22<00:26, 68.75it/s, est. speed input: 104454.69 toks/s, output: 102.01 toks/s]
Processed prompts:  57%|█████▋    | 2335/4096 [00:23<00:25, 68.73it/s, est. speed input: 103764.03 toks/s, output: 101.33 toks/s]
Processed prompts:  58%|█████▊    | 2367/4096 [00:23<00:25, 68.58it/s, est. speed input: 103088.39 toks/s, output: 100.67 toks/s]
Processed prompts:  59%|█████▊    | 2399/4096 [00:23<00:24, 68.50it/s, est. speed input: 102441.13 toks/s, output: 100.04 toks/s]
Processed prompts:  59%|█████▉    | 2431/4096 [00:24<00:24, 68.49it/s, est. speed input: 101823.33 toks/s, output: 99.44 toks/s] 
Processed prompts:  60%|██████    | 2463/4096 [00:24<00:23, 68.36it/s, est. speed input: 101217.13 toks/s, output: 98.84 toks/s]
Processed prompts:  61%|██████    | 2495/4096 [00:25<00:23, 68.32it/s, est. speed input: 100637.69 toks/s, output: 98.28 toks/s]
Processed prompts:  62%|██████▏   | 2527/4096 [00:25<00:22, 68.85it/s, est. speed input: 100128.38 toks/s, output: 97.78 toks/s]
Processed prompts:  62%|██████▏   | 2559/4096 [00:26<00:22, 68.64it/s, est. speed input: 99586.92 toks/s, output: 97.25 toks/s] 
Processed prompts:  63%|██████▎   | 2591/4096 [00:26<00:21, 69.15it/s, est. speed input: 99119.19 toks/s, output: 96.80 toks/s]
Processed prompts:  64%|██████▍   | 2623/4096 [00:27<00:21, 68.95it/s, est. speed input: 98621.77 toks/s, output: 96.31 toks/s]
Processed prompts:  65%|██████▍   | 2655/4096 [00:27<00:20, 68.73it/s, est. speed input: 98135.05 toks/s, output: 95.83 toks/s]
Processed prompts:  66%|██████▌   | 2687/4096 [00:28<00:20, 68.66it/s, est. speed input: 97670.41 toks/s, output: 95.38 toks/s]
Processed prompts:  66%|██████▋   | 2719/4096 [00:28<00:20, 68.49it/s, est. speed input: 97212.09 toks/s, output: 94.93 toks/s]
Processed prompts:  67%|██████▋   | 2751/4096 [00:29<00:19, 69.08it/s, est. speed input: 96821.57 toks/s, output: 94.55 toks/s]
Processed prompts:  68%|██████▊   | 2783/4096 [00:29<00:19, 68.87it/s, est. speed input: 96397.64 toks/s, output: 94.14 toks/s]
Processed prompts:  69%|██████▊   | 2815/4096 [00:30<00:18, 68.76it/s, est. speed input: 95989.41 toks/s, output: 93.74 toks/s]
Processed prompts:  70%|██████▉   | 2847/4096 [00:30<00:18, 68.53it/s, est. speed input: 95582.78 toks/s, output: 93.34 toks/s]
Processed prompts:  70%|███████   | 2879/4096 [00:30<00:17, 68.47it/s, est. speed input: 95195.59 toks/s, output: 92.96 toks/s]
Processed prompts:  71%|███████   | 2911/4096 [00:31<00:17, 69.23it/s, est. speed input: 94874.16 toks/s, output: 92.65 toks/s]
Processed prompts:  72%|███████▏  | 2943/4096 [00:31<00:16, 68.92it/s, est. speed input: 94505.62 toks/s, output: 92.29 toks/s]
Processed prompts:  73%|███████▎  | 2975/4096 [00:32<00:16, 68.67it/s, est. speed input: 94146.26 toks/s, output: 91.94 toks/s]
Processed prompts:  73%|███████▎  | 3007/4096 [00:32<00:15, 68.69it/s, est. speed input: 93809.11 toks/s, output: 91.61 toks/s]
Processed prompts:  74%|███████▍  | 3039/4096 [00:33<00:15, 68.63it/s, est. speed input: 93476.49 toks/s, output: 91.29 toks/s]
Processed prompts:  75%|███████▍  | 3071/4096 [00:33<00:14, 68.62it/s, est. speed input: 93155.94 toks/s, output: 90.97 toks/s]
Processed prompts:  76%|███████▌  | 3103/4096 [00:34<00:14, 68.64it/s, est. speed input: 92845.18 toks/s, output: 90.67 toks/s]
Processed prompts:  77%|███████▋  | 3135/4096 [00:34<00:14, 68.41it/s, est. speed input: 92528.19 toks/s, output: 90.36 toks/s]
Processed prompts:  77%|███████▋  | 3167/4096 [00:35<00:13, 68.31it/s, est. speed input: 92222.81 toks/s, output: 90.06 toks/s]
Processed prompts:  78%|███████▊  | 3199/4096 [00:35<00:13, 68.18it/s, est. speed input: 91922.66 toks/s, output: 89.77 toks/s]
Processed prompts:  79%|███████▉  | 3231/4096 [00:36<00:12, 68.15it/s, est. speed input: 91633.52 toks/s, output: 89.49 toks/s]
Processed prompts:  80%|███████▉  | 3263/4096 [00:36<00:12, 68.23it/s, est. speed input: 91357.43 toks/s, output: 89.22 toks/s]
Processed prompts:  80%|████████  | 3295/4096 [00:37<00:11, 68.31it/s, est. speed input: 91090.19 toks/s, output: 88.96 toks/s]
Processed prompts:  81%|████████  | 3327/4096 [00:37<00:11, 67.98it/s, est. speed input: 90807.61 toks/s, output: 88.68 toks/s]
Processed prompts:  82%|████████▏ | 3359/4096 [00:37<00:10, 68.07it/s, est. speed input: 90549.74 toks/s, output: 88.43 toks/s]
Processed prompts:  83%|████████▎ | 3391/4096 [00:38<00:10, 68.16it/s, est. speed input: 90299.91 toks/s, output: 88.18 toks/s]
Processed prompts:  84%|████████▎ | 3423/4096 [00:38<00:09, 68.38it/s, est. speed input: 90064.44 toks/s, output: 87.95 toks/s]
Processed prompts:  84%|████████▍ | 3455/4096 [00:39<00:09, 68.40it/s, est. speed input: 89827.06 toks/s, output: 87.72 toks/s]
Processed prompts:  85%|████████▌ | 3487/4096 [00:39<00:08, 68.14it/s, est. speed input: 89581.74 toks/s, output: 87.48 toks/s]
Processed prompts:  86%|████████▌ | 3519/4096 [00:40<00:08, 68.20it/s, est. speed input: 89353.80 toks/s, output: 87.26 toks/s]
Processed prompts:  87%|████████▋ | 3551/4096 [00:40<00:07, 68.38it/s, est. speed input: 89138.30 toks/s, output: 87.05 toks/s]
Processed prompts:  87%|████████▋ | 3583/4096 [00:41<00:07, 68.39it/s, est. speed input: 88921.98 toks/s, output: 86.84 toks/s]
Processed prompts:  88%|████████▊ | 3615/4096 [00:41<00:07, 68.28it/s, est. speed input: 88704.90 toks/s, output: 86.63 toks/s]
Processed prompts:  89%|████████▉ | 3647/4096 [00:42<00:06, 68.33it/s, est. speed input: 88498.84 toks/s, output: 86.42 toks/s]
Processed prompts:  90%|████████▉ | 3679/4096 [00:42<00:06, 68.88it/s, est. speed input: 88321.35 toks/s, output: 86.25 toks/s]
Processed prompts:  91%|█████████ | 3711/4096 [00:43<00:05, 68.78it/s, est. speed input: 88125.14 toks/s, output: 86.06 toks/s]
Processed prompts:  91%|█████████▏| 3743/4096 [00:43<00:05, 68.56it/s, est. speed input: 87925.96 toks/s, output: 85.87 toks/s]
Processed prompts:  92%|█████████▏| 3775/4096 [00:44<00:04, 68.51it/s, est. speed input: 87735.94 toks/s, output: 85.68 toks/s]
Processed prompts:  93%|█████████▎| 3807/4096 [00:44<00:04, 68.44it/s, est. speed input: 87548.47 toks/s, output: 85.50 toks/s]
Processed prompts:  94%|█████████▎| 3839/4096 [00:44<00:03, 68.34it/s, est. speed input: 87362.45 toks/s, output: 85.31 toks/s]
Processed prompts:  95%|█████████▍| 3871/4096 [00:45<00:03, 68.21it/s, est. speed input: 87177.80 toks/s, output: 85.13 toks/s]
Processed prompts:  95%|█████████▌| 3903/4096 [00:45<00:02, 68.73it/s, est. speed input: 87023.32 toks/s, output: 84.98 toks/s]
Processed prompts:  96%|█████████▌| 3935/4096 [00:46<00:02, 68.59it/s, est. speed input: 86850.12 toks/s, output: 84.81 toks/s]
Processed prompts:  97%|█████████▋| 3967/4096 [00:46<00:01, 68.43it/s, est. speed input: 86678.32 toks/s, output: 84.65 toks/s]
Processed prompts:  98%|█████████▊| 3999/4096 [00:47<00:01, 69.03it/s, est. speed input: 86538.85 toks/s, output: 84.51 toks/s]
Processed prompts:  98%|█████████▊| 4031/4096 [00:47<00:00, 68.56it/s, est. speed input: 86365.98 toks/s, output: 84.34 toks/s]
Processed prompts:  99%|█████████▉| 4063/4096 [00:48<00:00, 68.64it/s, est. speed input: 86212.73 toks/s, output: 84.19 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:48<00:00, 68.64it/s, est. speed input: 86813.13 toks/s, output: 84.78 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:48<00:00, 84.78it/s, est. speed input: 86813.13 toks/s, output: 84.78 toks/s]
[rank0]:[W126 17:56:15.386784376 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 17:56:17
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:57:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=875086) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=875086) WARNING 01-26 17:57:35 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=875086) WARNING 01-26 17:57:55 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 69.73 requests/s, 71470.19 total tokens/s, 69.73 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 17:57:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:57:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:57:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:57:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:57:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:57:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:57:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:57:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:57:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:57:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:57:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:57:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:57:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:57:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:57:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:57:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:57:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:57:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:57:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:57:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:57:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:57:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:57:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:57:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:57:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:57:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:57:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:57:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=875086) [2026-01-26 17:57:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=875086) [2026-01-26 17:57:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=875086) [2026-01-26 17:57:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=875086) [2026-01-26 17:57:21] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=875086) [2026-01-26 17:57:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=875086) [2026-01-26 17:57:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=875086) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=875086) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.36s/it]
(EngineCore_DP0 pid=875086) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
(EngineCore_DP0 pid=875086) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.03s/it]
(EngineCore_DP0 pid=875086) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.17s/it]
(EngineCore_DP0 pid=875086) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]
(EngineCore_DP0 pid=875086) 
(EngineCore_DP0 pid=875086) [2026-01-26 17:57:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=875086) [2026-01-26 17:57:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36700160 bytes
(EngineCore_DP0 pid=875086) [2026-01-26 17:57:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=875086) [2026-01-26 17:57:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26214400 bytes
(EngineCore_DP0 pid=875086) [2026-01-26 17:57:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=875086) [2026-01-26 17:57:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 141557760 bytes
(EngineCore_DP0 pid=875086) [2026-01-26 17:57:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=875086) [2026-01-26 17:57:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70860800 bytes
(EngineCore_DP0 pid=875086) [rank0]:W0126 17:57:43.638000 875086 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=875086) [rank0]:W0126 17:57:43.706000 875086 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=875086) [rank0]:W0126 17:57:44.496000 875086 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=875086) [rank0]:W0126 17:57:44.604000 875086 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=875086) 2026-01-26 17:57:54,420 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=875086) 2026-01-26 17:57:54,639 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=875086) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:04,  3.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  4.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:02,  5.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:02,  6.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  7.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  7.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  9.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  9.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:01<00:00, 10.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  9.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.40it/s]
(EngineCore_DP0 pid=875086) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  8.40it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  9.86it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 10.48it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 10.79it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 10.41it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00, 10.29it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 27/8192 [00:00<00:30, 268.69it/s]
Adding requests:   1%|          | 61/8192 [00:00<00:26, 310.02it/s]
Adding requests:   1%|          | 93/8192 [00:00<00:26, 310.77it/s]
Adding requests:   2%|▏         | 127/8192 [00:00<00:25, 317.96it/s]
Adding requests:   2%|▏         | 162/8192 [00:00<00:24, 326.94it/s]
Adding requests:   2%|▏         | 198/8192 [00:00<00:23, 335.63it/s]
Adding requests:   3%|▎         | 233/8192 [00:00<00:23, 339.21it/s]
Adding requests:   3%|▎         | 267/8192 [00:00<00:23, 339.10it/s]
Adding requests:   4%|▎         | 302/8192 [00:00<00:23, 342.06it/s]
Adding requests:   4%|▍         | 338/8192 [00:01<00:22, 346.92it/s]
Adding requests:   5%|▍         | 375/8192 [00:01<00:22, 350.25it/s]
Adding requests:   5%|▌         | 411/8192 [00:01<00:22, 348.71it/s]
Adding requests:   5%|▌         | 446/8192 [00:01<00:22, 348.94it/s]
Adding requests:   6%|▌         | 485/8192 [00:01<00:21, 359.04it/s]
Adding requests:   6%|▋         | 525/8192 [00:01<00:20, 368.98it/s]
Adding requests:   7%|▋         | 562/8192 [00:01<00:20, 368.79it/s]
Adding requests:   7%|▋         | 599/8192 [00:01<00:21, 357.20it/s]
Adding requests:   8%|▊         | 635/8192 [00:01<00:21, 355.54it/s]
Adding requests:   8%|▊         | 671/8192 [00:01<00:21, 346.79it/s]
Adding requests:   9%|▊         | 708/8192 [00:02<00:21, 353.48it/s]
Adding requests:   9%|▉         | 744/8192 [00:02<00:21, 348.67it/s]
Adding requests:  10%|▉         | 780/8192 [00:02<00:21, 351.10it/s]
Adding requests:  10%|▉         | 816/8192 [00:02<00:21, 348.21it/s]
Adding requests:  10%|█         | 855/8192 [00:02<00:20, 358.06it/s]
Adding requests:  11%|█         | 892/8192 [00:02<00:20, 359.06it/s]
Adding requests:  11%|█▏        | 928/8192 [00:02<00:20, 351.12it/s]
Adding requests:  12%|█▏        | 965/8192 [00:02<00:20, 356.00it/s]
Adding requests:  12%|█▏        | 1001/8192 [00:02<00:20, 349.24it/s]
Adding requests:  13%|█▎        | 1037/8192 [00:02<00:20, 350.04it/s]
Adding requests:  13%|█▎        | 1073/8192 [00:03<00:20, 348.61it/s]
Adding requests:  14%|█▎        | 1108/8192 [00:03<00:20, 345.76it/s]
Adding requests:  14%|█▍        | 1146/8192 [00:03<00:19, 353.76it/s]
Adding requests:  14%|█▍        | 1182/8192 [00:03<00:19, 351.49it/s]
Adding requests:  15%|█▍        | 1220/8192 [00:03<00:19, 357.13it/s]
Adding requests:  15%|█▌        | 1256/8192 [00:03<00:19, 354.95it/s]
Adding requests:  16%|█▌        | 1292/8192 [00:03<00:19, 346.34it/s]
Adding requests:  16%|█▌        | 1329/8192 [00:03<00:19, 350.91it/s]
Adding requests:  17%|█▋        | 1366/8192 [00:03<00:19, 354.93it/s]
Adding requests:  17%|█▋        | 1402/8192 [00:04<00:19, 351.76it/s]
Adding requests:  18%|█▊        | 1438/8192 [00:04<00:19, 353.75it/s]
Adding requests:  18%|█▊        | 1474/8192 [00:04<00:19, 351.32it/s]
Adding requests:  18%|█▊        | 1512/8192 [00:04<00:18, 359.03it/s]
Adding requests:  19%|█▉        | 1548/8192 [00:04<00:18, 356.56it/s]
Adding requests:  19%|█▉        | 1584/8192 [00:04<00:19, 340.04it/s]
Adding requests:  20%|█▉        | 1619/8192 [00:04<00:19, 336.75it/s]
Adding requests:  20%|██        | 1653/8192 [00:04<00:19, 334.72it/s]
Adding requests:  21%|██        | 1688/8192 [00:04<00:19, 336.43it/s]
Adding requests:  21%|██        | 1724/8192 [00:04<00:18, 342.38it/s]
Adding requests:  21%|██▏       | 1760/8192 [00:05<00:18, 345.71it/s]
Adding requests:  22%|██▏       | 1796/8192 [00:05<00:18, 349.22it/s]
Adding requests:  22%|██▏       | 1832/8192 [00:05<00:18, 351.97it/s]
Adding requests:  23%|██▎       | 1868/8192 [00:05<00:18, 344.77it/s]
Adding requests:  23%|██▎       | 1904/8192 [00:05<00:18, 348.39it/s]
Adding requests:  24%|██▎       | 1942/8192 [00:05<00:17, 355.22it/s]
Adding requests:  24%|██▍       | 1978/8192 [00:05<00:17, 355.11it/s]
Adding requests:  25%|██▍       | 2014/8192 [00:05<00:17, 352.02it/s]
Adding requests:  25%|██▌       | 2050/8192 [00:05<00:17, 345.90it/s]
Adding requests:  25%|██▌       | 2085/8192 [00:06<00:18, 337.56it/s]
Adding requests:  26%|██▌       | 2123/8192 [00:06<00:17, 348.78it/s]
Adding requests:  26%|██▋       | 2158/8192 [00:06<00:17, 343.16it/s]
Adding requests:  27%|██▋       | 2193/8192 [00:06<00:17, 340.55it/s]
Adding requests:  27%|██▋       | 2228/8192 [00:06<00:17, 341.34it/s]
Adding requests:  28%|██▊       | 2264/8192 [00:06<00:17, 344.94it/s]
Adding requests:  28%|██▊       | 2302/8192 [00:06<00:16, 353.21it/s]
Adding requests:  29%|██▊       | 2339/8192 [00:06<00:16, 356.89it/s]
Adding requests:  29%|██▉       | 2375/8192 [00:06<00:16, 354.86it/s]
Adding requests:  29%|██▉       | 2412/8192 [00:06<00:16, 359.19it/s]
Adding requests:  30%|██▉       | 2449/8192 [00:07<00:15, 359.06it/s]
Adding requests:  30%|███       | 2485/8192 [00:07<00:15, 357.26it/s]
Adding requests:  31%|███       | 2523/8192 [00:07<00:15, 362.37it/s]
Adding requests:  31%|███▏      | 2562/8192 [00:07<00:15, 369.66it/s]
Adding requests:  32%|███▏      | 2600/8192 [00:07<00:15, 372.68it/s]
Adding requests:  32%|███▏      | 2638/8192 [00:07<00:15, 359.29it/s]
Adding requests:  33%|███▎      | 2675/8192 [00:07<00:15, 358.09it/s]
Adding requests:  33%|███▎      | 2711/8192 [00:07<00:15, 352.07it/s]
Adding requests:  34%|███▎      | 2748/8192 [00:07<00:15, 356.38it/s]
Adding requests:  34%|███▍      | 2786/8192 [00:07<00:15, 360.38it/s]
Adding requests:  34%|███▍      | 2823/8192 [00:08<00:15, 352.62it/s]
Adding requests:  35%|███▍      | 2860/8192 [00:08<00:15, 355.31it/s]
Adding requests:  35%|███▌      | 2896/8192 [00:08<00:14, 354.21it/s]
Adding requests:  36%|███▌      | 2932/8192 [00:08<00:14, 354.78it/s]
Adding requests:  36%|███▋      | 2970/8192 [00:08<00:14, 359.68it/s]
Adding requests:  37%|███▋      | 3007/8192 [00:08<00:14, 362.19it/s]
Adding requests:  37%|███▋      | 3044/8192 [00:08<00:14, 357.13it/s]
Adding requests:  38%|███▊      | 3080/8192 [00:08<00:14, 353.26it/s]
Adding requests:  38%|███▊      | 3118/8192 [00:08<00:14, 358.09it/s]
Adding requests:  39%|███▊      | 3154/8192 [00:08<00:14, 356.53it/s]
Adding requests:  39%|███▉      | 3190/8192 [00:09<00:14, 356.02it/s]
Adding requests:  39%|███▉      | 3226/8192 [00:09<00:13, 354.83it/s]
Adding requests:  40%|███▉      | 3262/8192 [00:09<00:13, 355.99it/s]
Adding requests:  40%|████      | 3298/8192 [00:09<00:14, 344.91it/s]
Adding requests:  41%|████      | 3333/8192 [00:09<00:14, 343.45it/s]
Adding requests:  41%|████      | 3371/8192 [00:09<00:13, 353.74it/s]
Adding requests:  42%|████▏     | 3407/8192 [00:09<00:13, 353.54it/s]
Adding requests:  42%|████▏     | 3445/8192 [00:09<00:13, 359.61it/s]
Adding requests:  42%|████▏     | 3481/8192 [00:09<00:13, 358.24it/s]
Adding requests:  43%|████▎     | 3517/8192 [00:10<00:13, 358.35it/s]
Adding requests:  43%|████▎     | 3557/8192 [00:10<00:12, 368.12it/s]
Adding requests:  44%|████▍     | 3594/8192 [00:10<00:12, 363.00it/s]
Adding requests:  44%|████▍     | 3631/8192 [00:10<00:12, 363.35it/s]
Adding requests:  45%|████▍     | 3668/8192 [00:10<00:12, 356.79it/s]
Adding requests:  45%|████▌     | 3704/8192 [00:10<00:12, 353.09it/s]
Adding requests:  46%|████▌     | 3740/8192 [00:10<00:12, 352.19it/s]
Adding requests:  46%|████▌     | 3776/8192 [00:10<00:12, 343.83it/s]
Adding requests:  47%|████▋     | 3811/8192 [00:10<00:13, 334.30it/s]
Adding requests:  47%|████▋     | 3847/8192 [00:10<00:12, 337.50it/s]
Adding requests:  47%|████▋     | 3882/8192 [00:11<00:12, 339.26it/s]
Adding requests:  48%|████▊     | 3916/8192 [00:11<00:12, 334.60it/s]
Adding requests:  48%|████▊     | 3950/8192 [00:11<00:12, 335.40it/s]
Adding requests:  49%|████▊     | 3984/8192 [00:11<00:12, 335.83it/s]
Adding requests:  49%|████▉     | 4019/8192 [00:11<00:12, 339.47it/s]
Adding requests:  49%|████▉     | 4053/8192 [00:11<00:12, 335.33it/s]
Adding requests:  50%|████▉     | 4088/8192 [00:11<00:12, 337.89it/s]
Adding requests:  50%|█████     | 4125/8192 [00:11<00:11, 344.96it/s]
Adding requests:  51%|█████     | 4160/8192 [00:11<00:12, 331.71it/s]
Adding requests:  51%|█████     | 4196/8192 [00:12<00:11, 337.00it/s]
Adding requests:  52%|█████▏    | 4230/8192 [00:12<00:11, 332.77it/s]
Adding requests:  52%|█████▏    | 4266/8192 [00:12<00:11, 339.01it/s]
Adding requests:  53%|█████▎    | 4301/8192 [00:12<00:11, 341.97it/s]
Adding requests:  53%|█████▎    | 4336/8192 [00:12<00:11, 343.10it/s]
Adding requests:  53%|█████▎    | 4373/8192 [00:12<00:10, 349.73it/s]
Adding requests:  54%|█████▍    | 4409/8192 [00:12<00:10, 348.14it/s]
Adding requests:  54%|█████▍    | 4445/8192 [00:12<00:10, 349.17it/s]
Adding requests:  55%|█████▍    | 4482/8192 [00:12<00:10, 353.33it/s]
Adding requests:  55%|█████▌    | 4518/8192 [00:12<00:10, 351.52it/s]
Adding requests:  56%|█████▌    | 4554/8192 [00:13<00:10, 351.15it/s]
Adding requests:  56%|█████▌    | 4591/8192 [00:13<00:10, 353.77it/s]
Adding requests:  56%|█████▋    | 4627/8192 [00:13<00:10, 349.47it/s]
Adding requests:  57%|█████▋    | 4662/8192 [00:13<00:10, 345.03it/s]
Adding requests:  57%|█████▋    | 4697/8192 [00:13<00:10, 338.83it/s]
Adding requests:  58%|█████▊    | 4735/8192 [00:13<00:09, 350.35it/s]
Adding requests:  58%|█████▊    | 4771/8192 [00:13<00:09, 352.05it/s]
Adding requests:  59%|█████▊    | 4807/8192 [00:13<00:09, 347.97it/s]
Adding requests:  59%|█████▉    | 4842/8192 [00:13<00:09, 341.25it/s]
Adding requests:  60%|█████▉    | 4879/8192 [00:13<00:09, 348.13it/s]
Adding requests:  60%|█████▉    | 4914/8192 [00:14<00:09, 346.00it/s]
Adding requests:  60%|██████    | 4951/8192 [00:14<00:09, 351.90it/s]
Adding requests:  61%|██████    | 4987/8192 [00:14<00:09, 353.04it/s]
Adding requests:  61%|██████▏   | 5024/8192 [00:14<00:08, 356.02it/s]
Adding requests:  62%|██████▏   | 5060/8192 [00:14<00:08, 354.63it/s]
Adding requests:  62%|██████▏   | 5096/8192 [00:14<00:08, 355.40it/s]
Adding requests:  63%|██████▎   | 5133/8192 [00:14<00:08, 357.53it/s]
Adding requests:  63%|██████▎   | 5169/8192 [00:14<00:08, 353.26it/s]
Adding requests:  64%|██████▎   | 5205/8192 [00:14<00:08, 350.29it/s]
Adding requests:  64%|██████▍   | 5241/8192 [00:14<00:08, 348.39it/s]
Adding requests:  64%|██████▍   | 5277/8192 [00:15<00:08, 349.94it/s]
Adding requests:  65%|██████▍   | 5313/8192 [00:15<00:08, 349.67it/s]
Adding requests:  65%|██████▌   | 5349/8192 [00:15<00:08, 350.75it/s]
Adding requests:  66%|██████▌   | 5385/8192 [00:15<00:08, 349.17it/s]
Adding requests:  66%|██████▌   | 5420/8192 [00:15<00:08, 338.00it/s]
Adding requests:  67%|██████▋   | 5459/8192 [00:15<00:07, 349.79it/s]
Adding requests:  67%|██████▋   | 5495/8192 [00:15<00:07, 341.64it/s]
Adding requests:  68%|██████▊   | 5531/8192 [00:15<00:07, 345.51it/s]
Adding requests:  68%|██████▊   | 5566/8192 [00:15<00:07, 345.53it/s]
Adding requests:  68%|██████▊   | 5601/8192 [00:16<00:07, 345.35it/s]
Adding requests:  69%|██████▉   | 5638/8192 [00:16<00:07, 351.86it/s]
Adding requests:  69%|██████▉   | 5674/8192 [00:16<00:07, 352.05it/s]
Adding requests:  70%|██████▉   | 5710/8192 [00:16<00:07, 351.70it/s]
Adding requests:  70%|███████   | 5746/8192 [00:16<00:07, 347.37it/s]
Adding requests:  71%|███████   | 5783/8192 [00:16<00:06, 352.34it/s]
Adding requests:  71%|███████   | 5820/8192 [00:16<00:06, 357.45it/s]
Adding requests:  71%|███████▏  | 5856/8192 [00:16<00:06, 352.38it/s]
Adding requests:  72%|███████▏  | 5894/8192 [00:16<00:06, 358.43it/s]
Adding requests:  72%|███████▏  | 5930/8192 [00:16<00:06, 354.78it/s]
Adding requests:  73%|███████▎  | 5968/8192 [00:17<00:06, 359.67it/s]
Adding requests:  73%|███████▎  | 6004/8192 [00:17<00:06, 359.06it/s]
Adding requests:  74%|███████▎  | 6040/8192 [00:17<00:05, 359.33it/s]
Adding requests:  74%|███████▍  | 6076/8192 [00:17<00:05, 356.87it/s]
Adding requests:  75%|███████▍  | 6112/8192 [00:17<00:05, 351.43it/s]
Adding requests:  75%|███████▌  | 6149/8192 [00:17<00:05, 356.54it/s]
Adding requests:  76%|███████▌  | 6185/8192 [00:17<00:05, 353.14it/s]
Adding requests:  76%|███████▌  | 6221/8192 [00:17<00:05, 350.00it/s]
Adding requests:  76%|███████▋  | 6258/8192 [00:17<00:05, 352.70it/s]
Adding requests:  77%|███████▋  | 6294/8192 [00:17<00:05, 351.85it/s]
Adding requests:  77%|███████▋  | 6332/8192 [00:18<00:05, 359.43it/s]
Adding requests:  78%|███████▊  | 6369/8192 [00:18<00:05, 361.55it/s]
Adding requests:  78%|███████▊  | 6406/8192 [00:18<00:05, 347.70it/s]
Adding requests:  79%|███████▊  | 6441/8192 [00:18<00:05, 341.77it/s]
Adding requests:  79%|███████▉  | 6476/8192 [00:18<00:05, 341.05it/s]
Adding requests:  79%|███████▉  | 6512/8192 [00:18<00:04, 346.00it/s]
Adding requests:  80%|███████▉  | 6547/8192 [00:18<00:04, 347.07it/s]
Adding requests:  80%|████████  | 6583/8192 [00:18<00:04, 346.46it/s]
Adding requests:  81%|████████  | 6619/8192 [00:18<00:04, 347.30it/s]
Adding requests:  81%|████████▏ | 6656/8192 [00:19<00:04, 351.33it/s]
Adding requests:  82%|████████▏ | 6692/8192 [00:19<00:04, 351.66it/s]
Adding requests:  82%|████████▏ | 6728/8192 [00:19<00:04, 351.29it/s]
Adding requests:  83%|████████▎ | 6765/8192 [00:19<00:04, 356.18it/s]
Adding requests:  83%|████████▎ | 6801/8192 [00:19<00:03, 350.44it/s]
Adding requests:  83%|████████▎ | 6837/8192 [00:19<00:03, 339.42it/s]
Adding requests:  84%|████████▍ | 6874/8192 [00:19<00:03, 345.54it/s]
Adding requests:  84%|████████▍ | 6909/8192 [00:19<00:03, 343.50it/s]
Adding requests:  85%|████████▍ | 6946/8192 [00:19<00:03, 347.86it/s]
Adding requests:  85%|████████▌ | 6981/8192 [00:19<00:03, 339.44it/s]
Adding requests:  86%|████████▌ | 7017/8192 [00:20<00:03, 345.03it/s]
Adding requests:  86%|████████▌ | 7052/8192 [00:20<00:03, 342.78it/s]
Adding requests:  87%|████████▋ | 7087/8192 [00:20<00:03, 343.13it/s]
Adding requests:  87%|████████▋ | 7123/8192 [00:20<00:03, 345.37it/s]
Adding requests:  87%|████████▋ | 7158/8192 [00:20<00:03, 344.35it/s]
Adding requests:  88%|████████▊ | 7195/8192 [00:20<00:02, 349.56it/s]
Adding requests:  88%|████████▊ | 7233/8192 [00:20<00:02, 356.78it/s]
Adding requests:  89%|████████▊ | 7269/8192 [00:20<00:02, 356.58it/s]
Adding requests:  89%|████████▉ | 7305/8192 [00:20<00:02, 354.99it/s]
Adding requests:  90%|████████▉ | 7341/8192 [00:20<00:02, 356.37it/s]
Adding requests:  90%|█████████ | 7378/8192 [00:21<00:02, 358.31it/s]
Adding requests:  91%|█████████ | 7414/8192 [00:21<00:02, 352.96it/s]
Adding requests:  91%|█████████ | 7450/8192 [00:21<00:02, 349.34it/s]
Adding requests:  91%|█████████▏| 7485/8192 [00:21<00:02, 346.89it/s]
Adding requests:  92%|█████████▏| 7520/8192 [00:21<00:01, 342.66it/s]
Adding requests:  92%|█████████▏| 7555/8192 [00:21<00:01, 343.09it/s]
Adding requests:  93%|█████████▎| 7590/8192 [00:21<00:01, 342.87it/s]
Adding requests:  93%|█████████▎| 7625/8192 [00:21<00:01, 342.43it/s]
Adding requests:  94%|█████████▎| 7663/8192 [00:21<00:01, 351.89it/s]
Adding requests:  94%|█████████▍| 7701/8192 [00:22<00:01, 359.55it/s]
Adding requests:  94%|█████████▍| 7737/8192 [00:22<00:01, 356.27it/s]
Adding requests:  95%|█████████▍| 7773/8192 [00:22<00:01, 353.90it/s]
Adding requests:  95%|█████████▌| 7809/8192 [00:22<00:01, 354.58it/s]
Adding requests:  96%|█████████▌| 7845/8192 [00:22<00:01, 345.61it/s]
Adding requests:  96%|█████████▌| 7880/8192 [00:22<00:00, 344.05it/s]
Adding requests:  97%|█████████▋| 7918/8192 [00:22<00:00, 352.80it/s]
Adding requests:  97%|█████████▋| 7957/8192 [00:22<00:00, 360.51it/s]
Adding requests:  98%|█████████▊| 7995/8192 [00:22<00:00, 366.09it/s]
Adding requests:  98%|█████████▊| 8032/8192 [00:22<00:00, 357.40it/s]
Adding requests:  99%|█████████▊| 8070/8192 [00:23<00:00, 361.71it/s]
Adding requests:  99%|█████████▉| 8107/8192 [00:23<00:00, 355.76it/s]
Adding requests:  99%|█████████▉| 8143/8192 [00:23<00:00, 353.93it/s]
Adding requests: 100%|█████████▉| 8179/8192 [00:23<00:00, 333.98it/s]
Adding requests: 100%|██████████| 8192/8192 [00:23<00:00, 349.66it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 1619/8192 [00:00<00:02, 2261.40it/s, est. speed input: 2315708.65 toks/s, output: 2261.41 toks/s]
Processed prompts:  23%|██▎       | 1846/8192 [00:03<00:15, 420.59it/s, est. speed input: 548070.89 toks/s, output: 535.22 toks/s]   
Processed prompts:  24%|██▎       | 1945/8192 [00:05<00:24, 257.55it/s, est. speed input: 378170.98 toks/s, output: 369.31 toks/s]
Processed prompts:  24%|██▍       | 2003/8192 [00:06<00:29, 210.39it/s, est. speed input: 331742.68 toks/s, output: 323.97 toks/s]
Processed prompts:  25%|██▌       | 2067/8192 [00:07<00:35, 174.75it/s, est. speed input: 298636.05 toks/s, output: 291.64 toks/s]
Processed prompts:  26%|██▌       | 2131/8192 [00:08<00:41, 146.43it/s, est. speed input: 272604.00 toks/s, output: 266.21 toks/s]
Processed prompts:  27%|██▋       | 2195/8192 [00:08<00:47, 125.60it/s, est. speed input: 252293.19 toks/s, output: 246.38 toks/s]
Processed prompts:  28%|██▊       | 2259/8192 [00:09<00:54, 109.71it/s, est. speed input: 235434.62 toks/s, output: 229.92 toks/s]
Processed prompts:  28%|██▊       | 2323/8192 [00:10<00:59, 98.17it/s, est. speed input: 221451.20 toks/s, output: 216.26 toks/s] 
Processed prompts:  29%|██▉       | 2387/8192 [00:11<01:04, 89.69it/s, est. speed input: 209543.53 toks/s, output: 204.63 toks/s]
Processed prompts:  30%|██▉       | 2451/8192 [00:12<01:08, 84.01it/s, est. speed input: 199581.72 toks/s, output: 194.90 toks/s]
Processed prompts:  31%|███       | 2515/8192 [00:13<01:10, 80.13it/s, est. speed input: 191059.22 toks/s, output: 186.58 toks/s]
Processed prompts:  31%|███▏      | 2579/8192 [00:14<01:12, 77.26it/s, est. speed input: 183548.13 toks/s, output: 179.25 toks/s]
Processed prompts:  32%|███▏      | 2643/8192 [00:15<01:14, 74.90it/s, est. speed input: 176777.56 toks/s, output: 172.63 toks/s]
Processed prompts:  33%|███▎      | 2707/8192 [00:16<01:14, 73.61it/s, est. speed input: 170931.22 toks/s, output: 166.92 toks/s]
Processed prompts:  34%|███▍      | 2771/8192 [00:17<01:14, 72.37it/s, est. speed input: 165569.86 toks/s, output: 161.69 toks/s]
Processed prompts:  35%|███▍      | 2835/8192 [00:18<01:14, 71.46it/s, est. speed input: 160738.51 toks/s, output: 156.97 toks/s]
Processed prompts:  35%|███▌      | 2899/8192 [00:18<01:14, 71.50it/s, est. speed input: 156615.43 toks/s, output: 152.94 toks/s]
Processed prompts:  36%|███▌      | 2963/8192 [00:19<01:13, 70.91it/s, est. speed input: 152662.18 toks/s, output: 149.08 toks/s]
Processed prompts:  37%|███▋      | 3027/8192 [00:20<01:13, 70.50it/s, est. speed input: 149056.34 toks/s, output: 145.56 toks/s]
Processed prompts:  38%|███▊      | 3091/8192 [00:21<01:12, 70.23it/s, est. speed input: 145763.60 toks/s, output: 142.35 toks/s]
Processed prompts:  39%|███▊      | 3155/8192 [00:22<01:11, 70.00it/s, est. speed input: 142725.60 toks/s, output: 139.38 toks/s]
Processed prompts:  39%|███▉      | 3219/8192 [00:23<01:11, 69.84it/s, est. speed input: 139924.65 toks/s, output: 136.65 toks/s]
Processed prompts:  40%|████      | 3283/8192 [00:24<01:10, 69.72it/s, est. speed input: 137335.03 toks/s, output: 134.12 toks/s]
Processed prompts:  41%|████      | 3347/8192 [00:25<01:09, 69.74it/s, est. speed input: 134956.31 toks/s, output: 131.79 toks/s]
Processed prompts:  42%|████▏     | 3411/8192 [00:26<01:08, 69.58it/s, est. speed input: 132704.34 toks/s, output: 129.59 toks/s]
Processed prompts:  42%|████▏     | 3475/8192 [00:27<01:07, 69.54it/s, est. speed input: 130621.22 toks/s, output: 127.56 toks/s]
Processed prompts:  43%|████▎     | 3539/8192 [00:28<01:06, 69.58it/s, est. speed input: 128687.18 toks/s, output: 125.67 toks/s]
Processed prompts:  44%|████▍     | 3603/8192 [00:29<01:06, 69.51it/s, est. speed input: 126857.43 toks/s, output: 123.88 toks/s]
Processed prompts:  45%|████▍     | 3667/8192 [00:29<01:04, 69.79it/s, est. speed input: 125200.58 toks/s, output: 122.27 toks/s]
Processed prompts:  46%|████▌     | 3731/8192 [00:30<01:04, 69.68it/s, est. speed input: 123587.31 toks/s, output: 120.69 toks/s]
Processed prompts:  46%|████▋     | 3795/8192 [00:31<01:03, 69.59it/s, est. speed input: 122064.37 toks/s, output: 119.20 toks/s]
Processed prompts:  47%|████▋     | 3859/8192 [00:32<01:02, 69.56it/s, est. speed input: 120632.65 toks/s, output: 117.81 toks/s]
Processed prompts:  48%|████▊     | 3923/8192 [00:33<01:01, 69.72it/s, est. speed input: 119307.67 toks/s, output: 116.51 toks/s]
Processed prompts:  49%|████▊     | 3987/8192 [00:34<01:00, 69.93it/s, est. speed input: 118067.34 toks/s, output: 115.30 toks/s]
Processed prompts:  49%|████▉     | 4051/8192 [00:35<00:59, 70.03it/s, est. speed input: 116884.56 toks/s, output: 114.15 toks/s]
Processed prompts:  50%|█████     | 4115/8192 [00:36<00:58, 69.83it/s, est. speed input: 115721.88 toks/s, output: 113.01 toks/s]
Processed prompts:  51%|█████     | 4179/8192 [00:37<00:57, 69.76it/s, est. speed input: 114626.99 toks/s, output: 111.94 toks/s]
Processed prompts:  52%|█████▏    | 4243/8192 [00:38<00:56, 69.66it/s, est. speed input: 113578.23 toks/s, output: 110.92 toks/s]
Processed prompts:  53%|█████▎    | 4307/8192 [00:39<00:55, 69.65it/s, est. speed input: 112585.49 toks/s, output: 109.95 toks/s]
Processed prompts:  53%|█████▎    | 4371/8192 [00:40<00:55, 69.45it/s, est. speed input: 111615.61 toks/s, output: 109.00 toks/s]
Processed prompts:  54%|█████▍    | 4435/8192 [00:41<00:53, 69.66it/s, est. speed input: 110731.07 toks/s, output: 108.14 toks/s]
Processed prompts:  55%|█████▍    | 4499/8192 [00:41<00:52, 69.80it/s, est. speed input: 109883.57 toks/s, output: 107.31 toks/s]
Processed prompts:  56%|█████▌    | 4563/8192 [00:42<00:51, 70.04it/s, est. speed input: 109088.13 toks/s, output: 106.53 toks/s]
Processed prompts:  56%|█████▋    | 4627/8192 [00:43<00:51, 69.86it/s, est. speed input: 108287.86 toks/s, output: 105.75 toks/s]
Processed prompts:  57%|█████▋    | 4691/8192 [00:44<00:50, 69.72it/s, est. speed input: 107520.10 toks/s, output: 105.00 toks/s]
Processed prompts:  58%|█████▊    | 4755/8192 [00:45<00:49, 69.61it/s, est. speed input: 106781.29 toks/s, output: 104.28 toks/s]
Processed prompts:  59%|█████▉    | 4819/8192 [00:46<00:48, 69.51it/s, est. speed input: 106069.35 toks/s, output: 103.58 toks/s]
Processed prompts:  60%|█████▉    | 4883/8192 [00:47<00:47, 69.73it/s, est. speed input: 105413.66 toks/s, output: 102.94 toks/s]
Processed prompts:  60%|██████    | 4947/8192 [00:48<00:46, 69.95it/s, est. speed input: 104788.65 toks/s, output: 102.33 toks/s]
Processed prompts:  61%|██████    | 5011/8192 [00:49<00:45, 69.84it/s, est. speed input: 104162.89 toks/s, output: 101.72 toks/s]
Processed prompts:  62%|██████▏   | 5075/8192 [00:50<00:44, 69.75it/s, est. speed input: 103558.47 toks/s, output: 101.13 toks/s]
Processed prompts:  63%|██████▎   | 5139/8192 [00:51<00:43, 69.59it/s, est. speed input: 102967.20 toks/s, output: 100.55 toks/s]
Processed prompts:  64%|██████▎   | 5203/8192 [00:52<00:42, 69.97it/s, est. speed input: 102439.30 toks/s, output: 100.04 toks/s]
Processed prompts:  64%|██████▍   | 5267/8192 [00:52<00:41, 70.07it/s, est. speed input: 101915.85 toks/s, output: 99.53 toks/s] 
Processed prompts:  65%|██████▌   | 5331/8192 [00:53<00:40, 69.86it/s, est. speed input: 101386.91 toks/s, output: 99.01 toks/s]
Processed prompts:  66%|██████▌   | 5395/8192 [00:54<00:40, 69.81it/s, est. speed input: 100883.71 toks/s, output: 98.52 toks/s]
Processed prompts:  67%|██████▋   | 5459/8192 [00:55<00:39, 69.96it/s, est. speed input: 100410.98 toks/s, output: 98.06 toks/s]
Processed prompts:  67%|██████▋   | 5523/8192 [00:56<00:38, 70.10it/s, est. speed input: 99956.38 toks/s, output: 97.61 toks/s] 
Processed prompts:  68%|██████▊   | 5587/8192 [00:57<00:37, 69.95it/s, est. speed input: 99497.78 toks/s, output: 97.17 toks/s]
Processed prompts:  69%|██████▉   | 5651/8192 [00:58<00:36, 69.83it/s, est. speed input: 99052.64 toks/s, output: 96.73 toks/s]
Processed prompts:  70%|██████▉   | 5715/8192 [00:59<00:35, 69.73it/s, est. speed input: 98619.79 toks/s, output: 96.31 toks/s]
Processed prompts:  71%|███████   | 5779/8192 [01:00<00:34, 69.67it/s, est. speed input: 98200.76 toks/s, output: 95.90 toks/s]
Processed prompts:  71%|███████▏  | 5843/8192 [01:01<00:33, 69.63it/s, est. speed input: 97794.85 toks/s, output: 95.50 toks/s]
Processed prompts:  72%|███████▏  | 5907/8192 [01:02<00:32, 69.89it/s, est. speed input: 97420.33 toks/s, output: 95.14 toks/s]
Processed prompts:  73%|███████▎  | 5971/8192 [01:02<00:31, 70.05it/s, est. speed input: 97055.68 toks/s, output: 94.78 toks/s]
Processed prompts:  74%|███████▎  | 6035/8192 [01:03<00:30, 69.82it/s, est. speed input: 96678.54 toks/s, output: 94.41 toks/s]
Processed prompts:  74%|███████▍  | 6099/8192 [01:04<00:30, 69.76it/s, est. speed input: 96318.38 toks/s, output: 94.06 toks/s]
Processed prompts:  75%|███████▌  | 6163/8192 [01:05<00:29, 69.51it/s, est. speed input: 95955.23 toks/s, output: 93.71 toks/s]
Processed prompts:  76%|███████▌  | 6227/8192 [01:06<00:28, 69.69it/s, est. speed input: 95624.78 toks/s, output: 93.38 toks/s]
Processed prompts:  77%|███████▋  | 6291/8192 [01:07<00:27, 69.59it/s, est. speed input: 95289.11 toks/s, output: 93.06 toks/s]
Processed prompts:  78%|███████▊  | 6355/8192 [01:08<00:26, 69.48it/s, est. speed input: 94959.40 toks/s, output: 92.73 toks/s]
Processed prompts:  78%|███████▊  | 6419/8192 [01:09<00:25, 69.45it/s, est. speed input: 94642.12 toks/s, output: 92.42 toks/s]
Processed prompts:  79%|███████▉  | 6483/8192 [01:10<00:24, 69.46it/s, est. speed input: 94334.68 toks/s, output: 92.12 toks/s]
Processed prompts:  80%|███████▉  | 6547/8192 [01:11<00:23, 69.40it/s, est. speed input: 94031.18 toks/s, output: 91.83 toks/s]
Processed prompts:  81%|████████  | 6611/8192 [01:12<00:22, 69.36it/s, est. speed input: 93735.36 toks/s, output: 91.54 toks/s]
Processed prompts:  81%|████████▏ | 6675/8192 [01:13<00:21, 69.35it/s, est. speed input: 93448.47 toks/s, output: 91.26 toks/s]
Processed prompts:  82%|████████▏ | 6739/8192 [01:14<00:20, 69.36it/s, est. speed input: 93169.30 toks/s, output: 90.99 toks/s]
Processed prompts:  83%|████████▎ | 6803/8192 [01:14<00:20, 69.36it/s, est. speed input: 92896.79 toks/s, output: 90.72 toks/s]
Processed prompts:  84%|████████▍ | 6867/8192 [01:15<00:19, 69.40it/s, est. speed input: 92633.20 toks/s, output: 90.46 toks/s]
Processed prompts:  85%|████████▍ | 6931/8192 [01:16<00:18, 69.43it/s, est. speed input: 92376.02 toks/s, output: 90.21 toks/s]
Processed prompts:  85%|████████▌ | 6995/8192 [01:17<00:17, 69.38it/s, est. speed input: 92120.91 toks/s, output: 89.96 toks/s]
Processed prompts:  86%|████████▌ | 7059/8192 [01:18<00:16, 69.61it/s, est. speed input: 91885.79 toks/s, output: 89.73 toks/s]
Processed prompts:  87%|████████▋ | 7123/8192 [01:19<00:15, 69.88it/s, est. speed input: 91661.38 toks/s, output: 89.51 toks/s]
Processed prompts:  88%|████████▊ | 7187/8192 [01:20<00:14, 69.72it/s, est. speed input: 91424.61 toks/s, output: 89.28 toks/s]
Processed prompts:  89%|████████▊ | 7251/8192 [01:21<00:13, 69.76it/s, est. speed input: 91200.75 toks/s, output: 89.06 toks/s]
Processed prompts:  89%|████████▉ | 7315/8192 [01:22<00:12, 69.55it/s, est. speed input: 90970.26 toks/s, output: 88.84 toks/s]
Processed prompts:  90%|█████████ | 7379/8192 [01:23<00:11, 69.42it/s, est. speed input: 90745.73 toks/s, output: 88.62 toks/s]
Processed prompts:  91%|█████████ | 7443/8192 [01:24<00:10, 69.69it/s, est. speed input: 90543.25 toks/s, output: 88.42 toks/s]
Processed prompts:  92%|█████████▏| 7507/8192 [01:25<00:09, 69.50it/s, est. speed input: 90327.49 toks/s, output: 88.21 toks/s]
Processed prompts:  92%|█████████▏| 7571/8192 [01:26<00:08, 69.44it/s, est. speed input: 90119.62 toks/s, output: 88.01 toks/s]
Processed prompts:  93%|█████████▎| 7635/8192 [01:26<00:07, 69.71it/s, est. speed input: 89930.20 toks/s, output: 87.82 toks/s]
Processed prompts:  94%|█████████▍| 7699/8192 [01:27<00:07, 69.92it/s, est. speed input: 89745.76 toks/s, output: 87.64 toks/s]
Processed prompts:  95%|█████████▍| 7763/8192 [01:28<00:06, 69.73it/s, est. speed input: 89550.23 toks/s, output: 87.45 toks/s]
Processed prompts:  96%|█████████▌| 7827/8192 [01:29<00:05, 69.52it/s, est. speed input: 89355.56 toks/s, output: 87.26 toks/s]
Processed prompts:  96%|█████████▋| 7891/8192 [01:30<00:04, 69.46it/s, est. speed input: 89168.18 toks/s, output: 87.08 toks/s]
Processed prompts:  97%|█████████▋| 7955/8192 [01:31<00:03, 69.46it/s, est. speed input: 88986.65 toks/s, output: 86.90 toks/s]
Processed prompts:  98%|█████████▊| 8019/8192 [01:32<00:02, 69.44it/s, est. speed input: 88807.61 toks/s, output: 86.73 toks/s]
Processed prompts:  99%|█████████▊| 8083/8192 [01:33<00:01, 69.39it/s, est. speed input: 88630.91 toks/s, output: 86.55 toks/s]
Processed prompts:  99%|█████████▉| 8147/8192 [01:34<00:00, 75.68it/s, est. speed input: 88699.29 toks/s, output: 86.62 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:34<00:00, 75.68it/s, est. speed input: 89188.67 toks/s, output: 87.10 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:34<00:00, 87.10it/s, est. speed input: 89188.67 toks/s, output: 87.10 toks/s]
[rank0]:[W126 17:59:58.408565065 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

