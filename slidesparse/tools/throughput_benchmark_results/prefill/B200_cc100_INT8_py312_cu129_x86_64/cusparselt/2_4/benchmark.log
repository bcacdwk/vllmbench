
========== M=16 ==========
Time: 2026-01-26 13:00:07
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 17 --max-num-batched-tokens 17 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:00:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=442291) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=442291) WARNING 01-26 13:00:26 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=442291) WARNING 01-26 13:00:33 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 44.80 requests/s, 761.61 total tokens/s, 44.80 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128

STDERR:
[2026-01-26 13:00:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:00:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:00:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:00:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:00:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:00:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:00:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:00:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:00:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:00:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:00:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:00:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:00:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:00:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:00:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:00:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:00:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:00:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=442291) [2026-01-26 13:00:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=442291) [2026-01-26 13:00:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=442291) [2026-01-26 13:00:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=442291) [2026-01-26 13:00:21] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=442291) [2026-01-26 13:00:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=442291) [2026-01-26 13:00:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=442291) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=442291) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.25it/s]
(EngineCore_DP0 pid=442291) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.25it/s]
(EngineCore_DP0 pid=442291) 
(EngineCore_DP0 pid=442291) [2026-01-26 13:00:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=442291) [2026-01-26 13:00:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=442291) [2026-01-26 13:00:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=442291) [2026-01-26 13:00:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=442291) [2026-01-26 13:00:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=442291) [2026-01-26 13:00:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=442291) [2026-01-26 13:00:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=442291) [2026-01-26 13:00:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=442291) 2026-01-26 13:00:33,781 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=442291) 2026-01-26 13:00:33,803 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=442291) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.06it/s]
(EngineCore_DP0 pid=442291) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.84it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 514.41it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 607.36it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:00, 124.37it/s, est. speed input: 1990.06 toks/s, output: 124.37 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:01, 61.80it/s, est. speed input: 1072.74 toks/s, output: 67.05 toks/s]  
Processed prompts:  27%|██▋       | 35/128 [00:00<00:01, 54.80it/s, est. speed input: 962.78 toks/s, output: 60.17 toks/s] 
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 51.38it/s, est. speed input: 910.17 toks/s, output: 56.89 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 49.44it/s, est. speed input: 880.35 toks/s, output: 55.02 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 48.10it/s, est. speed input: 858.81 toks/s, output: 53.68 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 47.26it/s, est. speed input: 844.75 toks/s, output: 52.80 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 46.60it/s, est. speed input: 833.15 toks/s, output: 52.07 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 46.10it/s, est. speed input: 823.47 toks/s, output: 51.47 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 45.75it/s, est. speed input: 815.38 toks/s, output: 50.96 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:01, 45.53it/s, est. speed input: 808.60 toks/s, output: 50.54 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:00, 45.45it/s, est. speed input: 803.00 toks/s, output: 50.19 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 45.45it/s, est. speed input: 798.33 toks/s, output: 49.90 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:01<00:00, 45.43it/s, est. speed input: 794.15 toks/s, output: 49.63 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 45.37it/s, est. speed input: 790.24 toks/s, output: 49.39 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 45.34it/s, est. speed input: 786.80 toks/s, output: 49.17 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 45.37it/s, est. speed input: 783.83 toks/s, output: 48.99 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 45.28it/s, est. speed input: 780.87 toks/s, output: 48.80 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 45.29it/s, est. speed input: 778.35 toks/s, output: 48.65 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:02<00:00, 45.30it/s, est. speed input: 776.04 toks/s, output: 48.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.30it/s, est. speed input: 774.20 toks/s, output: 48.39 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 48.39it/s, est. speed input: 774.20 toks/s, output: 48.39 toks/s]
[rank0]:[W126 13:00:39.712820618 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 13:00:41
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=128, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 128 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 129 --max-num-batched-tokens 129 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:00:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=443458) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=443458) WARNING 01-26 13:01:00 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=443458) WARNING 01-26 13:01:07 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 40.16 requests/s, 5180.52 total tokens/s, 40.16 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128

STDERR:
[2026-01-26 13:00:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:00:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:00:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:00:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:00:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:00:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:00:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:00:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:00:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:00:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:00:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:00:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:00:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:00:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:00:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:00:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:00:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:00:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:00:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=443458) [2026-01-26 13:00:55] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=443458) [2026-01-26 13:00:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=443458) [2026-01-26 13:00:55] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=443458) [2026-01-26 13:00:55] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=443458) [2026-01-26 13:00:55] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=443458) [2026-01-26 13:00:55] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=443458) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=443458) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.36it/s]
(EngineCore_DP0 pid=443458) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.36it/s]
(EngineCore_DP0 pid=443458) 
(EngineCore_DP0 pid=443458) [2026-01-26 13:00:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=443458) [2026-01-26 13:00:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=443458) [2026-01-26 13:00:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=443458) [2026-01-26 13:00:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=443458) [2026-01-26 13:00:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=443458) [2026-01-26 13:00:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=443458) [2026-01-26 13:00:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=443458) [2026-01-26 13:00:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=443458) 2026-01-26 13:01:07,430 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=443458) 2026-01-26 13:01:07,452 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=443458) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.52it/s]
(EngineCore_DP0 pid=443458) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.44it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 357.79it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 415.64it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:01, 74.84it/s, est. speed input: 9579.84 toks/s, output: 74.84 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:02, 52.30it/s, est. speed input: 7011.69 toks/s, output: 54.78 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 48.38it/s, est. speed input: 6535.64 toks/s, output: 51.06 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 46.49it/s, est. speed input: 6298.27 toks/s, output: 49.20 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 45.52it/s, est. speed input: 6173.00 toks/s, output: 48.23 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:02, 44.81it/s, est. speed input: 6080.49 toks/s, output: 47.50 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 44.34it/s, est. speed input: 6012.11 toks/s, output: 46.97 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:01, 43.95it/s, est. speed input: 5955.71 toks/s, output: 46.53 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 43.70it/s, est. speed input: 5911.81 toks/s, output: 46.19 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 43.60it/s, est. speed input: 5878.94 toks/s, output: 45.93 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 43.56it/s, est. speed input: 5852.71 toks/s, output: 45.72 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 43.45it/s, est. speed input: 5827.39 toks/s, output: 45.53 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 43.45it/s, est. speed input: 5808.39 toks/s, output: 45.38 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 43.39it/s, est. speed input: 5790.22 toks/s, output: 45.24 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:01, 43.39it/s, est. speed input: 5775.39 toks/s, output: 45.12 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 43.39it/s, est. speed input: 5762.22 toks/s, output: 45.02 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 43.35it/s, est. speed input: 5749.75 toks/s, output: 44.92 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 43.39it/s, est. speed input: 5740.14 toks/s, output: 44.84 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 43.29it/s, est. speed input: 5728.65 toks/s, output: 44.75 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 43.28it/s, est. speed input: 5719.44 toks/s, output: 44.68 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 43.33it/s, est. speed input: 5712.12 toks/s, output: 44.63 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 43.37it/s, est. speed input: 5705.71 toks/s, output: 44.58 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 43.35it/s, est. speed input: 5698.97 toks/s, output: 44.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.32it/s, est. speed input: 5692.45 toks/s, output: 44.47 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.32it/s, est. speed input: 5692.45 toks/s, output: 44.47 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.47it/s, est. speed input: 5692.45 toks/s, output: 44.47 toks/s]
[rank0]:[W126 13:01:12.694057127 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 13:01:14
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=256, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 256 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 257 --max-num-batched-tokens 257 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:01:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=444509) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=444509) WARNING 01-26 13:01:33 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=444509) WARNING 01-26 13:01:40 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 36.00 requests/s, 9251.02 total tokens/s, 36.00 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128

STDERR:
[2026-01-26 13:01:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:01:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:01:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:01:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:01:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:01:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:01:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:01:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:01:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:01:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:01:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:01:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:01:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:01:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:01:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:01:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:01:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:01:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:01:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:01:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:01:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:01:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:01:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:01:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:01:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:01:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:01:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:01:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=444509) [2026-01-26 13:01:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=444509) [2026-01-26 13:01:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=444509) [2026-01-26 13:01:28] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=444509) [2026-01-26 13:01:28] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=444509) [2026-01-26 13:01:28] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=444509) [2026-01-26 13:01:28] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=444509) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=444509) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.39it/s]
(EngineCore_DP0 pid=444509) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.39it/s]
(EngineCore_DP0 pid=444509) 
(EngineCore_DP0 pid=444509) [2026-01-26 13:01:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=444509) [2026-01-26 13:01:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=444509) [2026-01-26 13:01:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=444509) [2026-01-26 13:01:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=444509) [2026-01-26 13:01:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=444509) [2026-01-26 13:01:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=444509) [2026-01-26 13:01:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=444509) [2026-01-26 13:01:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=444509) 2026-01-26 13:01:40,558 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=444509) 2026-01-26 13:01:40,580 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=444509) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.25it/s]
(EngineCore_DP0 pid=444509) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 19.87it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1278.98it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1277.15it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:43,  2.93it/s, est. speed input: 751.28 toks/s, output: 2.93 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:07, 15.76it/s, est. speed input: 3311.17 toks/s, output: 12.93 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:04, 23.92it/s, est. speed input: 4801.37 toks/s, output: 18.76 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 29.34it/s, est. speed input: 5782.62 toks/s, output: 22.59 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 32.93it/s, est. speed input: 6470.39 toks/s, output: 25.27 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 35.45it/s, est. speed input: 6988.46 toks/s, output: 27.30 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:02, 37.15it/s, est. speed input: 7386.87 toks/s, output: 28.85 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:02, 38.37it/s, est. speed input: 7707.47 toks/s, output: 30.11 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 39.21it/s, est. speed input: 7968.38 toks/s, output: 31.13 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 39.71it/s, est. speed input: 8180.09 toks/s, output: 31.95 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 40.03it/s, est. speed input: 8357.44 toks/s, output: 32.65 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 40.33it/s, est. speed input: 8512.78 toks/s, output: 33.25 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 40.20it/s, est. speed input: 8630.53 toks/s, output: 33.71 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 40.42it/s, est. speed input: 8747.29 toks/s, output: 34.17 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:01, 40.29it/s, est. speed input: 8837.92 toks/s, output: 34.52 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 40.51it/s, est. speed input: 8931.28 toks/s, output: 34.89 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 40.67it/s, est. speed input: 9014.53 toks/s, output: 35.21 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 40.73it/s, est. speed input: 9087.82 toks/s, output: 35.50 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 40.85it/s, est. speed input: 9156.59 toks/s, output: 35.77 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 40.86it/s, est. speed input: 9216.76 toks/s, output: 36.00 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 40.84it/s, est. speed input: 9270.59 toks/s, output: 36.21 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.84it/s, est. speed input: 9320.36 toks/s, output: 36.41 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:03<00:00, 40.79it/s, est. speed input: 9364.60 toks/s, output: 36.58 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 40.76it/s, est. speed input: 9405.61 toks/s, output: 36.74 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 40.34it/s, est. speed input: 9431.81 toks/s, output: 36.84 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 40.56it/s, est. speed input: 9470.59 toks/s, output: 36.99 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.56it/s, est. speed input: 9485.21 toks/s, output: 37.05 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.05it/s, est. speed input: 9485.21 toks/s, output: 37.05 toks/s]
[rank0]:[W126 13:01:45.129453354 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 13:32:19
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:32:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=501692) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=501692) WARNING 01-26 13:32:38 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=501692) WARNING 01-26 13:32:46 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 40.61 requests/s, 20832.83 total tokens/s, 40.61 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 13:32:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:32:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:32:26] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:32:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:32:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:32:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:32:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:32:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:32:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:32:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:32:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:32:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:32:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:32:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:32:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:32:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:32:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:32:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=501692) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=501692) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.31it/s]
(EngineCore_DP0 pid=501692) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.31it/s]
(EngineCore_DP0 pid=501692) 
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=501692) 2026-01-26 13:32:46,001 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=501692) 2026-01-26 13:32:46,023 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=501692) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.36it/s]
(EngineCore_DP0 pid=501692) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 19.86it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  57%|█████▋    | 73/128 [00:00<00:00, 728.30it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 326.47it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:01, 112.48it/s, est. speed input: 57595.55 toks/s, output: 112.49 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:01, 58.71it/s, est. speed input: 32379.74 toks/s, output: 63.24 toks/s]  
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 52.42it/s, est. speed input: 29190.41 toks/s, output: 57.01 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:01, 49.64it/s, est. speed input: 27843.13 toks/s, output: 54.38 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:00<00:01, 47.89it/s, est. speed input: 26973.26 toks/s, output: 52.68 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 46.84it/s, est. speed input: 26439.61 toks/s, output: 51.64 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 46.06it/s, est. speed input: 26022.86 toks/s, output: 50.83 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 45.42it/s, est. speed input: 25674.42 toks/s, output: 50.15 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 45.00it/s, est. speed input: 25396.42 toks/s, output: 49.60 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 44.62it/s, est. speed input: 25150.31 toks/s, output: 49.12 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 44.04it/s, est. speed input: 24896.11 toks/s, output: 48.63 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:01, 43.96it/s, est. speed input: 24723.13 toks/s, output: 48.29 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:01, 43.90it/s, est. speed input: 24571.89 toks/s, output: 47.99 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 43.53it/s, est. speed input: 24400.64 toks/s, output: 47.66 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:01<00:00, 43.68it/s, est. speed input: 24294.19 toks/s, output: 47.45 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 43.68it/s, est. speed input: 24188.90 toks/s, output: 47.24 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 43.77it/s, est. speed input: 24102.86 toks/s, output: 47.08 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 43.77it/s, est. speed input: 24019.54 toks/s, output: 46.91 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 43.71it/s, est. speed input: 23939.22 toks/s, output: 46.76 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 43.69it/s, est. speed input: 23867.45 toks/s, output: 46.62 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:02<00:00, 43.79it/s, est. speed input: 23810.76 toks/s, output: 46.51 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.79it/s, est. speed input: 23755.74 toks/s, output: 46.40 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 46.40it/s, est. speed input: 23755.74 toks/s, output: 46.40 toks/s]
[rank0]:[W126 13:32:51.236900172 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 13:32:52
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:33:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=502763) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=502763) WARNING 01-26 13:33:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=502763) WARNING 01-26 13:33:19 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 35.83 requests/s, 36728.93 total tokens/s, 35.83 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 13:32:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:33:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:33:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:33:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:33:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:33:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:33:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:33:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:33:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:33:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:33:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:33:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:33:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:33:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:07] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:07] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:07] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:07] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=502763) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=502763) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.42it/s]
(EngineCore_DP0 pid=502763) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.42it/s]
(EngineCore_DP0 pid=502763) 
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=502763) 2026-01-26 13:33:19,843 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=502763) 2026-01-26 13:33:19,864 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=502763) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.92it/s]
(EngineCore_DP0 pid=502763) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.53it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|██▉       | 38/128 [00:00<00:00, 379.68it/s]
Adding requests:  63%|██████▎   | 81/128 [00:00<00:00, 405.71it/s]
Adding requests:  96%|█████████▌| 123/128 [00:00<00:00, 411.40it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 407.88it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 16.26it/s, est. speed input: 16651.55 toks/s, output: 16.26 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:03, 30.43it/s, est. speed input: 28991.34 toks/s, output: 28.31 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:03, 34.78it/s, est. speed input: 32984.12 toks/s, output: 32.21 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 36.81it/s, est. speed input: 34963.41 toks/s, output: 34.14 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 37.97it/s, est. speed input: 36170.97 toks/s, output: 35.32 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 38.73it/s, est. speed input: 37003.20 toks/s, output: 36.14 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 39.17it/s, est. speed input: 37582.22 toks/s, output: 36.70 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 39.53it/s, est. speed input: 38039.99 toks/s, output: 37.15 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 39.64it/s, est. speed input: 38301.14 toks/s, output: 37.40 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 39.94it/s, est. speed input: 38628.90 toks/s, output: 37.72 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 40.04it/s, est. speed input: 38869.51 toks/s, output: 37.96 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 40.13it/s, est. speed input: 39075.68 toks/s, output: 38.16 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 40.13it/s, est. speed input: 39232.78 toks/s, output: 38.31 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 40.21it/s, est. speed input: 39387.07 toks/s, output: 38.46 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 40.44it/s, est. speed input: 39558.52 toks/s, output: 38.63 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 40.43it/s, est. speed input: 39671.98 toks/s, output: 38.74 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 40.16it/s, est. speed input: 39722.20 toks/s, output: 38.79 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 40.24it/s, est. speed input: 39815.41 toks/s, output: 38.88 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 40.23it/s, est. speed input: 39888.56 toks/s, output: 38.95 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 40.31it/s, est. speed input: 39968.31 toks/s, output: 39.03 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 39.97it/s, est. speed input: 39976.81 toks/s, output: 39.04 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.00it/s, est. speed input: 40025.50 toks/s, output: 39.09 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 40.12it/s, est. speed input: 40083.22 toks/s, output: 39.14 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 40.19it/s, est. speed input: 40135.55 toks/s, output: 39.19 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 40.24it/s, est. speed input: 40183.06 toks/s, output: 39.24 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 40.22it/s, est. speed input: 40219.99 toks/s, output: 39.28 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.22it/s, est. speed input: 40242.03 toks/s, output: 39.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.30it/s, est. speed input: 40242.03 toks/s, output: 39.30 toks/s]
[rank0]:[W126 13:33:25.514961944 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 13:33:27
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:33:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=503825) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=503825) WARNING 01-26 13:33:47 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=503825) WARNING 01-26 13:33:54 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 80.25 requests/s, 82259.06 total tokens/s, 80.25 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 13:33:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:33:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:33:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:33:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:33:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:33:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:33:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:33:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:33:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:33:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:33:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:33:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:33:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:33:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=503825) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=503825) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.73it/s]
(EngineCore_DP0 pid=503825) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.73it/s]
(EngineCore_DP0 pid=503825) 
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=503825) 2026-01-26 13:33:54,082 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=503825) 2026-01-26 13:33:54,104 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=503825) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.84it/s]
(EngineCore_DP0 pid=503825) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 22.78it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:53,  4.76it/s]
Adding requests:  16%|█▋        | 42/256 [00:00<00:01, 167.70it/s]
Adding requests:  34%|███▎      | 86/256 [00:00<00:00, 266.88it/s]
Adding requests:  51%|█████     | 131/256 [00:00<00:00, 328.57it/s]
Adding requests:  68%|██████▊   | 175/256 [00:00<00:00, 364.34it/s]
Adding requests:  84%|████████▍ | 215/256 [00:00<00:00, 372.03it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 315.12it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  21%|██        | 54/256 [00:00<00:00, 480.46it/s, est. speed input: 492024.89 toks/s, output: 480.47 toks/s]
Processed prompts:  40%|████      | 103/256 [00:00<00:01, 140.49it/s, est. speed input: 161883.68 toks/s, output: 158.09 toks/s]
Processed prompts:  50%|█████     | 129/256 [00:00<00:01, 119.64it/s, est. speed input: 139895.67 toks/s, output: 136.62 toks/s]
Processed prompts:  57%|█████▋    | 147/256 [00:01<00:00, 110.91it/s, est. speed input: 131292.01 toks/s, output: 128.21 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:01<00:00, 103.54it/s, est. speed input: 125036.43 toks/s, output: 122.11 toks/s]
Processed prompts:  68%|██████▊   | 175/256 [00:01<00:00, 102.10it/s, est. speed input: 122674.98 toks/s, output: 119.80 toks/s]
Processed prompts:  73%|███████▎  | 187/256 [00:01<00:00, 99.11it/s, est. speed input: 120081.32 toks/s, output: 117.27 toks/s] 
Processed prompts:  77%|███████▋  | 198/256 [00:01<00:00, 94.80it/s, est. speed input: 117297.56 toks/s, output: 114.55 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:01<00:00, 93.57it/s, est. speed input: 115742.53 toks/s, output: 113.03 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:01<00:00, 92.61it/s, est. speed input: 114378.88 toks/s, output: 111.70 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 91.86it/s, est. speed input: 113165.91 toks/s, output: 110.51 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:02<00:00, 91.21it/s, est. speed input: 112059.05 toks/s, output: 109.43 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:02<00:00, 90.74it/s, est. speed input: 111064.40 toks/s, output: 108.46 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 90.74it/s, est. speed input: 110326.77 toks/s, output: 107.74 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 107.73it/s, est. speed input: 110326.77 toks/s, output: 107.74 toks/s]
[rank0]:[W126 13:33:59.439597557 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 13:34:00
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:34:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=504880) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=504880) WARNING 01-26 13:34:22 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=504880) WARNING 01-26 13:34:29 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 162.21 requests/s, 166263.00 total tokens/s, 162.21 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 13:34:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:34:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:34:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:34:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:34:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:34:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:34:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:34:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:34:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:34:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:34:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:34:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:34:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:34:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:17] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:17] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:17] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:17] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:17] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=504880) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=504880) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.08it/s]
(EngineCore_DP0 pid=504880) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.08it/s]
(EngineCore_DP0 pid=504880) 
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=504880) 2026-01-26 13:34:29,127 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=504880) 2026-01-26 13:34:29,149 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=504880) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 22.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 20.01it/s]
(EngineCore_DP0 pid=504880) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 22.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 22.97it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 41/512 [00:00<00:01, 400.30it/s]
Adding requests:  17%|█▋        | 86/512 [00:00<00:01, 425.61it/s]
Adding requests:  26%|██▌       | 131/512 [00:00<00:00, 433.80it/s]
Adding requests:  34%|███▍      | 175/512 [00:00<00:00, 433.31it/s]
Adding requests:  43%|████▎     | 221/512 [00:00<00:00, 441.34it/s]
Adding requests:  53%|█████▎    | 269/512 [00:00<00:00, 451.95it/s]
Adding requests:  62%|██████▏   | 315/512 [00:00<00:00, 439.87it/s]
Adding requests:  71%|███████   | 361/512 [00:00<00:00, 445.89it/s]
Adding requests:  80%|███████▉  | 409/512 [00:00<00:00, 454.08it/s]
Adding requests:  89%|████████▉ | 456/512 [00:01<00:00, 458.27it/s]
Adding requests:  98%|█████████▊| 503/512 [00:01<00:00, 460.06it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 447.93it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:00<00:00, 1656.30it/s, est. speed input: 1696152.15 toks/s, output: 1656.32 toks/s]
Processed prompts:  69%|██████▉   | 352/512 [00:01<00:00, 286.20it/s, est. speed input: 337300.22 toks/s, output: 329.39 toks/s]   
Processed prompts:  84%|████████▍ | 431/512 [00:01<00:00, 242.09it/s, est. speed input: 289036.47 toks/s, output: 282.26 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:01<00:00, 220.11it/s, est. speed input: 268049.82 toks/s, output: 261.77 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 220.11it/s, est. speed input: 260550.59 toks/s, output: 254.44 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 254.43it/s, est. speed input: 260550.59 toks/s, output: 254.44 toks/s]
[rank0]:[W126 13:34:34.960237838 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 13:34:36
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:34:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=505960) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=505960) WARNING 01-26 13:35:00 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=505960) WARNING 01-26 13:35:06 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 291.81 requests/s, 299104.10 total tokens/s, 291.81 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 13:34:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:34:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:34:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:34:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:34:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:34:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:34:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:34:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:34:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:34:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:34:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:34:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:34:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:34:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=505960) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=505960) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.08it/s]
(EngineCore_DP0 pid=505960) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.08it/s]
(EngineCore_DP0 pid=505960) 
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=505960) 2026-01-26 13:35:06,839 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=505960) 2026-01-26 13:35:06,862 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=505960) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 22.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 20.59it/s]
(EngineCore_DP0 pid=505960) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 23.06it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 398.22it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 426.13it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 435.74it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:01, 436.61it/s]
Adding requests:  21%|██▏       | 220/1024 [00:00<00:01, 443.64it/s]
Adding requests:  26%|██▌       | 268/1024 [00:00<00:01, 455.44it/s]
Adding requests:  31%|███       | 314/1024 [00:00<00:01, 453.92it/s]
Adding requests:  35%|███▌      | 361/1024 [00:00<00:01, 456.56it/s]
Adding requests:  40%|███▉      | 408/1024 [00:00<00:01, 458.08it/s]
Adding requests:  45%|████▍     | 456/1024 [00:01<00:01, 461.86it/s]
Adding requests:  49%|████▉     | 503/1024 [00:01<00:01, 462.50it/s]
Adding requests:  54%|█████▎    | 550/1024 [00:01<00:01, 458.39it/s]
Adding requests:  58%|█████▊    | 599/1024 [00:01<00:00, 465.67it/s]
Adding requests:  63%|██████▎   | 648/1024 [00:01<00:00, 471.39it/s]
Adding requests:  68%|██████▊   | 697/1024 [00:01<00:00, 476.53it/s]
Adding requests:  73%|███████▎  | 745/1024 [00:01<00:00, 475.19it/s]
Adding requests:  77%|███████▋  | 793/1024 [00:01<00:00, 474.42it/s]
Adding requests:  82%|████████▏ | 841/1024 [00:01<00:00, 463.59it/s]
Adding requests:  87%|████████▋ | 889/1024 [00:01<00:00, 468.15it/s]
Adding requests:  91%|█████████▏| 936/1024 [00:02<00:00, 467.28it/s]
Adding requests:  96%|█████████▌| 983/1024 [00:02<00:00, 467.57it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 461.21it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:00<00:00, 5652.96it/s, est. speed input: 5789001.25 toks/s, output: 5653.07 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 5652.96it/s, est. speed input: 814336.94 toks/s, output: 795.25 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 795.18it/s, est. speed input: 814336.94 toks/s, output: 795.25 toks/s] 
[rank0]:[W126 13:35:12.149124092 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 13:35:14
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:35:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=507108) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=507108) WARNING 01-26 13:35:43 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=507108) WARNING 01-26 13:35:50 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 460.39 requests/s, 471899.46 total tokens/s, 460.39 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 13:35:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:35:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:35:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:35:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:35:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:35:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:35:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:35:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:35:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:35:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:35:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:35:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:35:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:35:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:35:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:35:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:35:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:35:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:38] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:38] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=507108) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=507108) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.11it/s]
(EngineCore_DP0 pid=507108) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.10it/s]
(EngineCore_DP0 pid=507108) 
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=507108) 2026-01-26 13:35:50,207 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=507108) 2026-01-26 13:35:50,231 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=507108) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 19.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 18.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 18.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 18.01it/s]
(EngineCore_DP0 pid=507108) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 18.31it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 18.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.96it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 39/2048 [00:00<00:05, 380.80it/s]
Adding requests:   4%|▍         | 83/2048 [00:00<00:04, 415.26it/s]
Adding requests:   6%|▋         | 128/2048 [00:00<00:04, 426.85it/s]
Adding requests:   8%|▊         | 172/2048 [00:00<00:04, 431.08it/s]
Adding requests:  11%|█         | 218/2048 [00:00<00:04, 439.07it/s]
Adding requests:  13%|█▎        | 266/2048 [00:00<00:03, 451.42it/s]
Adding requests:  15%|█▌        | 312/2048 [00:00<00:03, 451.63it/s]
Adding requests:  18%|█▊        | 359/2048 [00:00<00:03, 453.95it/s]
Adding requests:  20%|█▉        | 405/2048 [00:00<00:03, 450.20it/s]
Adding requests:  22%|██▏       | 452/2048 [00:01<00:03, 454.65it/s]
Adding requests:  24%|██▍       | 499/2048 [00:01<00:03, 456.93it/s]
Adding requests:  27%|██▋       | 545/2048 [00:01<00:03, 451.91it/s]
Adding requests:  29%|██▉       | 593/2048 [00:01<00:03, 457.74it/s]
Adding requests:  31%|███▏      | 641/2048 [00:01<00:03, 462.61it/s]
Adding requests:  34%|███▎      | 690/2048 [00:01<00:02, 468.65it/s]
Adding requests:  36%|███▌      | 739/2048 [00:01<00:02, 473.82it/s]
Adding requests:  38%|███▊      | 787/2048 [00:01<00:02, 470.24it/s]
Adding requests:  41%|████      | 835/2048 [00:01<00:02, 458.73it/s]
Adding requests:  43%|████▎     | 884/2048 [00:01<00:02, 465.81it/s]
Adding requests:  46%|████▌     | 933/2048 [00:02<00:02, 471.32it/s]
Adding requests:  48%|████▊     | 981/2048 [00:02<00:02, 471.95it/s]
Adding requests:  50%|█████     | 1030/2048 [00:02<00:02, 476.09it/s]
Adding requests:  53%|█████▎    | 1078/2048 [00:02<00:02, 472.21it/s]
Adding requests:  55%|█████▍    | 1126/2048 [00:02<00:01, 470.64it/s]
Adding requests:  57%|█████▋    | 1176/2048 [00:02<00:01, 476.44it/s]
Adding requests:  60%|█████▉    | 1226/2048 [00:02<00:01, 482.98it/s]
Adding requests:  62%|██████▏   | 1275/2048 [00:02<00:01, 476.34it/s]
Adding requests:  65%|██████▍   | 1325/2048 [00:02<00:01, 480.76it/s]
Adding requests:  67%|██████▋   | 1374/2048 [00:02<00:01, 481.01it/s]
Adding requests:  69%|██████▉   | 1423/2048 [00:03<00:01, 482.08it/s]
Adding requests:  72%|███████▏  | 1472/2048 [00:03<00:01, 464.23it/s]
Adding requests:  74%|███████▍  | 1521/2048 [00:03<00:01, 469.69it/s]
Adding requests:  77%|███████▋  | 1570/2048 [00:03<00:01, 474.09it/s]
Adding requests:  79%|███████▉  | 1618/2048 [00:03<00:00, 467.84it/s]
Adding requests:  81%|████████▏ | 1666/2048 [00:03<00:00, 469.91it/s]
Adding requests:  84%|████████▎ | 1714/2048 [00:03<00:00, 472.72it/s]
Adding requests:  86%|████████▌ | 1762/2048 [00:03<00:00, 473.12it/s]
Adding requests:  88%|████████▊ | 1810/2048 [00:03<00:00, 472.05it/s]
Adding requests:  91%|█████████ | 1858/2048 [00:03<00:00, 474.24it/s]
Adding requests:  93%|█████████▎| 1906/2048 [00:04<00:00, 472.65it/s]
Adding requests:  95%|█████████▌| 1954/2048 [00:04<00:00, 472.56it/s]
Adding requests:  98%|█████████▊| 2003/2048 [00:04<00:00, 477.17it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 466.06it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 39058.12it/s, est. speed input: 40006426.71 toks/s, output: 39065.76 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 38983.85it/s, est. speed input: 40006426.71 toks/s, output: 39065.76 toks/s]
[rank0]:[W126 13:35:57.393857278 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 13:35:59
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:36:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=508402) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=508402) WARNING 01-26 13:36:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=508402) WARNING 01-26 13:36:43 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 468.82 requests/s, 480542.79 total tokens/s, 468.82 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 13:36:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:36:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:36:24] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:36:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:36:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:36:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:36:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:36:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:36:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:36:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:36:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:36:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:36:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:36:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:36:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:36:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:36:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:36:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=508402) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=508402) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.07it/s]
(EngineCore_DP0 pid=508402) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.07it/s]
(EngineCore_DP0 pid=508402) 
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=508402) [rank0]:W0126 13:36:40.296000 508402 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=508402) [rank0]:W0126 13:36:40.368000 508402 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=508402) [rank0]:W0126 13:36:41.173000 508402 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=508402) [rank0]:W0126 13:36:41.270000 508402 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=508402) 2026-01-26 13:36:43,854 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=508402) 2026-01-26 13:36:43,878 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=508402) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 19.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 20.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 22.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 20.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 20.79it/s]
(EngineCore_DP0 pid=508402) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 20.45it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 21.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 21.83it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 38/4096 [00:00<00:10, 372.15it/s]
Adding requests:   2%|▏         | 82/4096 [00:00<00:09, 407.50it/s]
Adding requests:   3%|▎         | 127/4096 [00:00<00:09, 424.78it/s]
Adding requests:   4%|▍         | 171/4096 [00:00<00:09, 430.33it/s]
Adding requests:   5%|▌         | 216/4096 [00:00<00:08, 436.71it/s]
Adding requests:   6%|▋         | 264/4096 [00:00<00:08, 450.17it/s]
Adding requests:   8%|▊         | 310/4096 [00:00<00:08, 450.59it/s]
Adding requests:   9%|▊         | 357/4096 [00:00<00:08, 454.80it/s]
Adding requests:  10%|▉         | 405/4096 [00:00<00:08, 459.73it/s]
Adding requests:  11%|█         | 452/4096 [00:01<00:07, 461.22it/s]
Adding requests:  12%|█▏        | 499/4096 [00:01<00:07, 463.22it/s]
Adding requests:  13%|█▎        | 546/4096 [00:01<00:07, 455.05it/s]
Adding requests:  15%|█▍        | 596/4096 [00:01<00:07, 463.61it/s]
Adding requests:  16%|█▌        | 644/4096 [00:01<00:07, 467.79it/s]
Adding requests:  17%|█▋        | 694/4096 [00:01<00:07, 476.20it/s]
Adding requests:  18%|█▊        | 742/4096 [00:01<00:07, 473.70it/s]
Adding requests:  19%|█▉        | 790/4096 [00:01<00:07, 466.32it/s]
Adding requests:  20%|██        | 837/4096 [00:01<00:07, 459.18it/s]
Adding requests:  22%|██▏       | 886/4096 [00:01<00:06, 467.16it/s]
Adding requests:  23%|██▎       | 935/4096 [00:02<00:06, 472.24it/s]
Adding requests:  24%|██▍       | 983/4096 [00:02<00:06, 474.41it/s]
Adding requests:  25%|██▌       | 1032/4096 [00:02<00:06, 478.87it/s]
Adding requests:  26%|██▋       | 1080/4096 [00:02<00:06, 475.15it/s]
Adding requests:  28%|██▊       | 1128/4096 [00:02<00:06, 454.05it/s]
Adding requests:  29%|██▊       | 1177/4096 [00:02<00:06, 464.36it/s]
Adding requests:  30%|██▉       | 1227/4096 [00:02<00:06, 472.77it/s]
Adding requests:  31%|███       | 1275/4096 [00:02<00:05, 470.62it/s]
Adding requests:  32%|███▏      | 1324/4096 [00:02<00:05, 476.01it/s]
Adding requests:  34%|███▎      | 1373/4096 [00:02<00:05, 478.44it/s]
Adding requests:  35%|███▍      | 1422/4096 [00:03<00:05, 479.50it/s]
Adding requests:  36%|███▌      | 1471/4096 [00:03<00:05, 482.28it/s]
Adding requests:  37%|███▋      | 1521/4096 [00:03<00:05, 484.76it/s]
Adding requests:  38%|███▊      | 1570/4096 [00:03<00:05, 485.10it/s]
Adding requests:  40%|███▉      | 1620/4096 [00:03<00:05, 489.04it/s]
Adding requests:  41%|████      | 1669/4096 [00:03<00:05, 485.06it/s]
Adding requests:  42%|████▏     | 1718/4096 [00:03<00:04, 481.61it/s]
Adding requests:  43%|████▎     | 1767/4096 [00:03<00:04, 478.94it/s]
Adding requests:  44%|████▍     | 1816/4096 [00:03<00:04, 473.13it/s]
Adding requests:  46%|████▌     | 1864/4096 [00:03<00:04, 472.16it/s]
Adding requests:  47%|████▋     | 1913/4096 [00:04<00:04, 475.60it/s]
Adding requests:  48%|████▊     | 1962/4096 [00:04<00:04, 478.52it/s]
Adding requests:  49%|████▉     | 2012/4096 [00:04<00:04, 482.56it/s]
Adding requests:  50%|█████     | 2061/4096 [00:04<00:04, 484.68it/s]
Adding requests:  52%|█████▏    | 2111/4096 [00:04<00:04, 484.72it/s]
Adding requests:  53%|█████▎    | 2160/4096 [00:04<00:04, 476.50it/s]
Adding requests:  54%|█████▍    | 2208/4096 [00:04<00:03, 473.90it/s]
Adding requests:  55%|█████▌    | 2258/4096 [00:04<00:03, 480.90it/s]
Adding requests:  56%|█████▋    | 2307/4096 [00:04<00:03, 479.62it/s]
Adding requests:  57%|█████▋    | 2355/4096 [00:05<00:03, 478.41it/s]
Adding requests:  59%|█████▊    | 2403/4096 [00:05<00:03, 477.94it/s]
Adding requests:  60%|█████▉    | 2452/4096 [00:05<00:03, 479.90it/s]
Adding requests:  61%|██████    | 2501/4096 [00:05<00:03, 481.69it/s]
Adding requests:  62%|██████▏   | 2550/4096 [00:05<00:03, 483.45it/s]
Adding requests:  63%|██████▎   | 2599/4096 [00:05<00:03, 483.03it/s]
Adding requests:  65%|██████▍   | 2649/4096 [00:05<00:02, 485.27it/s]
Adding requests:  66%|██████▌   | 2698/4096 [00:05<00:02, 480.87it/s]
Adding requests:  67%|██████▋   | 2747/4096 [00:05<00:02, 480.68it/s]
Adding requests:  68%|██████▊   | 2796/4096 [00:05<00:02, 477.57it/s]
Adding requests:  69%|██████▉   | 2844/4096 [00:06<00:02, 473.95it/s]
Adding requests:  71%|███████   | 2892/4096 [00:06<00:02, 474.84it/s]
Adding requests:  72%|███████▏  | 2940/4096 [00:06<00:02, 472.63it/s]
Adding requests:  73%|███████▎  | 2988/4096 [00:06<00:02, 466.02it/s]
Adding requests:  74%|███████▍  | 3036/4096 [00:06<00:02, 468.16it/s]
Adding requests:  75%|███████▌  | 3084/4096 [00:06<00:02, 468.70it/s]
Adding requests:  76%|███████▋  | 3133/4096 [00:06<00:02, 474.07it/s]
Adding requests:  78%|███████▊  | 3181/4096 [00:06<00:01, 475.56it/s]
Adding requests:  79%|███████▉  | 3229/4096 [00:06<00:01, 474.42it/s]
Adding requests:  80%|████████  | 3278/4096 [00:06<00:01, 477.68it/s]
Adding requests:  81%|████████  | 3327/4096 [00:07<00:01, 478.52it/s]
Adding requests:  82%|████████▏ | 3377/4096 [00:07<00:01, 482.58it/s]
Adding requests:  84%|████████▎ | 3427/4096 [00:07<00:01, 485.56it/s]
Adding requests:  85%|████████▍ | 3476/4096 [00:07<00:01, 473.51it/s]
Adding requests:  86%|████████▌ | 3524/4096 [00:07<00:01, 464.03it/s]
Adding requests:  87%|████████▋ | 3571/4096 [00:07<00:01, 464.58it/s]
Adding requests:  88%|████████▊ | 3619/4096 [00:07<00:01, 466.57it/s]
Adding requests:  90%|████████▉ | 3667/4096 [00:07<00:00, 470.23it/s]
Adding requests:  91%|█████████ | 3716/4096 [00:07<00:00, 474.10it/s]
Adding requests:  92%|█████████▏| 3765/4096 [00:07<00:00, 477.51it/s]
Adding requests:  93%|█████████▎| 3815/4096 [00:08<00:00, 483.44it/s]
Adding requests:  94%|█████████▍| 3865/4096 [00:08<00:00, 487.95it/s]
Adding requests:  96%|█████████▌| 3914/4096 [00:08<00:00, 486.80it/s]
Adding requests:  97%|█████████▋| 3963/4096 [00:08<00:00, 486.80it/s]
Adding requests:  98%|█████████▊| 4012/4096 [00:08<00:00, 485.71it/s]
Adding requests:  99%|█████████▉| 4061/4096 [00:08<00:00, 477.29it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 473.02it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 54181.16it/s, est. speed input: 55492528.98 toks/s, output: 54188.50 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 54107.95it/s, est. speed input: 55492528.98 toks/s, output: 54188.50 toks/s]
[rank0]:[W126 13:36:55.534319533 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 13:36:57
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-1B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:37:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=510000) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=510000) WARNING 01-26 13:37:53 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=510000) WARNING 01-26 13:38:00 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 469.19 requests/s, 480920.83 total tokens/s, 469.19 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 13:37:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:37:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:37:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:37:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:37:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:37:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:37:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:37:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:37:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:37:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:37:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:37:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:37:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:37:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:37:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:37:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:37:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:37:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:48] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=510000) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=510000) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.11it/s]
(EngineCore_DP0 pid=510000) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.11it/s]
(EngineCore_DP0 pid=510000) 
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=510000) [rank0]:W0126 13:37:56.457000 510000 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=510000) [rank0]:W0126 13:37:56.527000 510000 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=510000) [rank0]:W0126 13:37:57.331000 510000 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=510000) [rank0]:W0126 13:37:57.429000 510000 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=510000) 2026-01-26 13:38:00,164 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=510000) 2026-01-26 13:38:00,188 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=510000) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:00, 17.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:00, 19.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 20.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 20.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 19.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:00<00:00, 19.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 19.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 19.55it/s]
(EngineCore_DP0 pid=510000) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00, 23.09it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 23.23it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 23.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 23.25it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 40/8192 [00:00<00:20, 391.73it/s]
Adding requests:   1%|          | 85/8192 [00:00<00:19, 420.63it/s]
Adding requests:   2%|▏         | 129/8192 [00:00<00:18, 426.71it/s]
Adding requests:   2%|▏         | 173/8192 [00:00<00:18, 430.56it/s]
Adding requests:   3%|▎         | 218/8192 [00:00<00:18, 436.62it/s]
Adding requests:   3%|▎         | 266/8192 [00:00<00:17, 449.60it/s]
Adding requests:   4%|▍         | 311/8192 [00:00<00:17, 447.41it/s]
Adding requests:   4%|▍         | 358/8192 [00:00<00:17, 451.40it/s]
Adding requests:   5%|▍         | 405/8192 [00:00<00:17, 456.11it/s]
Adding requests:   6%|▌         | 451/8192 [00:01<00:17, 454.54it/s]
Adding requests:   6%|▌         | 498/8192 [00:01<00:16, 457.30it/s]
Adding requests:   7%|▋         | 544/8192 [00:01<00:17, 449.78it/s]
Adding requests:   7%|▋         | 593/8192 [00:01<00:16, 459.88it/s]
Adding requests:   8%|▊         | 640/8192 [00:01<00:16, 462.35it/s]
Adding requests:   8%|▊         | 689/8192 [00:01<00:16, 468.16it/s]
Adding requests:   9%|▉         | 738/8192 [00:01<00:15, 472.80it/s]
Adding requests:  10%|▉         | 786/8192 [00:01<00:15, 467.44it/s]
Adding requests:  10%|█         | 833/8192 [00:01<00:16, 458.72it/s]
Adding requests:  11%|█         | 881/8192 [00:01<00:15, 462.52it/s]
Adding requests:  11%|█▏        | 930/8192 [00:02<00:15, 468.27it/s]
Adding requests:  12%|█▏        | 978/8192 [00:02<00:15, 468.69it/s]
Adding requests:  13%|█▎        | 1027/8192 [00:02<00:15, 472.77it/s]
Adding requests:  13%|█▎        | 1075/8192 [00:02<00:15, 467.35it/s]
Adding requests:  14%|█▎        | 1122/8192 [00:02<00:15, 466.30it/s]
Adding requests:  14%|█▍        | 1171/8192 [00:02<00:14, 471.54it/s]
Adding requests:  15%|█▍        | 1221/8192 [00:02<00:14, 479.05it/s]
Adding requests:  15%|█▌        | 1269/8192 [00:02<00:14, 472.28it/s]
Adding requests:  16%|█▌        | 1317/8192 [00:02<00:14, 474.26it/s]
Adding requests:  17%|█▋        | 1366/8192 [00:02<00:14, 477.96it/s]
Adding requests:  17%|█▋        | 1415/8192 [00:03<00:14, 480.30it/s]
Adding requests:  18%|█▊        | 1464/8192 [00:03<00:14, 479.74it/s]
Adding requests:  18%|█▊        | 1512/8192 [00:03<00:14, 469.85it/s]
Adding requests:  19%|█▉        | 1560/8192 [00:03<00:14, 470.50it/s]
Adding requests:  20%|█▉        | 1610/8192 [00:03<00:13, 477.58it/s]
Adding requests:  20%|██        | 1658/8192 [00:03<00:13, 474.68it/s]
Adding requests:  21%|██        | 1706/8192 [00:03<00:13, 473.49it/s]
Adding requests:  21%|██▏       | 1754/8192 [00:03<00:13, 473.47it/s]
Adding requests:  22%|██▏       | 1802/8192 [00:03<00:13, 474.07it/s]
Adding requests:  23%|██▎       | 1850/8192 [00:03<00:13, 474.34it/s]
Adding requests:  23%|██▎       | 1898/8192 [00:04<00:13, 474.14it/s]
Adding requests:  24%|██▍       | 1946/8192 [00:04<00:13, 474.58it/s]
Adding requests:  24%|██▍       | 1994/8192 [00:04<00:13, 473.48it/s]
Adding requests:  25%|██▍       | 2043/8192 [00:04<00:12, 477.31it/s]
Adding requests:  26%|██▌       | 2092/8192 [00:04<00:12, 480.13it/s]
Adding requests:  26%|██▌       | 2141/8192 [00:04<00:12, 473.38it/s]
Adding requests:  27%|██▋       | 2189/8192 [00:04<00:12, 468.72it/s]
Adding requests:  27%|██▋       | 2238/8192 [00:04<00:12, 474.23it/s]
Adding requests:  28%|██▊       | 2286/8192 [00:04<00:12, 470.75it/s]
Adding requests:  29%|██▊       | 2335/8192 [00:05<00:12, 473.96it/s]
Adding requests:  29%|██▉       | 2383/8192 [00:05<00:12, 475.49it/s]
Adding requests:  30%|██▉       | 2432/8192 [00:05<00:12, 477.52it/s]
Adding requests:  30%|███       | 2480/8192 [00:05<00:12, 475.14it/s]
Adding requests:  31%|███       | 2528/8192 [00:05<00:11, 474.60it/s]
Adding requests:  31%|███▏      | 2576/8192 [00:05<00:11, 470.57it/s]
Adding requests:  32%|███▏      | 2624/8192 [00:05<00:11, 471.19it/s]
Adding requests:  33%|███▎      | 2673/8192 [00:05<00:11, 473.13it/s]
Adding requests:  33%|███▎      | 2721/8192 [00:05<00:11, 463.02it/s]
Adding requests:  34%|███▍      | 2768/8192 [00:05<00:11, 464.73it/s]
Adding requests:  34%|███▍      | 2815/8192 [00:06<00:11, 463.69it/s]
Adding requests:  35%|███▍      | 2862/8192 [00:06<00:11, 465.15it/s]
Adding requests:  36%|███▌      | 2911/8192 [00:06<00:11, 471.82it/s]
Adding requests:  36%|███▌      | 2959/8192 [00:06<00:11, 466.25it/s]
Adding requests:  37%|███▋      | 3007/8192 [00:06<00:11, 469.49it/s]
Adding requests:  37%|███▋      | 3055/8192 [00:06<00:10, 471.12it/s]
Adding requests:  38%|███▊      | 3103/8192 [00:06<00:10, 468.01it/s]
Adding requests:  38%|███▊      | 3150/8192 [00:06<00:10, 467.98it/s]
Adding requests:  39%|███▉      | 3198/8192 [00:06<00:10, 469.79it/s]
Adding requests:  40%|███▉      | 3247/8192 [00:06<00:10, 474.06it/s]
Adding requests:  40%|████      | 3295/8192 [00:07<00:10, 474.68it/s]
Adding requests:  41%|████      | 3344/8192 [00:07<00:10, 476.64it/s]
Adding requests:  41%|████▏     | 3392/8192 [00:07<00:10, 475.94it/s]
Adding requests:  42%|████▏     | 3442/8192 [00:07<00:09, 480.58it/s]
Adding requests:  43%|████▎     | 3491/8192 [00:07<00:09, 470.59it/s]
Adding requests:  43%|████▎     | 3539/8192 [00:07<00:09, 472.80it/s]
Adding requests:  44%|████▍     | 3587/8192 [00:07<00:09, 470.92it/s]
Adding requests:  44%|████▍     | 3635/8192 [00:07<00:09, 462.06it/s]
Adding requests:  45%|████▍     | 3682/8192 [00:07<00:09, 458.05it/s]
Adding requests:  46%|████▌     | 3728/8192 [00:07<00:09, 455.45it/s]
Adding requests:  46%|████▌     | 3774/8192 [00:08<00:09, 446.07it/s]
Adding requests:  47%|████▋     | 3821/8192 [00:08<00:09, 450.02it/s]
Adding requests:  47%|████▋     | 3869/8192 [00:08<00:09, 456.49it/s]
Adding requests:  48%|████▊     | 3916/8192 [00:08<00:09, 457.61it/s]
Adding requests:  48%|████▊     | 3963/8192 [00:08<00:09, 460.79it/s]
Adding requests:  49%|████▉     | 4010/8192 [00:08<00:09, 462.04it/s]
Adding requests:  50%|████▉     | 4057/8192 [00:08<00:09, 458.35it/s]
Adding requests:  50%|█████     | 4105/8192 [00:08<00:08, 463.34it/s]
Adding requests:  51%|█████     | 4152/8192 [00:08<00:08, 465.15it/s]
Adding requests:  51%|█████▏    | 4202/8192 [00:09<00:08, 472.67it/s]
Adding requests:  52%|█████▏    | 4250/8192 [00:09<00:08, 472.87it/s]
Adding requests:  52%|█████▏    | 4298/8192 [00:09<00:08, 471.68it/s]
Adding requests:  53%|█████▎    | 4348/8192 [00:09<00:08, 477.16it/s]
Adding requests:  54%|█████▎    | 4397/8192 [00:09<00:07, 480.42it/s]
Adding requests:  54%|█████▍    | 4446/8192 [00:09<00:07, 479.82it/s]
Adding requests:  55%|█████▍    | 4494/8192 [00:09<00:07, 475.08it/s]
Adding requests:  55%|█████▌    | 4542/8192 [00:09<00:07, 473.41it/s]
Adding requests:  56%|█████▌    | 4591/8192 [00:09<00:07, 475.22it/s]
Adding requests:  57%|█████▋    | 4640/8192 [00:09<00:07, 478.62it/s]
Adding requests:  57%|█████▋    | 4688/8192 [00:10<00:07, 473.28it/s]
Adding requests:  58%|█████▊    | 4738/8192 [00:10<00:07, 478.99it/s]
Adding requests:  58%|█████▊    | 4786/8192 [00:10<00:07, 477.98it/s]
Adding requests:  59%|█████▉    | 4834/8192 [00:10<00:07, 475.79it/s]
Adding requests:  60%|█████▉    | 4882/8192 [00:10<00:06, 473.70it/s]
Adding requests:  60%|██████    | 4930/8192 [00:10<00:07, 463.92it/s]
Adding requests:  61%|██████    | 4978/8192 [00:10<00:06, 468.03it/s]
Adding requests:  61%|██████▏   | 5026/8192 [00:10<00:06, 471.37it/s]
Adding requests:  62%|██████▏   | 5075/8192 [00:10<00:06, 475.73it/s]
Adding requests:  63%|██████▎   | 5124/8192 [00:10<00:06, 479.67it/s]
Adding requests:  63%|██████▎   | 5172/8192 [00:11<00:06, 479.53it/s]
Adding requests:  64%|██████▎   | 5220/8192 [00:11<00:06, 477.53it/s]
Adding requests:  64%|██████▍   | 5268/8192 [00:11<00:06, 475.89it/s]
Adding requests:  65%|██████▍   | 5317/8192 [00:11<00:06, 479.03it/s]
Adding requests:  66%|██████▌   | 5366/8192 [00:11<00:05, 481.54it/s]
Adding requests:  66%|██████▌   | 5415/8192 [00:11<00:05, 480.48it/s]
Adding requests:  67%|██████▋   | 5464/8192 [00:11<00:05, 475.90it/s]
Adding requests:  67%|██████▋   | 5512/8192 [00:11<00:05, 471.63it/s]
Adding requests:  68%|██████▊   | 5560/8192 [00:11<00:05, 472.77it/s]
Adding requests:  68%|██████▊   | 5608/8192 [00:11<00:05, 471.56it/s]
Adding requests:  69%|██████▉   | 5656/8192 [00:12<00:05, 471.55it/s]
Adding requests:  70%|██████▉   | 5704/8192 [00:12<00:05, 473.24it/s]
Adding requests:  70%|███████   | 5753/8192 [00:12<00:05, 475.87it/s]
Adding requests:  71%|███████   | 5801/8192 [00:12<00:05, 474.50it/s]
Adding requests:  71%|███████▏  | 5849/8192 [00:12<00:04, 472.24it/s]
Adding requests:  72%|███████▏  | 5898/8192 [00:12<00:04, 477.28it/s]
Adding requests:  73%|███████▎  | 5947/8192 [00:12<00:04, 479.52it/s]
Adding requests:  73%|███████▎  | 5996/8192 [00:12<00:04, 481.35it/s]
Adding requests:  74%|███████▍  | 6046/8192 [00:12<00:04, 485.32it/s]
Adding requests:  74%|███████▍  | 6095/8192 [00:12<00:04, 470.43it/s]
Adding requests:  75%|███████▌  | 6144/8192 [00:13<00:04, 473.99it/s]
Adding requests:  76%|███████▌  | 6193/8192 [00:13<00:04, 477.06it/s]
Adding requests:  76%|███████▌  | 6243/8192 [00:13<00:04, 483.14it/s]
Adding requests:  77%|███████▋  | 6294/8192 [00:13<00:03, 488.40it/s]
Adding requests:  77%|███████▋  | 6344/8192 [00:13<00:03, 489.47it/s]
Adding requests:  78%|███████▊  | 6394/8192 [00:13<00:03, 491.34it/s]
Adding requests:  79%|███████▊  | 6445/8192 [00:13<00:03, 495.97it/s]
Adding requests:  79%|███████▉  | 6495/8192 [00:13<00:03, 496.49it/s]
Adding requests:  80%|███████▉  | 6545/8192 [00:13<00:03, 497.07it/s]
Adding requests:  81%|████████  | 6595/8192 [00:13<00:03, 492.34it/s]
Adding requests:  81%|████████  | 6645/8192 [00:14<00:03, 492.27it/s]
Adding requests:  82%|████████▏ | 6695/8192 [00:14<00:03, 489.10it/s]
Adding requests:  82%|████████▏ | 6744/8192 [00:14<00:02, 487.59it/s]
Adding requests:  83%|████████▎ | 6795/8192 [00:14<00:02, 492.53it/s]
Adding requests:  84%|████████▎ | 6845/8192 [00:14<00:02, 490.96it/s]
Adding requests:  84%|████████▍ | 6895/8192 [00:14<00:02, 493.61it/s]
Adding requests:  85%|████████▍ | 6946/8192 [00:14<00:02, 496.84it/s]
Adding requests:  85%|████████▌ | 6996/8192 [00:14<00:02, 490.64it/s]
Adding requests:  86%|████████▌ | 7046/8192 [00:14<00:02, 487.41it/s]
Adding requests:  87%|████████▋ | 7095/8192 [00:15<00:02, 485.55it/s]
Adding requests:  87%|████████▋ | 7144/8192 [00:15<00:02, 486.33it/s]
Adding requests:  88%|████████▊ | 7193/8192 [00:15<00:02, 483.48it/s]
Adding requests:  88%|████████▊ | 7242/8192 [00:15<00:01, 484.61it/s]
Adding requests:  89%|████████▉ | 7291/8192 [00:15<00:01, 477.73it/s]
Adding requests:  90%|████████▉ | 7340/8192 [00:15<00:01, 479.23it/s]
Adding requests:  90%|█████████ | 7388/8192 [00:15<00:01, 479.28it/s]
Adding requests:  91%|█████████ | 7439/8192 [00:15<00:01, 488.35it/s]
Adding requests:  91%|█████████▏| 7488/8192 [00:15<00:01, 487.44it/s]
Adding requests:  92%|█████████▏| 7537/8192 [00:15<00:01, 486.64it/s]
Adding requests:  93%|█████████▎| 7586/8192 [00:16<00:01, 484.68it/s]
Adding requests:  93%|█████████▎| 7635/8192 [00:16<00:01, 482.47it/s]
Adding requests:  94%|█████████▍| 7685/8192 [00:16<00:01, 486.85it/s]
Adding requests:  94%|█████████▍| 7734/8192 [00:16<00:00, 485.16it/s]
Adding requests:  95%|█████████▌| 7783/8192 [00:16<00:00, 483.19it/s]
Adding requests:  96%|█████████▌| 7832/8192 [00:16<00:00, 482.53it/s]
Adding requests:  96%|█████████▌| 7881/8192 [00:16<00:00, 480.55it/s]
Adding requests:  97%|█████████▋| 7930/8192 [00:16<00:00, 459.11it/s]
Adding requests:  97%|█████████▋| 7977/8192 [00:16<00:00, 460.66it/s]
Adding requests:  98%|█████████▊| 8025/8192 [00:16<00:00, 461.91it/s]
Adding requests:  99%|█████████▊| 8074/8192 [00:17<00:00, 469.47it/s]
Adding requests:  99%|█████████▉| 8124/8192 [00:17<00:00, 475.97it/s]
Adding requests: 100%|█████████▉| 8172/8192 [00:17<00:00, 475.73it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 473.18it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  69%|██████▉   | 5632/8192 [00:00<00:00, 56318.31it/s, est. speed input: 57673660.07 toks/s, output: 56319.38 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 56318.31it/s, est. speed input: 58178777.62 toks/s, output: 56813.33 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 56768.75it/s, est. speed input: 58178777.62 toks/s, output: 56813.33 toks/s]
[rank0]:[W126 13:38:21.350841771 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 14:34:41
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:34:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=598789) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=598789) WARNING 01-26 14:35:03 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=598789) WARNING 01-26 14:35:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 32.76 requests/s, 16806.50 total tokens/s, 32.76 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 14:34:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:34:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:34:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:34:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:34:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:34:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:34:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:34:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:34:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:34:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:34:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:34:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:34:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:34:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:34:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:34:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:34:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:34:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:34:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:34:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:34:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:34:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:34:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:34:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:34:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:34:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:34:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:34:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=598789) [2026-01-26 14:34:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=598789) [2026-01-26 14:34:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=598789) [2026-01-26 14:34:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=598789) [2026-01-26 14:34:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=598789) [2026-01-26 14:34:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=598789) [2026-01-26 14:34:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=598789) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=598789) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.55it/s]
(EngineCore_DP0 pid=598789) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.55it/s]
(EngineCore_DP0 pid=598789) 
(EngineCore_DP0 pid=598789) [2026-01-26 14:34:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=598789) [2026-01-26 14:34:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=598789) [2026-01-26 14:34:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=598789) [2026-01-26 14:34:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=598789) [2026-01-26 14:34:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=598789) [2026-01-26 14:34:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=598789) [2026-01-26 14:34:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=598789) [2026-01-26 14:34:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=598789) 2026-01-26 14:35:13,879 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=598789) 2026-01-26 14:35:13,901 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=598789) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.85it/s]
(EngineCore_DP0 pid=598789) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.76it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  55%|█████▍    | 70/128 [00:00<00:00, 697.82it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 334.14it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:19,  6.59it/s, est. speed input: 3372.32 toks/s, output: 6.59 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:05, 21.96it/s, est. speed input: 9860.88 toks/s, output: 19.26 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 28.44it/s, est. speed input: 12585.47 toks/s, output: 24.58 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 31.82it/s, est. speed input: 14079.59 toks/s, output: 27.50 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 33.78it/s, est. speed input: 15019.83 toks/s, output: 29.34 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 35.06it/s, est. speed input: 15679.50 toks/s, output: 30.62 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 35.86it/s, est. speed input: 16155.86 toks/s, output: 31.55 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 36.42it/s, est. speed input: 16525.04 toks/s, output: 32.28 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 36.87it/s, est. speed input: 16825.82 toks/s, output: 32.86 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 37.14it/s, est. speed input: 17065.40 toks/s, output: 33.33 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 37.32it/s, est. speed input: 17261.31 toks/s, output: 33.71 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 37.40it/s, est. speed input: 17421.10 toks/s, output: 34.03 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 37.44it/s, est. speed input: 17554.89 toks/s, output: 34.29 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 37.53it/s, est. speed input: 17676.71 toks/s, output: 34.52 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 37.62it/s, est. speed input: 17786.16 toks/s, output: 34.74 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 37.67it/s, est. speed input: 17880.11 toks/s, output: 34.92 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 37.64it/s, est. speed input: 17958.25 toks/s, output: 35.07 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 37.66it/s, est. speed input: 18031.43 toks/s, output: 35.22 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 37.67it/s, est. speed input: 18096.53 toks/s, output: 35.34 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 37.74it/s, est. speed input: 18159.91 toks/s, output: 35.47 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 37.70it/s, est. speed input: 18211.41 toks/s, output: 35.57 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 37.67it/s, est. speed input: 18257.55 toks/s, output: 35.66 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 37.60it/s, est. speed input: 18296.72 toks/s, output: 35.74 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 37.63it/s, est. speed input: 18337.64 toks/s, output: 35.82 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 37.74it/s, est. speed input: 18381.37 toks/s, output: 35.90 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 37.78it/s, est. speed input: 18419.21 toks/s, output: 35.97 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 37.78it/s, est. speed input: 18452.70 toks/s, output: 36.04 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 37.76it/s, est. speed input: 18483.08 toks/s, output: 36.10 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 37.74it/s, est. speed input: 18510.40 toks/s, output: 36.15 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 37.77it/s, est. speed input: 18538.61 toks/s, output: 36.21 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 37.80it/s, est. speed input: 18565.52 toks/s, output: 36.26 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 37.75it/s, est. speed input: 18587.21 toks/s, output: 36.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.75it/s, est. speed input: 18603.15 toks/s, output: 36.33 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.33it/s, est. speed input: 18603.15 toks/s, output: 36.33 toks/s]
[rank0]:[W126 14:35:20.151142406 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 14:35:22
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:35:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=600103) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=600103) WARNING 01-26 14:35:45 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=600103) WARNING 01-26 14:35:55 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 33.56 requests/s, 34402.78 total tokens/s, 33.56 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 14:35:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:35:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:35:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:35:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:35:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:35:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:35:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:35:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:35:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:35:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:35:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:35:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:35:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:35:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:35:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:35:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:35:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:35:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:35:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:35:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:35:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:35:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:35:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:35:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:35:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:35:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:35:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:35:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=600103) [2026-01-26 14:35:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=600103) [2026-01-26 14:35:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=600103) [2026-01-26 14:35:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=600103) [2026-01-26 14:35:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=600103) [2026-01-26 14:35:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=600103) [2026-01-26 14:35:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=600103) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=600103) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.40it/s]
(EngineCore_DP0 pid=600103) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.40it/s]
(EngineCore_DP0 pid=600103) 
(EngineCore_DP0 pid=600103) [2026-01-26 14:35:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=600103) [2026-01-26 14:35:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=600103) [2026-01-26 14:35:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=600103) [2026-01-26 14:35:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=600103) [2026-01-26 14:35:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=600103) [2026-01-26 14:35:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=600103) [2026-01-26 14:35:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=600103) [2026-01-26 14:35:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=600103) 2026-01-26 14:35:55,110 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=600103) 2026-01-26 14:35:55,134 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=600103) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.52it/s]
(EngineCore_DP0 pid=600103) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.97it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|███       | 39/128 [00:00<00:00, 380.33it/s]
Adding requests:  64%|██████▍   | 82/128 [00:00<00:00, 406.14it/s]
Adding requests:  96%|█████████▌| 123/128 [00:00<00:00, 203.51it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 237.72it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:01, 102.43it/s, est. speed input: 104892.17 toks/s, output: 102.43 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 50.75it/s, est. speed input: 56421.03 toks/s, output: 55.10 toks/s]   
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 44.87it/s, est. speed input: 50482.60 toks/s, output: 49.30 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 42.15it/s, est. speed input: 47778.66 toks/s, output: 46.66 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:02, 40.51it/s, est. speed input: 46203.46 toks/s, output: 45.12 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 39.43it/s, est. speed input: 45090.53 toks/s, output: 44.03 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 38.64it/s, est. speed input: 44231.41 toks/s, output: 43.19 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 38.17it/s, est. speed input: 43675.82 toks/s, output: 42.65 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 37.79it/s, est. speed input: 43203.96 toks/s, output: 42.19 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 37.54it/s, est. speed input: 42810.46 toks/s, output: 41.81 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 37.29it/s, est. speed input: 42454.03 toks/s, output: 41.46 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 37.15it/s, est. speed input: 42154.86 toks/s, output: 41.17 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 37.05it/s, est. speed input: 41888.27 toks/s, output: 40.91 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:01, 36.96it/s, est. speed input: 41649.58 toks/s, output: 40.67 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 36.94it/s, est. speed input: 41445.66 toks/s, output: 40.47 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 36.93it/s, est. speed input: 41261.80 toks/s, output: 40.29 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:01, 36.88it/s, est. speed input: 41088.39 toks/s, output: 40.13 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 36.89it/s, est. speed input: 40939.06 toks/s, output: 39.98 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 36.82it/s, est. speed input: 40788.73 toks/s, output: 39.83 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 36.84it/s, est. speed input: 40663.63 toks/s, output: 39.71 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 36.80it/s, est. speed input: 40539.09 toks/s, output: 39.59 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 36.81it/s, est. speed input: 40430.68 toks/s, output: 39.48 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 36.81it/s, est. speed input: 40328.49 toks/s, output: 39.38 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:03<00:00, 36.81it/s, est. speed input: 40233.37 toks/s, output: 39.29 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:03<00:00, 36.76it/s, est. speed input: 40138.96 toks/s, output: 39.20 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 36.74it/s, est. speed input: 40053.56 toks/s, output: 39.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.74it/s, est. speed input: 40032.99 toks/s, output: 39.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.09it/s, est. speed input: 40032.99 toks/s, output: 39.09 toks/s]
[rank0]:[W126 14:36:00.812542364 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 14:36:02
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:36:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=601257) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=601257) WARNING 01-26 14:36:25 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=601257) WARNING 01-26 14:36:35 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 67.76 requests/s, 69451.55 total tokens/s, 67.76 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 14:36:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:36:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:36:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:36:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:36:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:36:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:36:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:36:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:36:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:36:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:36:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:36:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:36:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:36:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:36:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:36:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:36:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:36:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=601257) [2026-01-26 14:36:17] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=601257) [2026-01-26 14:36:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=601257) [2026-01-26 14:36:17] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=601257) [2026-01-26 14:36:17] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=601257) [2026-01-26 14:36:17] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=601257) [2026-01-26 14:36:17] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=601257) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=601257) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=601257) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=601257) 
(EngineCore_DP0 pid=601257) [2026-01-26 14:36:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=601257) [2026-01-26 14:36:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=601257) [2026-01-26 14:36:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=601257) [2026-01-26 14:36:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=601257) [2026-01-26 14:36:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=601257) [2026-01-26 14:36:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=601257) [2026-01-26 14:36:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=601257) [2026-01-26 14:36:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=601257) 2026-01-26 14:36:35,049 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=601257) 2026-01-26 14:36:35,073 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=601257) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.59it/s]
(EngineCore_DP0 pid=601257) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.62it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 17.60it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:48,  5.31it/s]
Adding requests:  16%|█▋        | 42/256 [00:00<00:01, 179.14it/s]
Adding requests:  34%|███▎      | 86/256 [00:00<00:00, 277.91it/s]
Adding requests:  51%|█████     | 131/256 [00:00<00:00, 338.28it/s]
Adding requests:  68%|██████▊   | 175/256 [00:00<00:00, 371.85it/s]
Adding requests:  86%|████████▋ | 221/256 [00:00<00:00, 398.43it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 334.87it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:00<00:00, 389.87it/s, est. speed input: 399249.76 toks/s, output: 389.87 toks/s]
Processed prompts:  32%|███▏      | 81/256 [00:00<00:01, 115.50it/s, est. speed input: 132812.29 toks/s, output: 129.70 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:00<00:01, 96.55it/s, est. speed input: 113115.98 toks/s, output: 110.46 toks/s]
Processed prompts:  46%|████▌     | 117/256 [00:01<00:01, 91.80it/s, est. speed input: 107681.70 toks/s, output: 105.16 toks/s]
Processed prompts:  50%|█████     | 129/256 [00:01<00:01, 87.25it/s, est. speed input: 103580.20 toks/s, output: 101.15 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:01<00:01, 82.06it/s, est. speed input: 99700.77 toks/s, output: 97.36 toks/s]  
Processed prompts:  59%|█████▊    | 150/256 [00:01<00:01, 79.99it/s, est. speed input: 97582.27 toks/s, output: 95.29 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:01<00:01, 80.55it/s, est. speed input: 96740.64 toks/s, output: 94.47 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:01<00:01, 76.57it/s, est. speed input: 94580.24 toks/s, output: 92.36 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:01<00:01, 75.95it/s, est. speed input: 93523.41 toks/s, output: 91.33 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:02<00:00, 75.44it/s, est. speed input: 92577.13 toks/s, output: 90.41 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:02<00:00, 75.00it/s, est. speed input: 91717.53 toks/s, output: 89.57 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:02<00:00, 74.69it/s, est. speed input: 90944.96 toks/s, output: 88.81 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:02<00:00, 74.41it/s, est. speed input: 90233.49 toks/s, output: 88.12 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:02<00:00, 74.27it/s, est. speed input: 89595.62 toks/s, output: 87.50 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:02<00:00, 73.95it/s, est. speed input: 88974.49 toks/s, output: 86.89 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:02<00:00, 73.71it/s, est. speed input: 88401.89 toks/s, output: 86.33 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:02<00:00, 73.71it/s, est. speed input: 87899.75 toks/s, output: 85.84 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:02<00:00, 73.66it/s, est. speed input: 87428.72 toks/s, output: 85.38 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 73.81it/s, est. speed input: 87016.47 toks/s, output: 84.98 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 73.81it/s, est. speed input: 87016.47 toks/s, output: 84.98 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 84.97it/s, est. speed input: 87016.47 toks/s, output: 84.98 toks/s]
[rank0]:[W126 14:36:40.775707640 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 14:36:42
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:36:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=602382) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=602382) WARNING 01-26 14:37:06 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=602382) WARNING 01-26 14:37:16 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 129.23 requests/s, 132464.71 total tokens/s, 129.23 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 14:36:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:36:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:36:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:36:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:36:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:36:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:36:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:36:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:36:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:36:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:36:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:36:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:36:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:36:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:36:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:36:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:36:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:36:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:36:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=602382) [2026-01-26 14:36:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=602382) [2026-01-26 14:36:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=602382) [2026-01-26 14:36:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=602382) [2026-01-26 14:36:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=602382) [2026-01-26 14:36:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=602382) [2026-01-26 14:36:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=602382) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=602382) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.54it/s]
(EngineCore_DP0 pid=602382) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.54it/s]
(EngineCore_DP0 pid=602382) 
(EngineCore_DP0 pid=602382) [2026-01-26 14:37:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=602382) [2026-01-26 14:37:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=602382) [2026-01-26 14:37:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=602382) [2026-01-26 14:37:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=602382) [2026-01-26 14:37:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=602382) [2026-01-26 14:37:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=602382) [2026-01-26 14:37:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=602382) [2026-01-26 14:37:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=602382) 2026-01-26 14:37:16,259 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=602382) 2026-01-26 14:37:16,282 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=602382) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 16.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.22it/s]
(EngineCore_DP0 pid=602382) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 16.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 17.33it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 40/512 [00:00<00:01, 394.55it/s]
Adding requests:  16%|█▋        | 84/512 [00:00<00:01, 420.91it/s]
Adding requests:  25%|██▌       | 129/512 [00:00<00:00, 431.47it/s]
Adding requests:  34%|███▍      | 173/512 [00:00<00:00, 431.95it/s]
Adding requests:  42%|████▏     | 217/512 [00:00<00:00, 427.03it/s]
Adding requests:  52%|█████▏    | 264/512 [00:00<00:00, 440.72it/s]
Adding requests:  60%|██████    | 309/512 [00:00<00:00, 440.89it/s]
Adding requests:  70%|██████▉   | 356/512 [00:00<00:00, 446.94it/s]
Adding requests:  79%|███████▊  | 403/512 [00:00<00:00, 452.20it/s]
Adding requests:  88%|████████▊ | 450/512 [00:01<00:00, 455.62it/s]
Adding requests:  97%|█████████▋| 497/512 [00:01<00:00, 457.20it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 444.50it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:00<00:00, 1133.59it/s, est. speed input: 1160862.14 toks/s, output: 1133.60 toks/s]
Processed prompts:  51%|█████     | 260/512 [00:00<00:01, 238.39it/s, est. speed input: 281576.84 toks/s, output: 274.98 toks/s]   
Processed prompts:  62%|██████▏   | 315/512 [00:01<00:00, 198.02it/s, est. speed input: 238453.35 toks/s, output: 232.86 toks/s]
Processed prompts:  69%|██████▊   | 351/512 [00:01<00:00, 180.16it/s, est. speed input: 221203.12 toks/s, output: 216.02 toks/s]
Processed prompts:  74%|███████▍  | 379/512 [00:01<00:00, 170.59it/s, est. speed input: 212270.32 toks/s, output: 207.29 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:02<00:00, 162.20it/s, est. speed input: 205527.91 toks/s, output: 200.71 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:02<00:00, 157.23it/s, est. speed input: 201183.07 toks/s, output: 196.47 toks/s]
Processed prompts:  86%|████████▌ | 440/512 [00:02<00:00, 156.70it/s, est. speed input: 198966.31 toks/s, output: 194.30 toks/s]
Processed prompts:  89%|████████▉ | 457/512 [00:02<00:00, 151.53it/s, est. speed input: 195690.90 toks/s, output: 191.10 toks/s]
Processed prompts:  92%|█████████▏| 473/512 [00:02<00:00, 148.30it/s, est. speed input: 193144.87 toks/s, output: 188.62 toks/s]
Processed prompts:  96%|█████████▌| 489/512 [00:02<00:00, 145.32it/s, est. speed input: 190750.94 toks/s, output: 186.28 toks/s]
Processed prompts:  98%|█████████▊| 504/512 [00:02<00:00, 140.82it/s, est. speed input: 188209.37 toks/s, output: 183.80 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 140.82it/s, est. speed input: 186664.25 toks/s, output: 182.29 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 182.28it/s, est. speed input: 186664.25 toks/s, output: 182.29 toks/s]
[rank0]:[W126 14:37:22.483717849 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 14:37:24
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:37:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=603564) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=603564) WARNING 01-26 14:37:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=603564) WARNING 01-26 14:37:59 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 246.67 requests/s, 252832.29 total tokens/s, 246.67 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 14:37:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:37:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:37:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:37:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:37:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:37:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:37:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:37:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:37:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:37:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:37:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:37:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:37:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:37:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:37:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:37:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:37:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:37:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:37:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:37:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:37:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:37:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:37:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:37:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:37:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:37:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:37:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:37:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=603564) [2026-01-26 14:37:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=603564) [2026-01-26 14:37:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=603564) [2026-01-26 14:37:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=603564) [2026-01-26 14:37:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=603564) [2026-01-26 14:37:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=603564) [2026-01-26 14:37:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=603564) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=603564) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.72it/s]
(EngineCore_DP0 pid=603564) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.72it/s]
(EngineCore_DP0 pid=603564) 
(EngineCore_DP0 pid=603564) [2026-01-26 14:37:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=603564) [2026-01-26 14:37:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=603564) [2026-01-26 14:37:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=603564) [2026-01-26 14:37:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=603564) [2026-01-26 14:37:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=603564) [2026-01-26 14:37:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=603564) [2026-01-26 14:37:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=603564) [2026-01-26 14:37:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=603564) 2026-01-26 14:37:59,957 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=603564) 2026-01-26 14:37:59,980 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=603564) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 16.17it/s]
(EngineCore_DP0 pid=603564) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 14.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.58it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 399.57it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 425.29it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 434.12it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:01, 435.49it/s]
Adding requests:  21%|██▏       | 220/1024 [00:00<00:01, 441.80it/s]
Adding requests:  26%|██▌       | 268/1024 [00:00<00:01, 453.18it/s]
Adding requests:  31%|███       | 314/1024 [00:00<00:01, 448.61it/s]
Adding requests:  35%|███▌      | 360/1024 [00:00<00:01, 451.68it/s]
Adding requests:  40%|███▉      | 407/1024 [00:00<00:01, 456.67it/s]
Adding requests:  44%|████▍     | 454/1024 [00:01<00:01, 458.75it/s]
Adding requests:  49%|████▉     | 500/1024 [00:01<00:01, 457.53it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 453.92it/s]
Adding requests:  58%|█████▊    | 595/1024 [00:01<00:00, 463.07it/s]
Adding requests:  63%|██████▎   | 643/1024 [00:01<00:00, 465.56it/s]
Adding requests:  68%|██████▊   | 692/1024 [00:01<00:00, 472.17it/s]
Adding requests:  72%|███████▏  | 740/1024 [00:01<00:00, 473.26it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:01<00:00, 470.82it/s]
Adding requests:  82%|████████▏ | 836/1024 [00:01<00:00, 459.78it/s]
Adding requests:  86%|████████▋ | 885/1024 [00:01<00:00, 466.45it/s]
Adding requests:  91%|█████████ | 934/1024 [00:02<00:00, 471.69it/s]
Adding requests:  96%|█████████▌| 982/1024 [00:02<00:00, 471.15it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 460.05it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:00<00:00, 5221.94it/s, est. speed input: 5347641.92 toks/s, output: 5222.04 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 5221.94it/s, est. speed input: 544916.07 toks/s, output: 532.14 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 532.11it/s, est. speed input: 544916.07 toks/s, output: 532.14 toks/s] 
[rank0]:[W126 14:38:06.733695271 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 14:38:08
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:38:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=604800) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=604800) WARNING 01-26 14:38:39 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=604800) WARNING 01-26 14:38:49 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 279.69 requests/s, 286680.94 total tokens/s, 279.69 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 14:38:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:38:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:38:24] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:38:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:38:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:38:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:38:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:38:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:38:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:38:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:38:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:38:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:38:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:38:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:38:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:38:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:38:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:38:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:38:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:38:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:38:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:38:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:38:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:38:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:38:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:38:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:38:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:38:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=604800) [2026-01-26 14:38:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=604800) [2026-01-26 14:38:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=604800) [2026-01-26 14:38:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=604800) [2026-01-26 14:38:32] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=604800) [2026-01-26 14:38:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=604800) [2026-01-26 14:38:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=604800) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=604800) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.72it/s]
(EngineCore_DP0 pid=604800) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.72it/s]
(EngineCore_DP0 pid=604800) 
(EngineCore_DP0 pid=604800) [2026-01-26 14:38:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=604800) [2026-01-26 14:38:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=604800) [2026-01-26 14:38:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=604800) [2026-01-26 14:38:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=604800) [2026-01-26 14:38:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=604800) [2026-01-26 14:38:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=604800) [2026-01-26 14:38:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=604800) [2026-01-26 14:38:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=604800) 2026-01-26 14:38:49,523 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=604800) 2026-01-26 14:38:49,548 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=604800) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 15.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 17.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 17.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 16.23it/s]
(EngineCore_DP0 pid=604800) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 18.26it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 17.11it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 38/2048 [00:00<00:05, 379.45it/s]
Adding requests:   4%|▍         | 82/2048 [00:00<00:04, 413.73it/s]
Adding requests:   6%|▌         | 127/2048 [00:00<00:04, 426.06it/s]
Adding requests:   8%|▊         | 171/2048 [00:00<00:04, 429.47it/s]
Adding requests:  11%|█         | 216/2048 [00:00<00:04, 435.94it/s]
Adding requests:  13%|█▎        | 263/2048 [00:00<00:03, 447.15it/s]
Adding requests:  15%|█▌        | 309/2048 [00:00<00:03, 449.07it/s]
Adding requests:  17%|█▋        | 354/2048 [00:00<00:03, 447.52it/s]
Adding requests:  20%|█▉        | 400/2048 [00:00<00:03, 449.47it/s]
Adding requests:  22%|██▏       | 446/2048 [00:01<00:03, 452.63it/s]
Adding requests:  24%|██▍       | 493/2048 [00:01<00:03, 457.26it/s]
Adding requests:  26%|██▋       | 539/2048 [00:01<00:03, 447.92it/s]
Adding requests:  29%|██▊       | 588/2048 [00:01<00:03, 460.23it/s]
Adding requests:  31%|███       | 635/2048 [00:01<00:03, 457.28it/s]
Adding requests:  33%|███▎      | 684/2048 [00:01<00:02, 465.51it/s]
Adding requests:  36%|███▌      | 733/2048 [00:01<00:02, 470.45it/s]
Adding requests:  38%|███▊      | 781/2048 [00:01<00:02, 465.06it/s]
Adding requests:  40%|████      | 828/2048 [00:01<00:02, 455.80it/s]
Adding requests:  43%|████▎     | 875/2048 [00:01<00:02, 459.24it/s]
Adding requests:  45%|████▌     | 924/2048 [00:02<00:02, 465.52it/s]
Adding requests:  47%|████▋     | 971/2048 [00:02<00:02, 466.31it/s]
Adding requests:  50%|████▉     | 1019/2048 [00:02<00:02, 469.34it/s]
Adding requests:  52%|█████▏    | 1066/2048 [00:02<00:02, 468.76it/s]
Adding requests:  54%|█████▍    | 1113/2048 [00:02<00:02, 464.92it/s]
Adding requests:  57%|█████▋    | 1160/2048 [00:02<00:01, 465.96it/s]
Adding requests:  59%|█████▉    | 1210/2048 [00:02<00:01, 475.59it/s]
Adding requests:  61%|██████▏   | 1258/2048 [00:02<00:01, 468.56it/s]
Adding requests:  64%|██████▍   | 1306/2048 [00:02<00:01, 470.60it/s]
Adding requests:  66%|██████▌   | 1354/2048 [00:02<00:01, 455.65it/s]
Adding requests:  69%|██████▊   | 1403/2048 [00:03<00:01, 463.36it/s]
Adding requests:  71%|███████   | 1450/2048 [00:03<00:01, 462.94it/s]
Adding requests:  73%|███████▎  | 1499/2048 [00:03<00:01, 469.94it/s]
Adding requests:  76%|███████▌  | 1547/2048 [00:03<00:01, 470.11it/s]
Adding requests:  78%|███████▊  | 1597/2048 [00:03<00:00, 475.89it/s]
Adding requests:  80%|████████  | 1645/2048 [00:03<00:00, 476.15it/s]
Adding requests:  83%|████████▎ | 1693/2048 [00:03<00:00, 459.20it/s]
Adding requests:  85%|████████▌ | 1741/2048 [00:03<00:00, 464.53it/s]
Adding requests:  87%|████████▋ | 1788/2048 [00:03<00:00, 460.71it/s]
Adding requests:  90%|████████▉ | 1836/2048 [00:03<00:00, 464.20it/s]
Adding requests:  92%|█████████▏| 1883/2048 [00:04<00:00, 465.30it/s]
Adding requests:  94%|█████████▍| 1930/2048 [00:04<00:00, 465.80it/s]
Adding requests:  97%|█████████▋| 1977/2048 [00:04<00:00, 466.77it/s]
Adding requests:  99%|█████████▉| 2025/2048 [00:04<00:00, 470.66it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 460.80it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:00<00:00, 10400.62it/s, est. speed input: 10650974.93 toks/s, output: 10400.81 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:02<00:00, 10400.62it/s, est. speed input: 729124.62 toks/s, output: 712.03 toks/s]    
Processed prompts: 100%|██████████| 2048/2048 [00:02<00:00, 712.00it/s, est. speed input: 729124.62 toks/s, output: 712.03 toks/s]  
[rank0]:[W126 14:38:59.469487906 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 14:39:01
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:39:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=606191) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=606191) WARNING 01-26 14:39:41 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=606191) WARNING 01-26 14:39:51 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 293.90 requests/s, 301246.73 total tokens/s, 293.90 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 14:39:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:39:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:39:26] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:39:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:39:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:39:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:39:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:39:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:39:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:39:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:39:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:39:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:39:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:39:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:39:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:39:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:39:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:39:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:39:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:39:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:39:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:39:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:39:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:39:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:39:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:39:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:39:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:39:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=606191) [2026-01-26 14:39:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=606191) [2026-01-26 14:39:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=606191) [2026-01-26 14:39:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=606191) [2026-01-26 14:39:34] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=606191) [2026-01-26 14:39:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=606191) [2026-01-26 14:39:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=606191) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=606191) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.74it/s]
(EngineCore_DP0 pid=606191) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.74it/s]
(EngineCore_DP0 pid=606191) 
(EngineCore_DP0 pid=606191) [2026-01-26 14:39:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=606191) [2026-01-26 14:39:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=606191) [2026-01-26 14:39:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=606191) [2026-01-26 14:39:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=606191) [2026-01-26 14:39:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=606191) [2026-01-26 14:39:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=606191) [2026-01-26 14:39:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=606191) [2026-01-26 14:39:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=606191) [rank0]:W0126 14:39:46.855000 606191 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=606191) [rank0]:W0126 14:39:46.943000 606191 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=606191) [rank0]:W0126 14:39:47.893000 606191 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=606191) [rank0]:W0126 14:39:48.019000 606191 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=606191) 2026-01-26 14:39:51,416 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=606191) 2026-01-26 14:39:51,442 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=606191) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 11.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 14.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 15.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 16.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 16.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 15.32it/s]
(EngineCore_DP0 pid=606191) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.62it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.74it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 17.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 17.85it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 41/4096 [00:00<00:10, 401.22it/s]
Adding requests:   2%|▏         | 86/4096 [00:00<00:09, 424.12it/s]
Adding requests:   3%|▎         | 131/4096 [00:00<00:09, 435.78it/s]
Adding requests:   4%|▍         | 176/4096 [00:00<00:08, 438.54it/s]
Adding requests:   5%|▌         | 223/4096 [00:00<00:08, 447.64it/s]
Adding requests:   7%|▋         | 271/4096 [00:00<00:08, 457.86it/s]
Adding requests:   8%|▊         | 317/4096 [00:00<00:08, 453.02it/s]
Adding requests:   9%|▉         | 365/4096 [00:00<00:08, 460.46it/s]
Adding requests:  10%|█         | 412/4096 [00:00<00:07, 462.39it/s]
Adding requests:  11%|█         | 460/4096 [00:01<00:07, 466.13it/s]
Adding requests:  12%|█▏        | 507/4096 [00:01<00:07, 466.49it/s]
Adding requests:  14%|█▎        | 554/4096 [00:01<00:07, 456.19it/s]
Adding requests:  15%|█▍        | 600/4096 [00:01<00:07, 456.05it/s]
Adding requests:  16%|█▌        | 649/4096 [00:01<00:07, 465.40it/s]
Adding requests:  17%|█▋        | 698/4096 [00:01<00:07, 471.91it/s]
Adding requests:  18%|█▊        | 746/4096 [00:01<00:07, 470.95it/s]
Adding requests:  19%|█▉        | 794/4096 [00:01<00:07, 470.86it/s]
Adding requests:  21%|██        | 842/4096 [00:01<00:07, 461.41it/s]
Adding requests:  22%|██▏       | 892/4096 [00:01<00:06, 471.45it/s]
Adding requests:  23%|██▎       | 940/4096 [00:02<00:06, 463.36it/s]
Adding requests:  24%|██▍       | 989/4096 [00:02<00:06, 469.02it/s]
Adding requests:  25%|██▌       | 1038/4096 [00:02<00:06, 472.99it/s]
Adding requests:  27%|██▋       | 1086/4096 [00:02<00:06, 471.11it/s]
Adding requests:  28%|██▊       | 1134/4096 [00:02<00:06, 467.89it/s]
Adding requests:  29%|██▉       | 1185/4096 [00:02<00:06, 479.21it/s]
Adding requests:  30%|███       | 1234/4096 [00:02<00:05, 482.36it/s]
Adding requests:  31%|███▏      | 1283/4096 [00:02<00:05, 478.14it/s]
Adding requests:  33%|███▎      | 1332/4096 [00:02<00:05, 481.18it/s]
Adding requests:  34%|███▎      | 1381/4096 [00:02<00:05, 481.71it/s]
Adding requests:  35%|███▍      | 1431/4096 [00:03<00:05, 484.16it/s]
Adding requests:  36%|███▌      | 1480/4096 [00:03<00:05, 484.79it/s]
Adding requests:  37%|███▋      | 1530/4096 [00:03<00:05, 488.46it/s]
Adding requests:  39%|███▊      | 1579/4096 [00:03<00:05, 487.75it/s]
Adding requests:  40%|███▉      | 1630/4096 [00:03<00:05, 493.15it/s]
Adding requests:  41%|████      | 1680/4096 [00:03<00:05, 473.26it/s]
Adding requests:  42%|████▏     | 1728/4096 [00:03<00:05, 471.51it/s]
Adding requests:  43%|████▎     | 1776/4096 [00:03<00:04, 470.01it/s]
Adding requests:  45%|████▍     | 1825/4096 [00:03<00:04, 475.14it/s]
Adding requests:  46%|████▌     | 1873/4096 [00:03<00:04, 474.84it/s]
Adding requests:  47%|████▋     | 1921/4096 [00:04<00:04, 476.27it/s]
Adding requests:  48%|████▊     | 1969/4096 [00:04<00:04, 475.86it/s]
Adding requests:  49%|████▉     | 2019/4096 [00:04<00:04, 480.06it/s]
Adding requests:  50%|█████     | 2068/4096 [00:04<00:04, 467.49it/s]
Adding requests:  52%|█████▏    | 2117/4096 [00:04<00:04, 471.23it/s]
Adding requests:  53%|█████▎    | 2165/4096 [00:04<00:04, 465.05it/s]
Adding requests:  54%|█████▍    | 2212/4096 [00:04<00:04, 464.97it/s]
Adding requests:  55%|█████▌    | 2260/4096 [00:04<00:03, 468.81it/s]
Adding requests:  56%|█████▋    | 2307/4096 [00:04<00:03, 467.39it/s]
Adding requests:  57%|█████▋    | 2354/4096 [00:05<00:03, 467.85it/s]
Adding requests:  59%|█████▊    | 2402/4096 [00:05<00:03, 469.15it/s]
Adding requests:  60%|█████▉    | 2450/4096 [00:05<00:03, 471.52it/s]
Adding requests:  61%|██████    | 2499/4096 [00:05<00:03, 476.08it/s]
Adding requests:  62%|██████▏   | 2547/4096 [00:05<00:03, 475.57it/s]
Adding requests:  63%|██████▎   | 2596/4096 [00:05<00:03, 477.77it/s]
Adding requests:  65%|██████▍   | 2645/4096 [00:05<00:03, 479.42it/s]
Adding requests:  66%|██████▌   | 2693/4096 [00:05<00:02, 477.72it/s]
Adding requests:  67%|██████▋   | 2741/4096 [00:05<00:02, 476.82it/s]
Adding requests:  68%|██████▊   | 2789/4096 [00:05<00:02, 475.35it/s]
Adding requests:  69%|██████▉   | 2837/4096 [00:06<00:02, 472.97it/s]
Adding requests:  70%|███████   | 2886/4096 [00:06<00:02, 476.74it/s]
Adding requests:  72%|███████▏  | 2934/4096 [00:06<00:02, 474.13it/s]
Adding requests:  73%|███████▎  | 2982/4096 [00:06<00:02, 475.10it/s]
Adding requests:  74%|███████▍  | 3030/4096 [00:06<00:02, 476.29it/s]
Adding requests:  75%|███████▌  | 3078/4096 [00:06<00:02, 473.60it/s]
Adding requests:  76%|███████▋  | 3128/4096 [00:06<00:02, 479.38it/s]
Adding requests:  78%|███████▊  | 3176/4096 [00:06<00:01, 476.85it/s]
Adding requests:  79%|███████▊  | 3224/4096 [00:06<00:01, 476.95it/s]
Adding requests:  80%|███████▉  | 3272/4096 [00:06<00:01, 467.62it/s]
Adding requests:  81%|████████  | 3321/4096 [00:07<00:01, 471.51it/s]
Adding requests:  82%|████████▏ | 3369/4096 [00:07<00:01, 473.85it/s]
Adding requests:  83%|████████▎ | 3417/4096 [00:07<00:01, 475.50it/s]
Adding requests:  85%|████████▍ | 3465/4096 [00:07<00:01, 469.48it/s]
Adding requests:  86%|████████▌ | 3513/4096 [00:07<00:01, 471.11it/s]
Adding requests:  87%|████████▋ | 3561/4096 [00:07<00:01, 469.93it/s]
Adding requests:  88%|████████▊ | 3609/4096 [00:07<00:01, 470.83it/s]
Adding requests:  89%|████████▉ | 3657/4096 [00:07<00:00, 469.63it/s]
Adding requests:  90%|█████████ | 3706/4096 [00:07<00:00, 473.58it/s]
Adding requests:  92%|█████████▏| 3754/4096 [00:07<00:00, 474.64it/s]
Adding requests:  93%|█████████▎| 3804/4096 [00:08<00:00, 479.95it/s]
Adding requests:  94%|█████████▍| 3854/4096 [00:08<00:00, 483.79it/s]
Adding requests:  95%|█████████▌| 3903/4096 [00:08<00:00, 482.66it/s]
Adding requests:  96%|█████████▋| 3952/4096 [00:08<00:00, 482.48it/s]
Adding requests:  98%|█████████▊| 4001/4096 [00:08<00:00, 479.35it/s]
Adding requests:  99%|█████████▉| 4049/4096 [00:08<00:00, 478.34it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 472.05it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  62%|██████▏   | 2549/4096 [00:00<00:00, 16035.81it/s, est. speed input: 16421454.39 toks/s, output: 16036.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:05<00:00, 16035.81it/s, est. speed input: 797740.76 toks/s, output: 779.04 toks/s]    
Processed prompts: 100%|██████████| 4096/4096 [00:05<00:00, 779.02it/s, est. speed input: 797740.76 toks/s, output: 779.04 toks/s]  
[rank0]:[W126 14:40:08.488293385 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 14:40:10
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Llama3.2-3B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 14:40:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=607934) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=607934) WARNING 01-26 14:41:09 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=607934) WARNING 01-26 14:41:18 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 302.23 requests/s, 309788.92 total tokens/s, 302.23 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 14:40:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:40:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:40:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:40:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:40:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:40:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:40:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:40:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:40:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:40:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:40:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:40:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:40:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:40:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 14:41:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 14:41:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 14:41:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 14:41:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:41:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:41:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:41:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:41:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 14:41:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 14:41:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 14:41:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 14:41:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 14:41:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 14:41:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=607934) [2026-01-26 14:41:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=607934) [2026-01-26 14:41:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=607934) [2026-01-26 14:41:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=607934) [2026-01-26 14:41:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=607934) [2026-01-26 14:41:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=607934) [2026-01-26 14:41:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=607934) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=607934) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.42it/s]
(EngineCore_DP0 pid=607934) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.42it/s]
(EngineCore_DP0 pid=607934) 
(EngineCore_DP0 pid=607934) [2026-01-26 14:41:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=607934) [2026-01-26 14:41:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=607934) [2026-01-26 14:41:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=607934) [2026-01-26 14:41:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=607934) [2026-01-26 14:41:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=607934) [2026-01-26 14:41:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=607934) [2026-01-26 14:41:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=607934) [2026-01-26 14:41:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=607934) [rank0]:W0126 14:41:13.867000 607934 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=607934) [rank0]:W0126 14:41:13.955000 607934 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=607934) [rank0]:W0126 14:41:14.881000 607934 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=607934) [rank0]:W0126 14:41:15.006000 607934 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=607934) 2026-01-26 14:41:18,339 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=607934) 2026-01-26 14:41:18,365 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=607934) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:08,  2.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:07,  2.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:01<00:03,  4.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:01,  7.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  9.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:00, 11.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00, 12.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00, 13.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00, 15.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:01<00:00, 16.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 10.00it/s]
(EngineCore_DP0 pid=607934) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 17.85it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 14.57it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 15.50it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 16.58it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 17.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 16.77it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 40/8192 [00:00<00:20, 392.49it/s]
Adding requests:   1%|          | 84/8192 [00:00<00:19, 418.12it/s]
Adding requests:   2%|▏         | 127/8192 [00:00<00:19, 421.96it/s]
Adding requests:   2%|▏         | 170/8192 [00:00<00:18, 423.74it/s]
Adding requests:   3%|▎         | 215/8192 [00:00<00:18, 430.58it/s]
Adding requests:   3%|▎         | 262/8192 [00:00<00:17, 442.52it/s]
Adding requests:   4%|▎         | 307/8192 [00:00<00:17, 442.48it/s]
Adding requests:   4%|▍         | 354/8192 [00:00<00:17, 450.35it/s]
Adding requests:   5%|▍         | 400/8192 [00:00<00:17, 452.78it/s]
Adding requests:   5%|▌         | 447/8192 [00:01<00:16, 456.48it/s]
Adding requests:   6%|▌         | 493/8192 [00:01<00:16, 455.41it/s]
Adding requests:   7%|▋         | 539/8192 [00:01<00:17, 446.91it/s]
Adding requests:   7%|▋         | 588/8192 [00:01<00:16, 458.29it/s]
Adding requests:   8%|▊         | 636/8192 [00:01<00:16, 463.25it/s]
Adding requests:   8%|▊         | 685/8192 [00:01<00:15, 469.67it/s]
Adding requests:   9%|▉         | 734/8192 [00:01<00:15, 475.52it/s]
Adding requests:  10%|▉         | 782/8192 [00:01<00:15, 468.46it/s]
Adding requests:  10%|█         | 829/8192 [00:01<00:15, 461.63it/s]
Adding requests:  11%|█         | 877/8192 [00:01<00:15, 464.33it/s]
Adding requests:  11%|█▏        | 926/8192 [00:02<00:15, 469.66it/s]
Adding requests:  12%|█▏        | 974/8192 [00:02<00:15, 471.23it/s]
Adding requests:  12%|█▏        | 1022/8192 [00:02<00:15, 473.54it/s]
Adding requests:  13%|█▎        | 1070/8192 [00:02<00:15, 470.60it/s]
Adding requests:  14%|█▎        | 1118/8192 [00:02<00:15, 466.23it/s]
Adding requests:  14%|█▍        | 1168/8192 [00:02<00:14, 474.63it/s]
Adding requests:  15%|█▍        | 1217/8192 [00:02<00:14, 479.16it/s]
Adding requests:  15%|█▌        | 1265/8192 [00:02<00:14, 473.92it/s]
Adding requests:  16%|█▌        | 1313/8192 [00:02<00:14, 471.99it/s]
Adding requests:  17%|█▋        | 1362/8192 [00:02<00:14, 476.54it/s]
Adding requests:  17%|█▋        | 1411/8192 [00:03<00:14, 480.18it/s]
Adding requests:  18%|█▊        | 1460/8192 [00:03<00:14, 475.96it/s]
Adding requests:  18%|█▊        | 1509/8192 [00:03<00:13, 478.73it/s]
Adding requests:  19%|█▉        | 1557/8192 [00:03<00:14, 465.73it/s]
Adding requests:  20%|█▉        | 1607/8192 [00:03<00:13, 474.99it/s]
Adding requests:  20%|██        | 1655/8192 [00:03<00:13, 473.91it/s]
Adding requests:  21%|██        | 1703/8192 [00:03<00:13, 471.81it/s]
Adding requests:  21%|██▏       | 1751/8192 [00:03<00:13, 472.49it/s]
Adding requests:  22%|██▏       | 1799/8192 [00:03<00:13, 473.78it/s]
Adding requests:  23%|██▎       | 1847/8192 [00:03<00:13, 475.59it/s]
Adding requests:  23%|██▎       | 1895/8192 [00:04<00:13, 473.88it/s]
Adding requests:  24%|██▎       | 1943/8192 [00:04<00:13, 474.78it/s]
Adding requests:  24%|██▍       | 1991/8192 [00:04<00:13, 466.92it/s]
Adding requests:  25%|██▍       | 2039/8192 [00:04<00:13, 468.51it/s]
Adding requests:  25%|██▌       | 2088/8192 [00:04<00:12, 474.15it/s]
Adding requests:  26%|██▌       | 2136/8192 [00:04<00:12, 469.19it/s]
Adding requests:  27%|██▋       | 2183/8192 [00:04<00:12, 464.45it/s]
Adding requests:  27%|██▋       | 2231/8192 [00:04<00:12, 468.89it/s]
Adding requests:  28%|██▊       | 2279/8192 [00:04<00:12, 471.59it/s]
Adding requests:  28%|██▊       | 2327/8192 [00:05<00:12, 470.91it/s]
Adding requests:  29%|██▉       | 2376/8192 [00:05<00:12, 473.73it/s]
Adding requests:  30%|██▉       | 2424/8192 [00:05<00:12, 475.01it/s]
Adding requests:  30%|███       | 2472/8192 [00:05<00:12, 475.14it/s]
Adding requests:  31%|███       | 2520/8192 [00:05<00:11, 474.14it/s]
Adding requests:  31%|███▏      | 2569/8192 [00:05<00:11, 478.34it/s]
Adding requests:  32%|███▏      | 2617/8192 [00:05<00:11, 478.03it/s]
Adding requests:  33%|███▎      | 2666/8192 [00:05<00:11, 479.79it/s]
Adding requests:  33%|███▎      | 2714/8192 [00:05<00:11, 473.88it/s]
Adding requests:  34%|███▎      | 2762/8192 [00:05<00:11, 461.10it/s]
Adding requests:  34%|███▍      | 2809/8192 [00:06<00:11, 461.58it/s]
Adding requests:  35%|███▍      | 2856/8192 [00:06<00:11, 462.09it/s]
Adding requests:  35%|███▌      | 2905/8192 [00:06<00:11, 467.69it/s]
Adding requests:  36%|███▌      | 2952/8192 [00:06<00:11, 464.01it/s]
Adding requests:  37%|███▋      | 2999/8192 [00:06<00:11, 465.39it/s]
Adding requests:  37%|███▋      | 3047/8192 [00:06<00:11, 466.74it/s]
Adding requests:  38%|███▊      | 3095/8192 [00:06<00:10, 467.90it/s]
Adding requests:  38%|███▊      | 3143/8192 [00:06<00:10, 467.26it/s]
Adding requests:  39%|███▉      | 3191/8192 [00:06<00:10, 470.62it/s]
Adding requests:  40%|███▉      | 3240/8192 [00:06<00:10, 474.08it/s]
Adding requests:  40%|████      | 3288/8192 [00:07<00:10, 474.40it/s]
Adding requests:  41%|████      | 3336/8192 [00:07<00:10, 475.11it/s]
Adding requests:  41%|████▏     | 3385/8192 [00:07<00:10, 479.04it/s]
Adding requests:  42%|████▏     | 3434/8192 [00:07<00:09, 480.42it/s]
Adding requests:  43%|████▎     | 3483/8192 [00:07<00:10, 468.57it/s]
Adding requests:  43%|████▎     | 3530/8192 [00:07<00:10, 453.20it/s]
Adding requests:  44%|████▎     | 3577/8192 [00:07<00:10, 457.80it/s]
Adding requests:  44%|████▍     | 3624/8192 [00:07<00:09, 458.55it/s]
Adding requests:  45%|████▍     | 3671/8192 [00:07<00:09, 461.49it/s]
Adding requests:  45%|████▌     | 3718/8192 [00:07<00:09, 461.83it/s]
Adding requests:  46%|████▌     | 3767/8192 [00:08<00:09, 468.63it/s]
Adding requests:  47%|████▋     | 3816/8192 [00:08<00:09, 472.05it/s]
Adding requests:  47%|████▋     | 3865/8192 [00:08<00:09, 476.78it/s]
Adding requests:  48%|████▊     | 3913/8192 [00:08<00:09, 475.31it/s]
Adding requests:  48%|████▊     | 3962/8192 [00:08<00:08, 477.09it/s]
Adding requests:  49%|████▉     | 4010/8192 [00:08<00:09, 460.90it/s]
Adding requests:  50%|████▉     | 4057/8192 [00:08<00:08, 460.10it/s]
Adding requests:  50%|█████     | 4105/8192 [00:08<00:08, 464.79it/s]
Adding requests:  51%|█████     | 4153/8192 [00:08<00:08, 468.29it/s]
Adding requests:  51%|█████▏    | 4203/8192 [00:08<00:08, 474.80it/s]
Adding requests:  52%|█████▏    | 4252/8192 [00:09<00:08, 476.76it/s]
Adding requests:  52%|█████▏    | 4300/8192 [00:09<00:08, 475.20it/s]
Adding requests:  53%|█████▎    | 4350/8192 [00:09<00:08, 480.16it/s]
Adding requests:  54%|█████▎    | 4399/8192 [00:09<00:07, 482.85it/s]
Adding requests:  54%|█████▍    | 4448/8192 [00:09<00:07, 484.04it/s]
Adding requests:  55%|█████▍    | 4497/8192 [00:09<00:07, 478.35it/s]
Adding requests:  55%|█████▌    | 4545/8192 [00:09<00:07, 476.79it/s]
Adding requests:  56%|█████▌    | 4594/8192 [00:09<00:07, 479.56it/s]
Adding requests:  57%|█████▋    | 4644/8192 [00:09<00:07, 482.67it/s]
Adding requests:  57%|█████▋    | 4693/8192 [00:10<00:07, 476.96it/s]
Adding requests:  58%|█████▊    | 4742/8192 [00:10<00:07, 480.28it/s]
Adding requests:  58%|█████▊    | 4791/8192 [00:10<00:07, 480.82it/s]
Adding requests:  59%|█████▉    | 4840/8192 [00:10<00:06, 479.34it/s]
Adding requests:  60%|█████▉    | 4888/8192 [00:10<00:06, 476.08it/s]
Adding requests:  60%|██████    | 4936/8192 [00:10<00:06, 476.91it/s]
Adding requests:  61%|██████    | 4984/8192 [00:10<00:06, 477.13it/s]
Adding requests:  61%|██████▏   | 5034/8192 [00:10<00:06, 481.51it/s]
Adding requests:  62%|██████▏   | 5084/8192 [00:10<00:06, 485.40it/s]
Adding requests:  63%|██████▎   | 5134/8192 [00:10<00:06, 487.01it/s]
Adding requests:  63%|██████▎   | 5183/8192 [00:11<00:06, 483.98it/s]
Adding requests:  64%|██████▍   | 5232/8192 [00:11<00:06, 470.34it/s]
Adding requests:  64%|██████▍   | 5280/8192 [00:11<00:06, 470.01it/s]
Adding requests:  65%|██████▌   | 5330/8192 [00:11<00:05, 477.56it/s]
Adding requests:  66%|██████▌   | 5378/8192 [00:11<00:05, 475.88it/s]
Adding requests:  66%|██████▌   | 5427/8192 [00:11<00:05, 479.61it/s]
Adding requests:  67%|██████▋   | 5475/8192 [00:11<00:05, 475.53it/s]
Adding requests:  67%|██████▋   | 5523/8192 [00:11<00:05, 472.07it/s]
Adding requests:  68%|██████▊   | 5571/8192 [00:11<00:05, 471.69it/s]
Adding requests:  69%|██████▊   | 5620/8192 [00:11<00:05, 476.04it/s]
Adding requests:  69%|██████▉   | 5668/8192 [00:12<00:05, 467.06it/s]
Adding requests:  70%|██████▉   | 5718/8192 [00:12<00:05, 474.84it/s]
Adding requests:  70%|███████   | 5767/8192 [00:12<00:05, 477.79it/s]
Adding requests:  71%|███████   | 5815/8192 [00:12<00:05, 473.21it/s]
Adding requests:  72%|███████▏  | 5864/8192 [00:12<00:04, 475.70it/s]
Adding requests:  72%|███████▏  | 5914/8192 [00:12<00:04, 480.23it/s]
Adding requests:  73%|███████▎  | 5963/8192 [00:12<00:04, 478.18it/s]
Adding requests:  73%|███████▎  | 6013/8192 [00:12<00:04, 483.71it/s]
Adding requests:  74%|███████▍  | 6063/8192 [00:12<00:04, 487.59it/s]
Adding requests:  75%|███████▍  | 6112/8192 [00:12<00:04, 483.07it/s]
Adding requests:  75%|███████▌  | 6161/8192 [00:13<00:04, 481.24it/s]
Adding requests:  76%|███████▌  | 6212/8192 [00:13<00:04, 488.26it/s]
Adding requests:  76%|███████▋  | 6263/8192 [00:13<00:03, 493.38it/s]
Adding requests:  77%|███████▋  | 6313/8192 [00:13<00:03, 492.87it/s]
Adding requests:  78%|███████▊  | 6363/8192 [00:13<00:03, 494.63it/s]
Adding requests:  78%|███████▊  | 6413/8192 [00:13<00:03, 493.46it/s]
Adding requests:  79%|███████▉  | 6464/8192 [00:13<00:03, 495.92it/s]
Adding requests:  80%|███████▉  | 6514/8192 [00:13<00:03, 486.54it/s]
Adding requests:  80%|████████  | 6563/8192 [00:13<00:03, 484.93it/s]
Adding requests:  81%|████████  | 6612/8192 [00:14<00:03, 484.51it/s]
Adding requests:  81%|████████▏ | 6661/8192 [00:14<00:03, 481.29it/s]
Adding requests:  82%|████████▏ | 6710/8192 [00:14<00:03, 479.47it/s]
Adding requests:  83%|████████▎ | 6759/8192 [00:14<00:02, 480.30it/s]
Adding requests:  83%|████████▎ | 6809/8192 [00:14<00:02, 484.10it/s]
Adding requests:  84%|████████▎ | 6859/8192 [00:14<00:02, 487.98it/s]
Adding requests:  84%|████████▍ | 6908/8192 [00:14<00:02, 488.51it/s]
Adding requests:  85%|████████▍ | 6958/8192 [00:14<00:02, 490.50it/s]
Adding requests:  86%|████████▌ | 7008/8192 [00:14<00:02, 486.23it/s]
Adding requests:  86%|████████▌ | 7057/8192 [00:14<00:02, 484.66it/s]
Adding requests:  87%|████████▋ | 7106/8192 [00:15<00:02, 486.08it/s]
Adding requests:  87%|████████▋ | 7155/8192 [00:15<00:02, 485.03it/s]
Adding requests:  88%|████████▊ | 7204/8192 [00:15<00:02, 483.19it/s]
Adding requests:  89%|████████▊ | 7253/8192 [00:15<00:01, 484.78it/s]
Adding requests:  89%|████████▉ | 7303/8192 [00:15<00:01, 487.88it/s]
Adding requests:  90%|████████▉ | 7352/8192 [00:15<00:01, 483.74it/s]
Adding requests:  90%|█████████ | 7402/8192 [00:15<00:01, 488.15it/s]
Adding requests:  91%|█████████ | 7453/8192 [00:15<00:01, 493.27it/s]
Adding requests:  92%|█████████▏| 7503/8192 [00:15<00:01, 492.31it/s]
Adding requests:  92%|█████████▏| 7553/8192 [00:15<00:01, 491.47it/s]
Adding requests:  93%|█████████▎| 7603/8192 [00:16<00:01, 487.81it/s]
Adding requests:  93%|█████████▎| 7653/8192 [00:16<00:01, 490.73it/s]
Adding requests:  94%|█████████▍| 7703/8192 [00:16<00:00, 492.43it/s]
Adding requests:  95%|█████████▍| 7753/8192 [00:16<00:00, 475.92it/s]
Adding requests:  95%|█████████▌| 7801/8192 [00:16<00:00, 472.70it/s]
Adding requests:  96%|█████████▌| 7851/8192 [00:16<00:00, 477.96it/s]
Adding requests:  96%|█████████▋| 7900/8192 [00:16<00:00, 476.78it/s]
Adding requests:  97%|█████████▋| 7948/8192 [00:16<00:00, 473.96it/s]
Adding requests:  98%|█████████▊| 7996/8192 [00:16<00:00, 475.01it/s]
Adding requests:  98%|█████████▊| 8044/8192 [00:16<00:00, 471.73it/s]
Adding requests:  99%|█████████▉| 8094/8192 [00:17<00:00, 477.90it/s]
Adding requests:  99%|█████████▉| 8142/8192 [00:17<00:00, 477.45it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 482.88it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 474.21it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  64%|██████▎   | 5218/8192 [00:00<00:00, 20670.53it/s, est. speed input: 21167486.04 toks/s, output: 20670.77 toks/s]
Processed prompts:  89%|████████▉ | 7286/8192 [00:06<00:01, 828.09it/s, est. speed input: 1068287.89 toks/s, output: 1043.25 toks/s]    
Processed prompts: 100%|█████████▉| 8156/8192 [00:09<00:00, 643.77it/s, est. speed input: 859249.13 toks/s, output: 839.11 toks/s]  
Processed prompts: 100%|██████████| 8192/8192 [00:09<00:00, 643.77it/s, est. speed input: 853655.59 toks/s, output: 833.65 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:09<00:00, 833.64it/s, est. speed input: 853655.59 toks/s, output: 833.65 toks/s]
[rank0]:[W126 14:41:50.275516073 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 15:48:52
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:48:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=706317) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=706317) WARNING 01-26 15:49:15 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=706317) WARNING 01-26 15:49:26 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 37.39 requests/s, 19180.61 total tokens/s, 37.39 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 15:48:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:48:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:48:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:48:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:48:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:48:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:48:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:48:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:48:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:49:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:49:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:49:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:49:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:49:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:49:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:49:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:49:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:07] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:07] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:07] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:07] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=706317) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=706317) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=706317) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.19it/s]
(EngineCore_DP0 pid=706317) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.21it/s]
(EngineCore_DP0 pid=706317) 
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=706317) [2026-01-26 15:49:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=706317) 2026-01-26 15:49:26,114 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=706317) 2026-01-26 15:49:26,137 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=706317) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=706317) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.81it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  40%|███▉      | 51/128 [00:00<00:00, 507.21it/s]
Adding requests:  87%|████████▋ | 111/128 [00:00<00:00, 548.19it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 549.17it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 54.65it/s, est. speed input: 27981.46 toks/s, output: 54.65 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:02, 44.52it/s, est. speed input: 23446.53 toks/s, output: 45.79 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 42.28it/s, est. speed input: 22386.95 toks/s, output: 43.72 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 41.18it/s, est. speed input: 21847.36 toks/s, output: 42.67 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 40.54it/s, est. speed input: 21514.61 toks/s, output: 42.02 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 40.26it/s, est. speed input: 21319.31 toks/s, output: 41.64 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 40.07it/s, est. speed input: 21177.67 toks/s, output: 41.36 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 39.93it/s, est. speed input: 21068.92 toks/s, output: 41.15 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 39.82it/s, est. speed input: 20992.92 toks/s, output: 41.00 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:01, 39.69it/s, est. speed input: 20922.83 toks/s, output: 40.86 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 39.68it/s, est. speed input: 20875.23 toks/s, output: 40.77 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 39.61it/s, est. speed input: 20827.38 toks/s, output: 40.68 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 39.53it/s, est. speed input: 20781.25 toks/s, output: 40.59 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 39.55it/s, est. speed input: 20749.67 toks/s, output: 40.53 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 39.52it/s, est. speed input: 20717.85 toks/s, output: 40.46 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 39.55it/s, est. speed input: 20693.85 toks/s, output: 40.42 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 39.57it/s, est. speed input: 20672.41 toks/s, output: 40.38 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 39.58it/s, est. speed input: 20653.13 toks/s, output: 40.34 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 39.59it/s, est. speed input: 20635.37 toks/s, output: 40.30 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:00, 39.64it/s, est. speed input: 20622.63 toks/s, output: 40.28 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 39.66it/s, est. speed input: 20609.73 toks/s, output: 40.25 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 39.67it/s, est. speed input: 20598.45 toks/s, output: 40.23 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 39.73it/s, est. speed input: 20590.57 toks/s, output: 40.22 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 39.73it/s, est. speed input: 20581.23 toks/s, output: 40.20 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 39.72it/s, est. speed input: 20572.12 toks/s, output: 40.18 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 39.79it/s, est. speed input: 20567.90 toks/s, output: 40.17 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 39.80it/s, est. speed input: 20561.62 toks/s, output: 40.16 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 39.83it/s, est. speed input: 20557.40 toks/s, output: 40.15 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 39.84it/s, est. speed input: 20552.53 toks/s, output: 40.14 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.84it/s, est. speed input: 20548.96 toks/s, output: 40.13 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.13it/s, est. speed input: 20548.96 toks/s, output: 40.13 toks/s]
[rank0]:[W126 15:49:32.594181079 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 15:49:34
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:49:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=707578) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=707578) WARNING 01-26 15:49:58 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=707578) WARNING 01-26 15:50:07 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 38.76 requests/s, 39726.12 total tokens/s, 38.76 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 15:49:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:49:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:49:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:49:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:49:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:49:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:49:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:49:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:49:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:49:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:49:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:49:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:49:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:49:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:49:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:49:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:49:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:49] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:49] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:49] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:49] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:49] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=707578) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=707578) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.41it/s]
(EngineCore_DP0 pid=707578) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.27it/s]
(EngineCore_DP0 pid=707578) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.29it/s]
(EngineCore_DP0 pid=707578) 
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=707578) [2026-01-26 15:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=707578) 2026-01-26 15:50:07,914 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=707578) 2026-01-26 15:50:07,937 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=707578) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.70it/s]
(EngineCore_DP0 pid=707578) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.06it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 277.61it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 319.42it/s]
Adding requests:  75%|███████▌  | 96/128 [00:00<00:00, 316.11it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 316.17it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:00, 160.69it/s, est. speed input: 164560.94 toks/s, output: 160.70 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:01, 58.19it/s, est. speed input: 66088.10 toks/s, output: 64.54 toks/s]   
Processed prompts:  34%|███▍      | 44/128 [00:00<00:01, 51.02it/s, est. speed input: 58608.36 toks/s, output: 57.23 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:00<00:01, 47.52it/s, est. speed input: 55201.56 toks/s, output: 53.91 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 45.38it/s, est. speed input: 53179.66 toks/s, output: 51.93 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 43.75it/s, est. speed input: 51633.11 toks/s, output: 50.42 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 42.73it/s, est. speed input: 50623.37 toks/s, output: 49.44 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 41.92it/s, est. speed input: 49779.85 toks/s, output: 48.61 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 41.22it/s, est. speed input: 49041.06 toks/s, output: 47.89 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:01, 40.72it/s, est. speed input: 48416.34 toks/s, output: 47.28 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 40.36it/s, est. speed input: 47876.73 toks/s, output: 46.75 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 40.09it/s, est. speed input: 47404.43 toks/s, output: 46.29 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 39.91it/s, est. speed input: 46988.74 toks/s, output: 45.89 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 39.82it/s, est. speed input: 46695.62 toks/s, output: 45.60 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 39.67it/s, est. speed input: 46412.23 toks/s, output: 45.32 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 39.54it/s, est. speed input: 46150.21 toks/s, output: 45.07 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 39.50it/s, est. speed input: 45917.75 toks/s, output: 44.84 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 39.45it/s, est. speed input: 45701.22 toks/s, output: 44.63 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 39.47it/s, est. speed input: 45507.67 toks/s, output: 44.44 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 39.47it/s, est. speed input: 45327.26 toks/s, output: 44.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 39.47it/s, est. speed input: 45249.71 toks/s, output: 44.19 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.19it/s, est. speed input: 45249.71 toks/s, output: 44.19 toks/s]
[rank0]:[W126 15:50:12.151080401 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 15:50:14
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:50:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=708708) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=708708) WARNING 01-26 15:50:38 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=708708) WARNING 01-26 15:50:48 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 72.45 requests/s, 74257.96 total tokens/s, 72.45 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 15:50:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:50:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:50:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:50:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:50:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:50:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:50:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:50:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:50:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:50:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:50:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:50:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:50:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:50:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:50:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:50:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:50:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:30] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=708708) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=708708) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.56it/s]
(EngineCore_DP0 pid=708708) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.33it/s]
(EngineCore_DP0 pid=708708) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.36it/s]
(EngineCore_DP0 pid=708708) 
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=708708) [2026-01-26 15:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=708708) 2026-01-26 15:50:48,244 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=708708) 2026-01-26 15:50:48,267 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=708708) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 12.65it/s]
(EngineCore_DP0 pid=708708) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.72it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.71it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<01:01,  4.13it/s]
Adding requests:  11%|█▏        | 29/256 [00:00<00:02, 106.09it/s]
Adding requests:  23%|██▎       | 60/256 [00:00<00:01, 176.49it/s]
Adding requests:  35%|███▌      | 90/256 [00:00<00:00, 217.01it/s]
Adding requests:  48%|████▊     | 123/256 [00:00<00:00, 252.25it/s]
Adding requests:  61%|██████▏   | 157/256 [00:00<00:00, 278.11it/s]
Adding requests:  76%|███████▌  | 194/256 [00:00<00:00, 303.22it/s]
Adding requests:  90%|████████▉ | 230/256 [00:00<00:00, 319.19it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 248.61it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:00<00:00, 516.28it/s, est. speed input: 528694.73 toks/s, output: 516.28 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:00<00:01, 132.97it/s, est. speed input: 155243.96 toks/s, output: 151.60 toks/s]
Processed prompts:  56%|█████▌    | 143/256 [00:01<00:00, 114.18it/s, est. speed input: 134896.60 toks/s, output: 131.73 toks/s]
Processed prompts:  63%|██████▎   | 161/256 [00:01<00:00, 105.03it/s, est. speed input: 126091.00 toks/s, output: 123.14 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:01<00:00, 97.48it/s, est. speed input: 119759.98 toks/s, output: 116.95 toks/s] 
Processed prompts:  73%|███████▎  | 188/256 [00:01<00:00, 93.82it/s, est. speed input: 116425.10 toks/s, output: 113.70 toks/s]
Processed prompts:  78%|███████▊  | 199/256 [00:01<00:00, 92.85it/s, est. speed input: 114675.27 toks/s, output: 111.99 toks/s]
Processed prompts:  82%|████████▏ | 209/256 [00:01<00:00, 88.66it/s, est. speed input: 112072.23 toks/s, output: 109.45 toks/s]
Processed prompts:  86%|████████▌ | 219/256 [00:02<00:00, 86.80it/s, est. speed input: 110300.53 toks/s, output: 107.72 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 83.30it/s, est. speed input: 108280.99 toks/s, output: 105.74 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:02<00:00, 82.58it/s, est. speed input: 106881.63 toks/s, output: 104.38 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:02<00:00, 82.06it/s, est. speed input: 105633.68 toks/s, output: 103.16 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 82.06it/s, est. speed input: 104747.69 toks/s, output: 102.29 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 102.29it/s, est. speed input: 104747.69 toks/s, output: 102.29 toks/s]
[rank0]:[W126 15:50:53.906697463 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 15:50:55
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:51:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=709857) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=709857) WARNING 01-26 15:51:20 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=709857) WARNING 01-26 15:51:30 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 138.43 requests/s, 141893.64 total tokens/s, 138.43 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 15:51:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:51:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:51:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:51:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:51:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:51:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:51:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:51:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:51:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:51:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:51:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:51:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:12] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=709857) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=709857) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.73it/s]
(EngineCore_DP0 pid=709857) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=709857) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.42it/s]
(EngineCore_DP0 pid=709857) 
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=709857) [2026-01-26 15:51:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=709857) 2026-01-26 15:51:30,897 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=709857) 2026-01-26 15:51:30,920 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=709857) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 13.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.80it/s]
(EngineCore_DP0 pid=709857) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 18.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 18.25it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 28/512 [00:00<00:01, 277.62it/s]
Adding requests:  12%|█▏        | 63/512 [00:00<00:01, 318.95it/s]
Adding requests:  19%|█▊        | 95/512 [00:00<00:01, 310.17it/s]
Adding requests:  25%|██▌       | 128/512 [00:00<00:01, 317.68it/s]
Adding requests:  32%|███▏      | 163/512 [00:00<00:01, 327.19it/s]
Adding requests:  39%|███▉      | 199/512 [00:00<00:00, 337.85it/s]
Adding requests:  46%|████▌     | 235/512 [00:00<00:00, 344.14it/s]
Adding requests:  53%|█████▎    | 270/512 [00:00<00:00, 342.35it/s]
Adding requests:  60%|█████▉    | 306/512 [00:00<00:00, 347.27it/s]
Adding requests:  67%|██████▋   | 343/512 [00:01<00:00, 353.79it/s]
Adding requests:  74%|███████▍  | 380/512 [00:01<00:00, 357.30it/s]
Adding requests:  82%|████████▏ | 419/512 [00:01<00:00, 365.73it/s]
Adding requests:  89%|████████▉ | 456/512 [00:01<00:00, 361.51it/s]
Adding requests:  97%|█████████▋| 496/512 [00:01<00:00, 371.23it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 348.51it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  41%|████      | 210/512 [00:00<00:00, 1810.26it/s, est. speed input: 1853820.78 toks/s, output: 1810.29 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:01<00:00, 244.67it/s, est. speed input: 290987.55 toks/s, output: 284.17 toks/s]   
Processed prompts:  93%|█████████▎| 476/512 [00:01<00:00, 205.52it/s, est. speed input: 247735.76 toks/s, output: 241.93 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 205.52it/s, est. speed input: 235291.44 toks/s, output: 229.78 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 229.77it/s, est. speed input: 235291.44 toks/s, output: 229.78 toks/s]
[rank0]:[W126 15:51:36.136067208 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 15:51:38
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:51:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=711057) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=711057) WARNING 01-26 15:52:07 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=711057) WARNING 01-26 15:52:17 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 153.89 requests/s, 157735.61 total tokens/s, 153.89 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 15:51:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:51:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:51:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:51:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:51:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:51:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:51:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:51:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:51:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:51:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:51:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:51:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:51:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:51:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=711057) [2026-01-26 15:51:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=711057) [2026-01-26 15:51:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=711057) [2026-01-26 15:51:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=711057) [2026-01-26 15:51:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=711057) [2026-01-26 15:51:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=711057) [2026-01-26 15:51:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=711057) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=711057) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.74it/s]
(EngineCore_DP0 pid=711057) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=711057) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.43it/s]
(EngineCore_DP0 pid=711057) 
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=711057) [2026-01-26 15:52:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=711057) 2026-01-26 15:52:17,609 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=711057) 2026-01-26 15:52:17,633 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=711057) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 16.05it/s]
(EngineCore_DP0 pid=711057) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 18.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.45it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 277.66it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 319.03it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 315.94it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 321.07it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 329.80it/s]
Adding requests:  19%|█▉        | 199/1024 [00:00<00:02, 339.71it/s]
Adding requests:  23%|██▎       | 235/1024 [00:00<00:02, 345.39it/s]
Adding requests:  26%|██▋       | 270/1024 [00:00<00:02, 343.97it/s]
Adding requests:  30%|██▉       | 307/1024 [00:00<00:02, 348.72it/s]
Adding requests:  34%|███▎      | 345/1024 [00:01<00:01, 356.87it/s]
Adding requests:  37%|███▋      | 382/1024 [00:01<00:01, 358.28it/s]
Adding requests:  41%|████      | 420/1024 [00:01<00:01, 364.46it/s]
Adding requests:  45%|████▍     | 457/1024 [00:01<00:01, 361.11it/s]
Adding requests:  49%|████▊     | 497/1024 [00:01<00:01, 371.65it/s]
Adding requests:  52%|█████▏    | 536/1024 [00:01<00:01, 376.28it/s]
Adding requests:  56%|█████▌    | 574/1024 [00:01<00:01, 373.43it/s]
Adding requests:  60%|█████▉    | 612/1024 [00:01<00:01, 360.64it/s]
Adding requests:  63%|██████▎   | 649/1024 [00:01<00:01, 356.00it/s]
Adding requests:  67%|██████▋   | 686/1024 [00:01<00:00, 357.90it/s]
Adding requests:  71%|███████   | 722/1024 [00:02<00:00, 356.83it/s]
Adding requests:  74%|███████▍  | 758/1024 [00:02<00:00, 354.85it/s]
Adding requests:  78%|███████▊  | 794/1024 [00:02<00:00, 352.97it/s]
Adding requests:  81%|████████▏ | 832/1024 [00:02<00:00, 359.60it/s]
Adding requests:  85%|████████▍ | 868/1024 [00:02<00:00, 358.26it/s]
Adding requests:  89%|████████▊ | 907/1024 [00:02<00:00, 363.36it/s]
Adding requests:  92%|█████████▏| 944/1024 [00:02<00:00, 356.39it/s]
Adding requests:  96%|█████████▌| 980/1024 [00:02<00:00, 356.47it/s]
Adding requests:  99%|█████████▉| 1016/1024 [00:02<00:00, 353.42it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 353.23it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:00<00:00, 3512.10it/s, est. speed input: 3596598.89 toks/s, output: 3512.14 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:02<00:00, 287.30it/s, est. speed input: 348471.86 toks/s, output: 340.30 toks/s]   
Processed prompts:  94%|█████████▍| 964/1024 [00:03<00:00, 239.11it/s, est. speed input: 294269.74 toks/s, output: 287.37 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:03<00:00, 239.11it/s, est. speed input: 279320.41 toks/s, output: 272.77 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:03<00:00, 272.77it/s, est. speed input: 279320.41 toks/s, output: 272.77 toks/s]
[rank0]:[W126 15:52:26.684856732 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 15:52:28
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:52:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=712381) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=712381) WARNING 01-26 15:53:03 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=712381) WARNING 01-26 15:53:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 162.52 requests/s, 166584.17 total tokens/s, 162.52 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 15:52:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:52:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:52:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:52:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:52:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:52:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:52:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:52:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:52:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:52:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:52:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:52:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:52:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:52:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:52:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:52:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:52:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:55] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:55] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:55] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:55] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=712381) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=712381) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.75it/s]
(EngineCore_DP0 pid=712381) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=712381) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.43it/s]
(EngineCore_DP0 pid=712381) 
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=712381) [2026-01-26 15:52:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=712381) [rank0]:W0126 15:53:08.220000 712381 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=712381) [rank0]:W0126 15:53:08.303000 712381 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=712381) [rank0]:W0126 15:53:09.264000 712381 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=712381) [rank0]:W0126 15:53:09.393000 712381 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=712381) 2026-01-26 15:53:13,173 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=712381) 2026-01-26 15:53:13,200 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=712381) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 14.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 14.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 13.88it/s]
(EngineCore_DP0 pid=712381) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 18.01it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 18.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.21it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 26/2048 [00:00<00:08, 251.65it/s]
Adding requests:   3%|▎         | 60/2048 [00:00<00:06, 301.85it/s]
Adding requests:   4%|▍         | 92/2048 [00:00<00:06, 307.74it/s]
Adding requests:   6%|▌         | 126/2048 [00:00<00:06, 319.91it/s]
Adding requests:   8%|▊         | 161/2048 [00:00<00:05, 328.90it/s]
Adding requests:  10%|▉         | 198/2048 [00:00<00:05, 340.82it/s]
Adding requests:  11%|█▏        | 233/2048 [00:00<00:05, 341.73it/s]
Adding requests:  13%|█▎        | 268/2048 [00:00<00:05, 342.68it/s]
Adding requests:  15%|█▍        | 304/2048 [00:00<00:05, 347.88it/s]
Adding requests:  17%|█▋        | 342/2048 [00:01<00:04, 354.98it/s]
Adding requests:  19%|█▊        | 379/2048 [00:01<00:04, 358.30it/s]
Adding requests:  20%|██        | 418/2048 [00:01<00:04, 366.79it/s]
Adding requests:  22%|██▏       | 455/2048 [00:01<00:04, 362.25it/s]
Adding requests:  24%|██▍       | 495/2048 [00:01<00:04, 372.75it/s]
Adding requests:  26%|██▌       | 534/2048 [00:01<00:04, 376.99it/s]
Adding requests:  28%|██▊       | 572/2048 [00:01<00:03, 374.81it/s]
Adding requests:  30%|██▉       | 610/2048 [00:01<00:03, 361.12it/s]
Adding requests:  32%|███▏      | 647/2048 [00:01<00:04, 349.20it/s]
Adding requests:  33%|███▎      | 684/2048 [00:01<00:03, 352.64it/s]
Adding requests:  35%|███▌      | 720/2048 [00:02<00:03, 352.33it/s]
Adding requests:  37%|███▋      | 756/2048 [00:02<00:03, 351.48it/s]
Adding requests:  39%|███▊      | 792/2048 [00:02<00:03, 352.63it/s]
Adding requests:  41%|████      | 830/2048 [00:02<00:03, 359.45it/s]
Adding requests:  42%|████▏     | 867/2048 [00:02<00:03, 360.12it/s]
Adding requests:  44%|████▍     | 905/2048 [00:02<00:03, 364.60it/s]
Adding requests:  46%|████▌     | 942/2048 [00:02<00:03, 357.18it/s]
Adding requests:  48%|████▊     | 978/2048 [00:02<00:02, 357.60it/s]
Adding requests:  50%|████▉     | 1014/2048 [00:02<00:02, 352.15it/s]
Adding requests:  51%|█████▏    | 1050/2048 [00:02<00:02, 353.01it/s]
Adding requests:  53%|█████▎    | 1086/2048 [00:03<00:02, 352.60it/s]
Adding requests:  55%|█████▍    | 1122/2048 [00:03<00:02, 353.33it/s]
Adding requests:  57%|█████▋    | 1158/2048 [00:03<00:02, 353.57it/s]
Adding requests:  58%|█████▊    | 1194/2048 [00:03<00:02, 354.73it/s]
Adding requests:  60%|██████    | 1232/2048 [00:03<00:02, 360.06it/s]
Adding requests:  62%|██████▏   | 1269/2048 [00:03<00:02, 356.47it/s]
Adding requests:  64%|██████▎   | 1305/2048 [00:03<00:02, 355.64it/s]
Adding requests:  65%|██████▌   | 1341/2048 [00:03<00:01, 355.45it/s]
Adding requests:  67%|██████▋   | 1379/2048 [00:03<00:01, 360.06it/s]
Adding requests:  69%|██████▉   | 1416/2048 [00:04<00:01, 348.63it/s]
Adding requests:  71%|███████   | 1452/2048 [00:04<00:01, 350.19it/s]
Adding requests:  73%|███████▎  | 1490/2048 [00:04<00:01, 356.47it/s]
Adding requests:  75%|███████▍  | 1527/2048 [00:04<00:01, 357.31it/s]
Adding requests:  76%|███████▋  | 1563/2048 [00:04<00:01, 353.03it/s]
Adding requests:  78%|███████▊  | 1599/2048 [00:04<00:01, 349.03it/s]
Adding requests:  80%|███████▉  | 1634/2048 [00:04<00:01, 345.25it/s]
Adding requests:  81%|████████▏ | 1669/2048 [00:04<00:01, 339.41it/s]
Adding requests:  83%|████████▎ | 1706/2048 [00:04<00:00, 346.49it/s]
Adding requests:  85%|████████▌ | 1741/2048 [00:04<00:00, 340.87it/s]
Adding requests:  87%|████████▋ | 1779/2048 [00:05<00:00, 350.20it/s]
Adding requests:  89%|████████▊ | 1815/2048 [00:05<00:00, 348.41it/s]
Adding requests:  90%|█████████ | 1852/2048 [00:05<00:00, 351.74it/s]
Adding requests:  92%|█████████▏| 1889/2048 [00:05<00:00, 356.04it/s]
Adding requests:  94%|█████████▍| 1926/2048 [00:05<00:00, 359.19it/s]
Adding requests:  96%|█████████▌| 1964/2048 [00:05<00:00, 362.50it/s]
Adding requests:  98%|█████████▊| 2001/2048 [00:05<00:00, 356.73it/s]
Adding requests:  99%|█████████▉| 2037/2048 [00:05<00:00, 349.50it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 352.12it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  46%|████▌     | 940/2048 [00:00<00:00, 8821.07it/s, est. speed input: 9033781.58 toks/s, output: 8821.34 toks/s]
Processed prompts:  89%|████████▉ | 1823/2048 [00:05<00:00, 283.41it/s, est. speed input: 341319.01 toks/s, output: 333.32 toks/s]  
Processed prompts: 100%|██████████| 2048/2048 [00:06<00:00, 283.41it/s, est. speed input: 309149.01 toks/s, output: 301.90 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:06<00:00, 301.90it/s, est. speed input: 309149.01 toks/s, output: 301.90 toks/s]
[rank0]:[W126 15:53:28.625790482 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 15:53:30
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:54:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=713954) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=713954) WARNING 01-26 15:54:17 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=713954) WARNING 01-26 15:54:27 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 166.22 requests/s, 170373.20 total tokens/s, 166.22 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 15:54:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:54:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:54:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:54:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:54:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:54:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:54:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:54:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:54:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:54:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:54:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:54:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:54:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:54:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:54:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:54:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:54:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:08] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=713954) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=713954) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.74it/s]
(EngineCore_DP0 pid=713954) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=713954) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.43it/s]
(EngineCore_DP0 pid=713954) 
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=713954) [2026-01-26 15:54:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=713954) [rank0]:W0126 15:54:22.014000 713954 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=713954) [rank0]:W0126 15:54:22.097000 713954 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=713954) [rank0]:W0126 15:54:23.065000 713954 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=713954) [rank0]:W0126 15:54:23.196000 713954 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=713954) 2026-01-26 15:54:27,093 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=713954) 2026-01-26 15:54:27,119 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=713954) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 16.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 14.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 15.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 16.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 15.11it/s]
(EngineCore_DP0 pid=713954) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 17.90it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.10it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 18.33it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 18.25it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 274.94it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 317.08it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 314.67it/s]
Adding requests:   3%|▎         | 128/4096 [00:00<00:12, 320.21it/s]
Adding requests:   4%|▍         | 163/4096 [00:00<00:11, 329.91it/s]
Adding requests:   5%|▍         | 199/4096 [00:00<00:11, 337.46it/s]
Adding requests:   6%|▌         | 235/4096 [00:00<00:11, 343.33it/s]
Adding requests:   7%|▋         | 270/4096 [00:00<00:11, 341.92it/s]
Adding requests:   7%|▋         | 306/4096 [00:00<00:10, 345.81it/s]
Adding requests:   8%|▊         | 344/4096 [00:01<00:10, 354.03it/s]
Adding requests:   9%|▉         | 381/4096 [00:01<00:10, 356.46it/s]
Adding requests:  10%|█         | 420/4096 [00:01<00:10, 364.28it/s]
Adding requests:  11%|█         | 457/4096 [00:01<00:10, 359.98it/s]
Adding requests:  12%|█▏        | 497/4096 [00:01<00:09, 369.65it/s]
Adding requests:  13%|█▎        | 536/4096 [00:01<00:09, 375.27it/s]
Adding requests:  14%|█▍        | 574/4096 [00:01<00:09, 373.41it/s]
Adding requests:  15%|█▍        | 612/4096 [00:01<00:09, 360.42it/s]
Adding requests:  16%|█▌        | 649/4096 [00:01<00:09, 355.89it/s]
Adding requests:  17%|█▋        | 686/4096 [00:01<00:09, 357.52it/s]
Adding requests:  18%|█▊        | 722/4096 [00:02<00:09, 356.67it/s]
Adding requests:  19%|█▊        | 758/4096 [00:02<00:09, 354.45it/s]
Adding requests:  19%|█▉        | 794/4096 [00:02<00:09, 352.97it/s]
Adding requests:  20%|██        | 832/4096 [00:02<00:09, 359.53it/s]
Adding requests:  21%|██        | 868/4096 [00:02<00:09, 353.89it/s]
Adding requests:  22%|██▏       | 907/4096 [00:02<00:08, 360.15it/s]
Adding requests:  23%|██▎       | 944/4096 [00:02<00:08, 353.96it/s]
Adding requests:  24%|██▍       | 980/4096 [00:02<00:08, 354.16it/s]
Adding requests:  25%|██▍       | 1016/4096 [00:02<00:08, 351.42it/s]
Adding requests:  26%|██▌       | 1052/4096 [00:02<00:08, 350.99it/s]
Adding requests:  27%|██▋       | 1088/4096 [00:03<00:08, 349.53it/s]
Adding requests:  27%|██▋       | 1125/4096 [00:03<00:08, 353.96it/s]
Adding requests:  28%|██▊       | 1161/4096 [00:03<00:08, 350.39it/s]
Adding requests:  29%|██▉       | 1198/4096 [00:03<00:08, 353.58it/s]
Adding requests:  30%|███       | 1236/4096 [00:03<00:07, 359.60it/s]
Adding requests:  31%|███       | 1272/4096 [00:03<00:07, 355.81it/s]
Adding requests:  32%|███▏      | 1308/4096 [00:03<00:07, 355.31it/s]
Adding requests:  33%|███▎      | 1344/4096 [00:03<00:07, 356.24it/s]
Adding requests:  34%|███▎      | 1380/4096 [00:03<00:07, 353.82it/s]
Adding requests:  35%|███▍      | 1416/4096 [00:04<00:07, 351.18it/s]
Adding requests:  35%|███▌      | 1453/4096 [00:04<00:07, 355.00it/s]
Adding requests:  36%|███▋      | 1490/4096 [00:04<00:07, 358.75it/s]
Adding requests:  37%|███▋      | 1527/4096 [00:04<00:07, 359.16it/s]
Adding requests:  38%|███▊      | 1563/4096 [00:04<00:07, 354.99it/s]
Adding requests:  39%|███▉      | 1599/4096 [00:04<00:07, 349.24it/s]
Adding requests:  40%|███▉      | 1634/4096 [00:04<00:07, 345.70it/s]
Adding requests:  41%|████      | 1669/4096 [00:04<00:07, 340.35it/s]
Adding requests:  42%|████▏     | 1705/4096 [00:04<00:06, 345.28it/s]
Adding requests:  43%|████▎     | 1742/4096 [00:04<00:06, 349.68it/s]
Adding requests:  43%|████▎     | 1780/4096 [00:05<00:06, 356.40it/s]
Adding requests:  44%|████▍     | 1816/4096 [00:05<00:06, 352.37it/s]
Adding requests:  45%|████▌     | 1853/4096 [00:05<00:06, 355.74it/s]
Adding requests:  46%|████▌     | 1890/4096 [00:05<00:06, 357.56it/s]
Adding requests:  47%|████▋     | 1928/4096 [00:05<00:05, 362.99it/s]
Adding requests:  48%|████▊     | 1965/4096 [00:05<00:05, 363.90it/s]
Adding requests:  49%|████▉     | 2002/4096 [00:05<00:05, 351.38it/s]
Adding requests:  50%|████▉     | 2038/4096 [00:05<00:05, 347.54it/s]
Adding requests:  51%|█████     | 2073/4096 [00:05<00:05, 341.74it/s]
Adding requests:  51%|█████▏    | 2108/4096 [00:05<00:05, 336.58it/s]
Adding requests:  52%|█████▏    | 2143/4096 [00:06<00:05, 340.04it/s]
Adding requests:  53%|█████▎    | 2178/4096 [00:06<00:05, 334.72it/s]
Adding requests:  54%|█████▍    | 2213/4096 [00:06<00:05, 336.87it/s]
Adding requests:  55%|█████▍    | 2249/4096 [00:06<00:05, 343.57it/s]
Adding requests:  56%|█████▌    | 2286/4096 [00:06<00:05, 349.81it/s]
Adding requests:  57%|█████▋    | 2324/4096 [00:06<00:04, 356.33it/s]
Adding requests:  58%|█████▊    | 2361/4096 [00:06<00:04, 358.25it/s]
Adding requests:  59%|█████▊    | 2399/4096 [00:06<00:04, 364.26it/s]
Adding requests:  59%|█████▉    | 2436/4096 [00:06<00:04, 363.12it/s]
Adding requests:  60%|██████    | 2474/4096 [00:07<00:04, 364.77it/s]
Adding requests:  61%|██████▏   | 2511/4096 [00:07<00:04, 364.89it/s]
Adding requests:  62%|██████▏   | 2551/4096 [00:07<00:04, 372.30it/s]
Adding requests:  63%|██████▎   | 2590/4096 [00:07<00:04, 375.55it/s]
Adding requests:  64%|██████▍   | 2628/4096 [00:07<00:03, 367.93it/s]
Adding requests:  65%|██████▌   | 2665/4096 [00:07<00:03, 360.65it/s]
Adding requests:  66%|██████▌   | 2702/4096 [00:07<00:03, 356.97it/s]
Adding requests:  67%|██████▋   | 2738/4096 [00:07<00:03, 356.32it/s]
Adding requests:  68%|██████▊   | 2776/4096 [00:07<00:03, 361.91it/s]
Adding requests:  69%|██████▊   | 2815/4096 [00:07<00:03, 369.04it/s]
Adding requests:  70%|██████▉   | 2852/4096 [00:08<00:03, 362.68it/s]
Adding requests:  71%|███████   | 2889/4096 [00:08<00:03, 362.26it/s]
Adding requests:  71%|███████▏  | 2926/4096 [00:08<00:03, 362.20it/s]
Adding requests:  72%|███████▏  | 2964/4096 [00:08<00:03, 365.03it/s]
Adding requests:  73%|███████▎  | 3001/4096 [00:08<00:03, 364.89it/s]
Adding requests:  74%|███████▍  | 3039/4096 [00:08<00:02, 367.79it/s]
Adding requests:  75%|███████▌  | 3077/4096 [00:08<00:02, 371.28it/s]
Adding requests:  76%|███████▌  | 3115/4096 [00:08<00:02, 371.72it/s]
Adding requests:  77%|███████▋  | 3153/4096 [00:08<00:02, 369.03it/s]
Adding requests:  78%|███████▊  | 3190/4096 [00:08<00:02, 366.11it/s]
Adding requests:  79%|███████▉  | 3227/4096 [00:09<00:02, 365.20it/s]
Adding requests:  80%|███████▉  | 3264/4096 [00:09<00:02, 353.02it/s]
Adding requests:  81%|████████  | 3300/4096 [00:09<00:02, 347.34it/s]
Adding requests:  81%|████████▏ | 3336/4096 [00:09<00:02, 347.91it/s]
Adding requests:  82%|████████▏ | 3374/4096 [00:09<00:02, 355.16it/s]
Adding requests:  83%|████████▎ | 3412/4096 [00:09<00:01, 359.39it/s]
Adding requests:  84%|████████▍ | 3449/4096 [00:09<00:01, 361.34it/s]
Adding requests:  85%|████████▌ | 3486/4096 [00:09<00:01, 355.71it/s]
Adding requests:  86%|████████▌ | 3524/4096 [00:09<00:01, 362.23it/s]
Adding requests:  87%|████████▋ | 3564/4096 [00:10<00:01, 371.11it/s]
Adding requests:  88%|████████▊ | 3602/4096 [00:10<00:01, 365.47it/s]
Adding requests:  89%|████████▉ | 3639/4096 [00:10<00:01, 366.30it/s]
Adding requests:  90%|████████▉ | 3676/4096 [00:10<00:01, 359.60it/s]
Adding requests:  91%|█████████ | 3713/4096 [00:10<00:01, 357.33it/s]
Adding requests:  92%|█████████▏| 3749/4096 [00:10<00:00, 356.61it/s]
Adding requests:  92%|█████████▏| 3785/4096 [00:10<00:00, 346.92it/s]
Adding requests:  93%|█████████▎| 3820/4096 [00:10<00:00, 340.90it/s]
Adding requests:  94%|█████████▍| 3856/4096 [00:10<00:00, 345.58it/s]
Adding requests:  95%|█████████▍| 3891/4096 [00:10<00:00, 345.23it/s]
Adding requests:  96%|█████████▌| 3926/4096 [00:11<00:00, 343.08it/s]
Adding requests:  97%|█████████▋| 3962/4096 [00:11<00:00, 345.91it/s]
Adding requests:  98%|█████████▊| 3997/4096 [00:11<00:00, 343.48it/s]
Adding requests:  98%|█████████▊| 4033/4096 [00:11<00:00, 347.25it/s]
Adding requests:  99%|█████████▉| 4068/4096 [00:11<00:00, 346.93it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 354.44it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  46%|████▌     | 1893/4096 [00:00<00:00, 18435.20it/s, est. speed input: 18879130.56 toks/s, output: 18435.58 toks/s]
Processed prompts:  91%|█████████ | 3737/4096 [00:11<00:01, 288.50it/s, est. speed input: 347384.35 toks/s, output: 339.24 toks/s]      
Processed prompts:  92%|█████████▏| 3754/4096 [00:11<00:01, 283.49it/s, est. speed input: 342869.56 toks/s, output: 334.83 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:13<00:00, 283.49it/s, est. speed input: 320573.87 toks/s, output: 313.06 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:13<00:00, 313.06it/s, est. speed input: 320573.87 toks/s, output: 313.06 toks/s]
[rank0]:[W126 15:54:54.084219949 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 15:54:56
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-7B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 15:55:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=716025) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=716025) WARNING 01-26 15:56:07 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     def forward(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     raise e
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/tmp/torchinductor_root/b4/cb4kg3jf6kelto57mv2kddkonp6x2kscwufazyb6dsg7i5kkzser.py", line 1090, in call
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     triton_poi_fused_mul_quant_slide_int8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_slide_int8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     torch.cuda.synchronize()
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=716025) ERROR 01-26 15:56:14 [core.py:866] 

STDERR:
[2026-01-26 15:55:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:55:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:55:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:55:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:55:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:55:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:55:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:55:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 15:55:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 15:55:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:55:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 15:55:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 15:55:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 15:55:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 15:55:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 15:55:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 15:55:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=716025) [2026-01-26 15:55:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=716025) [2026-01-26 15:55:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=716025) [2026-01-26 15:55:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=716025) [2026-01-26 15:55:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=716025) [2026-01-26 15:55:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=716025) [2026-01-26 15:55:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=716025) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=716025) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.74it/s]
(EngineCore_DP0 pid=716025) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=716025) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.43it/s]
(EngineCore_DP0 pid=716025) 
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=716025) [2026-01-26 15:56:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=716025) [rank0]:W0126 15:56:12.602000 716025 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=716025) [rank0]:W0126 15:56:12.684000 716025 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=716025) [rank0]:W0126 15:56:13.790000 716025 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=716025) [rank0]:W0126 15:56:13.916000 716025 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=716025) Process EngineCore_DP0:
(EngineCore_DP0 pid=716025) Traceback (most recent call last):
(EngineCore_DP0 pid=716025)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=716025)     self.run()
(EngineCore_DP0 pid=716025)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=716025)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=716025)     raise e
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=716025)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=716025)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=716025)     super().__init__(
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=716025)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=716025)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=716025)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=716025)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=716025)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=716025)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=716025)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=716025)     return func(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=716025)     return func(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=716025)     self.model_runner.profile_run()
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=716025)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=716025)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=716025)     return func(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=716025)     outputs = self.model(
(EngineCore_DP0 pid=716025)               ^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=716025)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=716025)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=716025)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=716025)     hidden_states = self.model(
(EngineCore_DP0 pid=716025)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=716025)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=716025)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=716025)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=716025)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=716025)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=716025)     def forward(
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=716025)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=716025)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=716025)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=716025)     raise e
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=716025)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=716025)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=716025)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=716025)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=716025)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=716025)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=716025)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=716025)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=716025)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=716025)     return compiled_fn(full_args)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=716025)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=716025)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=716025)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=716025)                             ^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=716025)     outs = compiled_fn(args)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=716025)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=716025)     return self.current_callable(inputs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=716025)     out = model(new_inputs)
(EngineCore_DP0 pid=716025)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/tmp/torchinductor_root/b4/cb4kg3jf6kelto57mv2kddkonp6x2kscwufazyb6dsg7i5kkzser.py", line 1090, in call
(EngineCore_DP0 pid=716025)     triton_poi_fused_mul_quant_slide_int8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_slide_int8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=716025)     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=716025)     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=716025)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=716025)     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=716025)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=716025)     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=716025)     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=716025)     torch.cuda.synchronize()
(EngineCore_DP0 pid=716025)   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=716025)     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=716025)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=716025) torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=716025) Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=716025) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=716025) For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=716025) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=716025) 
[rank0]:[W126 15:56:15.270880011 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=65536

========== M=512 ==========
Time: 2026-01-26 17:11:34
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:11:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=820260) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=820260) WARNING 01-26 17:12:03 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=820260) WARNING 01-26 17:12:18 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 21.76 requests/s, 11161.52 total tokens/s, 21.76 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-26 17:11:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:11:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:11:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:11:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:11:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:11:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:11:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:11:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:11:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:11:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:11:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:11:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:11:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:11:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:11:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:11:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:11:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:11:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:11:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:11:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:11:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:11:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:11:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:11:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:11:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:11:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:11:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:11:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=820260) [2026-01-26 17:11:49] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=820260) [2026-01-26 17:11:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=820260) [2026-01-26 17:11:49] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=820260) [2026-01-26 17:11:49] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=820260) [2026-01-26 17:11:49] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=820260) [2026-01-26 17:11:49] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=820260) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=820260) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.08it/s]
(EngineCore_DP0 pid=820260) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.68it/s]
(EngineCore_DP0 pid=820260) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.37it/s]
(EngineCore_DP0 pid=820260) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.22it/s]
(EngineCore_DP0 pid=820260) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.27it/s]
(EngineCore_DP0 pid=820260) 
(EngineCore_DP0 pid=820260) [2026-01-26 17:11:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=820260) [2026-01-26 17:11:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22937600 bytes
(EngineCore_DP0 pid=820260) [2026-01-26 17:11:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=820260) [2026-01-26 17:11:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16384000 bytes
(EngineCore_DP0 pid=820260) [2026-01-26 17:11:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=820260) [2026-01-26 17:11:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 88473600 bytes
(EngineCore_DP0 pid=820260) [2026-01-26 17:11:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=820260) [2026-01-26 17:11:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44236800 bytes
(EngineCore_DP0 pid=820260) 2026-01-26 17:12:18,862 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=820260) 2026-01-26 17:12:18,903 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=820260) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.98it/s]
(EngineCore_DP0 pid=820260) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 10.04it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 508.27it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 243.39it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 291.31it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:04, 26.49it/s, est. speed input: 13565.88 toks/s, output: 26.49 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:04, 24.66it/s, est. speed input: 12756.63 toks/s, output: 24.91 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 24.14it/s, est. speed input: 12515.92 toks/s, output: 24.44 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:04, 23.93it/s, est. speed input: 12406.98 toks/s, output: 24.23 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:04, 23.84it/s, est. speed input: 12348.37 toks/s, output: 24.12 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:04, 23.70it/s, est. speed input: 12289.17 toks/s, output: 24.00 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:04, 23.67it/s, est. speed input: 12259.16 toks/s, output: 23.94 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:04, 23.60it/s, est. speed input: 12227.54 toks/s, output: 23.88 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:04, 23.57it/s, est. speed input: 12205.07 toks/s, output: 23.84 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:04, 23.60it/s, est. speed input: 12196.19 toks/s, output: 23.82 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:04, 23.60it/s, est. speed input: 12186.67 toks/s, output: 23.80 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:03, 23.60it/s, est. speed input: 12178.16 toks/s, output: 23.79 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:03, 23.47it/s, est. speed input: 12153.42 toks/s, output: 23.74 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:03, 23.51it/s, est. speed input: 12148.65 toks/s, output: 23.73 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:03, 23.48it/s, est. speed input: 12137.98 toks/s, output: 23.71 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:03, 23.46it/s, est. speed input: 12127.92 toks/s, output: 23.69 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:03, 23.54it/s, est. speed input: 12129.68 toks/s, output: 23.69 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:02<00:03, 23.42it/s, est. speed input: 12113.59 toks/s, output: 23.66 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:02<00:03, 23.44it/s, est. speed input: 12108.51 toks/s, output: 23.65 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:02<00:02, 23.30it/s, est. speed input: 12091.23 toks/s, output: 23.62 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:02<00:02, 23.36it/s, est. speed input: 12088.39 toks/s, output: 23.61 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:02<00:02, 23.40it/s, est. speed input: 12085.96 toks/s, output: 23.61 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:02, 23.04it/s, est. speed input: 12053.62 toks/s, output: 23.54 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:03<00:02, 23.19it/s, est. speed input: 12053.39 toks/s, output: 23.54 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:03<00:02, 23.29it/s, est. speed input: 12053.08 toks/s, output: 23.54 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:03<00:02, 23.20it/s, est. speed input: 12041.85 toks/s, output: 23.52 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:03<00:02, 23.29it/s, est. speed input: 12041.99 toks/s, output: 23.52 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:03<00:01, 23.42it/s, est. speed input: 12045.88 toks/s, output: 23.53 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:03<00:01, 23.31it/s, est. speed input: 12037.07 toks/s, output: 23.51 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:03<00:01, 23.38it/s, est. speed input: 12037.55 toks/s, output: 23.51 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:03<00:01, 23.39it/s, est. speed input: 12036.08 toks/s, output: 23.51 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:04<00:01, 23.41it/s, est. speed input: 12035.19 toks/s, output: 23.51 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:04<00:01, 23.49it/s, est. speed input: 12038.04 toks/s, output: 23.51 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:04<00:01, 23.53it/s, est. speed input: 12039.79 toks/s, output: 23.52 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:04<00:00, 23.57it/s, est. speed input: 12041.93 toks/s, output: 23.52 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:04<00:00, 23.40it/s, est. speed input: 12034.74 toks/s, output: 23.51 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:04<00:00, 23.44it/s, est. speed input: 12034.95 toks/s, output: 23.51 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:04<00:00, 23.50it/s, est. speed input: 12036.70 toks/s, output: 23.51 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:04<00:00, 23.53it/s, est. speed input: 12037.91 toks/s, output: 23.51 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:05<00:00, 23.59it/s, est. speed input: 12040.55 toks/s, output: 23.52 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:05<00:00, 23.60it/s, est. speed input: 12042.07 toks/s, output: 23.52 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:05<00:00, 23.57it/s, est. speed input: 12041.80 toks/s, output: 23.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.57it/s, est. speed input: 12041.61 toks/s, output: 23.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.52it/s, est. speed input: 12041.61 toks/s, output: 23.52 toks/s]
[rank0]:[W126 17:12:28.520295789 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-26 17:12:30
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:12:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=821790) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=821790) WARNING 01-26 17:12:58 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=821790) WARNING 01-26 17:13:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 22.43 requests/s, 22989.67 total tokens/s, 22.43 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-26 17:12:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:12:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:12:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:12:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:12:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:12:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:12:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:12:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:12:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:12:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:12:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:12:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:12:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:12:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:12:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:12:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:12:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:12:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:12:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:12:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:12:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:12:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:12:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:12:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:12:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:12:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:12:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:12:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=821790) [2026-01-26 17:12:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=821790) [2026-01-26 17:12:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=821790) [2026-01-26 17:12:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=821790) [2026-01-26 17:12:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=821790) [2026-01-26 17:12:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=821790) [2026-01-26 17:12:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=821790) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=821790) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.12it/s]
(EngineCore_DP0 pid=821790) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.83it/s]
(EngineCore_DP0 pid=821790) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.52it/s]
(EngineCore_DP0 pid=821790) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.36it/s]
(EngineCore_DP0 pid=821790) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.41it/s]
(EngineCore_DP0 pid=821790) 
(EngineCore_DP0 pid=821790) [2026-01-26 17:12:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=821790) [2026-01-26 17:12:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22937600 bytes
(EngineCore_DP0 pid=821790) [2026-01-26 17:12:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=821790) [2026-01-26 17:12:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16384000 bytes
(EngineCore_DP0 pid=821790) [2026-01-26 17:12:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=821790) [2026-01-26 17:12:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 88473600 bytes
(EngineCore_DP0 pid=821790) [2026-01-26 17:12:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=821790) [2026-01-26 17:12:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44236800 bytes
(EngineCore_DP0 pid=821790) 2026-01-26 17:13:13,452 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=821790) 2026-01-26 17:13:13,490 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=821790) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.16it/s]
(EngineCore_DP0 pid=821790) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 10.15it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 273.61it/s]
Adding requests:  49%|████▉     | 63/128 [00:00<00:00, 313.93it/s]
Adding requests:  74%|███████▍  | 95/128 [00:00<00:00, 310.45it/s]
Adding requests:  99%|█████████▉| 127/128 [00:00<00:00, 310.59it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 307.87it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 55.61it/s, est. speed input: 56948.58 toks/s, output: 55.61 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:03, 30.86it/s, est. speed input: 33857.63 toks/s, output: 33.06 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:04, 27.74it/s, est. speed input: 30728.22 toks/s, output: 30.01 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:04, 26.12it/s, est. speed input: 29105.26 toks/s, output: 28.42 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:04, 25.40it/s, est. speed input: 28348.37 toks/s, output: 27.68 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:04, 24.87it/s, est. speed input: 27788.95 toks/s, output: 27.14 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:04, 24.48it/s, est. speed input: 27359.08 toks/s, output: 26.72 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:03, 24.19it/s, est. speed input: 27012.23 toks/s, output: 26.38 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:03, 24.01it/s, est. speed input: 26738.95 toks/s, output: 26.11 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:03, 23.87it/s, est. speed input: 26509.47 toks/s, output: 25.89 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:03, 23.81it/s, est. speed input: 26329.95 toks/s, output: 25.71 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:03, 23.77it/s, est. speed input: 26177.09 toks/s, output: 25.56 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:03, 23.71it/s, est. speed input: 26036.03 toks/s, output: 25.43 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:03, 23.68it/s, est. speed input: 25916.60 toks/s, output: 25.31 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:03, 23.68it/s, est. speed input: 25814.98 toks/s, output: 25.21 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:02<00:03, 23.65it/s, est. speed input: 25719.65 toks/s, output: 25.12 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:02<00:02, 23.65it/s, est. speed input: 25638.73 toks/s, output: 25.04 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:02<00:02, 23.64it/s, est. speed input: 25564.42 toks/s, output: 24.97 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:02, 23.61it/s, est. speed input: 25493.05 toks/s, output: 24.90 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:02<00:02, 23.58it/s, est. speed input: 25426.81 toks/s, output: 24.83 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:02, 23.60it/s, est. speed input: 25374.00 toks/s, output: 24.78 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:02, 23.57it/s, est. speed input: 25318.03 toks/s, output: 24.72 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:03<00:02, 23.56it/s, est. speed input: 25267.82 toks/s, output: 24.68 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:03<00:02, 23.58it/s, est. speed input: 25225.68 toks/s, output: 24.63 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:03<00:01, 23.57it/s, est. speed input: 25184.38 toks/s, output: 24.59 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:03<00:01, 23.58it/s, est. speed input: 25147.20 toks/s, output: 24.56 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:03<00:01, 23.58it/s, est. speed input: 25112.51 toks/s, output: 24.52 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:03<00:01, 23.59it/s, est. speed input: 25080.59 toks/s, output: 24.49 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:03<00:01, 23.61it/s, est. speed input: 25053.26 toks/s, output: 24.47 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:04<00:01, 23.60it/s, est. speed input: 25024.06 toks/s, output: 24.44 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:04<00:01, 23.57it/s, est. speed input: 24994.25 toks/s, output: 24.41 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:04<00:01, 23.57it/s, est. speed input: 24969.22 toks/s, output: 24.38 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:04<00:00, 23.58it/s, est. speed input: 24945.59 toks/s, output: 24.36 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:04<00:00, 23.55it/s, est. speed input: 24919.98 toks/s, output: 24.34 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:04<00:00, 23.56it/s, est. speed input: 24898.94 toks/s, output: 24.32 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:04<00:00, 23.58it/s, est. speed input: 24880.14 toks/s, output: 24.30 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:04<00:00, 23.53it/s, est. speed input: 24856.20 toks/s, output: 24.27 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:05<00:00, 23.54it/s, est. speed input: 24838.07 toks/s, output: 24.26 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:05<00:00, 23.56it/s, est. speed input: 24821.57 toks/s, output: 24.24 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.24it/s, est. speed input: 24777.30 toks/s, output: 24.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.24it/s, est. speed input: 24777.30 toks/s, output: 24.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 24.20it/s, est. speed input: 24777.30 toks/s, output: 24.20 toks/s]
[rank0]:[W126 17:13:21.372801326 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-26 17:13:23
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:13:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=823104) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=823104) WARNING 01-26 17:13:51 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=823104) WARNING 01-26 17:14:06 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 47.00 requests/s, 48175.75 total tokens/s, 47.00 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-26 17:13:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:13:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:13:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:13:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:13:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:13:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:13:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:13:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:13:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:13:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:13:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:13:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:13:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:13:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:13:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:13:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:13:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:13:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:13:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:13:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:13:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:13:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:13:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:13:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:13:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:13:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:13:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:13:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=823104) [2026-01-26 17:13:38] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=823104) [2026-01-26 17:13:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=823104) [2026-01-26 17:13:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=823104) [2026-01-26 17:13:38] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=823104) [2026-01-26 17:13:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=823104) [2026-01-26 17:13:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=823104) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=823104) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.31it/s]
(EngineCore_DP0 pid=823104) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:00,  2.03it/s]
(EngineCore_DP0 pid=823104) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.60it/s]
(EngineCore_DP0 pid=823104) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.40it/s]
(EngineCore_DP0 pid=823104) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.48it/s]
(EngineCore_DP0 pid=823104) 
(EngineCore_DP0 pid=823104) [2026-01-26 17:13:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=823104) [2026-01-26 17:13:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22937600 bytes
(EngineCore_DP0 pid=823104) [2026-01-26 17:13:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=823104) [2026-01-26 17:13:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16384000 bytes
(EngineCore_DP0 pid=823104) [2026-01-26 17:13:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=823104) [2026-01-26 17:13:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 88473600 bytes
(EngineCore_DP0 pid=823104) [2026-01-26 17:13:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=823104) [2026-01-26 17:13:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44236800 bytes
(EngineCore_DP0 pid=823104) 2026-01-26 17:14:06,484 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=823104) 2026-01-26 17:14:06,522 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=823104) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  9.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.54it/s]
(EngineCore_DP0 pid=823104) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 10.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 10.02it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  10%|▉         | 25/256 [00:00<00:00, 244.93it/s]
Adding requests:  23%|██▎       | 59/256 [00:00<00:00, 298.69it/s]
Adding requests:  36%|███▌      | 92/256 [00:00<00:00, 308.98it/s]
Adding requests:  48%|████▊     | 123/256 [00:00<00:00, 155.00it/s]
Adding requests:  61%|██████    | 155/256 [00:00<00:00, 190.79it/s]
Adding requests:  75%|███████▌  | 192/256 [00:00<00:00, 233.15it/s]
Adding requests:  89%|████████▊ | 227/256 [00:00<00:00, 261.16it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 243.66it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:00<00:00, 436.00it/s, est. speed input: 446488.40 toks/s, output: 436.00 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:01<00:02, 79.23it/s, est. speed input: 93046.53 toks/s, output: 90.87 toks/s]   
Processed prompts:  44%|████▍     | 113/256 [00:01<00:02, 68.70it/s, est. speed input: 81307.32 toks/s, output: 79.40 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:01<00:02, 63.24it/s, est. speed input: 76011.88 toks/s, output: 74.23 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:01<00:02, 58.55it/s, est. speed input: 72175.45 toks/s, output: 70.48 toks/s]
Processed prompts:  57%|█████▋    | 147/256 [00:02<00:01, 57.81it/s, est. speed input: 70923.06 toks/s, output: 69.26 toks/s]
Processed prompts:  61%|██████    | 155/256 [00:02<00:01, 55.92it/s, est. speed input: 69439.47 toks/s, output: 67.81 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:02<00:01, 52.82it/s, est. speed input: 67721.41 toks/s, output: 66.13 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:02<00:01, 51.96it/s, est. speed input: 66857.96 toks/s, output: 65.29 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:02<00:01, 51.22it/s, est. speed input: 66080.11 toks/s, output: 64.53 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:02<00:01, 50.59it/s, est. speed input: 65365.90 toks/s, output: 63.83 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:02<00:01, 50.12it/s, est. speed input: 64719.88 toks/s, output: 63.20 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:03<00:01, 49.74it/s, est. speed input: 64123.60 toks/s, output: 62.62 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:03<00:01, 49.47it/s, est. speed input: 63576.92 toks/s, output: 62.09 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:03<00:01, 48.25it/s, est. speed input: 62896.11 toks/s, output: 61.42 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:03<00:00, 48.38it/s, est. speed input: 62430.10 toks/s, output: 60.97 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:03<00:00, 48.46it/s, est. speed input: 61994.13 toks/s, output: 60.54 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:03<00:00, 48.45it/s, est. speed input: 61577.22 toks/s, output: 60.13 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:03<00:00, 48.15it/s, est. speed input: 61146.83 toks/s, output: 59.71 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:03<00:00, 48.35it/s, est. speed input: 60799.65 toks/s, output: 59.37 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:04<00:00, 48.50it/s, est. speed input: 60474.88 toks/s, output: 59.06 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:04<00:00, 48.36it/s, est. speed input: 60138.39 toks/s, output: 58.73 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:04<00:00, 48.51it/s, est. speed input: 59850.91 toks/s, output: 58.45 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 48.51it/s, est. speed input: 59651.07 toks/s, output: 58.25 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 58.25it/s, est. speed input: 59651.07 toks/s, output: 58.25 toks/s]
[rank0]:[W126 17:14:14.377974939 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-26 17:14:15
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:14:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=824418) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=824418) WARNING 01-26 17:14:46 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=824418) WARNING 01-26 17:15:01 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 78.63 requests/s, 80596.98 total tokens/s, 78.63 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-26 17:14:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:14:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:14:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:14:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:14:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:14:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:14:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:14:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:14:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:14:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:14:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:14:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:14:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:14:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:14:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:14:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:14:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:14:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:14:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:14:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:14:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:14:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:14:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:14:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:14:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:14:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:14:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:14:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=824418) [2026-01-26 17:14:33] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=824418) [2026-01-26 17:14:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=824418) [2026-01-26 17:14:33] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=824418) [2026-01-26 17:14:33] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=824418) [2026-01-26 17:14:33] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=824418) [2026-01-26 17:14:33] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=824418) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=824418) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.23it/s]
(EngineCore_DP0 pid=824418) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.91it/s]
(EngineCore_DP0 pid=824418) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.50it/s]
(EngineCore_DP0 pid=824418) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.32it/s]
(EngineCore_DP0 pid=824418) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.39it/s]
(EngineCore_DP0 pid=824418) 
(EngineCore_DP0 pid=824418) [2026-01-26 17:14:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=824418) [2026-01-26 17:14:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22937600 bytes
(EngineCore_DP0 pid=824418) [2026-01-26 17:14:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=824418) [2026-01-26 17:14:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16384000 bytes
(EngineCore_DP0 pid=824418) [2026-01-26 17:14:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=824418) [2026-01-26 17:14:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 88473600 bytes
(EngineCore_DP0 pid=824418) [2026-01-26 17:14:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=824418) [2026-01-26 17:14:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44236800 bytes
(EngineCore_DP0 pid=824418) 2026-01-26 17:15:01,783 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=824418) 2026-01-26 17:15:01,822 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=824418) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  6.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  7.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.34it/s]
(EngineCore_DP0 pid=824418) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 11.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 11.11it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 28/512 [00:00<00:01, 279.00it/s]
Adding requests:  12%|█▏        | 61/512 [00:00<00:01, 305.80it/s]
Adding requests:  18%|█▊        | 93/512 [00:00<00:01, 308.73it/s]
Adding requests:  25%|██▍       | 127/512 [00:00<00:01, 318.61it/s]
Adding requests:  32%|███▏      | 162/512 [00:00<00:01, 329.48it/s]
Adding requests:  39%|███▊      | 198/512 [00:00<00:00, 338.91it/s]
Adding requests:  45%|████▌     | 232/512 [00:00<00:00, 335.25it/s]
Adding requests:  52%|█████▏    | 266/512 [00:00<00:00, 335.58it/s]
Adding requests:  59%|█████▉    | 301/512 [00:00<00:00, 339.53it/s]
Adding requests:  66%|██████▌   | 338/512 [00:01<00:00, 348.52it/s]
Adding requests:  73%|███████▎  | 374/512 [00:01<00:00, 351.13it/s]
Adding requests:  80%|████████  | 411/512 [00:01<00:00, 353.12it/s]
Adding requests:  87%|████████▋ | 447/512 [00:01<00:00, 350.99it/s]
Adding requests:  95%|█████████▌| 487/512 [00:01<00:00, 365.23it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 343.56it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:00<00:00, 824.70it/s, est. speed input: 844539.38 toks/s, output: 824.72 toks/s]
Processed prompts:  40%|████      | 205/512 [00:01<00:02, 151.67it/s, est. speed input: 181796.69 toks/s, output: 177.54 toks/s]
Processed prompts:  48%|████▊     | 244/512 [00:01<00:02, 122.63it/s, est. speed input: 150812.77 toks/s, output: 147.28 toks/s]
Processed prompts:  53%|█████▎    | 269/512 [00:01<00:02, 112.53it/s, est. speed input: 140591.82 toks/s, output: 137.30 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [00:02<00:02, 103.98it/s, est. speed input: 133385.96 toks/s, output: 130.26 toks/s]
Processed prompts:  59%|█████▉    | 303/512 [00:02<00:02, 97.88it/s, est. speed input: 128646.37 toks/s, output: 125.63 toks/s] 
Processed prompts:  62%|██████▏   | 316/512 [00:02<00:02, 96.61it/s, est. speed input: 126635.68 toks/s, output: 123.67 toks/s]
Processed prompts:  64%|██████▍   | 328/512 [00:02<00:01, 93.02it/s, est. speed input: 124106.14 toks/s, output: 121.20 toks/s]
Processed prompts:  66%|██████▌   | 339/512 [00:02<00:01, 88.39it/s, est. speed input: 121489.51 toks/s, output: 118.64 toks/s]
Processed prompts:  68%|██████▊   | 349/512 [00:02<00:01, 90.29it/s, est. speed input: 120800.38 toks/s, output: 117.97 toks/s]
Processed prompts:  70%|███████   | 359/512 [00:03<00:01, 83.36it/s, est. speed input: 118165.72 toks/s, output: 115.40 toks/s]
Processed prompts:  72%|███████▏  | 368/512 [00:03<00:01, 84.61it/s, est. speed input: 117329.90 toks/s, output: 114.58 toks/s]
Processed prompts:  74%|███████▎  | 377/512 [00:03<00:01, 85.70it/s, est. speed input: 116542.96 toks/s, output: 113.81 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:03<00:01, 76.93it/s, est. speed input: 114090.90 toks/s, output: 111.42 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:03<00:01, 77.38it/s, est. speed input: 113139.96 toks/s, output: 110.49 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:03<00:01, 77.94it/s, est. speed input: 112272.68 toks/s, output: 109.64 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:03<00:01, 78.34it/s, est. speed input: 111445.92 toks/s, output: 108.83 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:03<00:01, 78.68it/s, est. speed input: 110666.55 toks/s, output: 108.07 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:03<00:01, 78.93it/s, est. speed input: 109927.42 toks/s, output: 107.35 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:04<00:00, 80.53it/s, est. speed input: 109065.01 toks/s, output: 106.51 toks/s]
Processed prompts:  87%|████████▋ | 447/512 [00:04<00:00, 82.88it/s, est. speed input: 108643.64 toks/s, output: 106.10 toks/s]
Processed prompts:  89%|████████▉ | 456/512 [00:04<00:00, 84.67it/s, est. speed input: 108242.76 toks/s, output: 105.71 toks/s]
Processed prompts:  91%|█████████ | 465/512 [00:04<00:00, 85.80it/s, est. speed input: 107841.18 toks/s, output: 105.31 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:04<00:00, 75.86it/s, est. speed input: 106285.09 toks/s, output: 103.79 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:04<00:00, 76.83it/s, est. speed input: 105750.20 toks/s, output: 103.27 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:04<00:00, 77.54it/s, est. speed input: 105234.61 toks/s, output: 102.77 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:04<00:00, 78.05it/s, est. speed input: 104737.30 toks/s, output: 102.28 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:04<00:00, 78.52it/s, est. speed input: 104270.77 toks/s, output: 101.83 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 78.52it/s, est. speed input: 104443.26 toks/s, output: 102.00 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 101.99it/s, est. speed input: 104443.26 toks/s, output: 102.00 toks/s]
[rank0]:[W126 17:15:10.985112943 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-26 17:15:12
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:15:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=825784) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=825784) WARNING 01-26 17:15:46 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=825784) WARNING 01-26 17:16:02 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 83.80 requests/s, 85893.34 total tokens/s, 83.80 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-26 17:15:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:15:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:15:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:15:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:15:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:15:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:15:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:15:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:15:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:15:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:15:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:15:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:15:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:15:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:15:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:15:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:15:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:15:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:15:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:15:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:15:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:15:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:15:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:15:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:15:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:15:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:15:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:15:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=825784) [2026-01-26 17:15:33] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=825784) [2026-01-26 17:15:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=825784) [2026-01-26 17:15:33] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=825784) [2026-01-26 17:15:33] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=825784) [2026-01-26 17:15:33] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=825784) [2026-01-26 17:15:33] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=825784) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=825784) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.29it/s]
(EngineCore_DP0 pid=825784) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:00,  2.01it/s]
(EngineCore_DP0 pid=825784) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.58it/s]
(EngineCore_DP0 pid=825784) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.39it/s]
(EngineCore_DP0 pid=825784) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.47it/s]
(EngineCore_DP0 pid=825784) 
(EngineCore_DP0 pid=825784) [2026-01-26 17:15:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=825784) [2026-01-26 17:15:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22937600 bytes
(EngineCore_DP0 pid=825784) [2026-01-26 17:15:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=825784) [2026-01-26 17:15:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16384000 bytes
(EngineCore_DP0 pid=825784) [2026-01-26 17:15:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=825784) [2026-01-26 17:15:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 88473600 bytes
(EngineCore_DP0 pid=825784) [2026-01-26 17:15:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=825784) [2026-01-26 17:15:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44236800 bytes
(EngineCore_DP0 pid=825784) 2026-01-26 17:16:02,375 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=825784) 2026-01-26 17:16:02,415 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=825784) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  9.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  9.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  9.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.06it/s]
(EngineCore_DP0 pid=825784) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 11.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 11.30it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 11.28it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 276.98it/s]
Adding requests:   6%|▌         | 63/1024 [00:00<00:03, 318.18it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 315.31it/s]
Adding requests:  12%|█▎        | 128/1024 [00:00<00:02, 320.30it/s]
Adding requests:  16%|█▌        | 163/1024 [00:00<00:02, 328.90it/s]
Adding requests:  19%|█▉        | 199/1024 [00:00<00:02, 339.13it/s]
Adding requests:  23%|██▎       | 235/1024 [00:00<00:02, 344.82it/s]
Adding requests:  26%|██▋       | 270/1024 [00:00<00:02, 343.78it/s]
Adding requests:  30%|██▉       | 306/1024 [00:00<00:02, 348.51it/s]
Adding requests:  33%|███▎      | 343/1024 [00:01<00:01, 353.04it/s]
Adding requests:  37%|███▋      | 380/1024 [00:01<00:01, 356.52it/s]
Adding requests:  41%|████      | 419/1024 [00:01<00:01, 364.27it/s]
Adding requests:  45%|████▍     | 456/1024 [00:01<00:01, 361.26it/s]
Adding requests:  48%|████▊     | 496/1024 [00:01<00:01, 371.30it/s]
Adding requests:  52%|█████▏    | 535/1024 [00:01<00:01, 375.21it/s]
Adding requests:  56%|█████▌    | 573/1024 [00:01<00:01, 372.49it/s]
Adding requests:  60%|█████▉    | 611/1024 [00:01<00:01, 358.91it/s]
Adding requests:  63%|██████▎   | 648/1024 [00:01<00:01, 355.27it/s]
Adding requests:  67%|██████▋   | 685/1024 [00:01<00:00, 357.07it/s]
Adding requests:  70%|███████   | 721/1024 [00:02<00:00, 355.57it/s]
Adding requests:  74%|███████▍  | 757/1024 [00:02<00:00, 354.31it/s]
Adding requests:  77%|███████▋  | 793/1024 [00:02<00:00, 352.50it/s]
Adding requests:  81%|████████  | 831/1024 [00:02<00:00, 359.04it/s]
Adding requests:  85%|████████▍ | 867/1024 [00:02<00:00, 358.60it/s]
Adding requests:  88%|████████▊ | 905/1024 [00:02<00:00, 362.96it/s]
Adding requests:  92%|█████████▏| 942/1024 [00:02<00:00, 355.73it/s]
Adding requests:  96%|█████████▌| 978/1024 [00:02<00:00, 356.42it/s]
Adding requests:  99%|█████████▉| 1014/1024 [00:02<00:00, 351.01it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 352.37it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:00<00:00, 1430.08it/s, est. speed input: 1464471.22 toks/s, output: 1430.10 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:01<00:03, 173.08it/s, est. speed input: 212337.05 toks/s, output: 207.36 toks/s]   
Processed prompts:  44%|████▍     | 450/1024 [00:02<00:04, 140.46it/s, est. speed input: 176447.32 toks/s, output: 172.31 toks/s]
Processed prompts:  48%|████▊     | 489/1024 [00:02<00:04, 132.58it/s, est. speed input: 167492.66 toks/s, output: 163.57 toks/s]
Processed prompts:  50%|█████     | 517/1024 [00:03<00:04, 118.91it/s, est. speed input: 157134.92 toks/s, output: 153.45 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:03<00:04, 110.08it/s, est. speed input: 150867.47 toks/s, output: 147.33 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:03<00:04, 106.05it/s, est. speed input: 147716.85 toks/s, output: 144.25 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:04<00:04, 102.14it/s, est. speed input: 144876.37 toks/s, output: 141.48 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:04<00:04, 98.60it/s, est. speed input: 142305.38 toks/s, output: 138.97 toks/s] 
Processed prompts:  59%|█████▉    | 602/1024 [00:04<00:04, 95.27it/s, est. speed input: 139888.04 toks/s, output: 136.61 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:04<00:04, 92.85it/s, est. speed input: 137750.10 toks/s, output: 134.52 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:04<00:04, 90.82it/s, est. speed input: 135758.89 toks/s, output: 132.58 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:04<00:04, 89.11it/s, est. speed input: 133886.14 toks/s, output: 130.75 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:05<00:04, 87.98it/s, est. speed input: 132180.40 toks/s, output: 129.08 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:05<00:03, 86.75it/s, est. speed input: 130521.15 toks/s, output: 127.46 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:05<00:03, 86.14it/s, est. speed input: 129024.43 toks/s, output: 126.00 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:05<00:03, 85.72it/s, est. speed input: 127629.73 toks/s, output: 124.64 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:05<00:03, 85.47it/s, est. speed input: 126333.00 toks/s, output: 123.37 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:06<00:03, 85.20it/s, est. speed input: 125101.49 toks/s, output: 122.17 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:06<00:03, 85.11it/s, est. speed input: 123958.33 toks/s, output: 121.05 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:06<00:02, 84.91it/s, est. speed input: 122863.04 toks/s, output: 119.98 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:06<00:02, 85.81it/s, est. speed input: 121970.07 toks/s, output: 119.11 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:06<00:02, 85.48it/s, est. speed input: 120999.85 toks/s, output: 118.16 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:07<00:02, 85.12it/s, est. speed input: 120065.25 toks/s, output: 117.25 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:07<00:02, 85.05it/s, est. speed input: 119201.95 toks/s, output: 116.41 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:07<00:01, 84.75it/s, est. speed input: 118352.17 toks/s, output: 115.58 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:07<00:01, 84.81it/s, est. speed input: 117576.47 toks/s, output: 114.82 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:07<00:01, 84.77it/s, est. speed input: 116828.90 toks/s, output: 114.09 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:07<00:01, 84.60it/s, est. speed input: 116100.66 toks/s, output: 113.38 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:08<00:01, 84.42it/s, est. speed input: 115400.41 toks/s, output: 112.70 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:08<00:01, 84.43it/s, est. speed input: 114745.53 toks/s, output: 112.06 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:08<00:00, 84.61it/s, est. speed input: 114137.66 toks/s, output: 111.46 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:08<00:00, 84.58it/s, est. speed input: 113540.53 toks/s, output: 110.88 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:08<00:00, 84.38it/s, est. speed input: 112951.32 toks/s, output: 110.30 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:09<00:00, 84.39it/s, est. speed input: 112401.09 toks/s, output: 109.77 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:09<00:00, 85.15it/s, est. speed input: 111939.73 toks/s, output: 109.32 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 85.15it/s, est. speed input: 112597.94 toks/s, output: 109.96 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 109.96it/s, est. speed input: 112597.94 toks/s, output: 109.96 toks/s]
[rank0]:[W126 17:16:17.900716546 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-26 17:16:19
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:16:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=827333) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=827333) WARNING 01-26 17:16:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=827333) WARNING 01-26 17:17:16 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 87.96 requests/s, 90159.06 total tokens/s, 87.96 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-26 17:16:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:16:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:16:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:16:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:16:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:16:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:16:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:16:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:16:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:16:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:16:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:16:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:16:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:16:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:16:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:16:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:16:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:16:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:16:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:16:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:16:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:16:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:16:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:16:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:16:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:16:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:16:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:16:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=827333) [2026-01-26 17:16:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=827333) [2026-01-26 17:16:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=827333) [2026-01-26 17:16:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=827333) [2026-01-26 17:16:46] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=827333) [2026-01-26 17:16:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=827333) [2026-01-26 17:16:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=827333) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=827333) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.31it/s]
(EngineCore_DP0 pid=827333) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:00,  2.03it/s]
(EngineCore_DP0 pid=827333) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.60it/s]
(EngineCore_DP0 pid=827333) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.40it/s]
(EngineCore_DP0 pid=827333) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.48it/s]
(EngineCore_DP0 pid=827333) 
(EngineCore_DP0 pid=827333) [2026-01-26 17:16:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=827333) [2026-01-26 17:16:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22937600 bytes
(EngineCore_DP0 pid=827333) [2026-01-26 17:16:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=827333) [2026-01-26 17:16:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16384000 bytes
(EngineCore_DP0 pid=827333) [2026-01-26 17:16:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=827333) [2026-01-26 17:16:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 88473600 bytes
(EngineCore_DP0 pid=827333) [2026-01-26 17:16:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=827333) [2026-01-26 17:16:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44236800 bytes
(EngineCore_DP0 pid=827333) [rank0]:W0126 17:17:06.897000 827333 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=827333) [rank0]:W0126 17:17:06.966000 827333 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=827333) [rank0]:W0126 17:17:07.747000 827333 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=827333) [rank0]:W0126 17:17:07.854000 827333 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=827333) 2026-01-26 17:17:16,718 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=827333) 2026-01-26 17:17:16,764 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=827333) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  6.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  9.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  9.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.42it/s]
(EngineCore_DP0 pid=827333) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 11.09it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 11.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 11.20it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 28/2048 [00:00<00:07, 275.07it/s]
Adding requests:   3%|▎         | 62/2048 [00:00<00:06, 312.76it/s]
Adding requests:   5%|▍         | 94/2048 [00:00<00:06, 310.86it/s]
Adding requests:   6%|▋         | 128/2048 [00:00<00:06, 319.62it/s]
Adding requests:   8%|▊         | 163/2048 [00:00<00:05, 328.62it/s]
Adding requests:  10%|▉         | 199/2048 [00:00<00:05, 338.64it/s]
Adding requests:  11%|█▏        | 235/2048 [00:00<00:05, 345.00it/s]
Adding requests:  13%|█▎        | 270/2048 [00:00<00:05, 342.90it/s]
Adding requests:  15%|█▍        | 306/2048 [00:00<00:05, 347.61it/s]
Adding requests:  17%|█▋        | 344/2048 [00:01<00:04, 355.38it/s]
Adding requests:  19%|█▊        | 380/2048 [00:01<00:04, 356.47it/s]
Adding requests:  20%|██        | 419/2048 [00:01<00:04, 365.09it/s]
Adding requests:  22%|██▏       | 456/2048 [00:01<00:04, 360.92it/s]
Adding requests:  24%|██▍       | 496/2048 [00:01<00:04, 371.18it/s]
Adding requests:  26%|██▌       | 535/2048 [00:01<00:04, 375.77it/s]
Adding requests:  28%|██▊       | 573/2048 [00:01<00:03, 372.93it/s]
Adding requests:  30%|██▉       | 611/2048 [00:01<00:04, 358.53it/s]
Adding requests:  32%|███▏      | 647/2048 [00:01<00:03, 351.27it/s]
Adding requests:  33%|███▎      | 684/2048 [00:01<00:03, 354.28it/s]
Adding requests:  35%|███▌      | 720/2048 [00:02<00:03, 353.67it/s]
Adding requests:  37%|███▋      | 756/2048 [00:02<00:03, 350.79it/s]
Adding requests:  39%|███▊      | 792/2048 [00:02<00:03, 351.14it/s]
Adding requests:  40%|████      | 829/2048 [00:02<00:03, 355.84it/s]
Adding requests:  42%|████▏     | 866/2048 [00:02<00:03, 358.30it/s]
Adding requests:  44%|████▍     | 904/2048 [00:02<00:03, 363.15it/s]
Adding requests:  46%|████▌     | 941/2048 [00:02<00:03, 352.71it/s]
Adding requests:  48%|████▊     | 977/2048 [00:02<00:03, 353.33it/s]
Adding requests:  49%|████▉     | 1013/2048 [00:02<00:02, 349.76it/s]
Adding requests:  51%|█████     | 1049/2048 [00:02<00:02, 350.11it/s]
Adding requests:  53%|█████▎    | 1085/2048 [00:03<00:02, 350.15it/s]
Adding requests:  55%|█████▍    | 1121/2048 [00:03<00:02, 349.22it/s]
Adding requests:  56%|█████▋    | 1156/2048 [00:03<00:02, 349.31it/s]
Adding requests:  58%|█████▊    | 1192/2048 [00:03<00:02, 350.79it/s]
Adding requests:  60%|██████    | 1229/2048 [00:03<00:02, 355.92it/s]
Adding requests:  62%|██████▏   | 1265/2048 [00:03<00:02, 355.04it/s]
Adding requests:  64%|██████▎   | 1301/2048 [00:03<00:02, 353.15it/s]
Adding requests:  65%|██████▌   | 1337/2048 [00:03<00:02, 351.63it/s]
Adding requests:  67%|██████▋   | 1375/2048 [00:03<00:01, 357.82it/s]
Adding requests:  69%|██████▉   | 1411/2048 [00:04<00:01, 354.22it/s]
Adding requests:  71%|███████   | 1447/2048 [00:04<00:01, 352.38it/s]
Adding requests:  73%|███████▎  | 1485/2048 [00:04<00:01, 357.70it/s]
Adding requests:  74%|███████▍  | 1522/2048 [00:04<00:01, 359.98it/s]
Adding requests:  76%|███████▌  | 1559/2048 [00:04<00:01, 348.29it/s]
Adding requests:  78%|███████▊  | 1594/2048 [00:04<00:01, 344.48it/s]
Adding requests:  80%|███████▉  | 1629/2048 [00:04<00:01, 342.06it/s]
Adding requests:  81%|████████▏ | 1664/2048 [00:04<00:01, 335.63it/s]
Adding requests:  83%|████████▎ | 1700/2048 [00:04<00:01, 340.65it/s]
Adding requests:  85%|████████▍ | 1737/2048 [00:04<00:00, 347.26it/s]
Adding requests:  87%|████████▋ | 1774/2048 [00:05<00:00, 353.79it/s]
Adding requests:  88%|████████▊ | 1810/2048 [00:05<00:00, 342.94it/s]
Adding requests:  90%|█████████ | 1847/2048 [00:05<00:00, 348.37it/s]
Adding requests:  92%|█████████▏| 1883/2048 [00:05<00:00, 349.96it/s]
Adding requests:  94%|█████████▍| 1920/2048 [00:05<00:00, 354.51it/s]
Adding requests:  96%|█████████▌| 1959/2048 [00:05<00:00, 361.47it/s]
Adding requests:  97%|█████████▋| 1996/2048 [00:05<00:00, 352.28it/s]
Adding requests:  99%|█████████▉| 2032/2048 [00:05<00:00, 345.41it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 350.70it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:00<00:00, 2053.33it/s, est. speed input: 2102681.00 toks/s, output: 2053.35 toks/s]
Processed prompts:  35%|███▌      | 720/2048 [00:02<00:05, 240.01it/s, est. speed input: 303096.43 toks/s, output: 295.99 toks/s]   
Processed prompts:  40%|███▉      | 811/2048 [00:03<00:06, 178.22it/s, est. speed input: 236248.75 toks/s, output: 230.71 toks/s]
Processed prompts:  42%|████▏     | 864/2048 [00:04<00:07, 160.22it/s, est. speed input: 217863.66 toks/s, output: 212.76 toks/s]
Processed prompts:  44%|████▍     | 900/2048 [00:04<00:08, 137.45it/s, est. speed input: 200001.91 toks/s, output: 195.31 toks/s]
Processed prompts:  45%|████▌     | 926/2048 [00:04<00:08, 137.99it/s, est. speed input: 197950.67 toks/s, output: 193.31 toks/s]
Processed prompts:  46%|████▋     | 948/2048 [00:05<00:09, 120.48it/s, est. speed input: 188391.97 toks/s, output: 183.98 toks/s]
Processed prompts:  47%|████▋     | 965/2048 [00:05<00:09, 116.86it/s, est. speed input: 185291.27 toks/s, output: 180.95 toks/s]
Processed prompts:  48%|████▊     | 980/2048 [00:05<00:09, 111.30it/s, est. speed input: 182016.95 toks/s, output: 177.75 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:05<00:10, 104.65it/s, est. speed input: 178708.74 toks/s, output: 174.52 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:05<00:10, 101.15it/s, est. speed input: 175990.07 toks/s, output: 171.87 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:06<00:10, 98.02it/s, est. speed input: 173403.82 toks/s, output: 169.34 toks/s] 
Processed prompts:  51%|█████     | 1042/2048 [00:06<00:10, 95.50it/s, est. speed input: 170977.83 toks/s, output: 166.97 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:06<00:10, 93.44it/s, est. speed input: 168673.19 toks/s, output: 164.72 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:06<00:10, 91.80it/s, est. speed input: 166483.54 toks/s, output: 162.58 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:06<00:10, 90.87it/s, est. speed input: 164461.38 toks/s, output: 160.61 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:06<00:10, 90.08it/s, est. speed input: 162526.59 toks/s, output: 158.72 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:07<00:10, 89.20it/s, est. speed input: 160642.10 toks/s, output: 156.88 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:07<00:10, 88.91it/s, est. speed input: 158903.42 toks/s, output: 155.18 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:07<00:10, 88.53it/s, est. speed input: 157222.16 toks/s, output: 153.54 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:07<00:09, 88.31it/s, est. speed input: 155628.15 toks/s, output: 151.98 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:07<00:09, 88.25it/s, est. speed input: 154121.44 toks/s, output: 150.51 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:08<00:09, 89.29it/s, est. speed input: 152822.28 toks/s, output: 149.24 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:08<00:09, 88.90it/s, est. speed input: 151437.25 toks/s, output: 147.89 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:08<00:09, 88.60it/s, est. speed input: 150109.74 toks/s, output: 146.59 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:08<00:09, 88.40it/s, est. speed input: 148839.10 toks/s, output: 145.35 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:08<00:08, 88.39it/s, est. speed input: 147635.52 toks/s, output: 144.18 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:08<00:08, 88.14it/s, est. speed input: 146454.10 toks/s, output: 143.02 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:09<00:08, 87.90it/s, est. speed input: 145312.34 toks/s, output: 141.91 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:09<00:08, 88.14it/s, est. speed input: 144258.88 toks/s, output: 140.88 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:09<00:08, 88.36it/s, est. speed input: 143250.92 toks/s, output: 139.89 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:09<00:07, 88.41it/s, est. speed input: 142269.63 toks/s, output: 138.94 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:09<00:07, 88.46it/s, est. speed input: 141325.17 toks/s, output: 138.01 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:10<00:07, 88.41it/s, est. speed input: 140407.46 toks/s, output: 137.12 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:10<00:07, 88.46it/s, est. speed input: 139529.50 toks/s, output: 136.26 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:10<00:07, 88.73it/s, est. speed input: 138703.98 toks/s, output: 135.45 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:10<00:07, 88.79it/s, est. speed input: 137894.91 toks/s, output: 134.66 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:10<00:06, 88.54it/s, est. speed input: 137087.07 toks/s, output: 133.87 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:10<00:06, 89.80it/s, est. speed input: 136426.33 toks/s, output: 133.23 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:11<00:06, 89.59it/s, est. speed input: 135696.90 toks/s, output: 132.52 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:11<00:06, 88.89it/s, est. speed input: 134946.01 toks/s, output: 131.78 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:11<00:06, 88.57it/s, est. speed input: 134232.54 toks/s, output: 131.09 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:11<00:05, 90.05it/s, est. speed input: 133672.07 toks/s, output: 130.54 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:11<00:05, 89.39it/s, est. speed input: 133000.82 toks/s, output: 129.88 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:12<00:05, 90.28it/s, est. speed input: 132448.12 toks/s, output: 129.34 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:12<00:05, 89.53it/s, est. speed input: 131813.34 toks/s, output: 128.72 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:12<00:05, 88.90it/s, est. speed input: 131189.02 toks/s, output: 128.11 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:12<00:05, 88.44it/s, est. speed input: 130580.87 toks/s, output: 127.52 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:12<00:04, 89.54it/s, est. speed input: 130088.67 toks/s, output: 127.04 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:12<00:04, 88.82it/s, est. speed input: 129508.93 toks/s, output: 126.47 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:13<00:04, 88.58it/s, est. speed input: 128962.40 toks/s, output: 125.94 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:13<00:04, 88.29it/s, est. speed input: 128423.20 toks/s, output: 125.41 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:13<00:04, 88.38it/s, est. speed input: 127917.56 toks/s, output: 124.92 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:13<00:03, 88.20it/s, est. speed input: 127409.65 toks/s, output: 124.42 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:13<00:03, 88.60it/s, est. speed input: 126948.27 toks/s, output: 123.97 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:13<00:03, 89.91it/s, est. speed input: 126560.25 toks/s, output: 123.59 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:14<00:03, 90.44it/s, est. speed input: 126158.19 toks/s, output: 123.20 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:14<00:03, 89.60it/s, est. speed input: 125696.30 toks/s, output: 122.75 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:14<00:03, 89.23it/s, est. speed input: 125257.62 toks/s, output: 122.32 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:14<00:02, 88.80it/s, est. speed input: 124820.33 toks/s, output: 121.89 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:14<00:02, 88.57it/s, est. speed input: 124397.16 toks/s, output: 121.48 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:15<00:02, 88.44it/s, est. speed input: 123986.05 toks/s, output: 121.08 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:15<00:02, 88.21it/s, est. speed input: 123577.18 toks/s, output: 120.68 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:15<00:02, 88.13it/s, est. speed input: 123182.31 toks/s, output: 120.30 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:15<00:01, 88.20it/s, est. speed input: 122803.23 toks/s, output: 119.92 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:15<00:01, 89.61it/s, est. speed input: 122504.57 toks/s, output: 119.63 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:15<00:01, 89.41it/s, est. speed input: 122150.75 toks/s, output: 119.29 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:16<00:01, 88.68it/s, est. speed input: 121775.01 toks/s, output: 118.92 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:16<00:01, 88.49it/s, est. speed input: 121423.43 toks/s, output: 118.58 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:16<00:01, 88.51it/s, est. speed input: 121087.27 toks/s, output: 118.25 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:16<00:00, 88.56it/s, est. speed input: 120760.27 toks/s, output: 117.93 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:16<00:00, 89.76it/s, est. speed input: 120496.11 toks/s, output: 117.67 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:17<00:00, 89.43it/s, est. speed input: 120181.81 toks/s, output: 117.36 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:17<00:00, 89.02it/s, est. speed input: 119865.74 toks/s, output: 117.06 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:17<00:00, 85.82it/s, est. speed input: 119416.19 toks/s, output: 116.62 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:17<00:00, 85.82it/s, est. speed input: 120236.61 toks/s, output: 117.42 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:17<00:00, 117.42it/s, est. speed input: 120236.61 toks/s, output: 117.42 toks/s]
[rank0]:[W126 17:17:43.340202363 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-26 17:17:45
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:18:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=829213) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=829213) WARNING 01-26 17:18:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=829213) WARNING 01-26 17:18:55 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 90.82 requests/s, 93093.14 total tokens/s, 90.82 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-26 17:18:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:18:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:18:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:18:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:18:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:18:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:18:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:18:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:18:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:18:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:18:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:18:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:18:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:18:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:18:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:18:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:18:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:18:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:18:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:18:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:18:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:18:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:18:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:18:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:18:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:18:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:18:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:18:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=829213) [2026-01-26 17:18:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=829213) [2026-01-26 17:18:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=829213) [2026-01-26 17:18:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=829213) [2026-01-26 17:18:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=829213) [2026-01-26 17:18:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=829213) [2026-01-26 17:18:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=829213) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=829213) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.31it/s]
(EngineCore_DP0 pid=829213) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:00,  2.03it/s]
(EngineCore_DP0 pid=829213) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.60it/s]
(EngineCore_DP0 pid=829213) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.40it/s]
(EngineCore_DP0 pid=829213) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.48it/s]
(EngineCore_DP0 pid=829213) 
(EngineCore_DP0 pid=829213) [2026-01-26 17:18:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=829213) [2026-01-26 17:18:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22937600 bytes
(EngineCore_DP0 pid=829213) [2026-01-26 17:18:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=829213) [2026-01-26 17:18:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16384000 bytes
(EngineCore_DP0 pid=829213) [2026-01-26 17:18:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=829213) [2026-01-26 17:18:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 88473600 bytes
(EngineCore_DP0 pid=829213) [2026-01-26 17:18:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=829213) [2026-01-26 17:18:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44236800 bytes
(EngineCore_DP0 pid=829213) [rank0]:W0126 17:18:44.487000 829213 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=829213) [rank0]:W0126 17:18:44.555000 829213 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=829213) [rank0]:W0126 17:18:45.495000 829213 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=829213) [rank0]:W0126 17:18:45.603000 829213 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=829213) 2026-01-26 17:18:55,194 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=829213) 2026-01-26 17:18:55,290 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=829213) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  6.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  9.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 10.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 10.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00, 10.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.68it/s]
(EngineCore_DP0 pid=829213) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  9.99it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  9.93it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 10.72it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 11.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 10.90it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 28/4096 [00:00<00:14, 275.07it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 317.32it/s]
Adding requests:   2%|▏         | 95/4096 [00:00<00:12, 314.52it/s]
Adding requests:   3%|▎         | 128/4096 [00:00<00:12, 320.05it/s]
Adding requests:   4%|▍         | 163/4096 [00:00<00:11, 329.83it/s]
Adding requests:   5%|▍         | 199/4096 [00:00<00:11, 339.66it/s]
Adding requests:   6%|▌         | 235/4096 [00:00<00:11, 344.81it/s]
Adding requests:   7%|▋         | 270/4096 [00:00<00:11, 343.11it/s]
Adding requests:   7%|▋         | 306/4096 [00:00<00:10, 347.45it/s]
Adding requests:   8%|▊         | 342/4096 [00:01<00:10, 348.08it/s]
Adding requests:   9%|▉         | 378/4096 [00:01<00:10, 351.56it/s]
Adding requests:  10%|█         | 416/4096 [00:01<00:10, 359.53it/s]
Adding requests:  11%|█         | 452/4096 [00:01<00:10, 356.35it/s]
Adding requests:  12%|█▏        | 492/4096 [00:01<00:09, 368.86it/s]
Adding requests:  13%|█▎        | 532/4096 [00:01<00:09, 374.55it/s]
Adding requests:  14%|█▍        | 570/4096 [00:01<00:09, 372.05it/s]
Adding requests:  15%|█▍        | 608/4096 [00:01<00:09, 360.19it/s]
Adding requests:  16%|█▌        | 645/4096 [00:01<00:09, 356.02it/s]
Adding requests:  17%|█▋        | 681/4096 [00:01<00:09, 355.09it/s]
Adding requests:  18%|█▊        | 718/4096 [00:02<00:09, 356.23it/s]
Adding requests:  18%|█▊        | 754/4096 [00:02<00:09, 352.81it/s]
Adding requests:  19%|█▉        | 790/4096 [00:02<00:09, 353.03it/s]
Adding requests:  20%|██        | 827/4096 [00:02<00:09, 355.97it/s]
Adding requests:  21%|██        | 864/4096 [00:02<00:09, 358.62it/s]
Adding requests:  22%|██▏       | 900/4096 [00:02<00:08, 356.72it/s]
Adding requests:  23%|██▎       | 936/4096 [00:02<00:09, 350.84it/s]
Adding requests:  24%|██▍       | 973/4096 [00:02<00:08, 356.16it/s]
Adding requests:  25%|██▍       | 1009/4096 [00:02<00:08, 350.17it/s]
Adding requests:  26%|██▌       | 1045/4096 [00:02<00:08, 351.41it/s]
Adding requests:  26%|██▋       | 1081/4096 [00:03<00:08, 351.47it/s]
Adding requests:  27%|██▋       | 1117/4096 [00:03<00:08, 348.84it/s]
Adding requests:  28%|██▊       | 1154/4096 [00:03<00:08, 351.54it/s]
Adding requests:  29%|██▉       | 1190/4096 [00:03<00:08, 351.54it/s]
Adding requests:  30%|██▉       | 1228/4096 [00:03<00:07, 358.73it/s]
Adding requests:  31%|███       | 1264/4096 [00:03<00:07, 356.14it/s]
Adding requests:  32%|███▏      | 1300/4096 [00:03<00:07, 353.10it/s]
Adding requests:  33%|███▎      | 1336/4096 [00:03<00:07, 354.00it/s]
Adding requests:  34%|███▎      | 1374/4096 [00:03<00:07, 360.09it/s]
Adding requests:  34%|███▍      | 1411/4096 [00:04<00:07, 356.21it/s]
Adding requests:  35%|███▌      | 1447/4096 [00:04<00:07, 342.88it/s]
Adding requests:  36%|███▌      | 1484/4096 [00:04<00:07, 348.84it/s]
Adding requests:  37%|███▋      | 1521/4096 [00:04<00:07, 354.48it/s]
Adding requests:  38%|███▊      | 1557/4096 [00:04<00:07, 350.56it/s]
Adding requests:  39%|███▉      | 1593/4096 [00:04<00:07, 348.60it/s]
Adding requests:  40%|███▉      | 1628/4096 [00:04<00:07, 343.82it/s]
Adding requests:  41%|████      | 1663/4096 [00:04<00:07, 337.04it/s]
Adding requests:  41%|████▏     | 1699/4096 [00:04<00:07, 342.08it/s]
Adding requests:  42%|████▏     | 1736/4096 [00:04<00:06, 348.42it/s]
Adding requests:  43%|████▎     | 1774/4096 [00:05<00:06, 354.35it/s]
Adding requests:  44%|████▍     | 1810/4096 [00:05<00:06, 352.33it/s]
Adding requests:  45%|████▌     | 1847/4096 [00:05<00:06, 354.61it/s]
Adding requests:  46%|████▌     | 1883/4096 [00:05<00:06, 355.44it/s]
Adding requests:  47%|████▋     | 1920/4096 [00:05<00:06, 358.83it/s]
Adding requests:  48%|████▊     | 1958/4096 [00:05<00:05, 364.72it/s]
Adding requests:  49%|████▊     | 1995/4096 [00:05<00:05, 356.51it/s]
Adding requests:  50%|████▉     | 2031/4096 [00:05<00:06, 339.17it/s]
Adding requests:  50%|█████     | 2066/4096 [00:05<00:05, 339.16it/s]
Adding requests:  51%|█████▏    | 2101/4096 [00:05<00:05, 341.70it/s]
Adding requests:  52%|█████▏    | 2137/4096 [00:06<00:05, 345.49it/s]
Adding requests:  53%|█████▎    | 2172/4096 [00:06<00:05, 340.79it/s]
Adding requests:  54%|█████▍    | 2207/4096 [00:06<00:05, 339.45it/s]
Adding requests:  55%|█████▍    | 2243/4096 [00:06<00:05, 345.28it/s]
Adding requests:  56%|█████▌    | 2280/4096 [00:06<00:05, 352.47it/s]
Adding requests:  57%|█████▋    | 2316/4096 [00:06<00:05, 354.58it/s]
Adding requests:  57%|█████▋    | 2354/4096 [00:06<00:04, 360.03it/s]
Adding requests:  58%|█████▊    | 2392/4096 [00:06<00:04, 363.12it/s]
Adding requests:  59%|█████▉    | 2429/4096 [00:06<00:04, 363.96it/s]
Adding requests:  60%|██████    | 2466/4096 [00:07<00:04, 360.10it/s]
Adding requests:  61%|██████    | 2504/4096 [00:07<00:04, 364.15it/s]
Adding requests:  62%|██████▏   | 2543/4096 [00:07<00:04, 369.35it/s]
Adding requests:  63%|██████▎   | 2581/4096 [00:07<00:04, 371.23it/s]
Adding requests:  64%|██████▍   | 2619/4096 [00:07<00:04, 367.21it/s]
Adding requests:  65%|██████▍   | 2656/4096 [00:07<00:04, 359.11it/s]
Adding requests:  66%|██████▌   | 2692/4096 [00:07<00:03, 355.52it/s]
Adding requests:  67%|██████▋   | 2728/4096 [00:07<00:03, 352.31it/s]
Adding requests:  68%|██████▊   | 2767/4096 [00:07<00:03, 361.69it/s]
Adding requests:  68%|██████▊   | 2805/4096 [00:07<00:03, 366.42it/s]
Adding requests:  69%|██████▉   | 2842/4096 [00:08<00:03, 363.89it/s]
Adding requests:  70%|███████   | 2879/4096 [00:08<00:03, 361.94it/s]
Adding requests:  71%|███████   | 2916/4096 [00:08<00:03, 363.24it/s]
Adding requests:  72%|███████▏  | 2954/4096 [00:08<00:03, 366.69it/s]
Adding requests:  73%|███████▎  | 2991/4096 [00:08<00:03, 363.42it/s]
Adding requests:  74%|███████▍  | 3028/4096 [00:08<00:02, 365.34it/s]
Adding requests:  75%|███████▍  | 3067/4096 [00:08<00:02, 369.76it/s]
Adding requests:  76%|███████▌  | 3105/4096 [00:08<00:02, 370.18it/s]
Adding requests:  77%|███████▋  | 3143/4096 [00:08<00:02, 372.36it/s]
Adding requests:  78%|███████▊  | 3181/4096 [00:08<00:02, 364.03it/s]
Adding requests:  79%|███████▊  | 3218/4096 [00:09<00:02, 363.19it/s]
Adding requests:  79%|███████▉  | 3256/4096 [00:09<00:02, 365.01it/s]
Adding requests:  80%|████████  | 3293/4096 [00:09<00:02, 352.09it/s]
Adding requests:  81%|████████▏ | 3329/4096 [00:09<00:02, 350.04it/s]
Adding requests:  82%|████████▏ | 3365/4096 [00:09<00:02, 344.59it/s]
Adding requests:  83%|████████▎ | 3403/4096 [00:09<00:01, 352.57it/s]
Adding requests:  84%|████████▍ | 3439/4096 [00:09<00:01, 354.69it/s]
Adding requests:  85%|████████▍ | 3476/4096 [00:09<00:01, 358.58it/s]
Adding requests:  86%|████████▌ | 3512/4096 [00:09<00:01, 358.03it/s]
Adding requests:  87%|████████▋ | 3551/4096 [00:10<00:01, 364.76it/s]
Adding requests:  88%|████████▊ | 3588/4096 [00:10<00:01, 360.85it/s]
Adding requests:  89%|████████▊ | 3625/4096 [00:10<00:01, 363.34it/s]
Adding requests:  89%|████████▉ | 3662/4096 [00:10<00:01, 361.68it/s]
Adding requests:  90%|█████████ | 3699/4096 [00:10<00:01, 355.19it/s]
Adding requests:  91%|█████████ | 3736/4096 [00:10<00:01, 358.75it/s]
Adding requests:  92%|█████████▏| 3772/4096 [00:10<00:00, 349.84it/s]
Adding requests:  93%|█████████▎| 3808/4096 [00:10<00:00, 336.99it/s]
Adding requests:  94%|█████████▍| 3845/4096 [00:10<00:00, 343.91it/s]
Adding requests:  95%|█████████▍| 3881/4096 [00:10<00:00, 346.30it/s]
Adding requests:  96%|█████████▌| 3916/4096 [00:11<00:00, 340.92it/s]
Adding requests:  96%|█████████▋| 3952/4096 [00:11<00:00, 343.17it/s]
Adding requests:  97%|█████████▋| 3987/4096 [00:11<00:00, 344.15it/s]
Adding requests:  98%|█████████▊| 4022/4096 [00:11<00:00, 344.49it/s]
Adding requests:  99%|█████████▉| 4057/4096 [00:11<00:00, 343.39it/s]
Adding requests: 100%|█████████▉| 4092/4096 [00:11<00:00, 343.42it/s]
Adding requests: 100%|██████████| 4096/4096 [00:11<00:00, 353.40it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  25%|██▌       | 1043/4096 [00:00<00:00, 3606.08it/s, est. speed input: 3692733.15 toks/s, output: 3606.11 toks/s]
Processed prompts:  34%|███▍      | 1404/4096 [00:04<00:10, 267.26it/s, est. speed input: 344831.01 toks/s, output: 336.75 toks/s]   
Processed prompts:  38%|███▊      | 1558/4096 [00:05<00:12, 199.55it/s, est. speed input: 269915.08 toks/s, output: 263.59 toks/s]
Processed prompts:  40%|████      | 1646/4096 [00:06<00:13, 186.37it/s, est. speed input: 254974.61 toks/s, output: 249.00 toks/s]
Processed prompts:  42%|████▏     | 1705/4096 [00:07<00:14, 165.36it/s, est. speed input: 238649.49 toks/s, output: 233.06 toks/s]
Processed prompts:  43%|████▎     | 1746/4096 [00:07<00:14, 159.52it/s, est. speed input: 233368.31 toks/s, output: 227.90 toks/s]
Processed prompts:  43%|████▎     | 1777/4096 [00:08<00:15, 148.85it/s, est. speed input: 227227.42 toks/s, output: 221.90 toks/s]
Processed prompts:  44%|████▍     | 1801/4096 [00:08<00:17, 134.04it/s, est. speed input: 220511.96 toks/s, output: 215.34 toks/s]
Processed prompts:  44%|████▍     | 1820/4096 [00:08<00:19, 117.69it/s, est. speed input: 213860.50 toks/s, output: 208.85 toks/s]
Processed prompts:  45%|████▍     | 1843/4096 [00:09<00:21, 106.01it/s, est. speed input: 208215.96 toks/s, output: 203.34 toks/s]
Processed prompts:  46%|████▌     | 1875/4096 [00:09<00:21, 102.62it/s, est. speed input: 204005.08 toks/s, output: 199.22 toks/s]
Processed prompts:  47%|████▋     | 1907/4096 [00:09<00:21, 99.74it/s, est. speed input: 200052.83 toks/s, output: 195.36 toks/s] 
Processed prompts:  47%|████▋     | 1939/4096 [00:10<00:22, 97.40it/s, est. speed input: 196344.18 toks/s, output: 191.74 toks/s]
Processed prompts:  48%|████▊     | 1971/4096 [00:10<00:22, 95.89it/s, est. speed input: 192942.63 toks/s, output: 188.42 toks/s]
Processed prompts:  49%|████▉     | 2003/4096 [00:10<00:22, 94.29it/s, est. speed input: 189651.58 toks/s, output: 185.21 toks/s]
Processed prompts:  50%|████▉     | 2035/4096 [00:11<00:22, 93.16it/s, est. speed input: 186575.31 toks/s, output: 182.20 toks/s]
Processed prompts:  50%|█████     | 2067/4096 [00:11<00:21, 92.94it/s, est. speed input: 183809.30 toks/s, output: 179.50 toks/s]
Processed prompts:  51%|█████     | 2099/4096 [00:11<00:21, 92.10it/s, est. speed input: 181071.79 toks/s, output: 176.83 toks/s]
Processed prompts:  52%|█████▏    | 2131/4096 [00:12<00:21, 91.75it/s, est. speed input: 178539.00 toks/s, output: 174.35 toks/s]
Processed prompts:  53%|█████▎    | 2163/4096 [00:12<00:21, 91.23it/s, est. speed input: 176096.52 toks/s, output: 171.97 toks/s]
Processed prompts:  54%|█████▎    | 2195/4096 [00:12<00:20, 91.55it/s, est. speed input: 173910.24 toks/s, output: 169.83 toks/s]
Processed prompts:  54%|█████▍    | 2227/4096 [00:13<00:20, 91.48it/s, est. speed input: 171787.23 toks/s, output: 167.76 toks/s]
Processed prompts:  55%|█████▌    | 2259/4096 [00:13<00:20, 91.13it/s, est. speed input: 169725.25 toks/s, output: 165.75 toks/s]
Processed prompts:  56%|█████▌    | 2291/4096 [00:13<00:19, 91.16it/s, est. speed input: 167810.40 toks/s, output: 163.88 toks/s]
Processed prompts:  57%|█████▋    | 2323/4096 [00:14<00:19, 91.17it/s, est. speed input: 165987.86 toks/s, output: 162.10 toks/s]
Processed prompts:  57%|█████▋    | 2355/4096 [00:14<00:19, 91.08it/s, est. speed input: 164238.61 toks/s, output: 160.39 toks/s]
Processed prompts:  58%|█████▊    | 2387/4096 [00:15<00:18, 91.07it/s, est. speed input: 162579.01 toks/s, output: 158.77 toks/s]
Processed prompts:  59%|█████▉    | 2419/4096 [00:15<00:18, 90.75it/s, est. speed input: 160952.28 toks/s, output: 157.18 toks/s]
Processed prompts:  60%|█████▉    | 2451/4096 [00:15<00:18, 90.97it/s, est. speed input: 159457.36 toks/s, output: 155.72 toks/s]
Processed prompts:  61%|██████    | 2483/4096 [00:16<00:17, 90.87it/s, est. speed input: 157995.92 toks/s, output: 154.29 toks/s]
Processed prompts:  61%|██████▏   | 2515/4096 [00:16<00:17, 91.24it/s, est. speed input: 156650.24 toks/s, output: 152.98 toks/s]
Processed prompts:  62%|██████▏   | 2547/4096 [00:16<00:17, 90.87it/s, est. speed input: 155285.06 toks/s, output: 151.65 toks/s]
Processed prompts:  63%|██████▎   | 2579/4096 [00:17<00:16, 91.56it/s, est. speed input: 154087.44 toks/s, output: 150.48 toks/s]
Processed prompts:  64%|██████▎   | 2611/4096 [00:17<00:16, 91.24it/s, est. speed input: 152845.38 toks/s, output: 149.26 toks/s]
Processed prompts:  65%|██████▍   | 2643/4096 [00:17<00:15, 90.89it/s, est. speed input: 151639.83 toks/s, output: 148.09 toks/s]
Processed prompts:  65%|██████▌   | 2675/4096 [00:18<00:15, 91.05it/s, est. speed input: 150523.64 toks/s, output: 147.00 toks/s]
Processed prompts:  66%|██████▌   | 2707/4096 [00:18<00:15, 90.70it/s, est. speed input: 149401.79 toks/s, output: 145.90 toks/s]
Processed prompts:  67%|██████▋   | 2739/4096 [00:18<00:14, 91.35it/s, est. speed input: 148413.15 toks/s, output: 144.93 toks/s]
Processed prompts:  68%|██████▊   | 2771/4096 [00:19<00:14, 91.30it/s, est. speed input: 147408.95 toks/s, output: 143.95 toks/s]
Processed prompts:  68%|██████▊   | 2803/4096 [00:19<00:14, 91.31it/s, est. speed input: 146446.21 toks/s, output: 143.01 toks/s]
Processed prompts:  69%|██████▉   | 2835/4096 [00:19<00:13, 91.28it/s, est. speed input: 145513.42 toks/s, output: 142.10 toks/s]
Processed prompts:  70%|██████▉   | 2867/4096 [00:20<00:13, 91.09it/s, est. speed input: 144597.02 toks/s, output: 141.21 toks/s]
Processed prompts:  71%|███████   | 2899/4096 [00:20<00:13, 89.68it/s, est. speed input: 143596.49 toks/s, output: 140.23 toks/s]
Processed prompts:  72%|███████▏  | 2931/4096 [00:21<00:12, 90.21it/s, est. speed input: 142765.08 toks/s, output: 139.42 toks/s]
Processed prompts:  72%|███████▏  | 2963/4096 [00:21<00:12, 90.47it/s, est. speed input: 141951.98 toks/s, output: 138.62 toks/s]
Processed prompts:  73%|███████▎  | 2995/4096 [00:21<00:12, 90.65it/s, est. speed input: 141164.35 toks/s, output: 137.86 toks/s]
Processed prompts:  74%|███████▍  | 3027/4096 [00:22<00:11, 90.92it/s, est. speed input: 140413.93 toks/s, output: 137.12 toks/s]
Processed prompts:  75%|███████▍  | 3059/4096 [00:22<00:11, 90.87it/s, est. speed input: 139666.89 toks/s, output: 136.39 toks/s]
Processed prompts:  75%|███████▌  | 3091/4096 [00:22<00:11, 90.75it/s, est. speed input: 138937.36 toks/s, output: 135.68 toks/s]
Processed prompts:  76%|███████▌  | 3123/4096 [00:23<00:10, 90.73it/s, est. speed input: 138234.65 toks/s, output: 134.99 toks/s]
Processed prompts:  77%|███████▋  | 3155/4096 [00:23<00:10, 90.76it/s, est. speed input: 137556.17 toks/s, output: 134.33 toks/s]
Processed prompts:  78%|███████▊  | 3187/4096 [00:23<00:10, 90.79it/s, est. speed input: 136898.59 toks/s, output: 133.69 toks/s]
Processed prompts:  79%|███████▊  | 3219/4096 [00:24<00:09, 90.92it/s, est. speed input: 136267.53 toks/s, output: 133.07 toks/s]
Processed prompts:  79%|███████▉  | 3251/4096 [00:24<00:09, 91.11it/s, est. speed input: 135662.44 toks/s, output: 132.48 toks/s]
Processed prompts:  80%|████████  | 3283/4096 [00:24<00:08, 90.91it/s, est. speed input: 135050.63 toks/s, output: 131.89 toks/s]
Processed prompts:  81%|████████  | 3315/4096 [00:25<00:08, 90.99it/s, est. speed input: 134470.72 toks/s, output: 131.32 toks/s]
Processed prompts:  82%|████████▏ | 3347/4096 [00:25<00:08, 90.74it/s, est. speed input: 133886.38 toks/s, output: 130.75 toks/s]
Processed prompts:  82%|████████▏ | 3379/4096 [00:25<00:07, 90.42it/s, est. speed input: 133308.39 toks/s, output: 130.18 toks/s]
Processed prompts:  83%|████████▎ | 3411/4096 [00:26<00:07, 90.41it/s, est. speed input: 132760.01 toks/s, output: 129.65 toks/s]
Processed prompts:  84%|████████▍ | 3443/4096 [00:26<00:07, 90.47it/s, est. speed input: 132230.29 toks/s, output: 129.13 toks/s]
Processed prompts:  85%|████████▍ | 3475/4096 [00:27<00:06, 90.37it/s, est. speed input: 131705.90 toks/s, output: 128.62 toks/s]
Processed prompts:  86%|████████▌ | 3507/4096 [00:27<00:06, 90.52it/s, est. speed input: 131208.37 toks/s, output: 128.13 toks/s]
Processed prompts:  86%|████████▋ | 3539/4096 [00:27<00:06, 91.07it/s, est. speed input: 130750.83 toks/s, output: 127.69 toks/s]
Processed prompts:  87%|████████▋ | 3571/4096 [00:28<00:05, 90.96it/s, est. speed input: 130274.79 toks/s, output: 127.22 toks/s]
Processed prompts:  88%|████████▊ | 3603/4096 [00:28<00:05, 90.97it/s, est. speed input: 129815.90 toks/s, output: 126.77 toks/s]
Processed prompts:  89%|████████▊ | 3635/4096 [00:28<00:05, 91.00it/s, est. speed input: 129369.17 toks/s, output: 126.34 toks/s]
Processed prompts:  90%|████████▉ | 3667/4096 [00:29<00:04, 91.18it/s, est. speed input: 128942.63 toks/s, output: 125.92 toks/s]
Processed prompts:  90%|█████████ | 3699/4096 [00:29<00:04, 91.19it/s, est. speed input: 128519.51 toks/s, output: 125.51 toks/s]
Processed prompts:  91%|█████████ | 3731/4096 [00:29<00:04, 91.22it/s, est. speed input: 128107.70 toks/s, output: 125.11 toks/s]
Processed prompts:  92%|█████████▏| 3763/4096 [00:30<00:03, 91.14it/s, est. speed input: 127699.97 toks/s, output: 124.71 toks/s]
Processed prompts:  93%|█████████▎| 3795/4096 [00:30<00:03, 91.10it/s, est. speed input: 127302.45 toks/s, output: 124.32 toks/s]
Processed prompts:  93%|█████████▎| 3827/4096 [00:30<00:02, 90.96it/s, est. speed input: 126908.01 toks/s, output: 123.93 toks/s]
Processed prompts:  94%|█████████▍| 3859/4096 [00:31<00:02, 90.73it/s, est. speed input: 126515.44 toks/s, output: 123.55 toks/s]
Processed prompts:  95%|█████████▍| 3891/4096 [00:31<00:02, 90.62it/s, est. speed input: 126134.43 toks/s, output: 123.18 toks/s]
Processed prompts:  96%|█████████▌| 3923/4096 [00:31<00:01, 91.04it/s, est. speed input: 125787.24 toks/s, output: 122.84 toks/s]
Processed prompts:  97%|█████████▋| 3955/4096 [00:32<00:01, 90.60it/s, est. speed input: 125410.63 toks/s, output: 122.47 toks/s]
Processed prompts:  97%|█████████▋| 3987/4096 [00:32<00:01, 90.98it/s, est. speed input: 125076.44 toks/s, output: 122.14 toks/s]
Processed prompts:  98%|█████████▊| 4019/4096 [00:32<00:00, 90.79it/s, est. speed input: 124726.94 toks/s, output: 121.80 toks/s]
Processed prompts:  99%|█████████▉| 4051/4096 [00:33<00:00, 90.74it/s, est. speed input: 124389.02 toks/s, output: 121.47 toks/s]
Processed prompts: 100%|█████████▉| 4083/4096 [00:33<00:00, 108.83it/s, est. speed input: 124782.97 toks/s, output: 121.86 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:33<00:00, 108.83it/s, est. speed input: 125179.49 toks/s, output: 122.25 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:33<00:00, 122.25it/s, est. speed input: 125179.49 toks/s, output: 122.25 toks/s]
[rank0]:[W126 17:19:44.610623331 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=65536 ==========
Time: 2026-01-26 17:19:46
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=8192, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 8192 --max-num-seqs 64 --max-model-len 1025 --max-num-batched-tokens 65536 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/Qwen2.5-14B-INT8_M65536.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 17:20:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=831739) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=831739) WARNING 01-26 17:21:01 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=831739) WARNING 01-26 17:21:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 92.03 requests/s, 94327.10 total tokens/s, 92.03 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192

STDERR:
[2026-01-26 17:20:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:20:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:20:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:20:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:20:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:20:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:20:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:20:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:20:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:20:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:20:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:20:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:20:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:20:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 17:20:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 17:20:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:20:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 17:20:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:20:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:20:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:20:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:20:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 17:20:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 17:20:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 17:20:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 17:20:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 17:20:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 17:20:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=831739) [2026-01-26 17:20:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=831739) [2026-01-26 17:20:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=831739) [2026-01-26 17:20:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=831739) [2026-01-26 17:20:48] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=831739) [2026-01-26 17:20:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=831739) [2026-01-26 17:20:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=831739) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=831739) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.30it/s]
(EngineCore_DP0 pid=831739) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:00,  2.01it/s]
(EngineCore_DP0 pid=831739) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.58it/s]
(EngineCore_DP0 pid=831739) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.39it/s]
(EngineCore_DP0 pid=831739) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.47it/s]
(EngineCore_DP0 pid=831739) 
(EngineCore_DP0 pid=831739) [2026-01-26 17:20:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=831739) [2026-01-26 17:20:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22937600 bytes
(EngineCore_DP0 pid=831739) [2026-01-26 17:20:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=831739) [2026-01-26 17:20:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16384000 bytes
(EngineCore_DP0 pid=831739) [2026-01-26 17:20:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=831739) [2026-01-26 17:20:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 88473600 bytes
(EngineCore_DP0 pid=831739) [2026-01-26 17:20:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=831739) [2026-01-26 17:20:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44236800 bytes
(EngineCore_DP0 pid=831739) [rank0]:W0126 17:21:09.744000 831739 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=831739) [rank0]:W0126 17:21:09.813000 831739 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=831739) [rank0]:W0126 17:21:10.746000 831739 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=831739) [rank0]:W0126 17:21:10.853000 831739 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=831739) 2026-01-26 17:21:21,080 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=831739) 2026-01-26 17:21:21,268 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=831739) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:05,  3.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:02,  6.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  7.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:01,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  9.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:01<00:00,  9.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:01<00:00, 10.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  9.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.74it/s]
(EngineCore_DP0 pid=831739) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.89it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  9.28it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  9.14it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  9.07it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  9.97it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 10.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00, 10.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00, 10.10it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 27/8192 [00:00<00:30, 269.52it/s]
Adding requests:   1%|          | 62/8192 [00:00<00:25, 313.22it/s]
Adding requests:   1%|          | 94/8192 [00:00<00:25, 312.01it/s]
Adding requests:   2%|▏         | 128/8192 [00:00<00:25, 319.08it/s]
Adding requests:   2%|▏         | 163/8192 [00:00<00:24, 328.81it/s]
Adding requests:   2%|▏         | 199/8192 [00:00<00:23, 338.10it/s]
Adding requests:   3%|▎         | 234/8192 [00:00<00:23, 341.77it/s]
Adding requests:   3%|▎         | 269/8192 [00:00<00:23, 341.32it/s]
Adding requests:   4%|▎         | 305/8192 [00:00<00:22, 344.36it/s]
Adding requests:   4%|▍         | 342/8192 [00:01<00:22, 350.76it/s]
Adding requests:   5%|▍         | 378/8192 [00:01<00:22, 352.98it/s]
Adding requests:   5%|▌         | 416/8192 [00:01<00:21, 359.81it/s]
Adding requests:   6%|▌         | 452/8192 [00:01<00:21, 355.75it/s]
Adding requests:   6%|▌         | 492/8192 [00:01<00:20, 367.35it/s]
Adding requests:   6%|▋         | 530/8192 [00:01<00:20, 370.09it/s]
Adding requests:   7%|▋         | 568/8192 [00:01<00:20, 369.30it/s]
Adding requests:   7%|▋         | 605/8192 [00:01<00:21, 356.16it/s]
Adding requests:   8%|▊         | 641/8192 [00:01<00:21, 355.49it/s]
Adding requests:   8%|▊         | 677/8192 [00:01<00:21, 350.87it/s]
Adding requests:   9%|▊         | 714/8192 [00:02<00:21, 355.95it/s]
Adding requests:   9%|▉         | 750/8192 [00:02<00:21, 349.34it/s]
Adding requests:  10%|▉         | 786/8192 [00:02<00:21, 352.16it/s]
Adding requests:  10%|█         | 822/8192 [00:02<00:20, 352.00it/s]
Adding requests:  10%|█         | 860/8192 [00:02<00:20, 358.48it/s]
Adding requests:  11%|█         | 897/8192 [00:02<00:20, 360.45it/s]
Adding requests:  11%|█▏        | 934/8192 [00:02<00:20, 353.15it/s]
Adding requests:  12%|█▏        | 971/8192 [00:02<00:20, 354.76it/s]
Adding requests:  12%|█▏        | 1007/8192 [00:02<00:20, 349.32it/s]
Adding requests:  13%|█▎        | 1043/8192 [00:02<00:20, 351.53it/s]
Adding requests:  13%|█▎        | 1079/8192 [00:03<00:20, 349.02it/s]
Adding requests:  14%|█▎        | 1115/8192 [00:03<00:20, 349.64it/s]
Adding requests:  14%|█▍        | 1152/8192 [00:03<00:19, 353.37it/s]
Adding requests:  15%|█▍        | 1188/8192 [00:03<00:19, 351.39it/s]
Adding requests:  15%|█▍        | 1226/8192 [00:03<00:19, 359.00it/s]
Adding requests:  15%|█▌        | 1262/8192 [00:03<00:19, 354.78it/s]
Adding requests:  16%|█▌        | 1298/8192 [00:03<00:19, 351.75it/s]
Adding requests:  16%|█▋        | 1334/8192 [00:03<00:19, 352.59it/s]
Adding requests:  17%|█▋        | 1372/8192 [00:03<00:19, 357.82it/s]
Adding requests:  17%|█▋        | 1408/8192 [00:04<00:19, 355.31it/s]
Adding requests:  18%|█▊        | 1444/8192 [00:04<00:19, 355.10it/s]
Adding requests:  18%|█▊        | 1481/8192 [00:04<00:18, 357.77it/s]
Adding requests:  19%|█▊        | 1518/8192 [00:04<00:18, 359.75it/s]
Adding requests:  19%|█▉        | 1554/8192 [00:04<00:18, 356.30it/s]
Adding requests:  19%|█▉        | 1590/8192 [00:04<00:18, 351.82it/s]
Adding requests:  20%|█▉        | 1626/8192 [00:04<00:19, 345.12it/s]
Adding requests:  20%|██        | 1661/8192 [00:04<00:19, 330.76it/s]
Adding requests:  21%|██        | 1696/8192 [00:04<00:19, 335.37it/s]
Adding requests:  21%|██        | 1733/8192 [00:04<00:18, 343.27it/s]
Adding requests:  22%|██▏       | 1771/8192 [00:05<00:18, 351.82it/s]
Adding requests:  22%|██▏       | 1807/8192 [00:05<00:18, 348.47it/s]
Adding requests:  23%|██▎       | 1844/8192 [00:05<00:17, 353.41it/s]
Adding requests:  23%|██▎       | 1880/8192 [00:05<00:17, 352.92it/s]
Adding requests:  23%|██▎       | 1917/8192 [00:05<00:17, 355.03it/s]
Adding requests:  24%|██▍       | 1955/8192 [00:05<00:17, 360.86it/s]
Adding requests:  24%|██▍       | 1992/8192 [00:05<00:17, 356.35it/s]
Adding requests:  25%|██▍       | 2028/8192 [00:05<00:17, 346.56it/s]
Adding requests:  25%|██▌       | 2063/8192 [00:05<00:17, 347.40it/s]
Adding requests:  26%|██▌       | 2098/8192 [00:05<00:17, 340.87it/s]
Adding requests:  26%|██▌       | 2133/8192 [00:06<00:17, 342.88it/s]
Adding requests:  26%|██▋       | 2168/8192 [00:06<00:17, 340.52it/s]
Adding requests:  27%|██▋       | 2203/8192 [00:06<00:17, 338.81it/s]
Adding requests:  27%|██▋       | 2240/8192 [00:06<00:17, 344.64it/s]
Adding requests:  28%|██▊       | 2277/8192 [00:06<00:16, 350.63it/s]
Adding requests:  28%|██▊       | 2313/8192 [00:06<00:16, 353.10it/s]
Adding requests:  29%|██▊       | 2351/8192 [00:06<00:16, 360.35it/s]
Adding requests:  29%|██▉       | 2388/8192 [00:06<00:15, 363.03it/s]
Adding requests:  30%|██▉       | 2425/8192 [00:06<00:15, 362.39it/s]
Adding requests:  30%|███       | 2462/8192 [00:07<00:16, 358.06it/s]
Adding requests:  31%|███       | 2501/8192 [00:07<00:15, 364.49it/s]
Adding requests:  31%|███       | 2538/8192 [00:07<00:15, 363.81it/s]
Adding requests:  31%|███▏      | 2579/8192 [00:07<00:14, 374.40it/s]
Adding requests:  32%|███▏      | 2617/8192 [00:07<00:15, 369.59it/s]
Adding requests:  32%|███▏      | 2654/8192 [00:07<00:15, 361.67it/s]
Adding requests:  33%|███▎      | 2691/8192 [00:07<00:15, 358.58it/s]
Adding requests:  33%|███▎      | 2727/8192 [00:07<00:15, 354.11it/s]
Adding requests:  34%|███▍      | 2766/8192 [00:07<00:14, 362.79it/s]
Adding requests:  34%|███▍      | 2805/8192 [00:07<00:14, 367.41it/s]
Adding requests:  35%|███▍      | 2842/8192 [00:08<00:14, 364.87it/s]
Adding requests:  35%|███▌      | 2879/8192 [00:08<00:14, 362.45it/s]
Adding requests:  36%|███▌      | 2916/8192 [00:08<00:14, 352.73it/s]
Adding requests:  36%|███▌      | 2954/8192 [00:08<00:14, 358.38it/s]
Adding requests:  36%|███▋      | 2990/8192 [00:08<00:14, 358.37it/s]
Adding requests:  37%|███▋      | 3026/8192 [00:08<00:14, 349.43it/s]
Adding requests:  37%|███▋      | 3064/8192 [00:08<00:14, 355.11it/s]
Adding requests:  38%|███▊      | 3101/8192 [00:08<00:14, 359.07it/s]
Adding requests:  38%|███▊      | 3138/8192 [00:08<00:13, 361.59it/s]
Adding requests:  39%|███▉      | 3175/8192 [00:08<00:14, 355.50it/s]
Adding requests:  39%|███▉      | 3211/8192 [00:09<00:14, 355.06it/s]
Adding requests:  40%|███▉      | 3249/8192 [00:09<00:13, 361.03it/s]
Adding requests:  40%|████      | 3286/8192 [00:09<00:13, 351.90it/s]
Adding requests:  41%|████      | 3322/8192 [00:09<00:14, 347.66it/s]
Adding requests:  41%|████      | 3358/8192 [00:09<00:13, 350.09it/s]
Adding requests:  41%|████▏     | 3396/8192 [00:09<00:13, 356.63it/s]
Adding requests:  42%|████▏     | 3432/8192 [00:09<00:13, 357.19it/s]
Adding requests:  42%|████▏     | 3469/8192 [00:09<00:13, 360.07it/s]
Adding requests:  43%|████▎     | 3506/8192 [00:09<00:13, 349.04it/s]
Adding requests:  43%|████▎     | 3542/8192 [00:10<00:13, 352.18it/s]
Adding requests:  44%|████▎     | 3578/8192 [00:10<00:13, 352.25it/s]
Adding requests:  44%|████▍     | 3615/8192 [00:10<00:12, 356.09it/s]
Adding requests:  45%|████▍     | 3651/8192 [00:10<00:12, 357.15it/s]
Adding requests:  45%|████▌     | 3687/8192 [00:10<00:12, 348.23it/s]
Adding requests:  45%|████▌     | 3724/8192 [00:10<00:12, 352.11it/s]
Adding requests:  46%|████▌     | 3760/8192 [00:10<00:12, 346.52it/s]
Adding requests:  46%|████▋     | 3795/8192 [00:10<00:13, 336.40it/s]
Adding requests:  47%|████▋     | 3829/8192 [00:10<00:13, 334.58it/s]
Adding requests:  47%|████▋     | 3865/8192 [00:10<00:12, 339.75it/s]
Adding requests:  48%|████▊     | 3900/8192 [00:11<00:12, 340.32it/s]
Adding requests:  48%|████▊     | 3935/8192 [00:11<00:12, 338.79it/s]
Adding requests:  48%|████▊     | 3970/8192 [00:11<00:12, 341.36it/s]
Adding requests:  49%|████▉     | 4005/8192 [00:11<00:12, 343.74it/s]
Adding requests:  49%|████▉     | 4040/8192 [00:11<00:12, 344.06it/s]
Adding requests:  50%|████▉     | 4075/8192 [00:11<00:11, 344.09it/s]
Adding requests:  50%|█████     | 4111/8192 [00:11<00:11, 346.97it/s]
Adding requests:  51%|█████     | 4146/8192 [00:11<00:11, 344.56it/s]
Adding requests:  51%|█████     | 4183/8192 [00:11<00:11, 350.52it/s]
Adding requests:  52%|█████▏    | 4219/8192 [00:12<00:11, 340.50it/s]
Adding requests:  52%|█████▏    | 4254/8192 [00:12<00:11, 339.42it/s]
Adding requests:  52%|█████▏    | 4289/8192 [00:12<00:11, 341.75it/s]
Adding requests:  53%|█████▎    | 4324/8192 [00:12<00:11, 343.10it/s]
Adding requests:  53%|█████▎    | 4361/8192 [00:12<00:10, 348.36it/s]
Adding requests:  54%|█████▎    | 4396/8192 [00:12<00:10, 348.28it/s]
Adding requests:  54%|█████▍    | 4432/8192 [00:12<00:10, 351.24it/s]
Adding requests:  55%|█████▍    | 4468/8192 [00:12<00:10, 351.87it/s]
Adding requests:  55%|█████▍    | 4505/8192 [00:12<00:10, 354.77it/s]
Adding requests:  55%|█████▌    | 4542/8192 [00:12<00:10, 356.98it/s]
Adding requests:  56%|█████▌    | 4578/8192 [00:13<00:10, 355.39it/s]
Adding requests:  56%|█████▋    | 4614/8192 [00:13<00:10, 352.84it/s]
Adding requests:  57%|█████▋    | 4650/8192 [00:13<00:10, 348.43it/s]
Adding requests:  57%|█████▋    | 4685/8192 [00:13<00:10, 342.70it/s]
Adding requests:  58%|█████▊    | 4721/8192 [00:13<00:10, 345.94it/s]
Adding requests:  58%|█████▊    | 4761/8192 [00:13<00:09, 358.64it/s]
Adding requests:  59%|█████▊    | 4797/8192 [00:13<00:09, 348.79it/s]
Adding requests:  59%|█████▉    | 4832/8192 [00:13<00:09, 347.79it/s]
Adding requests:  59%|█████▉    | 4868/8192 [00:13<00:09, 349.83it/s]
Adding requests:  60%|█████▉    | 4904/8192 [00:13<00:09, 348.38it/s]
Adding requests:  60%|██████    | 4940/8192 [00:14<00:09, 351.57it/s]
Adding requests:  61%|██████    | 4976/8192 [00:14<00:09, 352.28it/s]
Adding requests:  61%|██████    | 5012/8192 [00:14<00:09, 341.96it/s]
Adding requests:  62%|██████▏   | 5050/8192 [00:14<00:08, 350.45it/s]
Adding requests:  62%|██████▏   | 5086/8192 [00:14<00:08, 347.59it/s]
Adding requests:  63%|██████▎   | 5122/8192 [00:14<00:08, 349.27it/s]
Adding requests:  63%|██████▎   | 5159/8192 [00:14<00:08, 352.77it/s]
Adding requests:  63%|██████▎   | 5195/8192 [00:14<00:08, 344.71it/s]
Adding requests:  64%|██████▍   | 5231/8192 [00:14<00:08, 346.38it/s]
Adding requests:  64%|██████▍   | 5266/8192 [00:15<00:08, 344.84it/s]
Adding requests:  65%|██████▍   | 5302/8192 [00:15<00:08, 349.04it/s]
Adding requests:  65%|██████▌   | 5337/8192 [00:15<00:08, 349.17it/s]
Adding requests:  66%|██████▌   | 5373/8192 [00:15<00:08, 352.25it/s]
Adding requests:  66%|██████▌   | 5409/8192 [00:15<00:08, 345.81it/s]
Adding requests:  66%|██████▋   | 5445/8192 [00:15<00:07, 349.03it/s]
Adding requests:  67%|██████▋   | 5482/8192 [00:15<00:07, 353.14it/s]
Adding requests:  67%|██████▋   | 5520/8192 [00:15<00:07, 358.38it/s]
Adding requests:  68%|██████▊   | 5556/8192 [00:15<00:07, 339.74it/s]
Adding requests:  68%|██████▊   | 5591/8192 [00:15<00:07, 338.40it/s]
Adding requests:  69%|██████▊   | 5629/8192 [00:16<00:07, 348.04it/s]
Adding requests:  69%|██████▉   | 5665/8192 [00:16<00:07, 348.10it/s]
Adding requests:  70%|██████▉   | 5701/8192 [00:16<00:07, 350.53it/s]
Adding requests:  70%|███████   | 5737/8192 [00:16<00:07, 347.85it/s]
Adding requests:  70%|███████   | 5774/8192 [00:16<00:06, 352.15it/s]
Adding requests:  71%|███████   | 5810/8192 [00:16<00:06, 353.80it/s]
Adding requests:  71%|███████▏  | 5846/8192 [00:16<00:06, 354.25it/s]
Adding requests:  72%|███████▏  | 5882/8192 [00:16<00:06, 354.23it/s]
Adding requests:  72%|███████▏  | 5919/8192 [00:16<00:06, 358.36it/s]
Adding requests:  73%|███████▎  | 5957/8192 [00:16<00:06, 361.56it/s]
Adding requests:  73%|███████▎  | 5994/8192 [00:17<00:06, 362.37it/s]
Adding requests:  74%|███████▎  | 6031/8192 [00:17<00:06, 360.06it/s]
Adding requests:  74%|███████▍  | 6068/8192 [00:17<00:05, 356.42it/s]
Adding requests:  75%|███████▍  | 6104/8192 [00:17<00:05, 349.46it/s]
Adding requests:  75%|███████▍  | 6141/8192 [00:17<00:05, 353.03it/s]
Adding requests:  75%|███████▌  | 6177/8192 [00:17<00:05, 348.98it/s]
Adding requests:  76%|███████▌  | 6212/8192 [00:17<00:05, 344.80it/s]
Adding requests:  76%|███████▋  | 6247/8192 [00:17<00:05, 335.44it/s]
Adding requests:  77%|███████▋  | 6282/8192 [00:17<00:05, 337.05it/s]
Adding requests:  77%|███████▋  | 6319/8192 [00:18<00:05, 344.75it/s]
Adding requests:  78%|███████▊  | 6357/8192 [00:18<00:05, 353.51it/s]
Adding requests:  78%|███████▊  | 6393/8192 [00:18<00:05, 352.25it/s]
Adding requests:  78%|███████▊  | 6429/8192 [00:18<00:05, 344.06it/s]
Adding requests:  79%|███████▉  | 6464/8192 [00:18<00:05, 341.67it/s]
Adding requests:  79%|███████▉  | 6499/8192 [00:18<00:04, 340.91it/s]
Adding requests:  80%|███████▉  | 6536/8192 [00:18<00:04, 346.52it/s]
Adding requests:  80%|████████  | 6572/8192 [00:18<00:04, 347.31it/s]
Adding requests:  81%|████████  | 6607/8192 [00:18<00:04, 346.42it/s]
Adding requests:  81%|████████  | 6644/8192 [00:18<00:04, 353.21it/s]
Adding requests:  82%|████████▏ | 6680/8192 [00:19<00:04, 349.76it/s]
Adding requests:  82%|████████▏ | 6716/8192 [00:19<00:04, 350.61it/s]
Adding requests:  82%|████████▏ | 6753/8192 [00:19<00:04, 356.09it/s]
Adding requests:  83%|████████▎ | 6789/8192 [00:19<00:03, 355.24it/s]
Adding requests:  83%|████████▎ | 6825/8192 [00:19<00:03, 347.29it/s]
Adding requests:  84%|████████▍ | 6861/8192 [00:19<00:03, 336.06it/s]
Adding requests:  84%|████████▍ | 6897/8192 [00:19<00:03, 340.70it/s]
Adding requests:  85%|████████▍ | 6934/8192 [00:19<00:03, 348.10it/s]
Adding requests:  85%|████████▌ | 6969/8192 [00:19<00:03, 343.33it/s]
Adding requests:  86%|████████▌ | 7006/8192 [00:19<00:03, 350.48it/s]
Adding requests:  86%|████████▌ | 7042/8192 [00:20<00:03, 351.70it/s]
Adding requests:  86%|████████▋ | 7078/8192 [00:20<00:03, 350.74it/s]
Adding requests:  87%|████████▋ | 7114/8192 [00:20<00:03, 347.40it/s]
Adding requests:  87%|████████▋ | 7150/8192 [00:20<00:02, 349.41it/s]
Adding requests:  88%|████████▊ | 7187/8192 [00:20<00:02, 355.22it/s]
Adding requests:  88%|████████▊ | 7224/8192 [00:20<00:02, 358.82it/s]
Adding requests:  89%|████████▊ | 7261/8192 [00:20<00:02, 359.67it/s]
Adding requests:  89%|████████▉ | 7298/8192 [00:20<00:02, 359.90it/s]
Adding requests:  90%|████████▉ | 7335/8192 [00:20<00:02, 355.72it/s]
Adding requests:  90%|████████▉ | 7372/8192 [00:21<00:02, 358.01it/s]
Adding requests:  90%|█████████ | 7408/8192 [00:21<00:02, 355.35it/s]
Adding requests:  91%|█████████ | 7444/8192 [00:21<00:02, 353.73it/s]
Adding requests:  91%|█████████▏| 7480/8192 [00:21<00:02, 347.79it/s]
Adding requests:  92%|█████████▏| 7518/8192 [00:21<00:01, 353.34it/s]
Adding requests:  92%|█████████▏| 7554/8192 [00:21<00:01, 353.27it/s]
Adding requests:  93%|█████████▎| 7590/8192 [00:21<00:01, 351.14it/s]
Adding requests:  93%|█████████▎| 7626/8192 [00:21<00:01, 351.63it/s]
Adding requests:  94%|█████████▎| 7664/8192 [00:21<00:01, 357.42it/s]
Adding requests:  94%|█████████▍| 7702/8192 [00:21<00:01, 362.81it/s]
Adding requests:  94%|█████████▍| 7739/8192 [00:22<00:01, 359.63it/s]
Adding requests:  95%|█████████▍| 7775/8192 [00:22<00:01, 356.19it/s]
Adding requests:  95%|█████████▌| 7811/8192 [00:22<00:01, 356.45it/s]
Adding requests:  96%|█████████▌| 7847/8192 [00:22<00:00, 345.08it/s]
Adding requests:  96%|█████████▌| 7882/8192 [00:22<00:00, 343.34it/s]
Adding requests:  97%|█████████▋| 7917/8192 [00:22<00:00, 341.03it/s]
Adding requests:  97%|█████████▋| 7956/8192 [00:22<00:00, 352.21it/s]
Adding requests:  98%|█████████▊| 7993/8192 [00:22<00:00, 356.50it/s]
Adding requests:  98%|█████████▊| 8029/8192 [00:22<00:00, 351.07it/s]
Adding requests:  98%|█████████▊| 8067/8192 [00:22<00:00, 358.12it/s]
Adding requests:  99%|█████████▉| 8103/8192 [00:23<00:00, 353.49it/s]
Adding requests:  99%|█████████▉| 8139/8192 [00:23<00:00, 351.09it/s]
Adding requests: 100%|█████████▉| 8175/8192 [00:23<00:00, 329.16it/s]
Adding requests: 100%|██████████| 8192/8192 [00:23<00:00, 350.57it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  26%|██▌       | 2131/8192 [00:00<00:01, 4256.44it/s, est. speed input: 4358670.15 toks/s, output: 4256.46 toks/s]
Processed prompts:  31%|███       | 2557/8192 [00:04<00:13, 425.24it/s, est. speed input: 561899.73 toks/s, output: 548.73 toks/s]   
Processed prompts:  33%|███▎      | 2739/8192 [00:06<00:18, 291.55it/s, est. speed input: 416647.94 toks/s, output: 406.88 toks/s]
Processed prompts:  35%|███▍      | 2842/8192 [00:08<00:23, 231.63it/s, est. speed input: 358264.05 toks/s, output: 349.87 toks/s]
Processed prompts:  35%|███▌      | 2907/8192 [00:08<00:25, 209.70it/s, est. speed input: 338161.62 toks/s, output: 330.24 toks/s]
Processed prompts:  36%|███▌      | 2963/8192 [00:09<00:28, 184.99it/s, est. speed input: 319390.84 toks/s, output: 311.90 toks/s]
Processed prompts:  37%|███▋      | 3027/8192 [00:10<00:31, 165.25it/s, est. speed input: 304125.69 toks/s, output: 297.00 toks/s]
Processed prompts:  38%|███▊      | 3091/8192 [00:10<00:34, 148.10it/s, est. speed input: 290724.93 toks/s, output: 283.91 toks/s]
Processed prompts:  39%|███▊      | 3155/8192 [00:11<00:37, 133.83it/s, est. speed input: 278825.67 toks/s, output: 272.29 toks/s]
Processed prompts:  39%|███▉      | 3219/8192 [00:12<00:40, 122.67it/s, est. speed input: 268306.28 toks/s, output: 262.02 toks/s]
Processed prompts:  40%|████      | 3283/8192 [00:12<00:43, 114.08it/s, est. speed input: 258881.38 toks/s, output: 252.81 toks/s]
Processed prompts:  41%|████      | 3347/8192 [00:13<00:44, 107.69it/s, est. speed input: 250427.57 toks/s, output: 244.56 toks/s]
Processed prompts:  42%|████▏     | 3411/8192 [00:14<00:46, 103.15it/s, est. speed input: 242847.32 toks/s, output: 237.16 toks/s]
Processed prompts:  42%|████▏     | 3475/8192 [00:15<00:47, 99.90it/s, est. speed input: 235983.37 toks/s, output: 230.45 toks/s] 
Processed prompts:  43%|████▎     | 3539/8192 [00:15<00:47, 98.08it/s, est. speed input: 229904.93 toks/s, output: 224.52 toks/s]
Processed prompts:  44%|████▍     | 3603/8192 [00:16<00:47, 96.14it/s, est. speed input: 224121.14 toks/s, output: 218.87 toks/s]
Processed prompts:  45%|████▍     | 3667/8192 [00:17<00:47, 95.40it/s, est. speed input: 219006.38 toks/s, output: 213.87 toks/s]
Processed prompts:  46%|████▌     | 3731/8192 [00:17<00:47, 94.41it/s, est. speed input: 214147.46 toks/s, output: 209.13 toks/s]
Processed prompts:  46%|████▋     | 3795/8192 [00:18<00:47, 93.51it/s, est. speed input: 209597.94 toks/s, output: 204.69 toks/s]
Processed prompts:  47%|████▋     | 3859/8192 [00:19<00:46, 93.06it/s, est. speed input: 205423.64 toks/s, output: 200.61 toks/s]
Processed prompts:  48%|████▊     | 3923/8192 [00:19<00:45, 93.16it/s, est. speed input: 201649.17 toks/s, output: 196.92 toks/s]
Processed prompts:  49%|████▊     | 3987/8192 [00:20<00:45, 93.07it/s, est. speed input: 198086.01 toks/s, output: 193.44 toks/s]
Processed prompts:  49%|████▉     | 4051/8192 [00:21<00:44, 93.13it/s, est. speed input: 194780.77 toks/s, output: 190.22 toks/s]
Processed prompts:  50%|█████     | 4115/8192 [00:21<00:43, 92.69it/s, est. speed input: 191577.36 toks/s, output: 187.09 toks/s]
Processed prompts:  51%|█████     | 4179/8192 [00:22<00:43, 92.47it/s, est. speed input: 188589.48 toks/s, output: 184.17 toks/s]
Processed prompts:  52%|█████▏    | 4243/8192 [00:23<00:42, 92.28it/s, est. speed input: 185773.61 toks/s, output: 181.42 toks/s]
Processed prompts:  53%|█████▎    | 4307/8192 [00:24<00:42, 92.10it/s, est. speed input: 183110.17 toks/s, output: 178.82 toks/s]
Processed prompts:  53%|█████▎    | 4371/8192 [00:24<00:41, 91.96it/s, est. speed input: 180593.88 toks/s, output: 176.36 toks/s]
Processed prompts:  54%|█████▍    | 4435/8192 [00:25<00:40, 92.26it/s, est. speed input: 178286.97 toks/s, output: 174.11 toks/s]
Processed prompts:  55%|█████▍    | 4499/8192 [00:26<00:39, 92.64it/s, est. speed input: 176128.92 toks/s, output: 172.00 toks/s]
Processed prompts:  56%|█████▌    | 4563/8192 [00:26<00:39, 92.70it/s, est. speed input: 174046.72 toks/s, output: 169.97 toks/s]
Processed prompts:  56%|█████▋    | 4627/8192 [00:27<00:38, 92.43it/s, est. speed input: 172020.90 toks/s, output: 167.99 toks/s]
Processed prompts:  57%|█████▋    | 4691/8192 [00:28<00:37, 92.22it/s, est. speed input: 170091.90 toks/s, output: 166.11 toks/s]
Processed prompts:  58%|█████▊    | 4755/8192 [00:28<00:37, 92.12it/s, est. speed input: 168262.11 toks/s, output: 164.32 toks/s]
Processed prompts:  59%|█████▉    | 4819/8192 [00:29<00:36, 92.03it/s, est. speed input: 166515.82 toks/s, output: 162.61 toks/s]
Processed prompts:  60%|█████▉    | 4883/8192 [00:30<00:35, 92.24it/s, est. speed input: 164887.60 toks/s, output: 161.02 toks/s]
Processed prompts:  60%|██████    | 4947/8192 [00:31<00:35, 91.94it/s, est. speed input: 163272.36 toks/s, output: 159.45 toks/s]
Processed prompts:  61%|██████    | 5011/8192 [00:31<00:34, 91.93it/s, est. speed input: 161754.79 toks/s, output: 157.96 toks/s]
Processed prompts:  62%|██████▏   | 5075/8192 [00:32<00:33, 91.96it/s, est. speed input: 160306.50 toks/s, output: 156.55 toks/s]
Processed prompts:  63%|██████▎   | 5139/8192 [00:33<00:33, 91.77it/s, est. speed input: 158893.42 toks/s, output: 155.17 toks/s]
Processed prompts:  64%|██████▎   | 5203/8192 [00:33<00:32, 92.04it/s, est. speed input: 157586.19 toks/s, output: 153.89 toks/s]
Processed prompts:  64%|██████▍   | 5267/8192 [00:34<00:31, 92.34it/s, est. speed input: 156343.78 toks/s, output: 152.68 toks/s]
Processed prompts:  65%|██████▌   | 5331/8192 [00:35<00:31, 92.20it/s, est. speed input: 155111.82 toks/s, output: 151.48 toks/s]
Processed prompts:  66%|██████▌   | 5395/8192 [00:35<00:30, 91.95it/s, est. speed input: 153910.32 toks/s, output: 150.30 toks/s]
Processed prompts:  67%|██████▋   | 5459/8192 [00:36<00:29, 92.32it/s, est. speed input: 152812.91 toks/s, output: 149.23 toks/s]
Processed prompts:  67%|██████▋   | 5523/8192 [00:37<00:28, 92.54it/s, est. speed input: 151750.76 toks/s, output: 148.19 toks/s]
Processed prompts:  68%|██████▊   | 5587/8192 [00:37<00:28, 92.25it/s, est. speed input: 150683.90 toks/s, output: 147.15 toks/s]
Processed prompts:  69%|██████▉   | 5651/8192 [00:38<00:27, 91.92it/s, est. speed input: 149643.44 toks/s, output: 146.14 toks/s]
Processed prompts:  70%|██████▉   | 5715/8192 [00:39<00:26, 91.92it/s, est. speed input: 148660.74 toks/s, output: 145.18 toks/s]
Processed prompts:  71%|███████   | 5779/8192 [00:40<00:26, 91.89it/s, est. speed input: 147710.19 toks/s, output: 144.25 toks/s]
Processed prompts:  71%|███████▏  | 5843/8192 [00:40<00:25, 91.83it/s, est. speed input: 146788.76 toks/s, output: 143.35 toks/s]
Processed prompts:  72%|███████▏  | 5907/8192 [00:41<00:24, 92.06it/s, est. speed input: 145922.03 toks/s, output: 142.50 toks/s]
Processed prompts:  73%|███████▎  | 5971/8192 [00:42<00:24, 92.30it/s, est. speed input: 145091.12 toks/s, output: 141.69 toks/s]
Processed prompts:  74%|███████▎  | 6035/8192 [00:42<00:23, 92.04it/s, est. speed input: 144250.04 toks/s, output: 140.87 toks/s]
Processed prompts:  74%|███████▍  | 6099/8192 [00:43<00:22, 91.82it/s, est. speed input: 143433.12 toks/s, output: 140.07 toks/s]
Processed prompts:  75%|███████▌  | 6163/8192 [00:44<00:22, 91.61it/s, est. speed input: 142637.26 toks/s, output: 139.29 toks/s]
Processed prompts:  76%|███████▌  | 6227/8192 [00:44<00:21, 92.01it/s, est. speed input: 141910.73 toks/s, output: 138.58 toks/s]
Processed prompts:  77%|███████▋  | 6291/8192 [00:45<00:20, 91.84it/s, est. speed input: 141169.90 toks/s, output: 137.86 toks/s]
Processed prompts:  78%|███████▊  | 6355/8192 [00:46<00:20, 91.45it/s, est. speed input: 140431.56 toks/s, output: 137.14 toks/s]
Processed prompts:  78%|███████▊  | 6419/8192 [00:47<00:19, 91.48it/s, est. speed input: 139737.83 toks/s, output: 136.46 toks/s]
Processed prompts:  79%|███████▉  | 6483/8192 [00:47<00:18, 91.62it/s, est. speed input: 139072.70 toks/s, output: 135.81 toks/s]
Processed prompts:  80%|███████▉  | 6547/8192 [00:48<00:18, 91.34it/s, est. speed input: 138399.72 toks/s, output: 135.16 toks/s]
Processed prompts:  81%|████████  | 6611/8192 [00:49<00:17, 91.45it/s, est. speed input: 137767.62 toks/s, output: 134.54 toks/s]
Processed prompts:  81%|████████▏ | 6675/8192 [00:49<00:16, 91.62it/s, est. speed input: 137160.05 toks/s, output: 133.95 toks/s]
Processed prompts:  82%|████████▏ | 6739/8192 [00:50<00:15, 91.72it/s, est. speed input: 136568.04 toks/s, output: 133.37 toks/s]
Processed prompts:  83%|████████▎ | 6803/8192 [00:51<00:15, 91.58it/s, est. speed input: 135977.51 toks/s, output: 132.79 toks/s]
Processed prompts:  84%|████████▍ | 6867/8192 [00:51<00:14, 91.59it/s, est. speed input: 135410.19 toks/s, output: 132.24 toks/s]
Processed prompts:  85%|████████▍ | 6931/8192 [00:52<00:13, 91.55it/s, est. speed input: 134855.20 toks/s, output: 131.69 toks/s]
Processed prompts:  85%|████████▌ | 6995/8192 [00:53<00:13, 91.58it/s, est. speed input: 134318.39 toks/s, output: 131.17 toks/s]
Processed prompts:  86%|████████▌ | 7059/8192 [00:54<00:12, 91.89it/s, est. speed input: 133813.50 toks/s, output: 130.68 toks/s]
Processed prompts:  87%|████████▋ | 7123/8192 [00:54<00:11, 92.06it/s, est. speed input: 133318.08 toks/s, output: 130.19 toks/s]
Processed prompts:  88%|████████▊ | 7187/8192 [00:55<00:10, 91.99it/s, est. speed input: 132823.88 toks/s, output: 129.71 toks/s]
Processed prompts:  89%|████████▊ | 7251/8192 [00:56<00:10, 92.24it/s, est. speed input: 132359.59 toks/s, output: 129.26 toks/s]
Processed prompts:  89%|████████▉ | 7315/8192 [00:56<00:09, 92.09it/s, est. speed input: 131887.74 toks/s, output: 128.80 toks/s]
Processed prompts:  90%|█████████ | 7379/8192 [00:57<00:08, 92.03it/s, est. speed input: 131430.07 toks/s, output: 128.35 toks/s]
Processed prompts:  91%|█████████ | 7443/8192 [00:58<00:08, 92.30it/s, est. speed input: 131000.51 toks/s, output: 127.93 toks/s]
Processed prompts:  92%|█████████▏| 7507/8192 [00:58<00:07, 92.08it/s, est. speed input: 130558.91 toks/s, output: 127.50 toks/s]
Processed prompts:  92%|█████████▏| 7571/8192 [00:59<00:06, 91.91it/s, est. speed input: 130126.29 toks/s, output: 127.08 toks/s]
Processed prompts:  93%|█████████▎| 7635/8192 [01:00<00:06, 92.12it/s, est. speed input: 129721.46 toks/s, output: 126.68 toks/s]
Processed prompts:  94%|█████████▍| 7699/8192 [01:00<00:05, 92.33it/s, est. speed input: 129329.40 toks/s, output: 126.30 toks/s]
Processed prompts:  95%|█████████▍| 7763/8192 [01:01<00:04, 92.07it/s, est. speed input: 128924.58 toks/s, output: 125.90 toks/s]
Processed prompts:  96%|█████████▌| 7827/8192 [01:02<00:03, 92.02it/s, est. speed input: 128535.81 toks/s, output: 125.52 toks/s]
Processed prompts:  96%|█████████▋| 7891/8192 [01:03<00:03, 91.94it/s, est. speed input: 128153.17 toks/s, output: 125.15 toks/s]
Processed prompts:  97%|█████████▋| 7955/8192 [01:03<00:02, 91.97it/s, est. speed input: 127783.23 toks/s, output: 124.79 toks/s]
Processed prompts:  98%|█████████▊| 8019/8192 [01:04<00:01, 91.94it/s, est. speed input: 127419.21 toks/s, output: 124.43 toks/s]
Processed prompts:  99%|█████████▊| 8083/8192 [01:05<00:01, 91.79it/s, est. speed input: 127056.29 toks/s, output: 124.08 toks/s]
Processed prompts:  99%|█████████▉| 8147/8192 [01:05<00:00, 100.22it/s, est. speed input: 127083.51 toks/s, output: 124.10 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:05<00:00, 100.22it/s, est. speed input: 127784.31 toks/s, output: 124.79 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [01:05<00:00, 124.79it/s, est. speed input: 127784.31 toks/s, output: 124.79 toks/s]
[rank0]:[W126 17:22:56.463017367 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-28 13:25:03
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
Params: prompt_len=512, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4 --dataset-name random --input-len 512 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 513 --max-num-batched-tokens 513 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/BitNet-2B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:25:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3214747) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3214747) WARNING 01-28 13:25:25 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3214747) WARNING 01-28 13:25:35 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 32.42 requests/s, 16631.08 total tokens/s, 32.42 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128

STDERR:
[2026-01-28 13:25:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:25:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:25:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:25:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:25:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:25:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:25:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:25:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:25:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:25:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:25:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:25:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:25:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:25:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:25:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:25:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:25:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:25:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:17] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:17] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:17] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:17] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:17] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3214747) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3214747) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=3214747) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=3214747) 
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3214747) [2026-01-28 13:25:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3214747) 2026-01-28 13:25:35,870 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3214747) 2026-01-28 13:25:35,895 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3214747) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.55it/s]
(EngineCore_DP0 pid=3214747) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.57it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  59%|█████▊    | 75/128 [00:00<00:00, 743.88it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 755.73it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:34,  3.73it/s, est. speed input: 1908.62 toks/s, output: 3.73 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:07, 15.67it/s, est. speed input: 6730.52 toks/s, output: 13.15 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:05, 22.81it/s, est. speed input: 9403.69 toks/s, output: 18.37 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 27.24it/s, est. speed input: 11083.39 toks/s, output: 21.65 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 30.12it/s, est. speed input: 12243.53 toks/s, output: 23.91 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 32.11it/s, est. speed input: 13102.80 toks/s, output: 25.59 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 33.45it/s, est. speed input: 13758.16 toks/s, output: 26.87 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:02, 34.32it/s, est. speed input: 14269.16 toks/s, output: 27.87 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 34.97it/s, est. speed input: 14688.32 toks/s, output: 28.69 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 35.37it/s, est. speed input: 15029.13 toks/s, output: 29.35 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 35.69it/s, est. speed input: 15319.04 toks/s, output: 29.92 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 35.87it/s, est. speed input: 15561.39 toks/s, output: 30.39 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 36.09it/s, est. speed input: 15779.71 toks/s, output: 30.82 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 36.19it/s, est. speed input: 15965.10 toks/s, output: 31.18 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 36.25it/s, est. speed input: 16127.38 toks/s, output: 31.50 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 35.87it/s, est. speed input: 16234.67 toks/s, output: 31.71 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 36.08it/s, est. speed input: 16368.72 toks/s, output: 31.97 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 36.15it/s, est. speed input: 16482.99 toks/s, output: 32.19 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 36.24it/s, est. speed input: 16588.80 toks/s, output: 32.40 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 36.31it/s, est. speed input: 16686.14 toks/s, output: 32.59 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 36.04it/s, est. speed input: 16751.79 toks/s, output: 32.72 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 36.17it/s, est. speed input: 16833.37 toks/s, output: 32.88 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 36.23it/s, est. speed input: 16906.28 toks/s, output: 33.02 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 36.37it/s, est. speed input: 16979.64 toks/s, output: 33.16 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 36.34it/s, est. speed input: 17039.98 toks/s, output: 33.28 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 36.38it/s, est. speed input: 17099.39 toks/s, output: 33.40 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 35.69it/s, est. speed input: 17113.97 toks/s, output: 33.43 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 35.86it/s, est. speed input: 17163.38 toks/s, output: 33.52 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 36.03it/s, est. speed input: 17212.17 toks/s, output: 33.62 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 36.07it/s, est. speed input: 17253.78 toks/s, output: 33.70 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 36.10it/s, est. speed input: 17292.63 toks/s, output: 33.77 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 35.88it/s, est. speed input: 17317.98 toks/s, output: 33.82 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.88it/s, est. speed input: 17347.39 toks/s, output: 33.88 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.88it/s, est. speed input: 17347.39 toks/s, output: 33.88 toks/s]
[rank0]:[W128 13:25:42.913259009 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=1024 ==========
Time: 2026-01-28 13:25:44
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=128, max_num_seqs=1
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 128 --max-num-seqs 1 --max-model-len 1025 --max-num-batched-tokens 1025 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/BitNet-2B-INT8_M1024.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:25:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3216009) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3216009) WARNING 01-28 13:26:06 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3216009) WARNING 01-28 13:26:16 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 31.94 requests/s, 32733.51 total tokens/s, 31.94 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128

STDERR:
[2026-01-28 13:25:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:25:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:25:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:25:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:25:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:25:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:25:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:25:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:25:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:25:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:25:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:25:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:25:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:25:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:25:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:25:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:25:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:25:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:25:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3216009) [2026-01-28 13:25:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3216009) [2026-01-28 13:25:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3216009) [2026-01-28 13:25:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3216009) [2026-01-28 13:25:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3216009) [2026-01-28 13:25:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3216009) [2026-01-28 13:25:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3216009) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3216009) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3216009) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3216009) 
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3216009) [2026-01-28 13:26:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3216009) 2026-01-28 13:26:16,411 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3216009) 2026-01-28 13:26:16,436 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3216009) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.98it/s]
(EngineCore_DP0 pid=3216009) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.97it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:00, 393.77it/s]
Adding requests:  66%|██████▋   | 85/128 [00:00<00:00, 420.99it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 216.70it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 247.64it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:01, 64.58it/s, est. speed input: 66130.05 toks/s, output: 64.58 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:02, 43.94it/s, est. speed input: 47259.54 toks/s, output: 46.15 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 40.48it/s, est. speed input: 43909.30 toks/s, output: 42.88 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 38.69it/s, est. speed input: 42159.96 toks/s, output: 41.17 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 37.85it/s, est. speed input: 41291.82 toks/s, output: 40.32 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 37.23it/s, est. speed input: 40647.15 toks/s, output: 39.69 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 36.79it/s, est. speed input: 40155.24 toks/s, output: 39.21 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:02, 36.42it/s, est. speed input: 39749.47 toks/s, output: 38.82 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 36.21it/s, est. speed input: 39436.33 toks/s, output: 38.51 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 36.06it/s, est. speed input: 39182.00 toks/s, output: 38.26 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 36.01it/s, est. speed input: 38983.57 toks/s, output: 38.07 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 36.01it/s, est. speed input: 38824.41 toks/s, output: 37.91 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 35.99it/s, est. speed input: 38684.51 toks/s, output: 37.78 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 35.96it/s, est. speed input: 38557.79 toks/s, output: 37.65 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 35.92it/s, est. speed input: 38441.34 toks/s, output: 37.54 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 35.91it/s, est. speed input: 38343.99 toks/s, output: 37.45 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 35.95it/s, est. speed input: 38264.67 toks/s, output: 37.37 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 35.92it/s, est. speed input: 38183.39 toks/s, output: 37.29 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 35.93it/s, est. speed input: 38116.69 toks/s, output: 37.22 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 35.94it/s, est. speed input: 38056.30 toks/s, output: 37.16 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:01, 35.90it/s, est. speed input: 37992.82 toks/s, output: 37.10 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 35.90it/s, est. speed input: 37940.03 toks/s, output: 37.05 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 35.79it/s, est. speed input: 37875.05 toks/s, output: 36.99 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 35.85it/s, est. speed input: 37834.62 toks/s, output: 36.95 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 35.80it/s, est. speed input: 37786.11 toks/s, output: 36.90 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:03<00:00, 35.84it/s, est. speed input: 37749.42 toks/s, output: 36.86 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 35.84it/s, est. speed input: 37712.23 toks/s, output: 36.83 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 35.36it/s, est. speed input: 37619.15 toks/s, output: 36.74 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 35.46it/s, est. speed input: 37583.55 toks/s, output: 36.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.56it/s, est. speed input: 37554.08 toks/s, output: 36.67 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.56it/s, est. speed input: 37554.08 toks/s, output: 36.67 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.67it/s, est. speed input: 37554.08 toks/s, output: 36.67 toks/s]
[rank0]:[W128 13:26:22.367173620 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=2048 ==========
Time: 2026-01-28 13:26:23
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=256, max_num_seqs=2
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 256 --max-num-seqs 2 --max-model-len 1025 --max-num-batched-tokens 2048 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/BitNet-2B-INT8_M2048.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:26:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3217205) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3217205) WARNING 01-28 13:26:46 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3217205) WARNING 01-28 13:26:56 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 65.61 requests/s, 67248.78 total tokens/s, 65.61 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256

STDERR:
[2026-01-28 13:26:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:26:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:26:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:26:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:26:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:26:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:26:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:26:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:26:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:26:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:26:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:26:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:26:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:26:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:26:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:26:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:26:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:26:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:26:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3217205) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3217205) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3217205) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3217205) 
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3217205) [2026-01-28 13:26:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3217205) 2026-01-28 13:26:56,698 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3217205) 2026-01-28 13:26:56,724 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3217205) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 12.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 11.71it/s]
(EngineCore_DP0 pid=3217205) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.07it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:51,  4.96it/s]
Adding requests:  16%|█▌        | 41/256 [00:00<00:01, 167.95it/s]
Adding requests:  33%|███▎      | 85/256 [00:00<00:00, 267.60it/s]
Adding requests:  50%|█████     | 129/256 [00:00<00:00, 325.36it/s]
Adding requests:  68%|██████▊   | 173/256 [00:00<00:00, 361.33it/s]
Adding requests:  85%|████████▌ | 218/256 [00:00<00:00, 388.15it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 324.42it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:00<00:00, 354.74it/s, est. speed input: 363275.13 toks/s, output: 354.75 toks/s]
Processed prompts:  30%|███       | 78/256 [00:00<00:01, 111.34it/s, est. speed input: 128221.62 toks/s, output: 125.22 toks/s]
Processed prompts:  38%|███▊      | 97/256 [00:00<00:01, 97.68it/s, est. speed input: 113524.24 toks/s, output: 110.86 toks/s] 
Processed prompts:  43%|████▎     | 111/256 [00:01<00:01, 89.95it/s, est. speed input: 106178.22 toks/s, output: 103.69 toks/s]
Processed prompts:  48%|████▊     | 123/256 [00:01<00:01, 85.03it/s, est. speed input: 101709.50 toks/s, output: 99.33 toks/s] 
Processed prompts:  52%|█████▏    | 133/256 [00:01<00:01, 81.77it/s, est. speed input: 98820.22 toks/s, output: 96.50 toks/s] 
Processed prompts:  55%|█████▌    | 142/256 [00:01<00:01, 77.38it/s, est. speed input: 95818.22 toks/s, output: 93.57 toks/s]
Processed prompts:  59%|█████▉    | 151/256 [00:01<00:01, 78.13it/s, est. speed input: 94916.38 toks/s, output: 92.69 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:01<00:01, 74.10it/s, est. speed input: 92599.80 toks/s, output: 90.43 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:01<00:01, 73.45it/s, est. speed input: 91446.17 toks/s, output: 89.30 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:01<00:01, 72.92it/s, est. speed input: 90417.56 toks/s, output: 88.30 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:02<00:00, 72.55it/s, est. speed input: 89506.54 toks/s, output: 87.41 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:02<00:00, 72.36it/s, est. speed input: 88707.40 toks/s, output: 86.63 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:02<00:00, 72.09it/s, est. speed input: 87958.55 toks/s, output: 85.90 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:02<00:00, 71.96it/s, est. speed input: 87289.47 toks/s, output: 85.24 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:02<00:00, 71.73it/s, est. speed input: 86656.04 toks/s, output: 84.62 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:02<00:00, 71.55it/s, est. speed input: 86071.60 toks/s, output: 84.05 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:02<00:00, 71.52it/s, est. speed input: 85552.09 toks/s, output: 83.55 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:02<00:00, 71.61it/s, est. speed input: 85088.94 toks/s, output: 83.09 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:02<00:00, 71.65it/s, est. speed input: 84657.46 toks/s, output: 82.67 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 71.60it/s, est. speed input: 84244.18 toks/s, output: 82.27 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 71.60it/s, est. speed input: 84244.18 toks/s, output: 82.27 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 82.27it/s, est. speed input: 84244.18 toks/s, output: 82.27 toks/s]
[rank0]:[W128 13:27:02.791273297 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=4096 ==========
Time: 2026-01-28 13:27:04
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=512, max_num_seqs=4
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 512 --max-num-seqs 4 --max-model-len 1025 --max-num-batched-tokens 4096 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/BitNet-2B-INT8_M4096.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:27:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3218356) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3218356) WARNING 01-28 13:27:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3218356) WARNING 01-28 13:27:38 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 133.11 requests/s, 136437.10 total tokens/s, 133.11 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512

STDERR:
[2026-01-28 13:27:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:27:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:27:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:27:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:27:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:27:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:27:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:27:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:27:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:27:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:27:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:27:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:27:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:27:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:27:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:27:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:27:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:27:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3218356) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3218356) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3218356) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3218356) 
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3218356) [2026-01-28 13:27:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3218356) 2026-01-28 13:27:38,384 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3218356) 2026-01-28 13:27:38,410 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3218356) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 15.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.09it/s]
(EngineCore_DP0 pid=3218356) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 15.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 16.02it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 41/512 [00:00<00:01, 400.47it/s]
Adding requests:  17%|█▋        | 86/512 [00:00<00:01, 424.90it/s]
Adding requests:  26%|██▌       | 131/512 [00:00<00:00, 435.06it/s]
Adding requests:  34%|███▍      | 175/512 [00:00<00:00, 434.98it/s]
Adding requests:  43%|████▎     | 221/512 [00:00<00:00, 443.54it/s]
Adding requests:  53%|█████▎    | 269/512 [00:00<00:00, 454.80it/s]
Adding requests:  62%|██████▏   | 315/512 [00:00<00:00, 452.34it/s]
Adding requests:  71%|███████   | 362/512 [00:00<00:00, 457.80it/s]
Adding requests:  80%|████████  | 410/512 [00:00<00:00, 462.30it/s]
Adding requests:  89%|████████▉ | 458/512 [00:01<00:00, 464.56it/s]
Adding requests:  99%|█████████▊| 505/512 [00:01<00:00, 464.39it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 453.11it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:00<00:00, 1306.59it/s, est. speed input: 1338038.87 toks/s, output: 1306.61 toks/s]
Processed prompts:  54%|█████▍    | 277/512 [00:01<00:00, 236.51it/s, est. speed input: 278212.50 toks/s, output: 271.69 toks/s]   
Processed prompts:  66%|██████▋   | 340/512 [00:01<00:00, 197.94it/s, est. speed input: 236473.34 toks/s, output: 230.93 toks/s]
Processed prompts:  74%|███████▍  | 381/512 [00:01<00:00, 181.95it/s, est. speed input: 220678.65 toks/s, output: 215.51 toks/s]
Processed prompts:  80%|████████  | 411/512 [00:01<00:00, 170.78it/s, est. speed input: 211087.71 toks/s, output: 206.14 toks/s]
Processed prompts:  85%|████████▍ | 435/512 [00:02<00:00, 165.11it/s, est. speed input: 205936.33 toks/s, output: 201.11 toks/s]
Processed prompts:  89%|████████▉ | 456/512 [00:02<00:00, 160.39it/s, est. speed input: 201987.58 toks/s, output: 197.25 toks/s]
Processed prompts:  93%|█████████▎| 475/512 [00:02<00:00, 155.00it/s, est. speed input: 198314.35 toks/s, output: 193.67 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [00:02<00:00, 154.09it/s, est. speed input: 196359.94 toks/s, output: 191.76 toks/s]
Processed prompts:  99%|█████████▉| 509/512 [00:02<00:00, 153.33it/s, est. speed input: 194582.26 toks/s, output: 190.02 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 153.33it/s, est. speed input: 193079.02 toks/s, output: 188.55 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 188.55it/s, est. speed input: 193079.02 toks/s, output: 188.55 toks/s]
[rank0]:[W128 13:27:44.522079812 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=8192 ==========
Time: 2026-01-28 13:27:46
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=1024, max_num_seqs=8
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 1024 --max-num-seqs 8 --max-model-len 1025 --max-num-batched-tokens 8192 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/BitNet-2B-INT8_M8192.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:27:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3219494) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3219494) WARNING 01-28 13:28:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3219494) WARNING 01-28 13:28:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 253.18 requests/s, 259511.08 total tokens/s, 253.18 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024

STDERR:
[2026-01-28 13:27:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:27:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:27:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:27:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:27:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:27:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:27:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:27:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:27:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:27:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:28:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:28:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:28:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:28:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:28:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:28:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:28:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:28:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:28:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:04] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3219494) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3219494) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=3219494) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=3219494) 
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3219494) [2026-01-28 13:28:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3219494) 2026-01-28 13:28:21,954 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3219494) 2026-01-28 13:28:21,980 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3219494) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 15.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 16.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 13.59it/s]
(EngineCore_DP0 pid=3219494) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 15.35it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.31it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 397.12it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 423.92it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 433.02it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:01, 434.89it/s]
Adding requests:  21%|██▏       | 220/1024 [00:00<00:01, 441.63it/s]
Adding requests:  26%|██▌       | 268/1024 [00:00<00:01, 452.94it/s]
Adding requests:  31%|███       | 314/1024 [00:00<00:01, 450.77it/s]
Adding requests:  35%|███▌      | 360/1024 [00:00<00:01, 453.42it/s]
Adding requests:  40%|███▉      | 407/1024 [00:00<00:01, 455.58it/s]
Adding requests:  44%|████▍     | 454/1024 [00:01<00:01, 458.56it/s]
Adding requests:  49%|████▉     | 500/1024 [00:01<00:01, 458.01it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 454.39it/s]
Adding requests:  58%|█████▊    | 595/1024 [00:01<00:00, 462.93it/s]
Adding requests:  63%|██████▎   | 643/1024 [00:01<00:00, 465.09it/s]
Adding requests:  68%|██████▊   | 692/1024 [00:01<00:00, 471.85it/s]
Adding requests:  72%|███████▏  | 740/1024 [00:01<00:00, 472.89it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:01<00:00, 470.43it/s]
Adding requests:  82%|████████▏ | 836/1024 [00:01<00:00, 459.72it/s]
Adding requests:  86%|████████▋ | 884/1024 [00:01<00:00, 465.25it/s]
Adding requests:  91%|█████████ | 933/1024 [00:02<00:00, 471.39it/s]
Adding requests:  96%|█████████▌| 981/1024 [00:02<00:00, 471.22it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 459.98it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:00<00:00, 5068.29it/s, est. speed input: 5190292.80 toks/s, output: 5068.38 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 5068.29it/s, est. speed input: 577062.26 toks/s, output: 563.54 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 563.50it/s, est. speed input: 577062.26 toks/s, output: 563.54 toks/s] 
[rank0]:[W128 13:28:28.571943084 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=16384 ==========
Time: 2026-01-28 13:28:30
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=2048, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 2048 --max-num-seqs 16 --max-model-len 1025 --max-num-batched-tokens 16384 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/BitNet-2B-INT8_M16384.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:28:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3220715) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3220715) WARNING 01-28 13:29:01 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3220715) WARNING 01-28 13:29:11 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 315.34 requests/s, 323218.91 total tokens/s, 315.34 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048

STDERR:
[2026-01-28 13:28:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:28:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:28:45] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:28:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:45] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:45] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:28:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:28:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:28:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:28:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:28:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:28:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:28:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:28:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:28:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:28:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:28:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:28:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:28:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:28:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:28:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:53] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3220715) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3220715) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3220715) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=3220715) 
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3220715) [2026-01-28 13:28:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3220715) 2026-01-28 13:29:11,034 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3220715) 2026-01-28 13:29:11,060 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3220715) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  9.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 13.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 12.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 11.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 12.06it/s]
(EngineCore_DP0 pid=3220715) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 16.35it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.68it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.66it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 40/2048 [00:00<00:05, 398.16it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 422.28it/s]
Adding requests:   6%|▋         | 128/2048 [00:00<00:04, 426.84it/s]
Adding requests:   8%|▊         | 171/2048 [00:00<00:04, 426.09it/s]
Adding requests:  10%|█         | 215/2048 [00:00<00:04, 429.23it/s]
Adding requests:  13%|█▎        | 262/2048 [00:00<00:04, 441.62it/s]
Adding requests:  15%|█▍        | 307/2048 [00:00<00:03, 444.21it/s]
Adding requests:  17%|█▋        | 354/2048 [00:00<00:03, 450.40it/s]
Adding requests:  20%|█▉        | 401/2048 [00:00<00:03, 455.74it/s]
Adding requests:  22%|██▏       | 448/2048 [00:01<00:03, 458.92it/s]
Adding requests:  24%|██▍       | 495/2048 [00:01<00:03, 461.36it/s]
Adding requests:  26%|██▋       | 542/2048 [00:01<00:03, 452.08it/s]
Adding requests:  29%|██▉       | 592/2048 [00:01<00:03, 464.00it/s]
Adding requests:  31%|███       | 639/2048 [00:01<00:03, 461.58it/s]
Adding requests:  34%|███▎      | 688/2048 [00:01<00:02, 468.22it/s]
Adding requests:  36%|███▌      | 737/2048 [00:01<00:02, 472.39it/s]
Adding requests:  38%|███▊      | 785/2048 [00:01<00:02, 467.42it/s]
Adding requests:  41%|████      | 832/2048 [00:01<00:02, 458.71it/s]
Adding requests:  43%|████▎     | 879/2048 [00:01<00:02, 461.44it/s]
Adding requests:  45%|████▌     | 928/2048 [00:02<00:02, 467.71it/s]
Adding requests:  48%|████▊     | 976/2048 [00:02<00:02, 470.20it/s]
Adding requests:  50%|█████     | 1025/2048 [00:02<00:02, 473.13it/s]
Adding requests:  52%|█████▏    | 1073/2048 [00:02<00:02, 468.17it/s]
Adding requests:  55%|█████▍    | 1120/2048 [00:02<00:01, 466.79it/s]
Adding requests:  57%|█████▋    | 1169/2048 [00:02<00:01, 472.96it/s]
Adding requests:  60%|█████▉    | 1220/2048 [00:02<00:01, 481.48it/s]
Adding requests:  62%|██████▏   | 1269/2048 [00:02<00:01, 472.75it/s]
Adding requests:  64%|██████▍   | 1317/2048 [00:02<00:01, 473.54it/s]
Adding requests:  67%|██████▋   | 1365/2048 [00:02<00:01, 475.15it/s]
Adding requests:  69%|██████▉   | 1414/2048 [00:03<00:01, 477.76it/s]
Adding requests:  71%|███████▏  | 1462/2048 [00:03<00:01, 478.04it/s]
Adding requests:  74%|███████▍  | 1511/2048 [00:03<00:01, 479.55it/s]
Adding requests:  76%|███████▌  | 1559/2048 [00:03<00:01, 478.61it/s]
Adding requests:  79%|███████▊  | 1609/2048 [00:03<00:00, 483.82it/s]
Adding requests:  81%|████████  | 1658/2048 [00:03<00:00, 480.35it/s]
Adding requests:  83%|████████▎ | 1707/2048 [00:03<00:00, 466.73it/s]
Adding requests:  86%|████████▌ | 1755/2048 [00:03<00:00, 469.55it/s]
Adding requests:  88%|████████▊ | 1803/2048 [00:03<00:00, 467.83it/s]
Adding requests:  90%|█████████ | 1851/2048 [00:03<00:00, 469.50it/s]
Adding requests:  93%|█████████▎| 1899/2048 [00:04<00:00, 471.33it/s]
Adding requests:  95%|█████████▌| 1947/2048 [00:04<00:00, 471.17it/s]
Adding requests:  97%|█████████▋| 1995/2048 [00:04<00:00, 472.27it/s]
Adding requests: 100%|█████████▉| 2044/2048 [00:04<00:00, 474.73it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 465.42it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:00<00:00, 12824.37it/s, est. speed input: 13132987.22 toks/s, output: 12824.54 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:02<00:00, 12824.37it/s, est. speed input: 1002039.49 toks/s, output: 978.55 toks/s]   
Processed prompts: 100%|██████████| 2048/2048 [00:02<00:00, 978.50it/s, est. speed input: 1002039.49 toks/s, output: 978.55 toks/s]  
[rank0]:[W128 13:29:20.247219098 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=32768 ==========
Time: 2026-01-28 13:29:21
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
Params: prompt_len=1024, output_len=1, num_prompts=4096, max_num_seqs=32
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4 --dataset-name random --input-len 1024 --output-len 1 --num-prompts 4096 --max-num-seqs 32 --max-model-len 1025 --max-num-batched-tokens 32768 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/json/BitNet-2B-INT8_M32768.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:29:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3222101) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3222101) WARNING 01-28 13:30:01 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3222101) WARNING 01-28 13:30:12 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 336.08 requests/s, 344482.47 total tokens/s, 336.08 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096

STDERR:
[2026-01-28 13:29:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:29:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:29:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:29:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:29:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:29:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:29:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:29:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:29:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:29:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:29:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 13:29:53] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 13:29:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:53] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:53] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 13:29:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 13:29:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:29:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:29:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:29:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:29:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3222101) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3222101) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=3222101) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=3222101) 
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3222101) [2026-01-28 13:29:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3222101) [rank0]:W0128 13:30:06.735000 3222101 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3222101) [rank0]:W0128 13:30:06.818000 3222101 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3222101) [rank0]:W0128 13:30:07.934000 3222101 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3222101) [rank0]:W0128 13:30:08.064000 3222101 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3222101) 2026-01-28 13:30:12,003 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3222101) 2026-01-28 13:30:12,031 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3222101) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 14.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 14.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 15.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 15.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.81it/s]
(EngineCore_DP0 pid=3222101) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 15.40it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 15.30it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 15.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 15.18it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 41/4096 [00:00<00:10, 403.11it/s]
Adding requests:   2%|▏         | 85/4096 [00:00<00:09, 422.93it/s]
Adding requests:   3%|▎         | 130/4096 [00:00<00:09, 433.99it/s]
Adding requests:   4%|▍         | 175/4096 [00:00<00:08, 436.30it/s]
Adding requests:   5%|▌         | 221/4096 [00:00<00:08, 444.23it/s]
Adding requests:   7%|▋         | 270/4096 [00:00<00:08, 456.28it/s]
Adding requests:   8%|▊         | 316/4096 [00:00<00:08, 452.46it/s]
Adding requests:   9%|▉         | 364/4096 [00:00<00:08, 458.87it/s]
Adding requests:  10%|█         | 411/4096 [00:00<00:07, 460.87it/s]
Adding requests:  11%|█         | 459/4096 [00:01<00:07, 464.96it/s]
Adding requests:  12%|█▏        | 506/4096 [00:01<00:07, 464.60it/s]
Adding requests:  14%|█▎        | 553/4096 [00:01<00:07, 458.81it/s]
Adding requests:  15%|█▍        | 601/4096 [00:01<00:07, 464.08it/s]
Adding requests:  16%|█▌        | 651/4096 [00:01<00:07, 471.77it/s]
Adding requests:  17%|█▋        | 702/4096 [00:01<00:07, 480.44it/s]
Adding requests:  18%|█▊        | 751/4096 [00:01<00:06, 479.34it/s]
Adding requests:  20%|█▉        | 799/4096 [00:01<00:06, 476.74it/s]
Adding requests:  21%|██        | 847/4096 [00:01<00:06, 467.77it/s]
Adding requests:  22%|██▏       | 897/4096 [00:01<00:06, 476.98it/s]
Adding requests:  23%|██▎       | 945/4096 [00:02<00:06, 469.86it/s]
Adding requests:  24%|██▍       | 994/4096 [00:02<00:06, 474.07it/s]
Adding requests:  25%|██▌       | 1043/4096 [00:02<00:06, 476.17it/s]
Adding requests:  27%|██▋       | 1091/4096 [00:02<00:06, 474.45it/s]
Adding requests:  28%|██▊       | 1139/4096 [00:02<00:06, 469.36it/s]
Adding requests:  29%|██▉       | 1191/4096 [00:02<00:06, 482.22it/s]
Adding requests:  30%|███       | 1240/4096 [00:02<00:05, 482.74it/s]
Adding requests:  31%|███▏      | 1289/4096 [00:02<00:05, 478.12it/s]
Adding requests:  33%|███▎      | 1339/4096 [00:02<00:05, 483.01it/s]
Adding requests:  34%|███▍      | 1388/4096 [00:02<00:05, 483.06it/s]
Adding requests:  35%|███▌      | 1437/4096 [00:03<00:05, 481.16it/s]
Adding requests:  36%|███▋      | 1486/4096 [00:03<00:05, 483.33it/s]
Adding requests:  37%|███▋      | 1535/4096 [00:03<00:05, 484.71it/s]
Adding requests:  39%|███▊      | 1584/4096 [00:03<00:05, 485.69it/s]
Adding requests:  40%|███▉      | 1634/4096 [00:03<00:05, 489.76it/s]
Adding requests:  41%|████      | 1683/4096 [00:03<00:05, 474.92it/s]
Adding requests:  42%|████▏     | 1732/4096 [00:03<00:04, 477.43it/s]
Adding requests:  43%|████▎     | 1780/4096 [00:03<00:04, 474.58it/s]
Adding requests:  45%|████▍     | 1829/4096 [00:03<00:04, 477.77it/s]
Adding requests:  46%|████▌     | 1878/4096 [00:03<00:04, 479.37it/s]
Adding requests:  47%|████▋     | 1926/4096 [00:04<00:04, 479.02it/s]
Adding requests:  48%|████▊     | 1974/4096 [00:04<00:04, 479.26it/s]
Adding requests:  49%|████▉     | 2022/4096 [00:04<00:04, 472.71it/s]
Adding requests:  51%|█████     | 2072/4096 [00:04<00:04, 479.29it/s]
Adding requests:  52%|█████▏    | 2120/4096 [00:04<00:04, 476.63it/s]
Adding requests:  53%|█████▎    | 2168/4096 [00:04<00:04, 472.63it/s]
Adding requests:  54%|█████▍    | 2216/4096 [00:04<00:03, 471.57it/s]
Adding requests:  55%|█████▌    | 2265/4096 [00:04<00:03, 473.99it/s]
Adding requests:  57%|█████▋    | 2315/4096 [00:04<00:03, 479.51it/s]
Adding requests:  58%|█████▊    | 2363/4096 [00:05<00:03, 477.41it/s]
Adding requests:  59%|█████▉    | 2411/4096 [00:05<00:03, 476.25it/s]
Adding requests:  60%|██████    | 2461/4096 [00:05<00:03, 480.85it/s]
Adding requests:  61%|██████▏   | 2510/4096 [00:05<00:03, 478.53it/s]
Adding requests:  62%|██████▎   | 2560/4096 [00:05<00:03, 483.90it/s]
Adding requests:  64%|██████▎   | 2609/4096 [00:05<00:03, 482.41it/s]
Adding requests:  65%|██████▍   | 2659/4096 [00:05<00:02, 485.97it/s]
Adding requests:  66%|██████▌   | 2708/4096 [00:05<00:02, 479.94it/s]
Adding requests:  67%|██████▋   | 2757/4096 [00:05<00:02, 480.59it/s]
Adding requests:  69%|██████▊   | 2806/4096 [00:05<00:02, 477.15it/s]
Adding requests:  70%|██████▉   | 2855/4096 [00:06<00:02, 479.07it/s]
Adding requests:  71%|███████   | 2905/4096 [00:06<00:02, 482.75it/s]
Adding requests:  72%|███████▏  | 2954/4096 [00:06<00:02, 478.12it/s]
Adding requests:  73%|███████▎  | 3003/4096 [00:06<00:02, 479.77it/s]
Adding requests:  74%|███████▍  | 3051/4096 [00:06<00:02, 478.69it/s]
Adding requests:  76%|███████▌  | 3099/4096 [00:06<00:02, 476.09it/s]
Adding requests:  77%|███████▋  | 3147/4096 [00:06<00:02, 473.80it/s]
Adding requests:  78%|███████▊  | 3196/4096 [00:06<00:01, 475.41it/s]
Adding requests:  79%|███████▉  | 3244/4096 [00:06<00:01, 463.35it/s]
Adding requests:  80%|████████  | 3292/4096 [00:06<00:01, 467.60it/s]
Adding requests:  82%|████████▏ | 3339/4096 [00:07<00:01, 451.15it/s]
Adding requests:  83%|████████▎ | 3388/4096 [00:07<00:01, 459.37it/s]
Adding requests:  84%|████████▍ | 3437/4096 [00:07<00:01, 466.42it/s]
Adding requests:  85%|████████▌ | 3484/4096 [00:07<00:01, 459.57it/s]
Adding requests:  86%|████████▌ | 3532/4096 [00:07<00:01, 463.27it/s]
Adding requests:  87%|████████▋ | 3580/4096 [00:07<00:01, 466.17it/s]
Adding requests:  89%|████████▊ | 3627/4096 [00:07<00:01, 465.31it/s]
Adding requests:  90%|████████▉ | 3676/4096 [00:07<00:00, 470.12it/s]
Adding requests:  91%|█████████ | 3724/4096 [00:07<00:00, 471.50it/s]
Adding requests:  92%|█████████▏| 3773/4096 [00:07<00:00, 476.78it/s]
Adding requests:  93%|█████████▎| 3822/4096 [00:08<00:00, 478.83it/s]
Adding requests:  95%|█████████▍| 3873/4096 [00:08<00:00, 485.15it/s]
Adding requests:  96%|█████████▌| 3922/4096 [00:08<00:00, 483.63it/s]
Adding requests:  97%|█████████▋| 3971/4096 [00:08<00:00, 481.79it/s]
Adding requests:  98%|█████████▊| 4020/4096 [00:08<00:00, 482.30it/s]
Adding requests:  99%|█████████▉| 4069/4096 [00:08<00:00, 475.56it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 473.25it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  71%|███████   | 2903/4096 [00:00<00:00, 22552.36it/s, est. speed input: 23094985.94 toks/s, output: 22552.69 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:03<00:00, 22552.36it/s, est. speed input: 1187979.82 toks/s, output: 1160.13 toks/s]  
Processed prompts: 100%|██████████| 4096/4096 [00:03<00:00, 1160.09it/s, est. speed input: 1187979.82 toks/s, output: 1160.13 toks/s] 
[rank0]:[W128 13:30:27.416342380 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

