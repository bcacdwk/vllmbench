
========== M=16 ==========
Time: 2026-01-25 17:00:17
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 17:00:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 17:00:26 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=158322) WARNING 01-25 17:00:34 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=158322) WARNING 01-25 17:00:41 [backends.py:609] Failed to read file <frozen os>
Throughput: 11.00 requests/s, 2992.47 total tokens/s, 2816.44 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096

STDERR:
[2026-01-25 17:00:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:00:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 17:00:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 17:00:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:00:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:00:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:00:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:00:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:00:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 17:00:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:00:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:00:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:00:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:00:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 17:00:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:00:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 17:00:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 17:00:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:00:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:00:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:00:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:00:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:00:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 17:00:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:00:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:00:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:00:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:00:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=158322) [2026-01-25 17:00:35] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=158322) [2026-01-25 17:00:35] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=158322) [2026-01-25 17:00:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=158322) [2026-01-25 17:00:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=158322) [2026-01-25 17:00:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=158322) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=158322) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.96it/s]
(EngineCore_DP0 pid=158322) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.96it/s]
(EngineCore_DP0 pid=158322) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=158322) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  6.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  7.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  9.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  9.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.54it/s]
(EngineCore_DP0 pid=158322) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  8.11it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  9.00it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  9.20it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  9.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.07it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 2252.81it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:01<00:21,  1.44s/it, est. speed input: 11.11 toks/s, output: 177.74 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00,  1.44s/it, est. speed input: 177.09 toks/s, output: 2833.42 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00, 11.07it/s, est. speed input: 177.09 toks/s, output: 2833.42 toks/s]
[rank0]:[W125 17:00:56.046914016 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 17:00:59
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 17:01:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 17:01:08 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=159133) WARNING 01-25 17:01:15 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=159133) WARNING 01-25 17:01:21 [backends.py:609] Failed to read file <frozen os>
Throughput: 51.08 requests/s, 13892.45 total tokens/s, 13075.25 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-25 17:01:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:01:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 17:01:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 17:01:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 17:01:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:01:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:01:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:01:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:01:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 17:01:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:01:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 17:01:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 17:01:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 17:01:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:01:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:01:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:01:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:01:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=159133) [2026-01-25 17:01:16] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=159133) [2026-01-25 17:01:16] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=159133) [2026-01-25 17:01:16] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=159133) [2026-01-25 17:01:16] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=159133) [2026-01-25 17:01:16] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=159133) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=159133) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]
(EngineCore_DP0 pid=159133) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]
(EngineCore_DP0 pid=159133) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=159133) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:09,  3.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:08,  3.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:06,  4.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:05,  5.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:04,  6.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:01<00:04,  7.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:03,  7.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  7.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  7.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:03,  8.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:02<00:02,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:02<00:02,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:02,  8.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:01,  8.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:01,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  8.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  9.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01,  9.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:02<00:01,  8.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:03<00:01,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  6.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  6.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:01,  6.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:01,  6.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  7.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:03<00:00,  7.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:04<00:00,  7.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:04<00:00,  7.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:04<00:00,  8.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:04<00:00,  8.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  7.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  7.60it/s]
(EngineCore_DP0 pid=159133) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.36it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  7.69it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  8.06it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  7.97it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  8.07it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  8.19it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  8.20it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:01,  8.34it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:01,  8.34it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00,  8.52it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  8.54it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00,  8.44it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:01<00:00,  7.93it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00,  7.52it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:02<00:00,  7.30it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:02<00:00,  6.95it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:02<00:00,  7.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  7.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  7.91it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2963.40it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<05:04,  2.40s/it, est. speed input: 6.67 toks/s, output: 106.79 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.40s/it, est. speed input: 832.10 toks/s, output: 13313.59 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 52.00it/s, est. speed input: 832.10 toks/s, output: 13313.59 toks/s]
[rank0]:[W125 17:01:40.490541786 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 17:01:43
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 17:01:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 17:01:50 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=159961) WARNING 01-25 17:01:58 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=159961) WARNING 01-25 17:02:06 [backends.py:609] Failed to read file <frozen os>
Throughput: 74.54 requests/s, 20275.09 total tokens/s, 19082.44 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-25 17:01:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:01:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 17:01:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 17:01:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 17:01:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:01:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:01:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:01:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:01:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 17:01:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:01:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 17:01:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 17:01:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 17:01:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 17:01:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:01:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:01:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:01:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:01:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=159961) [2026-01-25 17:01:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=159961) [2026-01-25 17:01:59] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=159961) [2026-01-25 17:01:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=159961) [2026-01-25 17:01:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=159961) [2026-01-25 17:01:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=159961) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=159961) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=159961) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=159961) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=159961) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:04,  7.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:04,  7.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:04,  6.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:05,  6.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:04,  6.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:01<00:04,  6.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:01<00:04,  6.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  7.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:03,  7.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:03,  7.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:03,  7.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:02,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:01<00:02,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:02<00:02,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:02<00:02,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:02<00:02,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:02<00:02,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:02<00:02,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:02<00:01,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:03<00:01,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:03<00:01,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:03<00:01,  8.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:03<00:01,  8.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:03<00:00,  8.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:03<00:00,  8.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:03<00:00,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  7.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:04<00:00,  7.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:04<00:00,  7.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:04<00:00,  7.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:04<00:00,  7.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  7.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  7.82it/s]
(EngineCore_DP0 pid=159961) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.39it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:04,  8.24it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  8.54it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  8.75it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.84it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:03,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:02,  8.92it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  8.94it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  9.02it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  8.95it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  8.84it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:01<00:00, 42.41it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:02<00:00, 21.67it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:02<00:00, 14.40it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 2611.71it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<12:48,  3.01s/it, est. speed input: 5.31 toks/s, output: 84.99 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:03<00:02, 53.21it/s, est. speed input: 605.98 toks/s, output: 9695.54 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:03<00:00, 107.42it/s, est. speed input: 1058.87 toks/s, output: 16941.92 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 107.42it/s, est. speed input: 1228.32 toks/s, output: 19652.99 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 76.77it/s, est. speed input: 1228.32 toks/s, output: 19652.99 toks/s] 
[rank0]:[W125 17:02:27.928612603 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-26 08:45:22
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:45:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:45:31 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=417421) WARNING 01-26 08:45:47 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=417421) WARNING 01-26 08:45:56 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.43 requests/s, 9365.68 total tokens/s, 8814.76 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-26 08:45:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:45:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 08:45:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 08:45:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:45:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:45:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:45:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:45:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:45:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 08:45:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:45:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:45:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:45:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:45:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:45:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:45:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 08:45:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 08:45:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:45:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:45:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:45:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:45:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:45:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 08:45:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:45:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:45:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:45:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:45:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W126 08:45:47.015272738 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=417421) [2026-01-26 08:45:47] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=417421) [2026-01-26 08:45:48] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=417421) [2026-01-26 08:45:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=417421) [2026-01-26 08:45:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=417421) [2026-01-26 08:45:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=417421) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=417421) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.40s/it]
(EngineCore_DP0 pid=417421) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.40s/it]
(EngineCore_DP0 pid=417421) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=417421) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:02,  7.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:02,  7.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:02,  7.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:01,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:01,  8.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:01,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  8.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:00,  9.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  9.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  6.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00,  6.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  5.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  5.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  3.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  4.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  5.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  6.54it/s]
(EngineCore_DP0 pid=417421) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.35it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  8.41it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  8.81it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  9.02it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  9.07it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  9.09it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  9.25it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  9.30it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00,  9.36it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  9.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  9.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  9.12it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2817.04it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:01<01:53,  1.81s/it, est. speed input: 8.85 toks/s, output: 141.62 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:01<00:00,  1.81s/it, est. speed input: 558.23 toks/s, output: 8931.73 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:01<00:00, 34.89it/s, est. speed input: 558.23 toks/s, output: 8931.73 toks/s]
[rank0]:[W126 08:46:11.014982012 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 08:46:14
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:46:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:46:23 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=418364) WARNING 01-26 08:46:32 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=418364) WARNING 01-26 08:46:39 [backends.py:609] Failed to read file <frozen os>
Throughput: 51.47 requests/s, 13999.59 total tokens/s, 13176.09 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 08:46:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:46:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 08:46:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 08:46:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:46:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:46:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:46:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:46:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:46:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 08:46:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:46:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:46:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:46:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:46:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:46:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:46:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 08:46:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 08:46:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:46:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:46:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:46:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:46:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:46:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 08:46:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:46:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:46:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:46:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:46:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=418364) [2026-01-26 08:46:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=418364) [2026-01-26 08:46:32] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=418364) [2026-01-26 08:46:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=418364) [2026-01-26 08:46:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=418364) [2026-01-26 08:46:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=418364) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=418364) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.14it/s]
(EngineCore_DP0 pid=418364) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.14it/s]
(EngineCore_DP0 pid=418364) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=418364) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:04,  7.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:04,  7.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:04,  7.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:03,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:03,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:03,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:03,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:03,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:02,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:02,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:03,  6.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:04,  4.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:02<00:03,  5.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:02<00:04,  4.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:04,  3.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:03,  4.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:03<00:02,  5.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:03<00:02,  6.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:03<00:02,  6.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:03<00:01,  7.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:03<00:01,  7.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:03<00:01,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:04<00:00,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:04<00:00,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:04<00:00,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:04<00:00,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:04<00:00,  7.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:04<00:00,  7.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:05<00:00,  4.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:05<00:00,  4.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:05<00:00,  6.49it/s]
(EngineCore_DP0 pid=418364) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:06,  2.65it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:04,  3.81it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:03,  5.11it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:02,  6.09it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:02,  6.86it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:01<00:01,  7.44it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:01<00:01,  7.83it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:01<00:01,  8.14it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:01,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:01<00:00,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:02<00:00,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:02<00:00,  8.79it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:02<00:00,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:02<00:00,  7.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  4.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  6.67it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2847.26it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<05:01,  2.37s/it, est. speed input: 6.74 toks/s, output: 107.91 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.37s/it, est. speed input: 839.27 toks/s, output: 13428.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 52.45it/s, est. speed input: 839.27 toks/s, output: 13428.26 toks/s]
[rank0]:[W126 08:46:58.870430276 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 08:47:01
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:47:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:47:10 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=419210) WARNING 01-26 08:47:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=419210) WARNING 01-26 08:47:24 [backends.py:609] Failed to read file <frozen os>
Throughput: 74.17 requests/s, 20173.24 total tokens/s, 18986.58 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 08:47:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:47:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 08:47:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 08:47:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:47:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:47:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:47:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:47:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:47:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 08:47:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:47:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:47:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:47:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:47:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:47:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:47:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 08:47:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 08:47:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:47:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:47:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:47:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:47:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:47:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 08:47:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:47:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:47:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:47:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:47:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=419210) [2026-01-26 08:47:17] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=419210) [2026-01-26 08:47:17] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=419210) [2026-01-26 08:47:17] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=419210) [2026-01-26 08:47:17] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=419210) [2026-01-26 08:47:17] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=419210) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=419210) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=419210) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=419210) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=419210) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:04,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:04,  7.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:04,  7.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:03,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:03,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:03,  7.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:03,  7.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:05,  4.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:02<00:04,  4.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:02<00:06,  3.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:02<00:05,  3.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:02<00:04,  4.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:03<00:03,  5.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:03<00:02,  5.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:03<00:02,  6.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:03<00:02,  6.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:03<00:01,  7.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:03<00:01,  7.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:03<00:01,  7.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:03<00:01,  7.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:04<00:01,  7.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:04<00:01,  7.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:04<00:01,  7.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:04<00:00,  7.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:04<00:00,  7.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:04<00:00,  6.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:05<00:00,  4.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:05<00:00,  4.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:05<00:00,  4.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:05<00:00,  4.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:05<00:00,  4.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:05<00:00,  6.05it/s]
(EngineCore_DP0 pid=419210) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:05,  5.89it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:05,  6.43it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:04,  6.81it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:04,  7.16it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:04,  7.35it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  7.53it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  7.79it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:01<00:03,  7.86it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:03,  8.03it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:03,  8.12it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  8.18it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  8.21it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  8.28it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  7.59it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:02<00:04,  4.52it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:02<00:04,  4.69it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:02<00:04,  3.97it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:03<00:04,  3.96it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:03<00:03,  4.73it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:03<00:02,  5.43it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:03<00:02,  6.02it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:03<00:01,  6.57it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:03<00:01,  7.02it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:03<00:01,  7.45it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:03<00:01,  7.75it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:04<00:01,  7.94it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:04<00:00,  8.09it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:04<00:00,  8.20it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:04<00:00,  8.31it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:04<00:00,  8.38it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:04<00:00,  8.46it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:04<00:00,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:04<00:00,  8.53it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:05<00:00,  7.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:05<00:00,  4.64it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:05<00:00,  6.47it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 2844.13it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<12:52,  3.03s/it, est. speed input: 5.28 toks/s, output: 84.54 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:03<00:02, 52.89it/s, est. speed input: 602.47 toks/s, output: 9639.49 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:03<00:00, 106.70it/s, est. speed input: 1052.34 toks/s, output: 16837.35 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 106.70it/s, est. speed input: 1219.11 toks/s, output: 19505.64 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 76.19it/s, est. speed input: 1219.11 toks/s, output: 19505.64 toks/s] 
[rank0]:[W126 08:47:47.108859165 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 08:47:50
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 08:47:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 08:47:59 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=420118) WARNING 01-26 08:48:08 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=420118) WARNING 01-26 08:48:13 [backends.py:609] Failed to read file <frozen os>
Throughput: 73.77 requests/s, 20065.85 total tokens/s, 18885.51 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-26 08:47:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:47:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 08:47:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 08:47:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:47:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:47:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:47:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:47:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:47:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 08:47:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:47:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:47:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:47:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:47:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 08:48:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 08:48:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 08:48:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 08:48:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:48:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:48:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:48:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:48:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 08:48:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 08:48:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 08:48:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 08:48:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 08:48:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 08:48:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=420118) [2026-01-26 08:48:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=420118) [2026-01-26 08:48:08] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=420118) [2026-01-26 08:48:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=420118) [2026-01-26 08:48:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=420118) [2026-01-26 08:48:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=420118) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=420118) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.19it/s]
(EngineCore_DP0 pid=420118) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.19it/s]
(EngineCore_DP0 pid=420118) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=420118) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  9.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:05,  9.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  9.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:05,  9.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:04,  9.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:04,  9.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:04,  9.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:04,  9.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:04,  9.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:04,  9.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  9.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  9.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  9.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:07,  4.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:02<00:07,  4.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:10,  3.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:08,  4.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:06,  4.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:03<00:05,  5.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:03<00:04,  6.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:03<00:04,  6.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:03<00:03,  7.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:03<00:03,  7.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:03<00:03,  8.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:03,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:02,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:04<00:02,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:04<00:02,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:04<00:02,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:04<00:02,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:04<00:02,  7.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:05<00:04,  3.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:05<00:03,  4.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:05<00:04,  3.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:05<00:03,  3.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:06<00:02,  4.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:06<00:02,  5.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:06<00:01,  5.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:06<00:01,  6.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:06<00:01,  6.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:06<00:01,  7.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:06<00:00,  7.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:06<00:00,  7.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:07<00:00,  7.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:07<00:00,  7.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:07<00:00,  8.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:07<00:00,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:07<00:00,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:07<00:00,  7.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:07<00:00,  6.65it/s]
(EngineCore_DP0 pid=420118) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:27,  1.81it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:17,  2.74it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:01<00:19,  2.41it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:01<00:14,  3.15it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:01<00:12,  3.77it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:01<00:10,  4.39it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:01<00:08,  4.96it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:02<00:07,  5.41it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:02<00:06,  6.21it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:02<00:05,  6.90it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:02<00:05,  7.42it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:02<00:04,  7.93it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:02<00:04,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:02<00:04,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:02<00:04,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:02<00:03,  8.89it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:03<00:03,  8.96it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:03<00:03,  9.02it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:03<00:04,  7.37it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:03<00:06,  5.04it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:03<00:05,  5.08it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:04<00:07,  3.69it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:04<00:06,  4.09it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:04<00:05,  4.90it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:04<00:04,  5.72it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:04<00:03,  6.43it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:04<00:03,  7.11it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:05<00:03,  7.64it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:05<00:02,  8.01it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:05<00:02,  8.32it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:05<00:02,  8.45it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:05<00:02,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:05<00:02,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:05<00:01,  8.81it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:05<00:01,  8.96it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:05<00:01,  8.99it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:06<00:01,  9.10it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:06<00:01,  9.07it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:06<00:01,  9.15it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:06<00:01,  8.22it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:06<00:02,  4.66it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:06<00:01,  5.41it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:07<00:01,  5.69it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:07<00:01,  4.20it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:07<00:01,  4.57it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:07<00:00,  5.42it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:07<00:00,  6.18it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:07<00:00,  6.90it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:08<00:00,  7.52it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:08<00:00,  7.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:08<00:00,  8.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:08<00:00,  6.15it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  59%|█████▊    | 300/512 [00:00<00:00, 2992.86it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3035.93it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:03<31:59,  3.76s/it, est. speed input: 4.26 toks/s, output: 68.16 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:03<00:19, 22.93it/s, est. speed input: 261.05 toks/s, output: 4176.81 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:03<00:03, 81.80it/s, est. speed input: 744.62 toks/s, output: 11913.85 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:04<00:01, 161.36it/s, est. speed input: 1235.05 toks/s, output: 19760.69 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:04<00:00, 235.73it/s, est. speed input: 1599.64 toks/s, output: 25594.09 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 235.73it/s, est. speed input: 1537.30 toks/s, output: 24596.80 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 96.08it/s, est. speed input: 1537.30 toks/s, output: 24596.80 toks/s] 
[rank0]:[W126 08:48:48.540091886 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-26 09:19:38
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-INT8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:19:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:19:47 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=454600) WARNING 01-26 09:19:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=454600) WARNING 01-26 09:20:08 [backends.py:609] Failed to read file <frozen os>
Throughput: 20.88 requests/s, 5678.72 total tokens/s, 5344.68 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-26 09:19:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:19:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 09:19:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 09:19:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:19:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:19:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:19:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:19:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:19:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 09:19:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:19:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:19:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:19:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:19:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:19:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:19:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 09:19:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 09:19:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:19:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:19:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:19:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:19:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:19:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 09:19:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:19:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:19:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:19:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:19:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=454600) [2026-01-26 09:19:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=454600) [2026-01-26 09:19:56] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=454600) [2026-01-26 09:19:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=454600) [2026-01-26 09:19:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=454600) [2026-01-26 09:19:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=454600) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=454600) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.91s/it]
(EngineCore_DP0 pid=454600) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.91s/it]
(EngineCore_DP0 pid=454600) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=454600) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:02,  6.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:02,  6.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:02,  6.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:01<00:05,  2.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:04,  3.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:04,  2.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:03,  3.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:02<00:02,  4.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:02<00:02,  4.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:02<00:01,  5.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:02<00:01,  6.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:02<00:01,  6.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:02<00:00,  6.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:02<00:00,  7.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  7.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:03<00:00,  7.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:03<00:00,  7.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:03<00:00,  7.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:03<00:00,  6.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:03<00:00,  5.39it/s]
(EngineCore_DP0 pid=454600) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:02,  4.40it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:03,  2.98it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:02,  3.64it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:01<00:02,  3.19it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:01<00:01,  3.50it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:01<00:01,  4.31it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:01<00:00,  5.11it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:01<00:00,  5.83it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  6.43it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:02<00:00,  6.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:02<00:00,  7.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:02<00:00,  5.03it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2758.84it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<03:08,  2.99s/it, est. speed input: 5.35 toks/s, output: 85.56 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00,  2.99s/it, est. speed input: 336.76 toks/s, output: 5388.17 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00, 21.05it/s, est. speed input: 336.76 toks/s, output: 5388.17 toks/s]
[rank0]:[W126 09:20:33.991134210 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 09:20:36
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:20:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:20:45 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=455630) WARNING 01-26 09:20:52 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=455630) WARNING 01-26 09:21:03 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.12 requests/s, 8191.47 total tokens/s, 7709.61 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 09:20:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:20:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 09:20:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 09:20:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:20:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:20:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:20:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:20:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:20:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 09:20:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:20:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:20:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:20:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:20:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:20:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:20:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 09:20:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 09:20:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:20:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:20:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:20:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:20:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:20:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 09:20:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:20:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:20:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:20:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:20:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=455630) [2026-01-26 09:20:52] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=455630) [2026-01-26 09:20:53] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=455630) [2026-01-26 09:20:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=455630) [2026-01-26 09:20:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=455630) [2026-01-26 09:20:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=455630) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=455630) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.32it/s]
(EngineCore_DP0 pid=455630) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.32it/s]
(EngineCore_DP0 pid=455630) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=455630) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:04,  6.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:06,  5.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:05,  6.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:05,  6.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:05,  5.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:01<00:08,  3.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:07,  3.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:02<00:08,  3.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:02<00:07,  3.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:02<00:05,  4.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:02<00:04,  5.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:02<00:04,  5.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:02<00:03,  6.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:02<00:03,  6.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:02<00:02,  6.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:03<00:02,  7.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:03<00:02,  7.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:03<00:02,  7.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:03<00:02,  7.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:03<00:02,  7.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:03<00:02,  6.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:04<00:02,  5.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:04<00:02,  4.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:04<00:02,  4.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:05<00:03,  3.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:05<00:02,  3.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:05<00:01,  4.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:05<00:01,  4.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:05<00:01,  5.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:05<00:00,  6.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:05<00:00,  6.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:06<00:00,  6.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:06<00:00,  7.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:06<00:00,  7.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:06<00:00,  7.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:06<00:00,  5.38it/s]
(EngineCore_DP0 pid=455630) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.47it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  7.31it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:02,  7.05it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:02,  6.40it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:00<00:00, 20.23it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00, 13.10it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00, 11.28it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:01<00:00, 10.35it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:01<00:00,  9.76it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:01<00:00, 10.49it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2829.40it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:04<08:38,  4.08s/it, est. speed input: 3.92 toks/s, output: 62.72 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:04<00:00, 36.36it/s, est. speed input: 412.75 toks/s, output: 6603.95 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 36.36it/s, est. speed input: 487.22 toks/s, output: 7795.48 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 30.45it/s, est. speed input: 487.22 toks/s, output: 7795.48 toks/s]
[rank0]:[W126 09:21:29.734595592 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 09:21:33
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:21:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:21:42 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=456609) WARNING 01-26 09:21:50 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=456609) WARNING 01-26 09:21:59 [backends.py:609] Failed to read file <frozen os>
Throughput: 42.43 requests/s, 11541.89 total tokens/s, 10862.96 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 09:21:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:21:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 09:21:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 09:21:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:21:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:21:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:21:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:21:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:21:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 09:21:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:21:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:21:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:21:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:21:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:21:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:21:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 09:21:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 09:21:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:21:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:21:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:21:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:21:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:21:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 09:21:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:21:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:21:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:21:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:21:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=456609) [2026-01-26 09:21:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=456609) [2026-01-26 09:21:51] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=456609) [2026-01-26 09:21:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=456609) [2026-01-26 09:21:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=456609) [2026-01-26 09:21:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=456609) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=456609) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.33it/s]
(EngineCore_DP0 pid=456609) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.32it/s]
(EngineCore_DP0 pid=456609) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=456609) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  7.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:04,  7.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:04,  7.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:04,  7.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  7.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  7.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  7.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:01<00:03,  7.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:07,  3.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:06,  3.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:02<00:08,  2.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:02<00:06,  3.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:02<00:05,  4.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:02<00:04,  4.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:02<00:03,  5.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:03<00:03,  6.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:03<00:02,  6.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:03<00:02,  6.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:03<00:02,  6.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:03<00:02,  7.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:03<00:02,  7.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:03<00:01,  7.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:04<00:01,  7.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:04<00:01,  7.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:04<00:01,  5.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:04<00:02,  3.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:05<00:02,  3.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:05<00:02,  3.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:05<00:01,  3.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:05<00:01,  4.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:06<00:01,  4.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:06<00:00,  5.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:06<00:00,  5.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:06<00:00,  5.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:06<00:00,  6.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:06<00:00,  5.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:06<00:00,  5.28it/s]
(EngineCore_DP0 pid=456609) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:06,  5.59it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:05,  6.17it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:05,  6.28it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:06,  5.03it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:01<00:08,  3.72it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:01<00:07,  4.09it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:01<00:09,  3.04it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:02<00:07,  3.58it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:02<00:06,  4.11it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:02<00:05,  4.47it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:02<00:04,  5.16it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:02<00:04,  5.74it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:02<00:03,  6.22it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:02<00:03,  6.56it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:03<00:02,  6.82it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:03<00:02,  7.04it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:03<00:02,  7.25it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:03<00:02,  7.38it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:03<00:02,  7.51it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:03<00:01,  7.66it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:03<00:02,  6.37it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:04<00:02,  4.44it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:04<00:02,  4.57it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:04<00:02,  3.76it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:05<00:02,  3.71it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:05<00:02,  4.40it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:05<00:01,  5.07it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:05<00:01,  5.68it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:05<00:00,  6.17it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:05<00:00,  6.61it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:05<00:00,  6.99it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:05<00:00,  7.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:04<00:00,  7.12it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 2988.09it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:05<22:35,  5.32s/it, est. speed input: 3.01 toks/s, output: 48.14 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:05<00:12, 16.10it/s, est. speed input: 182.76 toks/s, output: 2924.18 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:05<00:03, 38.87it/s, est. speed input: 367.56 toks/s, output: 5880.91 toks/s]
Processed prompts:  71%|███████▏  | 183/256 [00:05<00:01, 64.19it/s, est. speed input: 519.87 toks/s, output: 8317.94 toks/s]
Processed prompts:  91%|█████████ | 233/256 [00:05<00:00, 90.20it/s, est. speed input: 645.85 toks/s, output: 10333.52 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 90.20it/s, est. speed input: 688.92 toks/s, output: 11022.62 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 43.06it/s, est. speed input: 688.92 toks/s, output: 11022.62 toks/s]
[rank0]:[W126 09:22:30.948324149 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 09:22:34
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:22:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:22:43 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=457672) WARNING 01-26 09:22:51 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=457672) WARNING 01-26 09:23:01 [backends.py:609] Failed to read file <frozen os>
Throughput: 42.76 requests/s, 11630.77 total tokens/s, 10946.61 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-26 09:22:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:22:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 09:22:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 09:22:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:22:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:22:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:22:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:22:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:22:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 09:22:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:22:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:22:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:22:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:22:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:22:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:22:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-26 09:22:50] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-26 09:22:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:22:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:22:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:22:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:22:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-26 09:22:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-26 09:22:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:22:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:22:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:22:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:22:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=457672) [2026-01-26 09:22:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=457672) [2026-01-26 09:22:52] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=457672) [2026-01-26 09:22:52] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=457672) [2026-01-26 09:22:52] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=457672) [2026-01-26 09:22:52] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=457672) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=457672) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.33it/s]
(EngineCore_DP0 pid=457672) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.33it/s]
(EngineCore_DP0 pid=457672) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=457672) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:07,  6.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:10,  4.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:14,  3.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:12,  3.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:01<00:13,  3.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:01<00:12,  3.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:01<00:10,  4.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:01<00:08,  5.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:02<00:07,  5.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:02<00:06,  6.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:02<00:05,  6.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:02<00:05,  7.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:02<00:05,  7.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:02<00:04,  7.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:02<00:04,  7.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:02<00:04,  7.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:03<00:04,  7.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:03<00:04,  7.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:03<00:04,  7.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:03<00:08,  3.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:04<00:07,  3.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:04<00:10,  2.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:04<00:08,  3.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:04<00:06,  4.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:05<00:05,  4.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:05<00:04,  5.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:05<00:04,  5.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:05<00:03,  5.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:05<00:03,  6.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:05<00:03,  6.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:05<00:03,  6.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:06<00:02,  6.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:06<00:02,  6.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:06<00:02,  5.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:07<00:04,  3.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:07<00:04,  3.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:07<00:00, 13.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:07<00:00, 11.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:08<00:00,  9.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:08<00:00,  6.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:08<00:00,  5.77it/s]
(EngineCore_DP0 pid=457672) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:23,  2.16it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:21,  2.32it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:01<00:14,  3.26it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:01<00:11,  4.07it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:01<00:09,  4.69it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:01<00:08,  5.19it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:01<00:07,  5.62it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:01<00:07,  5.93it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:06,  6.18it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:02<00:06,  6.43it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:02<00:05,  6.84it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:02<00:05,  6.79it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:02<00:06,  6.32it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:02<00:09,  3.99it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:03<00:08,  4.20it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:03<00:11,  3.09it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:03<00:09,  3.65it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:04<00:07,  4.34it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:04<00:06,  5.01it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:04<00:05,  5.65it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:04<00:04,  6.21it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:04<00:04,  6.67it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:04<00:04,  6.92it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:04<00:03,  7.15it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:04<00:03,  7.39it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:05<00:03,  7.58it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:05<00:03,  7.70it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:05<00:02,  7.73it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:05<00:02,  7.77it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:05<00:02,  7.82it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:05<00:03,  6.26it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:06<00:04,  4.44it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:06<00:03,  4.85it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:06<00:04,  3.75it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:07<00:04,  3.64it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:07<00:03,  4.34it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:07<00:02,  5.03it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:07<00:02,  5.60it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:07<00:01,  6.14it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:07<00:01,  6.61it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:07<00:01,  6.96it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:07<00:01,  7.19it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:08<00:01,  7.33it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:08<00:00,  7.53it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:08<00:00,  7.66it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:08<00:00,  7.65it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:08<00:00,  7.72it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:08<00:00,  7.82it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:08<00:00,  7.26it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:09<00:00,  4.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:09<00:00,  5.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:09<00:00,  5.46it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  59%|█████▉    | 301/512 [00:00<00:00, 3004.12it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3046.67it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:08<1:15:08,  8.82s/it, est. speed input: 1.81 toks/s, output: 29.01 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:08<00:45,  9.97it/s, est. speed input: 112.79 toks/s, output: 1804.63 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:09<00:10, 31.99it/s, est. speed input: 289.54 toks/s, output: 4632.60 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:09<00:05, 49.24it/s, est. speed input: 390.80 toks/s, output: 6252.76 toks/s]
Processed prompts:  54%|█████▍    | 279/512 [00:09<00:03, 69.97it/s, est. speed input: 481.18 toks/s, output: 7698.90 toks/s]
Processed prompts:  65%|██████▌   | 333/512 [00:09<00:01, 95.28it/s, est. speed input: 566.96 toks/s, output: 9071.34 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [00:09<00:01, 121.80it/s, est. speed input: 643.91 toks/s, output: 10302.55 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:09<00:00, 147.03it/s, est. speed input: 710.32 toks/s, output: 11365.08 toks/s]
Processed prompts:  92%|█████████▏| 472/512 [00:09<00:00, 170.94it/s, est. speed input: 768.96 toks/s, output: 12303.30 toks/s]
Processed prompts: 100%|█████████▉| 511/512 [00:11<00:00, 56.90it/s, est. speed input: 692.66 toks/s, output: 11082.49 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 56.90it/s, est. speed input: 694.00 toks/s, output: 11104.03 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 43.37it/s, est. speed input: 694.00 toks/s, output: 11104.03 toks/s]
[rank0]:[W126 09:23:50.456649136 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-26 10:02:49
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Qwen2.5-7B-INT8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:02:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:02:58 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=499133) WARNING 01-26 10:03:06 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=499133) WARNING 01-26 10:03:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 14.66 requests/s, 3987.80 total tokens/s, 3753.22 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-26 10:02:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:02:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:02:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:02:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:02:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:02:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:02:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:02:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:02:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:02:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:02:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:02:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:02:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:02:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:03:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:03:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:03:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:03:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:03:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:03:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:03:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:03:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:03:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:03:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:03:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:03:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:03:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:03:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=499133) [2026-01-26 10:03:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=499133) [2026-01-26 10:03:08] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=499133) [2026-01-26 10:03:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=499133) [2026-01-26 10:03:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=499133) [2026-01-26 10:03:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=499133) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=499133) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.95s/it]
(EngineCore_DP0 pid=499133) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.45s/it]
(EngineCore_DP0 pid=499133) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.37s/it]
(EngineCore_DP0 pid=499133) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=499133) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:07,  2.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:07,  2.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:01<00:08,  1.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:01<00:05,  2.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:04,  3.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:03,  4.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:02<00:02,  5.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:02<00:01,  5.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:02<00:01,  6.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:02<00:01,  6.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:02<00:01,  7.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:02<00:00,  7.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:02<00:00,  7.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:02<00:00,  7.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:03<00:00,  7.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:03<00:00,  7.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:03<00:00,  5.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:03<00:00,  3.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:04<00:00,  2.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:04<00:00,  4.25it/s]
(EngineCore_DP0 pid=499133) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:03,  3.22it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  5.00it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:01,  6.08it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:01,  6.89it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  7.37it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  7.77it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:01<00:00,  8.06it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:01<00:00,  8.13it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  8.29it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  8.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  8.33it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  7.36it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2373.06it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:57,  2.81s/it, est. speed input: 5.69 toks/s, output: 91.11 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.81s/it, est. speed input: 355.14 toks/s, output: 5682.26 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 22.19it/s, est. speed input: 355.14 toks/s, output: 5682.26 toks/s]
[rank0]:[W126 10:03:50.982834026 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 10:03:54
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Qwen2.5-7B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:04:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:04:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=500244) WARNING 01-26 10:04:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=500244) WARNING 01-26 10:04:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 23.27 requests/s, 6329.03 total tokens/s, 5956.74 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 10:04:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:04:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:04:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:04:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:04:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:04:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:04:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:04:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:04:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:04:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:04:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:04:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:04:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:04:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:04:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:04:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:04:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:04:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:04:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:04:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:04:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:04:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:04:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:04:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:04:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:04:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:04:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:04:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=500244) [2026-01-26 10:04:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=500244) [2026-01-26 10:04:12] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=500244) [2026-01-26 10:04:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=500244) [2026-01-26 10:04:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=500244) [2026-01-26 10:04:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=500244) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=500244) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.16it/s]
(EngineCore_DP0 pid=500244) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.05s/it]
(EngineCore_DP0 pid=500244) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.02s/it]
(EngineCore_DP0 pid=500244) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=500244) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:04,  7.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:04,  7.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:04,  7.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:03,  7.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:03,  7.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:03,  7.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:03,  7.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  7.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  7.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:04,  5.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:06,  3.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:02<00:06,  3.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:02<00:06,  3.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:02<00:05,  3.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:02<00:04,  4.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:03<00:03,  4.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:03<00:03,  5.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:03<00:02,  6.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:03<00:02,  6.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:03<00:02,  6.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:03<00:01,  7.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:03<00:01,  7.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:03<00:01,  7.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:04<00:01,  7.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:04<00:01,  7.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:04<00:01,  7.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:04<00:02,  3.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:05<00:01,  3.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:05<00:02,  2.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:05<00:01,  3.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:06<00:01,  3.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:06<00:00,  4.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:06<00:00,  5.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:06<00:00,  6.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:06<00:00,  6.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:06<00:00,  5.30it/s]
(EngineCore_DP0 pid=500244) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.04it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  7.45it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:02,  7.70it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  7.93it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  8.01it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  8.01it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  8.04it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:01<00:01,  6.70it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:02,  4.07it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  4.75it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:01,  4.70it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:02<00:01,  4.42it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:02<00:01,  4.08it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:02<00:01,  4.83it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:02<00:00,  5.50it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:02<00:00,  6.12it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:02<00:00,  6.54it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:03<00:00,  6.92it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:03<00:00,  7.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:03<00:00,  5.97it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2617.22it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:03<08:07,  3.84s/it, est. speed input: 4.17 toks/s, output: 66.68 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:03<00:01, 26.75it/s, est. speed input: 304.27 toks/s, output: 4868.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 26.75it/s, est. speed input: 510.15 toks/s, output: 8162.46 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 31.88it/s, est. speed input: 510.15 toks/s, output: 8162.46 toks/s]
[rank0]:[W126 10:04:50.325217975 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 10:04:54
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Qwen2.5-7B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:05:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:05:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=501302) WARNING 01-26 10:05:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=501302) WARNING 01-26 10:05:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.95 requests/s, 8691.29 total tokens/s, 8180.04 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 10:05:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:05:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:05:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:05:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:05:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:05:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:05:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:05:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:05:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:05:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:05:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:05:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:05:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:05:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:05:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:05:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:05:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:05:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:05:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:05:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:05:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:05:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:05:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:05:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:05:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:05:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:05:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:05:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=501302) [2026-01-26 10:05:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=501302) [2026-01-26 10:05:12] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=501302) [2026-01-26 10:05:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=501302) [2026-01-26 10:05:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=501302) [2026-01-26 10:05:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=501302) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=501302) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.41it/s]
(EngineCore_DP0 pid=501302) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.10it/s]
(EngineCore_DP0 pid=501302) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
(EngineCore_DP0 pid=501302) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=501302) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  7.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:04,  7.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:04,  7.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:04,  7.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  7.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  7.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:01<00:03,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  8.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:03,  7.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:03,  7.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:04,  5.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:02<00:06,  3.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:02<00:05,  3.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:02<00:07,  2.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:03<00:05,  3.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:03<00:04,  4.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:03<00:03,  5.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:03<00:03,  5.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:03<00:02,  6.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:03<00:02,  6.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:03<00:02,  6.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:03<00:01,  7.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:04<00:01,  7.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:04<00:01,  7.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:04<00:01,  7.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:04<00:01,  7.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:04<00:01,  7.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:05<00:02,  3.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:05<00:01,  3.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:05<00:01,  3.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:06<00:01,  3.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:06<00:00,  4.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:06<00:00,  4.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:06<00:00,  5.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:06<00:00,  5.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:06<00:00,  5.44it/s]
(EngineCore_DP0 pid=501302) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:05,  6.14it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:04,  7.01it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:04,  7.51it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:04,  7.73it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  7.96it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.04it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  8.12it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:01<00:03,  7.66it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:04,  5.71it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:06,  3.98it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:05,  4.43it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:02<00:04,  4.72it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:02<00:05,  3.79it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:02<00:04,  4.30it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:02<00:03,  5.01it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:02<00:03,  5.66it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:03<00:02,  6.19it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:03<00:02,  6.71it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:03<00:02,  7.09it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:03<00:02,  7.38it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:03<00:01,  7.62it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:03<00:01,  7.80it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:03<00:01,  7.96it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:03<00:01,  7.99it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:03<00:01,  8.06it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:04<00:01,  8.11it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:04<00:00,  8.18it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:04<00:01,  6.88it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:04<00:01,  5.46it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:05<00:01,  4.47it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:05<00:00,  4.49it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:05<00:00,  3.61it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:05<00:00,  4.34it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:05<00:00,  5.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:05<00:00,  5.82it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:05<00:00,  5.85it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 2785.80it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:05<23:47,  5.60s/it, est. speed input: 2.86 toks/s, output: 45.72 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:05<00:17, 11.85it/s, est. speed input: 134.65 toks/s, output: 2154.36 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:05<00:04, 31.87it/s, est. speed input: 297.05 toks/s, output: 4752.70 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:05<00:01, 53.69it/s, est. speed input: 428.76 toks/s, output: 6860.13 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:06<00:00, 75.08it/s, est. speed input: 529.15 toks/s, output: 8466.45 toks/s]
Processed prompts:  93%|█████████▎| 239/256 [00:06<00:00, 92.71it/s, est. speed input: 611.75 toks/s, output: 9788.06 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 92.71it/s, est. speed input: 633.79 toks/s, output: 10140.69 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 39.61it/s, est. speed input: 633.79 toks/s, output: 10140.69 toks/s]
[rank0]:[W126 10:05:55.257607131 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 10:05:58
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Qwen2.5-7B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:06:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:06:08 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=502410) WARNING 01-26 10:06:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=502410) WARNING 01-26 10:06:28 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.99 requests/s, 8701.43 total tokens/s, 8189.58 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-26 10:06:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:06:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:06:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:06:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:06:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:06:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:06:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:06:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:06:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:06:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:06:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:06:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:06:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:06:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:06:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:06:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:06:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:06:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:06:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:06:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:06:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:06:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:06:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:06:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:06:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:06:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:06:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:06:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=502410) [2026-01-26 10:06:18] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=502410) [2026-01-26 10:06:18] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=502410) [2026-01-26 10:06:18] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=502410) [2026-01-26 10:06:18] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=502410) [2026-01-26 10:06:18] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=502410) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=502410) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.41it/s]
(EngineCore_DP0 pid=502410) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.11it/s]
(EngineCore_DP0 pid=502410) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]
(EngineCore_DP0 pid=502410) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=502410) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:09,  5.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:22,  2.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:01<00:18,  2.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:01<00:20,  2.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:01<00:14,  3.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:01<00:11,  3.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:02<00:09,  4.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:02<00:07,  5.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:02<00:06,  6.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:02<00:06,  6.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:02<00:05,  6.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:02<00:05,  7.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:02<00:05,  7.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:02<00:04,  7.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:03<00:04,  7.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:03<00:04,  7.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:03<00:04,  7.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:03<00:09,  3.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:04<00:08,  3.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:04<00:11,  2.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:04<00:09,  3.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:05<00:07,  4.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:05<00:05,  4.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:05<00:04,  5.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:05<00:04,  6.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:05<00:03,  6.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:05<00:03,  6.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:05<00:03,  7.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:05<00:03,  7.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:06<00:02,  7.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:06<00:02,  7.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:06<00:02,  7.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:06<00:02,  7.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:06<00:02,  6.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:07<00:04,  3.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:07<00:03,  3.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:07<00:04,  2.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:08<00:03,  3.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:08<00:02,  4.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:08<00:02,  4.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:08<00:01,  5.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:08<00:01,  6.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:08<00:00, 14.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:08<00:00,  5.69it/s]
(EngineCore_DP0 pid=502410) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:13,  3.60it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:16,  2.88it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:14,  3.42it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:01<00:11,  4.21it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:01<00:09,  5.07it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:01<00:07,  5.82it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:01<00:06,  6.42it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:01<00:06,  6.86it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:05,  7.18it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:05,  7.42it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:05,  7.64it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:02<00:05,  7.74it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:02<00:04,  7.75it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:02<00:04,  7.77it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:02<00:04,  7.84it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:02<00:05,  6.41it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:03<00:07,  4.34it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:03<00:07,  4.45it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:03<00:11,  2.91it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:04<00:08,  3.59it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:04<00:06,  4.32it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:04<00:05,  5.03it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:04<00:04,  5.68it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:04<00:04,  6.24it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:04<00:03,  6.71it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:04<00:03,  7.11it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:04<00:03,  7.33it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:04<00:03,  7.55it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:05<00:02,  7.79it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:05<00:02,  7.97it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:05<00:02,  8.06it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:05<00:02,  7.48it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:05<00:02,  6.56it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:06<00:04,  3.82it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:06<00:03,  4.02it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:06<00:04,  3.44it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:07<00:04,  3.29it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:07<00:03,  4.01it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:07<00:02,  4.69it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:07<00:02,  5.38it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:07<00:01,  6.02it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:07<00:01,  6.54it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:07<00:01,  6.86it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:08<00:00,  7.03it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:08<00:00,  7.32it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:08<00:00,  7.62it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:08<00:00,  7.77it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:08<00:00,  7.83it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:08<00:00,  7.88it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:08<00:00,  6.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:09<00:00,  3.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:09<00:00,  5.45it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  54%|█████▍    | 279/512 [00:00<00:00, 2788.95it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 2832.31it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:11<1:40:02, 11.75s/it, est. speed input: 1.36 toks/s, output: 21.79 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:11<02:02,  3.90it/s, est. speed input: 44.33 toks/s, output: 709.32 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:12<00:31, 13.45it/s, est. speed input: 120.78 toks/s, output: 1932.45 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:12<00:15, 24.70it/s, est. speed input: 185.52 toks/s, output: 2968.33 toks/s]
Processed prompts:  40%|████      | 205/512 [00:12<00:06, 44.34it/s, est. speed input: 267.40 toks/s, output: 4278.42 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:12<00:03, 71.51it/s, est. speed input: 352.68 toks/s, output: 5642.86 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:12<00:01, 104.17it/s, est. speed input: 432.54 toks/s, output: 6920.70 toks/s]
Processed prompts:  76%|███████▋  | 391/512 [00:12<00:00, 136.07it/s, est. speed input: 496.20 toks/s, output: 7939.17 toks/s]
Processed prompts:  87%|████████▋ | 443/512 [00:12<00:00, 162.10it/s, est. speed input: 554.59 toks/s, output: 8873.52 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [00:13<00:00, 157.81it/s, est. speed input: 596.65 toks/s, output: 9546.36 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:15<00:00, 157.81it/s, est. speed input: 517.75 toks/s, output: 8284.07 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:15<00:00, 32.36it/s, est. speed input: 517.75 toks/s, output: 8284.07 toks/s] 
[rank0]:[W126 10:07:21.198930748 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-26 10:50:19
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-INT8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Qwen2.5-14B-INT8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:50:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:50:30 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=547557) WARNING 01-26 10:50:37 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=547557) WARNING 01-26 10:51:06 [backends.py:609] Failed to read file <frozen os>
Throughput: 5.63 requests/s, 1530.16 total tokens/s, 1440.15 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-26 10:50:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:50:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:50:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 10:50:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:50:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:50:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:50:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:50:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:50:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:50:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:50:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:50:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:50:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:50:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:50:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:50:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:50:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 10:50:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:50:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:50:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:50:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:50:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:50:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:50:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:50:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:50:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:50:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:50:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=547557) [2026-01-26 10:50:38] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=547557) [2026-01-26 10:50:38] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=547557) [2026-01-26 10:50:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=547557) [2026-01-26 10:50:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=547557) [2026-01-26 10:50:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=547557) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=547557) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.08s/it]
(EngineCore_DP0 pid=547557) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.07s/it]
(EngineCore_DP0 pid=547557) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:02,  2.81s/it]
(EngineCore_DP0 pid=547557) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.28s/it]
(EngineCore_DP0 pid=547557) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.36s/it]
(EngineCore_DP0 pid=547557) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=547557) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:08,  2.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:05,  3.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:01<00:08,  1.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:02<00:09,  1.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:02<00:08,  1.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:02<00:05,  2.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:03<00:04,  2.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:03<00:03,  3.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:03<00:02,  3.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:03<00:02,  3.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:04<00:02,  3.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:04<00:02,  2.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:05<00:03,  1.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:05<00:02,  2.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:05<00:01,  2.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:06<00:00,  3.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:06<00:00,  3.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:06<00:00,  3.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:06<00:00,  3.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:06<00:00,  2.77it/s]
(EngineCore_DP0 pid=547557) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:02,  4.29it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:02,  3.99it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:02,  2.84it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:01<00:02,  3.26it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:01<00:02,  2.78it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  7.00it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2576.35it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:07<07:35,  7.24s/it, est. speed input: 2.21 toks/s, output: 35.37 toks/s]
Processed prompts:   3%|▎         | 2/64 [00:07<03:15,  3.16s/it, est. speed input: 4.24 toks/s, output: 67.88 toks/s]
Processed prompts:  52%|█████▏    | 33/64 [00:07<00:03,  8.25it/s, est. speed input: 68.71 toks/s, output: 1099.29 toks/s]
Processed prompts:  73%|███████▎  | 47/64 [00:09<00:01,  8.94it/s, est. speed input: 83.19 toks/s, output: 1331.03 toks/s]
Processed prompts:  84%|████████▍ | 54/64 [00:10<00:01,  7.80it/s, est. speed input: 83.45 toks/s, output: 1335.14 toks/s]
Processed prompts:  92%|█████████▏| 59/64 [00:10<00:00,  8.11it/s, est. speed input: 86.93 toks/s, output: 1390.94 toks/s]
Processed prompts:  98%|█████████▊| 63/64 [00:11<00:00,  8.24it/s, est. speed input: 89.16 toks/s, output: 1426.57 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:11<00:00,  8.24it/s, est. speed input: 90.22 toks/s, output: 1443.51 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:11<00:00,  5.64it/s, est. speed input: 90.22 toks/s, output: 1443.51 toks/s]
[rank0]:[W126 10:51:52.809749035 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 10:51:55
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-INT8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Qwen2.5-14B-INT8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:52:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:52:04 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=549130) WARNING 01-26 10:52:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=549130) WARNING 01-26 10:52:38 [backends.py:609] Failed to read file <frozen os>
Throughput: 8.77 requests/s, 2384.21 total tokens/s, 2243.97 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 10:52:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:52:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:52:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 10:52:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:52:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:52:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:52:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:52:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:52:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:52:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:52:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:52:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:52:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:52:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:52:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:52:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:52:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 10:52:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:52:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:52:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:52:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:52:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:52:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:52:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:52:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:52:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:52:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:52:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=549130) [2026-01-26 10:52:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=549130) [2026-01-26 10:52:12] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=549130) [2026-01-26 10:52:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=549130) [2026-01-26 10:52:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=549130) [2026-01-26 10:52:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=549130) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=549130) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.91s/it]
(EngineCore_DP0 pid=549130) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.89s/it]
(EngineCore_DP0 pid=549130) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:02,  2.71s/it]
(EngineCore_DP0 pid=549130) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.46s/it]
(EngineCore_DP0 pid=549130) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.42s/it]
(EngineCore_DP0 pid=549130) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=549130) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:16,  2.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:11,  2.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:09,  3.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:01<00:07,  3.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:01<00:07,  4.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:01<00:06,  4.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:06,  4.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:02<00:10,  2.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:03<00:13,  1.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:03<00:10,  2.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:03<00:08,  2.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:04<00:07,  3.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:04<00:06,  3.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:04<00:05,  3.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:04<00:04,  4.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:04<00:04,  4.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:05<00:07,  2.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:06<00:08,  1.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:06<00:06,  2.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:06<00:05,  2.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:07<00:04,  3.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:07<00:03,  3.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:07<00:03,  3.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:07<00:02,  4.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:07<00:02,  4.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:08<00:02,  3.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:08<00:02,  2.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:09<00:03,  2.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:09<00:02,  2.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:10<00:01,  2.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:10<00:01,  3.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:10<00:00,  3.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:10<00:00,  3.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:10<00:00,  4.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:11<00:00,  3.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:11<00:00,  3.12it/s]
(EngineCore_DP0 pid=549130) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:12,  1.43it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:01<00:08,  2.06it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:01<00:01, 10.06it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:01,  6.58it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:02<00:01,  4.70it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:03<00:01,  3.74it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:03<00:00,  3.87it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:03<00:00,  4.03it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:04<00:00,  4.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:04<00:00,  4.33it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:04<00:00,  4.41it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2647.91it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:08<17:42,  8.37s/it, est. speed input: 1.91 toks/s, output: 30.60 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:08<00:10,  7.89it/s, est. speed input: 89.75 toks/s, output: 1436.01 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:09<00:06,  9.97it/s, est. speed input: 109.86 toks/s, output: 1757.78 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:10<00:06,  8.91it/s, est. speed input: 108.76 toks/s, output: 1740.16 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:11<00:05,  8.94it/s, est. speed input: 110.92 toks/s, output: 1774.67 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:11<00:05,  8.90it/s, est. speed input: 112.00 toks/s, output: 1791.97 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:11<00:04,  9.57it/s, est. speed input: 114.61 toks/s, output: 1833.83 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:12<00:04,  9.16it/s, est. speed input: 114.75 toks/s, output: 1836.02 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:12<00:03, 10.30it/s, est. speed input: 117.45 toks/s, output: 1879.14 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:12<00:02, 11.65it/s, est. speed input: 120.09 toks/s, output: 1921.41 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:12<00:02, 10.56it/s, est. speed input: 120.32 toks/s, output: 1925.18 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:13<00:02, 11.29it/s, est. speed input: 121.66 toks/s, output: 1946.62 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:13<00:01, 13.30it/s, est. speed input: 124.21 toks/s, output: 1987.29 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:13<00:02, 11.52it/s, est. speed input: 124.17 toks/s, output: 1986.67 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:13<00:01, 13.39it/s, est. speed input: 126.40 toks/s, output: 2022.43 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:13<00:01, 15.09it/s, est. speed input: 128.61 toks/s, output: 2057.68 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:13<00:00, 18.28it/s, est. speed input: 131.94 toks/s, output: 2110.99 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:14<00:00, 15.30it/s, est. speed input: 132.75 toks/s, output: 2123.98 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:14<00:00, 15.73it/s, est. speed input: 133.94 toks/s, output: 2142.98 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:14<00:00, 18.02it/s, est. speed input: 136.23 toks/s, output: 2179.66 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:14<00:00, 21.15it/s, est. speed input: 139.39 toks/s, output: 2230.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:14<00:00, 21.15it/s, est. speed input: 140.73 toks/s, output: 2251.72 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:14<00:00,  8.80it/s, est. speed input: 140.73 toks/s, output: 2251.72 toks/s]
[rank0]:[W126 10:53:30.055369234 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 10:53:34
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-INT8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Qwen2.5-14B-INT8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:53:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:53:41 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=550711) WARNING 01-26 10:53:59 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=550711) WARNING 01-26 10:54:17 [backends.py:609] Failed to read file <frozen os>
Throughput: 8.92 requests/s, 2425.35 total tokens/s, 2282.68 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 10:53:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:53:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:53:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 10:53:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:53:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:53:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:53:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:53:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:53:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:53:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:53:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:53:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:53:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:53:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:53:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:53:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:53:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 10:53:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:53:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:53:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:53:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:53:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:53:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:53:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:53:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:53:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:53:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:53:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W126 10:53:59.796416406 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=550711) [2026-01-26 10:54:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=550711) [2026-01-26 10:54:00] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=550711) [2026-01-26 10:54:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=550711) [2026-01-26 10:54:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=550711) [2026-01-26 10:54:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=550711) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=550711) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.03it/s]
(EngineCore_DP0 pid=550711) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.01s/it]
(EngineCore_DP0 pid=550711) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.37it/s]
(EngineCore_DP0 pid=550711) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.21it/s]
(EngineCore_DP0 pid=550711) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.18it/s]
(EngineCore_DP0 pid=550711) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=550711) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:28,  1.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:01<00:26,  1.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:01<00:17,  1.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:01<00:12,  2.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:02<00:10,  3.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:02<00:08,  3.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:02<00:07,  3.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:02<00:06,  4.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:03<00:06,  4.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:03<00:10,  2.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:04<00:12,  1.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:04<00:10,  2.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:05<00:08,  2.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:05<00:07,  3.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:05<00:06,  3.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:05<00:05,  3.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:05<00:04,  4.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:06<00:04,  4.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:06<00:06,  2.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:07<00:08,  1.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:08<00:01,  5.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:08<00:01,  4.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:09<00:01,  3.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:09<00:01,  3.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:09<00:01,  3.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:10<00:00,  4.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:10<00:00,  4.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:10<00:00,  4.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:10<00:00,  4.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:10<00:00,  3.33it/s]
(EngineCore_DP0 pid=550711) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:15,  2.24it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:15,  2.20it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:01<00:11,  2.69it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:01<00:11,  2.64it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:01<00:11,  2.72it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:02<00:09,  3.19it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:02<00:07,  3.60it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:02<00:06,  3.92it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:02<00:06,  4.19it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:02<00:05,  4.37it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:03<00:05,  4.49it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:03<00:05,  3.91it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:04<00:07,  2.87it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:04<00:06,  3.07it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:04<00:08,  2.49it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:05<00:06,  2.91it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:05<00:05,  3.32it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:05<00:04,  3.67it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:05<00:04,  3.94it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:05<00:03,  4.18it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:06<00:03,  4.39it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:06<00:02,  4.53it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:06<00:02,  4.30it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:07<00:03,  2.91it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:07<00:03,  2.95it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:08<00:03,  2.60it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:08<00:02,  2.90it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:08<00:02,  3.30it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:08<00:01,  3.64it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:08<00:01,  3.94it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:09<00:00,  4.18it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:09<00:00,  4.38it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:09<00:00,  4.52it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:09<00:00,  3.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:10<00:00,  3.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:10<00:00,  3.38it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 2647.48it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:08<35:05,  8.26s/it, est. speed input: 1.94 toks/s, output: 31.01 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:08<00:27,  7.45it/s, est. speed input: 86.47 toks/s, output: 1383.53 toks/s]
Processed prompts:  21%|██        | 53/256 [00:09<00:27,  7.36it/s, est. speed input: 88.22 toks/s, output: 1411.55 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:10<00:29,  6.87it/s, est. speed input: 86.90 toks/s, output: 1390.41 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:10<00:31,  6.24it/s, est. speed input: 84.85 toks/s, output: 1357.67 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:11<00:29,  6.72it/s, est. speed input: 86.87 toks/s, output: 1389.85 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:11<00:31,  6.16it/s, est. speed input: 85.90 toks/s, output: 1374.33 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:11<00:28,  6.78it/s, est. speed input: 87.59 toks/s, output: 1401.42 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:11<00:24,  7.66it/s, est. speed input: 89.44 toks/s, output: 1431.03 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:12<00:28,  6.71it/s, est. speed input: 88.88 toks/s, output: 1422.08 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:12<00:24,  7.65it/s, est. speed input: 90.45 toks/s, output: 1447.27 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:12<00:20,  8.91it/s, est. speed input: 92.19 toks/s, output: 1475.11 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:12<00:25,  7.11it/s, est. speed input: 91.53 toks/s, output: 1464.48 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:13<00:21,  8.29it/s, est. speed input: 93.03 toks/s, output: 1488.46 toks/s]
Processed prompts:  30%|███       | 78/256 [00:13<00:18,  9.77it/s, est. speed input: 94.68 toks/s, output: 1514.80 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:13<00:23,  7.53it/s, est. speed input: 94.12 toks/s, output: 1505.95 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:13<00:19,  9.05it/s, est. speed input: 95.69 toks/s, output: 1531.01 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:13<00:16, 10.18it/s, est. speed input: 97.05 toks/s, output: 1552.81 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:13<00:14, 11.51it/s, est. speed input: 98.51 toks/s, output: 1576.18 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:14<00:13, 12.28it/s, est. speed input: 99.82 toks/s, output: 1597.14 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:14<00:19,  8.60it/s, est. speed input: 99.30 toks/s, output: 1588.74 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:14<00:16, 10.16it/s, est. speed input: 100.72 toks/s, output: 1611.52 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:14<00:13, 11.59it/s, est. speed input: 102.10 toks/s, output: 1633.64 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:14<00:13, 12.20it/s, est. speed input: 103.27 toks/s, output: 1652.28 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:15<00:12, 12.78it/s, est. speed input: 104.44 toks/s, output: 1671.05 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:15<00:02, 59.53it/s, est. speed input: 129.02 toks/s, output: 2064.37 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:15<00:02, 43.01it/s, est. speed input: 134.68 toks/s, output: 2154.81 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:17<00:09, 12.38it/s, est. speed input: 127.55 toks/s, output: 2040.77 toks/s]
Processed prompts:  55%|█████▌    | 141/256 [00:18<00:13,  8.77it/s, est. speed input: 123.76 toks/s, output: 1980.20 toks/s]
Processed prompts:  57%|█████▋    | 145/256 [00:18<00:12,  8.85it/s, est. speed input: 124.32 toks/s, output: 1989.08 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:18<00:11,  9.49it/s, est. speed input: 125.47 toks/s, output: 2007.57 toks/s]
Processed prompts:  59%|█████▉    | 151/256 [00:19<00:10, 10.21it/s, est. speed input: 126.64 toks/s, output: 2026.29 toks/s]
Processed prompts:  60%|█████▉    | 153/256 [00:19<00:10,  9.79it/s, est. speed input: 126.67 toks/s, output: 2026.74 toks/s]
Processed prompts:  61%|██████    | 155/256 [00:19<00:10,  9.47it/s, est. speed input: 126.75 toks/s, output: 2027.99 toks/s]
Processed prompts:  61%|██████▏   | 157/256 [00:19<00:09,  9.91it/s, est. speed input: 127.30 toks/s, output: 2036.82 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:19<00:08, 11.04it/s, est. speed input: 128.19 toks/s, output: 2051.06 toks/s]
Processed prompts:  63%|██████▎   | 161/256 [00:19<00:08, 11.78it/s, est. speed input: 128.93 toks/s, output: 2062.80 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:20<00:06, 14.25it/s, est. speed input: 130.49 toks/s, output: 2087.89 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:20<00:05, 15.05it/s, est. speed input: 131.36 toks/s, output: 2101.82 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:20<00:06, 14.01it/s, est. speed input: 131.83 toks/s, output: 2109.36 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:20<00:07, 12.16it/s, est. speed input: 131.96 toks/s, output: 2111.40 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:20<00:07, 10.53it/s, est. speed input: 131.88 toks/s, output: 2110.10 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:20<00:06, 11.77it/s, est. speed input: 132.65 toks/s, output: 2122.44 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:21<00:06, 12.89it/s, est. speed input: 133.42 toks/s, output: 2134.76 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:21<00:04, 17.87it/s, est. speed input: 135.69 toks/s, output: 2170.97 toks/s]
Processed prompts:  72%|███████▏  | 185/256 [00:21<00:02, 24.61it/s, est. speed input: 138.75 toks/s, output: 2219.93 toks/s]
Processed prompts:  74%|███████▍  | 189/256 [00:21<00:02, 27.75it/s, est. speed input: 141.03 toks/s, output: 2256.49 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:21<00:01, 37.88it/s, est. speed input: 145.54 toks/s, output: 2328.66 toks/s]
Processed prompts:  79%|███████▊  | 201/256 [00:21<00:01, 38.38it/s, est. speed input: 148.38 toks/s, output: 2374.12 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:22<00:04, 10.24it/s, est. speed input: 143.63 toks/s, output: 2298.04 toks/s]
Processed prompts:  82%|████████▏ | 209/256 [00:23<00:05,  8.63it/s, est. speed input: 142.30 toks/s, output: 2276.74 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:24<00:06,  6.68it/s, est. speed input: 139.67 toks/s, output: 2234.74 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:24<00:06,  6.66it/s, est. speed input: 139.24 toks/s, output: 2227.82 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:24<00:05,  7.00it/s, est. speed input: 139.25 toks/s, output: 2227.96 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:24<00:04,  7.84it/s, est. speed input: 139.68 toks/s, output: 2234.94 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:25<00:04,  8.71it/s, est. speed input: 140.12 toks/s, output: 2241.95 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:25<00:03,  9.57it/s, est. speed input: 140.55 toks/s, output: 2248.86 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:25<00:03, 10.36it/s, est. speed input: 140.98 toks/s, output: 2255.74 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:25<00:02, 10.69it/s, est. speed input: 141.29 toks/s, output: 2260.65 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:25<00:01, 14.14it/s, est. speed input: 142.85 toks/s, output: 2285.63 toks/s]
Processed prompts:  91%|█████████ | 233/256 [00:25<00:01, 16.45it/s, est. speed input: 144.05 toks/s, output: 2304.73 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:26<00:01, 17.65it/s, est. speed input: 145.10 toks/s, output: 2321.58 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:26<00:01, 15.95it/s, est. speed input: 145.40 toks/s, output: 2326.45 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:26<00:01, 13.10it/s, est. speed input: 145.32 toks/s, output: 2325.10 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:26<00:01, 12.39it/s, est. speed input: 145.51 toks/s, output: 2328.09 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:26<00:01, 11.91it/s, est. speed input: 145.69 toks/s, output: 2331.07 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:26<00:00, 16.70it/s, est. speed input: 147.45 toks/s, output: 2359.12 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:27<00:00, 15.52it/s, est. speed input: 147.77 toks/s, output: 2364.27 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:27<00:00, 15.52it/s, est. speed input: 151.07 toks/s, output: 2417.07 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:27<00:00,  9.44it/s, est. speed input: 151.07 toks/s, output: 2417.07 toks/s]
[rank0]:[W126 10:55:27.473031374 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 10:55:30
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-INT8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/json/Qwen2.5-14B-INT8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:55:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:55:39 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=552561) WARNING 01-26 10:55:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=552561) WARNING 01-26 10:56:15 [backends.py:609] Failed to read file <frozen os>
Throughput: 5.03 requests/s, 1368.21 total tokens/s, 1287.73 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-26 10:55:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:55:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:55:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 10:55:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:55:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:55:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:55:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:55:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:55:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:55:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:55:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:55:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:55:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:55:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:55:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:55:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:55:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-26 10:55:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:55:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:55:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:55:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:55:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-26 10:55:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-26 10:55:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:55:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:55:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:55:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:55:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W126 10:55:55.201427797 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=552561) [2026-01-26 10:55:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=552561) [2026-01-26 10:55:56] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=552561) [2026-01-26 10:55:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=552561) [2026-01-26 10:55:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=552561) [2026-01-26 10:55:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=552561) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=552561) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.14s/it]
(EngineCore_DP0 pid=552561) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.11s/it]
(EngineCore_DP0 pid=552561) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.27it/s]
(EngineCore_DP0 pid=552561) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.15it/s]
(EngineCore_DP0 pid=552561) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.10it/s]
(EngineCore_DP0 pid=552561) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=552561) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:43,  1.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:01<00:36,  1.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:01<00:24,  1.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:01<00:18,  2.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:02<00:15,  3.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:02<00:12,  3.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:02<00:11,  3.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:02<00:10,  4.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:03<00:11,  3.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:03<00:16,  2.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:04<00:21,  1.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:04<00:16,  2.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:05<00:13,  2.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:05<00:11,  3.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:05<00:10,  3.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:05<00:09,  3.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:05<00:08,  4.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:06<00:09,  3.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:06<00:12,  2.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:07<00:15,  2.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:07<00:13,  2.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:08<00:10,  2.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:08<00:08,  3.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:08<00:07,  3.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:08<00:06,  3.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:09<00:06,  4.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:09<00:05,  4.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:10<00:09,  2.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:10<00:08,  2.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:10<00:09,  2.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:11<00:07,  2.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:11<00:06,  3.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:11<00:05,  3.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:11<00:04,  3.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:12<00:02,  5.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:12<00:02,  5.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:13<00:01,  5.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:13<00:01,  4.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:13<00:01,  4.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:13<00:01,  4.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:13<00:01,  4.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:14<00:01,  4.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:14<00:01,  2.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:15<00:01,  2.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:15<00:00,  2.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:16<00:00,  2.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:16<00:00,  3.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:16<00:00,  3.11it/s]
(EngineCore_DP0 pid=552561) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:21,  2.34it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:14,  3.28it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:12,  3.85it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:01<00:12,  3.83it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:01<00:15,  3.01it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:01<00:14,  3.19it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:02<00:18,  2.39it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:02<00:15,  2.85it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:02<00:12,  3.28it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:03<00:11,  3.65it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:03<00:10,  3.98it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:03<00:09,  4.22it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:03<00:08,  4.45it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:03<00:08,  4.58it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:04<00:09,  3.84it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:04<00:11,  3.07it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:05<00:10,  3.22it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:05<00:12,  2.63it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:05<00:11,  2.84it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:06<00:09,  3.26it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:06<00:08,  3.63it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:06<00:07,  3.92it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:06<00:06,  4.19it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:06<00:06,  4.39it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:07<00:05,  4.49it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:07<00:06,  3.80it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:07<00:07,  3.11it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:08<00:07,  3.25it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:08<00:08,  2.68it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:08<00:07,  2.98it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:09<00:05,  3.38it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:09<00:05,  3.75it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:09<00:04,  4.07it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:09<00:03,  4.31it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:09<00:03,  4.45it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:10<00:03,  4.61it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:10<00:03,  4.28it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:11<00:04,  2.89it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:11<00:04,  2.96it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:11<00:04,  2.61it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:12<00:03,  2.82it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:12<00:02,  3.25it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:12<00:02,  3.64it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:12<00:01,  3.95it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:12<00:01,  4.20it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:13<00:01,  4.39it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:13<00:00,  4.54it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:13<00:00,  3.99it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:14<00:00,  3.07it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:14<00:00,  3.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:14<00:00,  2.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:14<00:00,  3.44it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  55%|█████▍    | 281/512 [00:00<00:00, 2807.73it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 2849.29it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<55:56,  6.57s/it, est. speed input: 2.44 toks/s, output: 38.98 toks/s]
Processed prompts:   4%|▍         | 20/512 [00:07<02:11,  3.75it/s, est. speed input: 44.32 toks/s, output: 709.15 toks/s]
Processed prompts:   4%|▍         | 22/512 [00:08<02:19,  3.51it/s, est. speed input: 43.76 toks/s, output: 700.09 toks/s]
Processed prompts:   5%|▍         | 24/512 [00:08<02:13,  3.64it/s, est. speed input: 45.29 toks/s, output: 724.65 toks/s]
Processed prompts:   5%|▌         | 26/512 [00:08<02:08,  3.79it/s, est. speed input: 46.68 toks/s, output: 746.87 toks/s]
Processed prompts:   5%|▌         | 28/512 [00:09<02:03,  3.93it/s, est. speed input: 47.93 toks/s, output: 766.90 toks/s]
Processed prompts:   8%|▊         | 41/512 [00:09<00:50,  9.36it/s, est. speed input: 67.48 toks/s, output: 1079.61 toks/s]
Processed prompts:   9%|▉         | 45/512 [00:09<00:41, 11.24it/s, est. speed input: 73.25 toks/s, output: 1171.92 toks/s]
Processed prompts:   9%|▉         | 48/512 [00:10<00:49,  9.34it/s, est. speed input: 74.00 toks/s, output: 1184.00 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:11<01:11,  6.47it/s, est. speed input: 71.41 toks/s, output: 1142.50 toks/s]
Processed prompts:  10%|█         | 52/512 [00:12<01:36,  4.76it/s, est. speed input: 68.62 toks/s, output: 1097.89 toks/s]
Processed prompts:  10%|█         | 53/512 [00:12<01:47,  4.27it/s, est. speed input: 67.60 toks/s, output: 1081.60 toks/s]
Processed prompts:  11%|█         | 54/512 [00:12<01:44,  4.36it/s, est. speed input: 67.80 toks/s, output: 1084.86 toks/s]
Processed prompts:  11%|█         | 55/512 [00:13<02:01,  3.77it/s, est. speed input: 66.73 toks/s, output: 1067.64 toks/s]
Processed prompts:  11%|█         | 57/512 [00:13<01:37,  4.68it/s, est. speed input: 68.07 toks/s, output: 1089.08 toks/s]
Processed prompts:  11%|█▏        | 58/512 [00:13<01:35,  4.74it/s, est. speed input: 68.24 toks/s, output: 1091.87 toks/s]
Processed prompts:  12%|█▏        | 60/512 [00:13<01:12,  6.27it/s, est. speed input: 69.95 toks/s, output: 1119.13 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:13<00:55,  8.12it/s, est. speed input: 71.75 toks/s, output: 1147.96 toks/s]
Processed prompts:  12%|█▎        | 64/512 [00:13<00:46,  9.72it/s, est. speed input: 73.42 toks/s, output: 1174.75 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:14<01:14,  6.00it/s, est. speed input: 72.54 toks/s, output: 1160.71 toks/s]
Processed prompts:  13%|█▎        | 68/512 [00:14<00:59,  7.48it/s, est. speed input: 74.13 toks/s, output: 1186.01 toks/s]
Processed prompts:  14%|█▍        | 72/512 [00:14<00:39, 11.25it/s, est. speed input: 77.67 toks/s, output: 1242.78 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:15<00:40, 10.87it/s, est. speed input: 78.76 toks/s, output: 1260.08 toks/s]
Processed prompts:  15%|█▍        | 76/512 [00:15<00:46,  9.45it/s, est. speed input: 79.36 toks/s, output: 1269.74 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:16<01:46,  4.09it/s, est. speed input: 75.42 toks/s, output: 1206.79 toks/s]
Processed prompts:  15%|█▌        | 79/512 [00:17<02:05,  3.46it/s, est. speed input: 74.08 toks/s, output: 1185.27 toks/s]
Processed prompts:  16%|█▌        | 80/512 [00:17<02:08,  3.37it/s, est. speed input: 73.59 toks/s, output: 1177.40 toks/s]
Processed prompts:  16%|█▌        | 81/512 [00:17<02:08,  3.36it/s, est. speed input: 73.24 toks/s, output: 1171.84 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:18<02:27,  2.91it/s, est. speed input: 72.13 toks/s, output: 1154.05 toks/s]
Processed prompts:  16%|█▌        | 83/512 [00:18<02:11,  3.25it/s, est. speed input: 72.22 toks/s, output: 1155.57 toks/s]
Processed prompts:  16%|█▋        | 84/512 [00:18<01:59,  3.58it/s, est. speed input: 72.31 toks/s, output: 1156.91 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:18<01:27,  4.89it/s, est. speed input: 73.20 toks/s, output: 1171.12 toks/s]
Processed prompts:  17%|█▋        | 87/512 [00:19<01:26,  4.89it/s, est. speed input: 73.25 toks/s, output: 1172.05 toks/s]
Processed prompts:  17%|█▋        | 88/512 [00:19<01:21,  5.19it/s, est. speed input: 73.48 toks/s, output: 1175.74 toks/s]
Processed prompts:  17%|█▋        | 89/512 [00:19<01:17,  5.48it/s, est. speed input: 73.73 toks/s, output: 1179.60 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:19<01:10,  5.95it/s, est. speed input: 74.06 toks/s, output: 1184.92 toks/s]
Processed prompts:  18%|█▊        | 92/512 [00:19<00:55,  7.56it/s, est. speed input: 75.04 toks/s, output: 1200.58 toks/s]
Processed prompts:  19%|█▊        | 95/512 [00:19<00:40, 10.21it/s, est. speed input: 76.76 toks/s, output: 1228.19 toks/s]
Processed prompts:  19%|█▉        | 97/512 [00:20<00:48,  8.55it/s, est. speed input: 77.14 toks/s, output: 1234.30 toks/s]
Processed prompts:  19%|█▉        | 99/512 [00:20<00:41,  9.95it/s, est. speed input: 78.24 toks/s, output: 1251.81 toks/s]
Processed prompts:  20%|█▉        | 101/512 [00:20<00:43,  9.54it/s, est. speed input: 78.93 toks/s, output: 1262.80 toks/s]
Processed prompts:  20%|██        | 103/512 [00:20<00:37, 10.80it/s, est. speed input: 79.98 toks/s, output: 1279.72 toks/s]
Processed prompts:  21%|██        | 105/512 [00:20<00:39, 10.28it/s, est. speed input: 80.69 toks/s, output: 1291.04 toks/s]
Processed prompts:  21%|██        | 107/512 [00:22<01:48,  3.72it/s, est. speed input: 77.30 toks/s, output: 1236.77 toks/s]
Processed prompts:  21%|██        | 108/512 [00:22<01:54,  3.54it/s, est. speed input: 76.82 toks/s, output: 1229.11 toks/s]
Processed prompts:  21%|██▏       | 109/512 [00:22<02:01,  3.32it/s, est. speed input: 76.26 toks/s, output: 1220.16 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:23<02:25,  2.77it/s, est. speed input: 75.09 toks/s, output: 1201.44 toks/s]
Processed prompts:  22%|██▏       | 111/512 [00:23<02:16,  2.94it/s, est. speed input: 74.90 toks/s, output: 1198.41 toks/s]
Processed prompts:  22%|██▏       | 112/512 [00:23<02:01,  3.29it/s, est. speed input: 74.94 toks/s, output: 1199.11 toks/s]
Processed prompts:  22%|██▏       | 113/512 [00:24<01:47,  3.71it/s, est. speed input: 75.06 toks/s, output: 1200.95 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:24<01:42,  3.87it/s, est. speed input: 75.01 toks/s, output: 1200.17 toks/s]
Processed prompts:  23%|██▎       | 116/512 [00:24<01:11,  5.51it/s, est. speed input: 75.80 toks/s, output: 1212.77 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:24<01:07,  5.85it/s, est. speed input: 76.03 toks/s, output: 1216.42 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:24<01:03,  6.22it/s, est. speed input: 76.27 toks/s, output: 1220.29 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:24<01:10,  5.59it/s, est. speed input: 76.21 toks/s, output: 1219.38 toks/s]
Processed prompts:  24%|██▎       | 121/512 [00:25<00:54,  7.21it/s, est. speed input: 76.96 toks/s, output: 1231.39 toks/s]
Processed prompts:  24%|██▍       | 124/512 [00:25<00:37, 10.22it/s, est. speed input: 78.37 toks/s, output: 1253.87 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:25<00:40,  9.49it/s, est. speed input: 78.87 toks/s, output: 1261.95 toks/s]
Processed prompts:  25%|██▌       | 128/512 [00:25<00:35, 10.95it/s, est. speed input: 79.75 toks/s, output: 1275.99 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:26<00:43,  8.88it/s, est. speed input: 79.99 toks/s, output: 1279.89 toks/s]
Processed prompts:  26%|██▌       | 132/512 [00:26<00:42,  8.91it/s, est. speed input: 80.54 toks/s, output: 1288.56 toks/s]
Processed prompts:  26%|██▌       | 133/512 [00:26<00:42,  8.96it/s, est. speed input: 80.81 toks/s, output: 1292.96 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:26<00:59,  6.35it/s, est. speed input: 80.39 toks/s, output: 1286.22 toks/s]
Processed prompts:  26%|██▋       | 135/512 [00:27<01:37,  3.89it/s, est. speed input: 79.20 toks/s, output: 1267.26 toks/s]
Processed prompts:  27%|██▋       | 136/512 [00:27<01:59,  3.16it/s, est. speed input: 78.35 toks/s, output: 1253.56 toks/s]
Processed prompts:  27%|██▋       | 137/512 [00:28<02:16,  2.75it/s, est. speed input: 77.54 toks/s, output: 1240.61 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:28<02:13,  2.80it/s, est. speed input: 77.17 toks/s, output: 1234.70 toks/s]
Processed prompts:  27%|██▋       | 139/512 [00:28<02:09,  2.88it/s, est. speed input: 76.86 toks/s, output: 1229.82 toks/s]
Processed prompts:  27%|██▋       | 140/512 [00:29<01:48,  3.43it/s, est. speed input: 77.02 toks/s, output: 1232.31 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:29<01:35,  3.88it/s, est. speed input: 77.11 toks/s, output: 1233.73 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:29<01:30,  4.10it/s, est. speed input: 77.10 toks/s, output: 1233.61 toks/s]
Processed prompts:  28%|██▊       | 143/512 [00:29<01:33,  3.96it/s, est. speed input: 76.93 toks/s, output: 1230.83 toks/s]
Processed prompts:  28%|██▊       | 144/512 [00:29<01:27,  4.22it/s, est. speed input: 76.95 toks/s, output: 1231.18 toks/s]
Processed prompts:  28%|██▊       | 145/512 [00:30<01:25,  4.28it/s, est. speed input: 76.91 toks/s, output: 1230.48 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:30<01:14,  4.93it/s, est. speed input: 77.10 toks/s, output: 1233.60 toks/s]
Processed prompts:  29%|██▊       | 147/512 [00:30<01:03,  5.72it/s, est. speed input: 77.35 toks/s, output: 1237.59 toks/s]
Processed prompts:  29%|██▉       | 148/512 [00:30<00:58,  6.18it/s, est. speed input: 77.54 toks/s, output: 1240.65 toks/s]
Processed prompts:  29%|██▉       | 149/512 [00:30<00:58,  6.25it/s, est. speed input: 77.67 toks/s, output: 1242.70 toks/s]
Processed prompts:  29%|██▉       | 151/512 [00:30<00:41,  8.67it/s, est. speed input: 78.39 toks/s, output: 1254.23 toks/s]
Processed prompts:  30%|██▉       | 152/512 [00:30<00:45,  7.93it/s, est. speed input: 78.50 toks/s, output: 1256.07 toks/s]
Processed prompts:  30%|███       | 155/512 [00:31<00:30, 11.83it/s, est. speed input: 79.70 toks/s, output: 1275.25 toks/s]
Processed prompts:  31%|███       | 159/512 [00:31<00:23, 15.34it/s, est. speed input: 81.28 toks/s, output: 1300.55 toks/s]
Processed prompts:  31%|███▏      | 161/512 [00:31<00:24, 14.32it/s, est. speed input: 81.87 toks/s, output: 1309.98 toks/s]
Processed prompts:  32%|███▏      | 163/512 [00:31<00:37,  9.40it/s, est. speed input: 81.83 toks/s, output: 1309.23 toks/s]
Processed prompts:  32%|███▏      | 165/512 [00:33<01:40,  3.44it/s, est. speed input: 79.04 toks/s, output: 1264.65 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:33<01:56,  2.97it/s, est. speed input: 78.19 toks/s, output: 1251.08 toks/s]
Processed prompts:  33%|███▎      | 167/512 [00:34<02:17,  2.50it/s, est. speed input: 77.15 toks/s, output: 1234.37 toks/s]
Processed prompts:  33%|███▎      | 168/512 [00:34<02:06,  2.71it/s, est. speed input: 77.04 toks/s, output: 1232.69 toks/s]
Processed prompts:  33%|███▎      | 169/512 [00:35<01:57,  2.93it/s, est. speed input: 76.94 toks/s, output: 1231.09 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:35<01:40,  3.41it/s, est. speed input: 77.07 toks/s, output: 1233.12 toks/s]
Processed prompts:  33%|███▎      | 171/512 [00:35<01:33,  3.63it/s, est. speed input: 77.03 toks/s, output: 1232.51 toks/s]
Processed prompts:  34%|███▎      | 172/512 [00:35<01:20,  4.20it/s, est. speed input: 77.18 toks/s, output: 1234.89 toks/s]
Processed prompts:  34%|███▍      | 173/512 [00:35<01:22,  4.11it/s, est. speed input: 77.07 toks/s, output: 1233.17 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:36<01:08,  4.91it/s, est. speed input: 77.29 toks/s, output: 1236.70 toks/s]
Processed prompts:  34%|███▍      | 175/512 [00:36<01:01,  5.46it/s, est. speed input: 77.45 toks/s, output: 1239.23 toks/s]
Processed prompts:  35%|███▍      | 177/512 [00:36<00:46,  7.18it/s, est. speed input: 77.96 toks/s, output: 1247.43 toks/s]
Processed prompts:  35%|███▍      | 179/512 [00:36<00:42,  7.88it/s, est. speed input: 78.38 toks/s, output: 1254.07 toks/s]
Processed prompts:  35%|███▌      | 180/512 [00:36<00:44,  7.44it/s, est. speed input: 78.47 toks/s, output: 1255.54 toks/s]
Processed prompts:  35%|███▌      | 181/512 [00:36<00:46,  7.15it/s, est. speed input: 78.57 toks/s, output: 1257.14 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:36<00:43,  7.57it/s, est. speed input: 78.77 toks/s, output: 1260.33 toks/s]
Processed prompts:  36%|███▌      | 183/512 [00:37<00:41,  7.90it/s, est. speed input: 78.97 toks/s, output: 1263.47 toks/s]
Processed prompts:  36%|███▌      | 184/512 [00:37<00:44,  7.31it/s, est. speed input: 79.05 toks/s, output: 1264.79 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:37<00:42,  7.67it/s, est. speed input: 79.24 toks/s, output: 1267.78 toks/s]
Processed prompts:  37%|███▋      | 187/512 [00:37<00:39,  8.17it/s, est. speed input: 79.61 toks/s, output: 1273.80 toks/s]
Processed prompts:  37%|███▋      | 189/512 [00:37<00:32, 10.03it/s, est. speed input: 80.19 toks/s, output: 1283.05 toks/s]
Processed prompts:  37%|███▋      | 191/512 [00:37<00:26, 12.09it/s, est. speed input: 80.82 toks/s, output: 1293.04 toks/s]
Processed prompts:  38%|███▊      | 193/512 [00:38<01:00,  5.31it/s, est. speed input: 79.99 toks/s, output: 1279.86 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:38<01:10,  4.53it/s, est. speed input: 79.67 toks/s, output: 1274.74 toks/s]
Processed prompts:  38%|███▊      | 195/512 [00:39<01:26,  3.65it/s, est. speed input: 79.13 toks/s, output: 1266.16 toks/s]
Processed prompts:  38%|███▊      | 196/512 [00:39<01:37,  3.23it/s, est. speed input: 78.69 toks/s, output: 1259.09 toks/s]
Processed prompts:  38%|███▊      | 197/512 [00:40<01:40,  3.14it/s, est. speed input: 78.42 toks/s, output: 1254.71 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:40<01:26,  3.62it/s, est. speed input: 78.50 toks/s, output: 1256.07 toks/s]
Processed prompts:  39%|███▉      | 199/512 [00:40<01:22,  3.79it/s, est. speed input: 78.45 toks/s, output: 1255.21 toks/s]
Processed prompts:  41%|████      | 210/512 [00:40<00:19, 15.39it/s, est. speed input: 82.36 toks/s, output: 1317.83 toks/s]
Processed prompts:  41%|████▏     | 212/512 [00:40<00:19, 15.02it/s, est. speed input: 82.85 toks/s, output: 1325.57 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:41<00:21, 13.80it/s, est. speed input: 83.24 toks/s, output: 1331.83 toks/s]
Processed prompts:  42%|████▏     | 216/512 [00:41<00:22, 13.15it/s, est. speed input: 83.66 toks/s, output: 1338.51 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:41<00:24, 11.77it/s, est. speed input: 83.97 toks/s, output: 1343.55 toks/s]
Processed prompts:  43%|████▎     | 220/512 [00:41<00:28, 10.08it/s, est. speed input: 84.17 toks/s, output: 1346.76 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:42<00:46,  6.27it/s, est. speed input: 83.64 toks/s, output: 1338.22 toks/s]
Processed prompts:  44%|████▎     | 223/512 [00:43<01:06,  4.37it/s, est. speed input: 82.86 toks/s, output: 1325.80 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:43<01:13,  3.91it/s, est. speed input: 82.51 toks/s, output: 1320.23 toks/s]
Processed prompts:  44%|████▍     | 225/512 [00:43<01:19,  3.63it/s, est. speed input: 82.22 toks/s, output: 1315.54 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:43<01:12,  3.94it/s, est. speed input: 82.25 toks/s, output: 1315.99 toks/s]
Processed prompts:  44%|████▍     | 227/512 [00:44<01:12,  3.95it/s, est. speed input: 82.14 toks/s, output: 1314.27 toks/s]
Processed prompts:  45%|████▍     | 228/512 [00:44<01:17,  3.67it/s, est. speed input: 81.90 toks/s, output: 1310.38 toks/s]
Processed prompts:  45%|████▍     | 229/512 [00:44<01:13,  3.86it/s, est. speed input: 81.85 toks/s, output: 1309.54 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:45<01:12,  3.88it/s, est. speed input: 81.74 toks/s, output: 1307.83 toks/s]
Processed prompts:  45%|████▌     | 231/512 [00:45<01:10,  4.01it/s, est. speed input: 81.68 toks/s, output: 1306.91 toks/s]
Processed prompts:  45%|████▌     | 232/512 [00:45<01:00,  4.64it/s, est. speed input: 81.79 toks/s, output: 1308.71 toks/s]
Processed prompts:  46%|████▌     | 233/512 [00:45<00:55,  5.05it/s, est. speed input: 81.87 toks/s, output: 1309.87 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:45<00:57,  4.84it/s, est. speed input: 81.81 toks/s, output: 1308.94 toks/s]
Processed prompts:  46%|████▌     | 235/512 [00:45<00:52,  5.23it/s, est. speed input: 81.88 toks/s, output: 1310.13 toks/s]
Processed prompts:  46%|████▌     | 236/512 [00:46<00:51,  5.33it/s, est. speed input: 81.91 toks/s, output: 1310.58 toks/s]
Processed prompts:  46%|████▋     | 237/512 [00:46<01:02,  4.42it/s, est. speed input: 81.70 toks/s, output: 1307.16 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:46<00:58,  4.70it/s, est. speed input: 81.72 toks/s, output: 1307.56 toks/s]
Processed prompts:  47%|████▋     | 239/512 [00:46<00:49,  5.50it/s, est. speed input: 81.87 toks/s, output: 1309.99 toks/s]
Processed prompts:  47%|████▋     | 240/512 [00:46<00:49,  5.48it/s, est. speed input: 81.89 toks/s, output: 1310.31 toks/s]
Processed prompts:  47%|████▋     | 241/512 [00:47<00:47,  5.69it/s, est. speed input: 81.96 toks/s, output: 1311.28 toks/s]
Processed prompts:  47%|████▋     | 243/512 [00:47<00:33,  8.15it/s, est. speed input: 82.42 toks/s, output: 1318.76 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:47<00:21, 12.53it/s, est. speed input: 83.25 toks/s, output: 1331.96 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:47<00:15, 16.82it/s, est. speed input: 84.33 toks/s, output: 1349.29 toks/s]
Processed prompts:  49%|████▉     | 252/512 [00:47<00:15, 17.10it/s, est. speed input: 84.81 toks/s, output: 1356.90 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:48<00:39,  6.51it/s, est. speed input: 84.02 toks/s, output: 1344.39 toks/s]
Processed prompts:  50%|█████     | 256/512 [00:49<01:21,  3.15it/s, est. speed input: 82.16 toks/s, output: 1314.48 toks/s]
Processed prompts:  50%|█████     | 257/512 [00:50<01:21,  3.14it/s, est. speed input: 81.94 toks/s, output: 1311.11 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:50<01:21,  3.13it/s, est. speed input: 81.74 toks/s, output: 1307.86 toks/s]
Processed prompts:  51%|█████     | 259/512 [00:50<01:20,  3.16it/s, est. speed input: 81.56 toks/s, output: 1304.98 toks/s]
Processed prompts:  51%|█████     | 260/512 [00:51<01:14,  3.36it/s, est. speed input: 81.50 toks/s, output: 1303.99 toks/s]
Processed prompts:  51%|█████     | 261/512 [00:51<01:16,  3.27it/s, est. speed input: 81.29 toks/s, output: 1300.62 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:51<01:15,  3.30it/s, est. speed input: 81.13 toks/s, output: 1298.12 toks/s]
Processed prompts:  51%|█████▏    | 263/512 [00:51<01:10,  3.53it/s, est. speed input: 81.08 toks/s, output: 1297.27 toks/s]
Processed prompts:  52%|█████▏    | 264/512 [00:52<01:03,  3.93it/s, est. speed input: 81.11 toks/s, output: 1297.71 toks/s]
Processed prompts:  52%|█████▏    | 265/512 [00:52<00:59,  4.18it/s, est. speed input: 81.10 toks/s, output: 1297.60 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:52<00:52,  4.66it/s, est. speed input: 81.17 toks/s, output: 1298.67 toks/s]
Processed prompts:  52%|█████▏    | 267/512 [00:52<00:47,  5.12it/s, est. speed input: 81.24 toks/s, output: 1299.83 toks/s]
Processed prompts:  52%|█████▏    | 268/512 [00:52<00:41,  5.83it/s, est. speed input: 81.37 toks/s, output: 1301.88 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:52<00:29,  8.22it/s, est. speed input: 81.78 toks/s, output: 1308.49 toks/s]
Processed prompts:  53%|█████▎    | 271/512 [00:52<00:28,  8.42it/s, est. speed input: 81.91 toks/s, output: 1310.61 toks/s]
Processed prompts:  53%|█████▎    | 272/512 [00:53<00:29,  8.13it/s, est. speed input: 82.01 toks/s, output: 1312.09 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:53<00:24,  9.76it/s, est. speed input: 82.38 toks/s, output: 1318.02 toks/s]
Processed prompts:  54%|█████▍    | 276/512 [00:53<00:23,  9.87it/s, est. speed input: 82.67 toks/s, output: 1322.70 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:53<00:23,  9.92it/s, est. speed input: 82.96 toks/s, output: 1327.33 toks/s]
Processed prompts:  55%|█████▍    | 280/512 [00:53<00:24,  9.65it/s, est. speed input: 83.22 toks/s, output: 1331.44 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:53<00:19, 11.58it/s, est. speed input: 83.65 toks/s, output: 1338.46 toks/s]
Processed prompts:  56%|█████▌    | 285/512 [00:54<00:33,  6.73it/s, est. speed input: 83.44 toks/s, output: 1334.99 toks/s]
Processed prompts:  56%|█████▌    | 287/512 [00:56<01:16,  2.95it/s, est. speed input: 81.55 toks/s, output: 1304.80 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [00:56<01:15,  2.97it/s, est. speed input: 81.37 toks/s, output: 1301.90 toks/s]
Processed prompts:  56%|█████▋    | 289/512 [00:56<01:12,  3.07it/s, est. speed input: 81.25 toks/s, output: 1299.97 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:57<01:06,  3.34it/s, est. speed input: 81.24 toks/s, output: 1299.81 toks/s]
Processed prompts:  57%|█████▋    | 291/512 [00:57<00:59,  3.70it/s, est. speed input: 81.27 toks/s, output: 1300.29 toks/s]
Processed prompts:  57%|█████▋    | 292/512 [00:57<00:54,  4.02it/s, est. speed input: 81.28 toks/s, output: 1300.54 toks/s]
Processed prompts:  57%|█████▋    | 293/512 [00:57<00:48,  4.48it/s, est. speed input: 81.35 toks/s, output: 1301.58 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:57<00:41,  5.19it/s, est. speed input: 81.47 toks/s, output: 1303.48 toks/s]
Processed prompts:  58%|█████▊    | 297/512 [00:57<00:26,  8.16it/s, est. speed input: 82.03 toks/s, output: 1312.50 toks/s]
Processed prompts:  58%|█████▊    | 299/512 [00:58<00:28,  7.41it/s, est. speed input: 82.13 toks/s, output: 1314.12 toks/s]
Processed prompts:  59%|█████▊    | 300/512 [00:58<00:28,  7.45it/s, est. speed input: 82.22 toks/s, output: 1315.55 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:58<00:23,  8.89it/s, est. speed input: 82.56 toks/s, output: 1320.99 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:58<00:16, 12.30it/s, est. speed input: 83.37 toks/s, output: 1333.85 toks/s]
Processed prompts:  60%|██████    | 308/512 [00:59<00:20, 10.06it/s, est. speed input: 83.49 toks/s, output: 1335.77 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:59<00:20, 10.05it/s, est. speed input: 83.74 toks/s, output: 1339.92 toks/s]
Processed prompts:  61%|██████    | 312/512 [01:00<00:42,  4.74it/s, est. speed input: 82.90 toks/s, output: 1326.43 toks/s]
Processed prompts:  61%|██████    | 313/512 [01:00<00:52,  3.77it/s, est. speed input: 82.43 toks/s, output: 1318.92 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [01:01<00:57,  3.43it/s, est. speed input: 82.16 toks/s, output: 1314.49 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [01:01<01:03,  3.13it/s, est. speed input: 81.85 toks/s, output: 1309.61 toks/s]
Processed prompts:  62%|██████▏   | 316/512 [01:01<00:58,  3.37it/s, est. speed input: 81.82 toks/s, output: 1309.07 toks/s]
Processed prompts:  62%|██████▏   | 317/512 [01:01<00:49,  3.92it/s, est. speed input: 81.90 toks/s, output: 1310.42 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [01:02<00:45,  4.27it/s, est. speed input: 81.93 toks/s, output: 1310.80 toks/s]
Processed prompts:  62%|██████▏   | 319/512 [01:02<00:39,  4.86it/s, est. speed input: 82.01 toks/s, output: 1312.17 toks/s]
Processed prompts:  62%|██████▎   | 320/512 [01:02<00:35,  5.37it/s, est. speed input: 82.09 toks/s, output: 1313.40 toks/s]
Processed prompts:  63%|██████▎   | 321/512 [01:02<00:37,  5.03it/s, est. speed input: 82.04 toks/s, output: 1312.67 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [01:02<00:35,  5.34it/s, est. speed input: 82.09 toks/s, output: 1313.41 toks/s]
Processed prompts:  63%|██████▎   | 323/512 [01:02<00:36,  5.16it/s, est. speed input: 82.07 toks/s, output: 1313.11 toks/s]
Processed prompts:  63%|██████▎   | 324/512 [01:03<00:37,  5.07it/s, est. speed input: 82.06 toks/s, output: 1312.88 toks/s]
Processed prompts:  63%|██████▎   | 325/512 [01:03<00:34,  5.39it/s, est. speed input: 82.10 toks/s, output: 1313.66 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [01:03<00:28,  6.39it/s, est. speed input: 82.29 toks/s, output: 1316.66 toks/s]
Processed prompts:  64%|██████▍   | 328/512 [01:03<00:34,  5.36it/s, est. speed input: 82.18 toks/s, output: 1314.94 toks/s]
Processed prompts:  64%|██████▍   | 329/512 [01:03<00:31,  5.80it/s, est. speed input: 82.26 toks/s, output: 1316.22 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [01:04<00:30,  5.93it/s, est. speed input: 82.31 toks/s, output: 1316.96 toks/s]
Processed prompts:  65%|██████▍   | 332/512 [01:04<00:22,  7.87it/s, est. speed input: 82.62 toks/s, output: 1321.92 toks/s]
Processed prompts:  65%|██████▌   | 335/512 [01:04<00:15, 11.19it/s, est. speed input: 83.18 toks/s, output: 1330.81 toks/s]
Processed prompts:  66%|██████▌   | 337/512 [01:04<00:13, 12.65it/s, est. speed input: 83.53 toks/s, output: 1336.41 toks/s]
Processed prompts:  66%|██████▌   | 339/512 [01:04<00:15, 11.15it/s, est. speed input: 83.73 toks/s, output: 1339.64 toks/s]
Processed prompts:  67%|██████▋   | 341/512 [01:05<00:29,  5.82it/s, est. speed input: 83.30 toks/s, output: 1332.87 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [01:05<00:37,  4.48it/s, est. speed input: 82.97 toks/s, output: 1327.49 toks/s]
Processed prompts:  67%|██████▋   | 343/512 [01:06<00:44,  3.77it/s, est. speed input: 82.68 toks/s, output: 1322.80 toks/s]
Processed prompts:  67%|██████▋   | 344/512 [01:06<00:46,  3.59it/s, est. speed input: 82.51 toks/s, output: 1320.22 toks/s]
Processed prompts:  67%|██████▋   | 345/512 [01:07<00:49,  3.37it/s, est. speed input: 82.32 toks/s, output: 1317.05 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [01:07<00:42,  3.94it/s, est. speed input: 82.40 toks/s, output: 1318.33 toks/s]
Processed prompts:  68%|██████▊   | 347/512 [01:07<00:51,  3.19it/s, est. speed input: 82.05 toks/s, output: 1312.85 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [01:07<00:50,  3.24it/s, est. speed input: 81.93 toks/s, output: 1310.88 toks/s]
Processed prompts:  68%|██████▊   | 349/512 [01:08<00:49,  3.27it/s, est. speed input: 81.81 toks/s, output: 1308.89 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [01:08<00:47,  3.44it/s, est. speed input: 81.74 toks/s, output: 1307.78 toks/s]
Processed prompts:  69%|██████▊   | 351/512 [01:08<00:40,  3.98it/s, est. speed input: 81.79 toks/s, output: 1308.56 toks/s]
Processed prompts:  69%|██████▉   | 352/512 [01:08<00:33,  4.79it/s, est. speed input: 81.89 toks/s, output: 1310.24 toks/s]
Processed prompts:  69%|██████▉   | 353/512 [01:08<00:29,  5.38it/s, est. speed input: 81.97 toks/s, output: 1311.46 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [01:09<00:27,  5.66it/s, est. speed input: 82.01 toks/s, output: 1312.23 toks/s]
Processed prompts:  69%|██████▉   | 355/512 [01:09<00:26,  5.89it/s, est. speed input: 82.06 toks/s, output: 1313.02 toks/s]
Processed prompts:  70%|██████▉   | 356/512 [01:09<00:28,  5.54it/s, est. speed input: 82.05 toks/s, output: 1312.82 toks/s]
Processed prompts:  70%|██████▉   | 357/512 [01:09<00:30,  5.11it/s, est. speed input: 82.01 toks/s, output: 1312.12 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [01:09<00:28,  5.40it/s, est. speed input: 82.05 toks/s, output: 1312.77 toks/s]
Processed prompts:  70%|███████   | 359/512 [01:09<00:24,  6.16it/s, est. speed input: 82.15 toks/s, output: 1314.39 toks/s]
Processed prompts:  70%|███████   | 360/512 [01:10<00:24,  6.19it/s, est. speed input: 82.19 toks/s, output: 1315.05 toks/s]
Processed prompts:  71%|███████   | 361/512 [01:10<00:22,  6.83it/s, est. speed input: 82.29 toks/s, output: 1316.62 toks/s]
Processed prompts:  71%|███████   | 362/512 [01:10<00:20,  7.30it/s, est. speed input: 82.38 toks/s, output: 1318.11 toks/s]
Processed prompts:  71%|███████   | 364/512 [01:10<00:17,  8.42it/s, est. speed input: 82.61 toks/s, output: 1321.72 toks/s]
Processed prompts:  71%|███████▏  | 365/512 [01:10<00:18,  8.12it/s, est. speed input: 82.67 toks/s, output: 1322.79 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [01:10<00:17,  8.26it/s, est. speed input: 82.77 toks/s, output: 1324.26 toks/s]
Processed prompts:  72%|███████▏  | 368/512 [01:10<00:14,  9.87it/s, est. speed input: 83.04 toks/s, output: 1328.68 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [01:11<00:12, 11.49it/s, est. speed input: 83.34 toks/s, output: 1333.51 toks/s]
Processed prompts:  73%|███████▎  | 373/512 [01:11<00:13, 10.54it/s, est. speed input: 83.65 toks/s, output: 1338.41 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [01:11<00:13, 10.37it/s, est. speed input: 83.97 toks/s, output: 1343.59 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [01:12<00:19,  6.72it/s, est. speed input: 83.74 toks/s, output: 1339.91 toks/s]
Processed prompts:  74%|███████▍  | 379/512 [01:12<00:23,  5.54it/s, est. speed input: 83.57 toks/s, output: 1337.09 toks/s]
Processed prompts:  74%|███████▍  | 381/512 [01:12<00:24,  5.35it/s, est. speed input: 83.55 toks/s, output: 1336.74 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [01:13<00:25,  5.16it/s, est. speed input: 83.51 toks/s, output: 1336.13 toks/s]
Processed prompts:  75%|███████▍  | 383/512 [01:13<00:29,  4.32it/s, est. speed input: 83.30 toks/s, output: 1332.80 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [01:13<00:25,  4.95it/s, est. speed input: 83.40 toks/s, output: 1334.43 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [01:13<00:19,  6.39it/s, est. speed input: 83.64 toks/s, output: 1338.21 toks/s]
Processed prompts:  76%|███████▌  | 387/512 [01:13<00:19,  6.58it/s, est. speed input: 83.70 toks/s, output: 1339.20 toks/s]
Processed prompts:  76%|███████▌  | 388/512 [01:14<00:19,  6.29it/s, est. speed input: 83.71 toks/s, output: 1339.38 toks/s]
Processed prompts:  76%|███████▌  | 389/512 [01:14<00:21,  5.83it/s, est. speed input: 83.69 toks/s, output: 1339.09 toks/s]
Processed prompts:  76%|███████▋  | 391/512 [01:14<00:16,  7.36it/s, est. speed input: 83.93 toks/s, output: 1342.83 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [01:14<00:11, 10.35it/s, est. speed input: 84.39 toks/s, output: 1350.25 toks/s]
Processed prompts:  77%|███████▋  | 396/512 [01:14<00:11, 10.22it/s, est. speed input: 84.59 toks/s, output: 1353.46 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [01:15<00:10, 11.38it/s, est. speed input: 84.87 toks/s, output: 1357.92 toks/s]
Processed prompts:  78%|███████▊  | 400/512 [01:15<00:09, 11.67it/s, est. speed input: 85.11 toks/s, output: 1361.81 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [01:15<00:10, 10.61it/s, est. speed input: 85.28 toks/s, output: 1364.49 toks/s]
Processed prompts:  79%|███████▉  | 404/512 [01:16<00:20,  5.37it/s, est. speed input: 84.81 toks/s, output: 1356.93 toks/s]
Processed prompts:  79%|███████▉  | 405/512 [01:17<00:37,  2.85it/s, est. speed input: 83.80 toks/s, output: 1340.73 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [01:17<00:42,  2.49it/s, est. speed input: 83.35 toks/s, output: 1333.68 toks/s]
Processed prompts:  79%|███████▉  | 407/512 [01:18<00:39,  2.64it/s, est. speed input: 83.24 toks/s, output: 1331.81 toks/s]
Processed prompts:  80%|███████▉  | 408/512 [01:18<00:35,  2.92it/s, est. speed input: 83.20 toks/s, output: 1331.19 toks/s]
Processed prompts:  80%|███████▉  | 409/512 [01:18<00:30,  3.34it/s, est. speed input: 83.22 toks/s, output: 1331.48 toks/s]
Processed prompts:  80%|████████  | 410/512 [01:18<00:27,  3.65it/s, est. speed input: 83.20 toks/s, output: 1331.25 toks/s]
Processed prompts:  80%|████████  | 411/512 [01:18<00:23,  4.36it/s, est. speed input: 83.29 toks/s, output: 1332.59 toks/s]
Processed prompts:  80%|████████  | 412/512 [01:19<00:20,  4.79it/s, est. speed input: 83.32 toks/s, output: 1333.20 toks/s]
Processed prompts:  81%|████████  | 413/512 [01:19<00:22,  4.34it/s, est. speed input: 83.23 toks/s, output: 1331.64 toks/s]
Processed prompts:  81%|████████  | 414/512 [01:19<00:21,  4.53it/s, est. speed input: 83.22 toks/s, output: 1331.55 toks/s]
Processed prompts:  81%|████████  | 415/512 [01:19<00:23,  4.22it/s, est. speed input: 83.13 toks/s, output: 1330.16 toks/s]
Processed prompts:  81%|████████▏ | 416/512 [01:20<00:21,  4.53it/s, est. speed input: 83.15 toks/s, output: 1330.32 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [01:20<00:15,  5.88it/s, est. speed input: 83.32 toks/s, output: 1333.06 toks/s]
Processed prompts:  82%|████████▏ | 420/512 [01:20<00:12,  7.54it/s, est. speed input: 83.56 toks/s, output: 1336.92 toks/s]
Processed prompts:  83%|████████▎ | 423/512 [01:20<00:08, 10.82it/s, est. speed input: 84.01 toks/s, output: 1344.22 toks/s]
Processed prompts:  83%|████████▎ | 425/512 [01:20<00:06, 12.52it/s, est. speed input: 84.30 toks/s, output: 1348.86 toks/s]
Processed prompts:  84%|████████▎ | 428/512 [01:20<00:06, 13.37it/s, est. speed input: 84.69 toks/s, output: 1355.00 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [01:20<00:05, 13.92it/s, est. speed input: 84.95 toks/s, output: 1359.19 toks/s]
Processed prompts:  84%|████████▍ | 432/512 [01:21<00:11,  7.23it/s, est. speed input: 84.70 toks/s, output: 1355.18 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [01:22<00:18,  4.20it/s, est. speed input: 84.09 toks/s, output: 1345.41 toks/s]
Processed prompts:  85%|████████▍ | 435/512 [01:22<00:20,  3.76it/s, est. speed input: 83.87 toks/s, output: 1341.94 toks/s]
Processed prompts:  85%|████████▌ | 436/512 [01:23<00:22,  3.32it/s, est. speed input: 83.62 toks/s, output: 1337.84 toks/s]
Processed prompts:  85%|████████▌ | 437/512 [01:23<00:26,  2.79it/s, est. speed input: 83.24 toks/s, output: 1331.91 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [01:24<00:25,  2.85it/s, est. speed input: 83.11 toks/s, output: 1329.79 toks/s]
Processed prompts:  86%|████████▌ | 439/512 [01:24<00:21,  3.33it/s, est. speed input: 83.15 toks/s, output: 1330.47 toks/s]
Processed prompts:  86%|████████▌ | 440/512 [01:24<00:19,  3.70it/s, est. speed input: 83.16 toks/s, output: 1330.53 toks/s]
Processed prompts:  86%|████████▌ | 441/512 [01:24<00:18,  3.84it/s, est. speed input: 83.12 toks/s, output: 1329.88 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [01:25<00:17,  4.09it/s, est. speed input: 83.11 toks/s, output: 1329.70 toks/s]
Processed prompts:  87%|████████▋ | 443/512 [01:25<00:14,  4.70it/s, est. speed input: 83.16 toks/s, output: 1330.60 toks/s]
Processed prompts:  87%|████████▋ | 444/512 [01:25<00:13,  4.93it/s, est. speed input: 83.18 toks/s, output: 1330.81 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [01:25<00:09,  6.69it/s, est. speed input: 83.38 toks/s, output: 1334.14 toks/s]
Processed prompts:  87%|████████▋ | 447/512 [01:25<00:10,  6.38it/s, est. speed input: 83.40 toks/s, output: 1334.34 toks/s]
Processed prompts:  88%|████████▊ | 448/512 [01:25<00:10,  6.39it/s, est. speed input: 83.43 toks/s, output: 1334.90 toks/s]
Processed prompts:  88%|████████▊ | 449/512 [01:26<00:09,  6.38it/s, est. speed input: 83.46 toks/s, output: 1335.43 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [01:26<00:08,  6.93it/s, est. speed input: 83.54 toks/s, output: 1336.67 toks/s]
Processed prompts:  88%|████████▊ | 453/512 [01:26<00:05, 10.04it/s, est. speed input: 83.92 toks/s, output: 1342.68 toks/s]
Processed prompts:  89%|████████▉ | 457/512 [01:26<00:03, 14.96it/s, est. speed input: 84.52 toks/s, output: 1352.33 toks/s]
Processed prompts:  90%|████████▉ | 460/512 [01:26<00:03, 17.15it/s, est. speed input: 84.95 toks/s, output: 1359.20 toks/s]
Processed prompts:  90%|█████████ | 462/512 [01:27<00:07,  7.04it/s, est. speed input: 84.56 toks/s, output: 1352.90 toks/s]
Processed prompts:  91%|█████████ | 464/512 [01:28<00:12,  3.90it/s, est. speed input: 83.82 toks/s, output: 1341.12 toks/s]
Processed prompts:  91%|█████████ | 465/512 [01:29<00:13,  3.45it/s, est. speed input: 83.56 toks/s, output: 1336.95 toks/s]
Processed prompts:  91%|█████████ | 466/512 [01:29<00:13,  3.36it/s, est. speed input: 83.43 toks/s, output: 1334.86 toks/s]
Processed prompts:  91%|█████████ | 467/512 [01:29<00:14,  3.10it/s, est. speed input: 83.22 toks/s, output: 1331.50 toks/s]
Processed prompts:  91%|█████████▏| 468/512 [01:30<00:13,  3.34it/s, est. speed input: 83.19 toks/s, output: 1331.05 toks/s]
Processed prompts:  92%|█████████▏| 469/512 [01:30<00:11,  3.72it/s, est. speed input: 83.21 toks/s, output: 1331.30 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [01:30<00:10,  3.89it/s, est. speed input: 83.18 toks/s, output: 1330.83 toks/s]
Processed prompts:  92%|█████████▏| 471/512 [01:30<00:10,  3.99it/s, est. speed input: 83.14 toks/s, output: 1330.23 toks/s]
Processed prompts:  92%|█████████▏| 472/512 [01:30<00:09,  4.22it/s, est. speed input: 83.13 toks/s, output: 1330.11 toks/s]
Processed prompts:  92%|█████████▏| 473/512 [01:31<00:09,  4.02it/s, est. speed input: 83.05 toks/s, output: 1328.86 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [01:31<00:08,  4.40it/s, est. speed input: 83.07 toks/s, output: 1329.12 toks/s]
Processed prompts:  93%|█████████▎| 475/512 [01:31<00:07,  5.06it/s, est. speed input: 83.13 toks/s, output: 1330.09 toks/s]
Processed prompts:  93%|█████████▎| 476/512 [01:31<00:06,  5.84it/s, est. speed input: 83.21 toks/s, output: 1331.32 toks/s]
Processed prompts:  93%|█████████▎| 477/512 [01:31<00:05,  6.58it/s, est. speed input: 83.29 toks/s, output: 1332.56 toks/s]
Processed prompts:  94%|█████████▎| 479/512 [01:31<00:03,  8.67it/s, est. speed input: 83.50 toks/s, output: 1336.05 toks/s]
Processed prompts:  94%|█████████▍| 481/512 [01:31<00:03,  9.68it/s, est. speed input: 83.70 toks/s, output: 1339.15 toks/s]
Processed prompts:  95%|█████████▍| 484/512 [01:32<00:02, 13.38it/s, est. speed input: 84.11 toks/s, output: 1345.70 toks/s]
Processed prompts:  95%|█████████▌| 487/512 [01:32<00:01, 14.96it/s, est. speed input: 84.48 toks/s, output: 1351.62 toks/s]
Processed prompts:  96%|█████████▌| 491/512 [01:32<00:01, 17.09it/s, est. speed input: 85.00 toks/s, output: 1359.93 toks/s]
Processed prompts:  96%|█████████▋| 493/512 [01:33<00:03,  5.96it/s, est. speed input: 84.39 toks/s, output: 1350.17 toks/s]
Processed prompts:  97%|█████████▋| 495/512 [01:34<00:04,  3.41it/s, est. speed input: 83.54 toks/s, output: 1336.59 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [01:35<00:04,  3.40it/s, est. speed input: 83.44 toks/s, output: 1335.05 toks/s]
Processed prompts:  97%|█████████▋| 497/512 [01:35<00:04,  3.39it/s, est. speed input: 83.35 toks/s, output: 1333.53 toks/s]
Processed prompts:  97%|█████████▋| 499/512 [01:35<00:03,  4.13it/s, est. speed input: 83.44 toks/s, output: 1335.03 toks/s]
Processed prompts:  98%|█████████▊| 500/512 [01:35<00:02,  4.33it/s, est. speed input: 83.45 toks/s, output: 1335.15 toks/s]
Processed prompts:  98%|█████████▊| 501/512 [01:36<00:02,  4.35it/s, est. speed input: 83.42 toks/s, output: 1334.66 toks/s]
Processed prompts:  98%|█████████▊| 503/512 [01:36<00:01,  5.38it/s, est. speed input: 83.55 toks/s, output: 1336.80 toks/s]
Processed prompts:  98%|█████████▊| 504/512 [01:36<00:01,  5.88it/s, est. speed input: 83.62 toks/s, output: 1337.89 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [01:36<00:00,  7.38it/s, est. speed input: 83.81 toks/s, output: 1341.00 toks/s]
Processed prompts:  99%|█████████▉| 508/512 [01:36<00:00,  8.39it/s, est. speed input: 83.99 toks/s, output: 1343.82 toks/s]
Processed prompts:  99%|█████████▉| 509/512 [01:36<00:00,  8.17it/s, est. speed input: 84.04 toks/s, output: 1344.59 toks/s]
Processed prompts: 100%|█████████▉| 511/512 [01:37<00:00, 10.22it/s, est. speed input: 84.27 toks/s, output: 1348.33 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:37<00:00, 10.22it/s, est. speed input: 84.32 toks/s, output: 1349.15 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:37<00:00,  5.27it/s, est. speed input: 84.32 toks/s, output: 1349.15 toks/s]
[rank0]:[W126 10:58:47.241304543 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

