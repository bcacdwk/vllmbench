
========== M=16 ==========
Time: 2026-01-25 18:45:00
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-FP8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:45:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:45:04 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) ================================================================
(EngineCore_DP0 pid=296467) Internal Triton PTX codegen error
(EngineCore_DP0 pid=296467) `ptxas` stderr:
(EngineCore_DP0 pid=296467) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpwjw7gma8.ptx -o /tmp/tmpwjw7gma8.ptx.o
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) //
(EngineCore_DP0 pid=296467) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=296467) //
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) .version 8.7
(EngineCore_DP0 pid=296467) .target sm_121a
(EngineCore_DP0 pid=296467) .address_size 64
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) 	// .globl	_quant_only_fp8_kernel  // -- Begin function _quant_only_fp8_kernel
(EngineCore_DP0 pid=296467) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=296467)                                         // @_quant_only_fp8_kernel
(EngineCore_DP0 pid=296467) .visible .entry _quant_only_fp8_kernel(
(EngineCore_DP0 pid=296467) 	.param .u64 .ptr .global .align 1 _quant_only_fp8_kernel_param_0,
(EngineCore_DP0 pid=296467) 	.param .u64 .ptr .global .align 1 _quant_only_fp8_kernel_param_1,
(EngineCore_DP0 pid=296467) 	.param .u64 .ptr .global .align 1 _quant_only_fp8_kernel_param_2,
(EngineCore_DP0 pid=296467) 	.param .u32 _quant_only_fp8_kernel_param_3,
(EngineCore_DP0 pid=296467) 	.param .u32 _quant_only_fp8_kernel_param_4,
(EngineCore_DP0 pid=296467) 	.param .u32 _quant_only_fp8_kernel_param_5,
(EngineCore_DP0 pid=296467) 	.param .u64 .ptr .global .align 1 _quant_only_fp8_kernel_param_6,
(EngineCore_DP0 pid=296467) 	.param .u64 .ptr .global .align 1 _quant_only_fp8_kernel_param_7
(EngineCore_DP0 pid=296467) )
(EngineCore_DP0 pid=296467) .reqntid 256
(EngineCore_DP0 pid=296467) {
(EngineCore_DP0 pid=296467) 	.reg .pred 	%p<10>;
(EngineCore_DP0 pid=296467) 	.reg .b16 	%rs<72>;
(EngineCore_DP0 pid=296467) 	.reg .b32 	%r<125>;
(EngineCore_DP0 pid=296467) 	.reg .b64 	%rd<14>;
(EngineCore_DP0 pid=296467) 	.loc	1 72 0                          // quant_only_tuned_Llama3.2-1B.py:72:0
(EngineCore_DP0 pid=296467) $L__func_begin0:
(EngineCore_DP0 pid=296467) 	.loc	1 72 0                          // quant_only_tuned_Llama3.2-1B.py:72:0
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) // %bb.0:
(EngineCore_DP0 pid=296467) 	ld.param.b64 	%rd7, [_quant_only_fp8_kernel_param_0];
(EngineCore_DP0 pid=296467) 	ld.param.b64 	%rd8, [_quant_only_fp8_kernel_param_1];
(EngineCore_DP0 pid=296467) $L__tmp0:
(EngineCore_DP0 pid=296467) 	.loc	1 79 24                         // quant_only_tuned_Llama3.2-1B.py:79:24
(EngineCore_DP0 pid=296467) 	mov.u32 	%r60, %ctaid.x;
(EngineCore_DP0 pid=296467) 	ld.param.b32 	%r61, [_quant_only_fp8_kernel_param_4];
(EngineCore_DP0 pid=296467) 	.loc	1 84 30                         // quant_only_tuned_Llama3.2-1B.py:84:30
(EngineCore_DP0 pid=296467) 	mul.lo.s32 	%r62, %r61, %r60;
(EngineCore_DP0 pid=296467) 	ld.param.b64 	%rd9, [_quant_only_fp8_kernel_param_2];
(EngineCore_DP0 pid=296467) 	.loc	1 84 24                         // quant_only_tuned_Llama3.2-1B.py:84:24
(EngineCore_DP0 pid=296467) 	mad.wide.s32 	%rd10, %r62, 2, %rd7;
(EngineCore_DP0 pid=296467) 	ld.param.b32 	%r63, [_quant_only_fp8_kernel_param_5];
(EngineCore_DP0 pid=296467) 	.loc	1 85 34                         // quant_only_tuned_Llama3.2-1B.py:85:34
(EngineCore_DP0 pid=296467) 	mul.lo.s32 	%r64, %r63, %r60;
(EngineCore_DP0 pid=296467) 	.loc	1 85 28                         // quant_only_tuned_Llama3.2-1B.py:85:28
(EngineCore_DP0 pid=296467) 	cvt.s64.s32 	%rd11, %r64;
(EngineCore_DP0 pid=296467) 	add.s64 	%rd12, %rd8, %rd11;
(EngineCore_DP0 pid=296467) 	.loc	1 91 40                         // quant_only_tuned_Llama3.2-1B.py:91:40
(EngineCore_DP0 pid=296467) 	mov.u32 	%r65, %tid.x;
(EngineCore_DP0 pid=296467) 	and.b32 	%r66, %r65, 31;
(EngineCore_DP0 pid=296467) 	shl.b32 	%r67, %r65, 4;
(EngineCore_DP0 pid=296467) 	and.b32 	%r68, %r67, 4080;
(EngineCore_DP0 pid=296467) 	.loc	1 92 26                         // quant_only_tuned_Llama3.2-1B.py:92:26
(EngineCore_DP0 pid=296467) 	setp.lt.u32 	%p1, %r68, 2048;
(EngineCore_DP0 pid=296467) 	.loc	1 93 36                         // quant_only_tuned_Llama3.2-1B.py:93:36
(EngineCore_DP0 pid=296467) 	cvt.u64.u32 	%rd13, %r68;
(EngineCore_DP0 pid=296467) 	mad.wide.u32 	%rd1, %r68, 2, %rd10;
(EngineCore_DP0 pid=296467) 	add.s64 	%rd2, %rd1, 16;
(EngineCore_DP0 pid=296467) 	mov.b32 	%r5, 0;
(EngineCore_DP0 pid=296467) 	.loc	1 93 24                         // quant_only_tuned_Llama3.2-1B.py:93:24
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	mov.u32 %r1, %r5;
(EngineCore_DP0 pid=296467) 	mov.u32 %r2, %r5;
(EngineCore_DP0 pid=296467) 	mov.u32 %r3, %r5;
(EngineCore_DP0 pid=296467) 	mov.u32 %r4, %r5;
(EngineCore_DP0 pid=296467) 	@%p1 ld.global.v4.b32 { %r1, %r2, %r3, %r4 }, [ %rd1 + 0 ];
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs9, %rs10}, %r1;
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs11, %rs12}, %r2;
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs13, %rs14}, %r3;
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs15, %rs16}, %r4;
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	mov.u32 %r9, %r5;
(EngineCore_DP0 pid=296467) 	mov.u32 %r10, %r5;
(EngineCore_DP0 pid=296467) 	mov.u32 %r11, %r5;
(EngineCore_DP0 pid=296467) 	mov.u32 %r12, %r5;
(EngineCore_DP0 pid=296467) 	@%p1 ld.global.v4.b32 { %r9, %r10, %r11, %r12 }, [ %rd2 + 0 ];
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs17, %rs18}, %r9;
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs19, %rs20}, %r10;
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs21, %rs22}, %r11;
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs23, %rs24}, %r12;
(EngineCore_DP0 pid=296467) 	.loc	1 94 50                         // quant_only_tuned_Llama3.2-1B.py:94:50
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs25, %rs9;
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs26, %rs10;
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs27, %rs11;
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs28, %rs12;
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs29, %rs13;
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs30, %rs14;
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs31, %rs15;
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs32, %rs16;
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs33, %rs17;
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs34, %rs18;
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs35, %rs19;
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs36, %rs20;
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs37, %rs21;
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs38, %rs22;
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs39, %rs23;
(EngineCore_DP0 pid=296467) 	abs.bf16 	%rs40, %rs24;
(EngineCore_DP0 pid=296467) $L__tmp1:
(EngineCore_DP0 pid=296467) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	max.bf16 	%rs41, %rs25, %rs26;
(EngineCore_DP0 pid=296467) 	max.bf16 	%rs42, %rs41, %rs27;
(EngineCore_DP0 pid=296467) 	max.bf16 	%rs43, %rs42, %rs28;
(EngineCore_DP0 pid=296467) 	max.bf16 	%rs44, %rs43, %rs29;
(EngineCore_DP0 pid=296467) 	max.bf16 	%rs45, %rs44, %rs30;
(EngineCore_DP0 pid=296467) 	max.bf16 	%rs46, %rs45, %rs31;
(EngineCore_DP0 pid=296467) 	max.bf16 	%rs47, %rs46, %rs32;
(EngineCore_DP0 pid=296467) 	max.bf16 	%rs48, %rs47, %rs33;
(EngineCore_DP0 pid=296467) 	max.bf16 	%rs49, %rs48, %rs34;
(EngineCore_DP0 pid=296467) 	max.bf16 	%rs50, %rs49, %rs35;
(EngineCore_DP0 pid=296467) 	max.bf16 	%rs51, %rs50, %rs36;
(EngineCore_DP0 pid=296467) 	max.bf16 	%rs52, %rs51, %rs37;
(EngineCore_DP0 pid=296467) 	max.bf16 	%rs53, %rs52, %rs38;
(EngineCore_DP0 pid=296467) 	max.bf16 	%rs54, %rs53, %rs39;
(EngineCore_DP0 pid=296467) 	max.bf16 	%rs55, %rs54, %rs40;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r69, %rs55;
(EngineCore_DP0 pid=296467) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	shfl.sync.bfly.b32 	%r70, %r69, 16, 31, -1;
(EngineCore_DP0 pid=296467) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	max.f32 	%r71, %r69, %r70;
(EngineCore_DP0 pid=296467) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	shfl.sync.bfly.b32 	%r72, %r71, 8, 31, -1;
(EngineCore_DP0 pid=296467) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	max.f32 	%r73, %r71, %r72;
(EngineCore_DP0 pid=296467) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	shfl.sync.bfly.b32 	%r74, %r73, 4, 31, -1;
(EngineCore_DP0 pid=296467) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	max.f32 	%r75, %r73, %r74;
(EngineCore_DP0 pid=296467) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	shfl.sync.bfly.b32 	%r76, %r75, 2, 31, -1;
(EngineCore_DP0 pid=296467) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	max.f32 	%r77, %r75, %r76;
(EngineCore_DP0 pid=296467) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	shfl.sync.bfly.b32 	%r78, %r77, 1, 31, -1;
(EngineCore_DP0 pid=296467) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	max.f32 	%r18, %r77, %r78;
(EngineCore_DP0 pid=296467) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	setp.eq.b32 	%p3, %r66, 0;
(EngineCore_DP0 pid=296467) 	shr.u32 	%r79, %r65, 3;
(EngineCore_DP0 pid=296467) 	and.b32 	%r80, %r79, 28;
(EngineCore_DP0 pid=296467) 	mov.b32 	%r81, global_smem;
(EngineCore_DP0 pid=296467) 	add.s32 	%r17, %r81, %r80;
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	@%p3 st.shared.b32 [ %r17 + 0 ], %r18;
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	bar.sync 	0;
(EngineCore_DP0 pid=296467) 	setp.lt.u32 	%p4, %r65, 8;
(EngineCore_DP0 pid=296467) 	shl.b32 	%r82, %r65, 2;
(EngineCore_DP0 pid=296467) 	add.s32 	%r20, %r81, %r82;
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	@%p4 ld.shared.b32 %r19, [ %r20 + 0 ];
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	shfl.sync.bfly.b32 	%r83, %r19, 4, 31, -1;
(EngineCore_DP0 pid=296467) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	max.f32 	%r84, %r19, %r83;
(EngineCore_DP0 pid=296467) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	shfl.sync.bfly.b32 	%r85, %r84, 2, 31, -1;
(EngineCore_DP0 pid=296467) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	max.f32 	%r86, %r84, %r85;
(EngineCore_DP0 pid=296467) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	shfl.sync.bfly.b32 	%r87, %r86, 1, 31, -1;
(EngineCore_DP0 pid=296467) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	max.f32 	%r22, %r86, %r87;
(EngineCore_DP0 pid=296467) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296467) 	setp.eq.b32 	%p5, %r65, 0;
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	@%p5 st.shared.b32 [ %r20 + 0 ], %r22;
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	bar.sync 	0;
(EngineCore_DP0 pid=296467) 	ld.shared.b32 	%r88, [global_smem];
(EngineCore_DP0 pid=296467) $L__tmp2:
(EngineCore_DP0 pid=296467) 	.loc	1 97 32                         // quant_only_tuned_Llama3.2-1B.py:97:32
(EngineCore_DP0 pid=296467) 	max.f32 	%r89, %r88, 0f2B8CBCCC;
(EngineCore_DP0 pid=296467) 	mov.b32 	%r90, 0f43E00000;
(EngineCore_DP0 pid=296467) 	.loc	1 98 32                         // quant_only_tuned_Llama3.2-1B.py:98:32
(EngineCore_DP0 pid=296467) 	div.full.f32 	%r91, %r89, %r90;
(EngineCore_DP0 pid=296467) 	.loc	1 98 41                         // quant_only_tuned_Llama3.2-1B.py:98:41
(EngineCore_DP0 pid=296467) 	max.f32 	%r23, %r91, 0f36924925;
(EngineCore_DP0 pid=296467) 	.loc	1 99 26                         // quant_only_tuned_Llama3.2-1B.py:99:26
(EngineCore_DP0 pid=296467) 	div.full.f32 	%r92, %r90, %r89;
(EngineCore_DP0 pid=296467) 	.loc	1 101 25                        // quant_only_tuned_Llama3.2-1B.py:101:25
(EngineCore_DP0 pid=296467) 	mad.wide.u32 	%rd3, %r60, 4, %rd9;
(EngineCore_DP0 pid=296467) 	.loc	1 101 30                        // quant_only_tuned_Llama3.2-1B.py:101:30
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	@%p5 st.global.b32 [ %rd3 + 0 ], { %r23 };
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	.loc	1 107 24                        // quant_only_tuned_Llama3.2-1B.py:107:24
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	mov.u32 %r24, %r5;
(EngineCore_DP0 pid=296467) 	mov.u32 %r25, %r5;
(EngineCore_DP0 pid=296467) 	mov.u32 %r26, %r5;
(EngineCore_DP0 pid=296467) 	mov.u32 %r27, %r5;
(EngineCore_DP0 pid=296467) 	@%p1 ld.global.v4.b32 { %r24, %r25, %r26, %r27 }, [ %rd1 + 0 ];
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs56, %rs57}, %r24;
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs58, %rs59}, %r25;
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs60, %rs61}, %r26;
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs62, %rs63}, %r27;
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	mov.u32 %r32, %r5;
(EngineCore_DP0 pid=296467) 	mov.u32 %r33, %r5;
(EngineCore_DP0 pid=296467) 	mov.u32 %r34, %r5;
(EngineCore_DP0 pid=296467) 	mov.u32 %r35, %r5;
(EngineCore_DP0 pid=296467) 	@%p1 ld.global.v4.b32 { %r32, %r33, %r34, %r35 }, [ %rd2 + 0 ];
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs64, %rs65}, %r32;
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs66, %rs67}, %r33;
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs68, %rs69}, %r34;
(EngineCore_DP0 pid=296467) 	mov.b32 	{%rs70, %rs71}, %r35;
(EngineCore_DP0 pid=296467) 	.loc	1 107 71                        // quant_only_tuned_Llama3.2-1B.py:107:71
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r93, %rs56;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r94, %rs57;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r95, %rs58;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r96, %rs59;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r97, %rs60;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r98, %rs61;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r99, %rs62;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r100, %rs63;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r101, %rs64;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r102, %rs65;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r103, %rs66;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r104, %rs67;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r105, %rs68;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r106, %rs69;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r107, %rs70;
(EngineCore_DP0 pid=296467) 	cvt.f32.bf16 	%r108, %rs71;
(EngineCore_DP0 pid=296467) 	.loc	1 108 33                        // quant_only_tuned_Llama3.2-1B.py:108:33
(EngineCore_DP0 pid=296467) 	mul.f32 	%r109, %r92, %r93;
(EngineCore_DP0 pid=296467) 	mul.f32 	%r110, %r92, %r94;
(EngineCore_DP0 pid=296467) 	mul.f32 	%r111, %r92, %r95;
(EngineCore_DP0 pid=296467) 	mul.f32 	%r112, %r92, %r96;
(EngineCore_DP0 pid=296467) 	mul.f32 	%r113, %r92, %r97;
(EngineCore_DP0 pid=296467) 	mul.f32 	%r114, %r92, %r98;
(EngineCore_DP0 pid=296467) 	mul.f32 	%r115, %r92, %r99;
(EngineCore_DP0 pid=296467) 	mul.f32 	%r116, %r92, %r100;
(EngineCore_DP0 pid=296467) 	mul.f32 	%r117, %r92, %r101;
(EngineCore_DP0 pid=296467) 	mul.f32 	%r118, %r92, %r102;
(EngineCore_DP0 pid=296467) 	mul.f32 	%r119, %r92, %r103;
(EngineCore_DP0 pid=296467) 	mul.f32 	%r120, %r92, %r104;
(EngineCore_DP0 pid=296467) 	mul.f32 	%r121, %r92, %r105;
(EngineCore_DP0 pid=296467) 	mul.f32 	%r122, %r92, %r106;
(EngineCore_DP0 pid=296467) 	mul.f32 	%r123, %r92, %r107;
(EngineCore_DP0 pid=296467) 	mul.f32 	%r124, %r92, %r108;
(EngineCore_DP0 pid=296467) 	.loc	1 108 54                        // quant_only_tuned_Llama3.2-1B.py:108:54
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r40, %r109, %r90;
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r41, %r110, %r90;
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r42, %r111, %r90;
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r43, %r112, %r90;
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r44, %r113, %r90;
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r45, %r114, %r90;
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r46, %r115, %r90;
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r47, %r116, %r90;
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r48, %r117, %r90;
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r49, %r118, %r90;
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r50, %r119, %r90;
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r51, %r120, %r90;
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r52, %r121, %r90;
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r53, %r122, %r90;
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r54, %r123, %r90;
(EngineCore_DP0 pid=296467) 	min.xorsign.abs.f32 	%r55, %r124, %r90;
(EngineCore_DP0 pid=296467) 	.loc	1 109 31                        // quant_only_tuned_Llama3.2-1B.py:109:31
(EngineCore_DP0 pid=296467) 	add.s64 	%rd6, %rd12, %rd13;
(EngineCore_DP0 pid=296467) 	.loc	1 109 48                        // quant_only_tuned_Llama3.2-1B.py:109:48
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	cvt.rn.satfinite.e4m3x2.f32  %rs1, %r41, %r40; 
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	cvt.rn.satfinite.e4m3x2.f32  %rs2, %r43, %r42; 
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	cvt.rn.satfinite.e4m3x2.f32  %rs3, %r45, %r44; 
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	cvt.rn.satfinite.e4m3x2.f32  %rs4, %r47, %r46; 
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	cvt.rn.satfinite.e4m3x2.f32  %rs5, %r49, %r48; 
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	cvt.rn.satfinite.e4m3x2.f32  %rs6, %r51, %r50; 
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	cvt.rn.satfinite.e4m3x2.f32  %rs7, %r53, %r52; 
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	cvt.rn.satfinite.e4m3x2.f32  %rs8, %r55, %r54; 
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	.loc	1 109 39                        // quant_only_tuned_Llama3.2-1B.py:109:39
(EngineCore_DP0 pid=296467) 	mov.b32 	%r56, {%rs1, %rs2};
(EngineCore_DP0 pid=296467) 	mov.b32 	%r57, {%rs3, %rs4};
(EngineCore_DP0 pid=296467) 	mov.b32 	%r58, {%rs5, %rs6};
(EngineCore_DP0 pid=296467) 	mov.b32 	%r59, {%rs7, %rs8};
(EngineCore_DP0 pid=296467) 	// begin inline asm
(EngineCore_DP0 pid=296467) 	@%p1 st.global.v4.b32 [ %rd6 + 0 ], { %r56, %r57, %r58, %r59 };
(EngineCore_DP0 pid=296467) 	// end inline asm
(EngineCore_DP0 pid=296467) 	.loc	1 104 4                         // quant_only_tuned_Llama3.2-1B.py:104:4
(EngineCore_DP0 pid=296467) 	ret;
(EngineCore_DP0 pid=296467) $L__tmp3:
(EngineCore_DP0 pid=296467) $L__func_end0:
(EngineCore_DP0 pid=296467)                                         // -- End function
(EngineCore_DP0 pid=296467) }
(EngineCore_DP0 pid=296467) 	.file	1 "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=296467) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=296467) 	.section	.debug_abbrev
(EngineCore_DP0 pid=296467) 	{
(EngineCore_DP0 pid=296467) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=296467) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=296467) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=296467) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=296467) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=296467) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=296467) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=296467) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=296467) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=296467) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=296467) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=296467) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=296467) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=296467) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=296467) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=296467) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=296467) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=296467) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=296467) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=296467) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=296467) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=296467) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=296467) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=296467) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=296467) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=296467) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=296467) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=296467) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=296467) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=296467) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=296467) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=296467) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=296467) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=296467) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=296467) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=296467) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=296467) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=296467) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=296467) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=296467) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=296467) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=296467) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=296467) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=296467) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=296467) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=296467) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=296467) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=296467) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=296467) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=296467) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=296467) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=296467) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=296467) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=296467) 	}
(EngineCore_DP0 pid=296467) 	.section	.debug_info
(EngineCore_DP0 pid=296467) 	{
(EngineCore_DP0 pid=296467) .b32 213                                // Length of Unit
(EngineCore_DP0 pid=296467) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=296467) .b8 0
(EngineCore_DP0 pid=296467) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=296467) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=296467) .b8 1                                   // Abbrev [1] 0xb:0xce DW_TAG_compile_unit
(EngineCore_DP0 pid=296467) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=296467) .b8 114
(EngineCore_DP0 pid=296467) .b8 105
(EngineCore_DP0 pid=296467) .b8 116
(EngineCore_DP0 pid=296467) .b8 111
(EngineCore_DP0 pid=296467) .b8 110
(EngineCore_DP0 pid=296467) .b8 0
(EngineCore_DP0 pid=296467) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=296467) .b8 0
(EngineCore_DP0 pid=296467) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=296467) .b8 117
(EngineCore_DP0 pid=296467) .b8 97
(EngineCore_DP0 pid=296467) .b8 110
(EngineCore_DP0 pid=296467) .b8 116
(EngineCore_DP0 pid=296467) .b8 95
(EngineCore_DP0 pid=296467) .b8 111
(EngineCore_DP0 pid=296467) .b8 110
(EngineCore_DP0 pid=296467) .b8 108
(EngineCore_DP0 pid=296467) .b8 121
(EngineCore_DP0 pid=296467) .b8 95
(EngineCore_DP0 pid=296467) .b8 116
(EngineCore_DP0 pid=296467) .b8 117
(EngineCore_DP0 pid=296467) .b8 110
(EngineCore_DP0 pid=296467) .b8 101
(EngineCore_DP0 pid=296467) .b8 100
(EngineCore_DP0 pid=296467) .b8 95
(EngineCore_DP0 pid=296467) .b8 76
(EngineCore_DP0 pid=296467) .b8 108
(EngineCore_DP0 pid=296467) .b8 97
(EngineCore_DP0 pid=296467) .b8 109
(EngineCore_DP0 pid=296467) .b8 97
(EngineCore_DP0 pid=296467) .b8 51
(EngineCore_DP0 pid=296467) .b8 46
(EngineCore_DP0 pid=296467) .b8 50
(EngineCore_DP0 pid=296467) .b8 45
(EngineCore_DP0 pid=296467) .b8 49
(EngineCore_DP0 pid=296467) .b8 66
(EngineCore_DP0 pid=296467) .b8 46
(EngineCore_DP0 pid=296467) .b8 112
(EngineCore_DP0 pid=296467) .b8 121
(EngineCore_DP0 pid=296467) .b8 0
(EngineCore_DP0 pid=296467) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=296467) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=296467) .b8 114
(EngineCore_DP0 pid=296467) .b8 111
(EngineCore_DP0 pid=296467) .b8 111
(EngineCore_DP0 pid=296467) .b8 116
(EngineCore_DP0 pid=296467) .b8 47
(EngineCore_DP0 pid=296467) .b8 118
(EngineCore_DP0 pid=296467) .b8 108
(EngineCore_DP0 pid=296467) .b8 108
(EngineCore_DP0 pid=296467) .b8 109
(EngineCore_DP0 pid=296467) .b8 98
(EngineCore_DP0 pid=296467) .b8 101
(EngineCore_DP0 pid=296467) .b8 110
(EngineCore_DP0 pid=296467) .b8 99
(EngineCore_DP0 pid=296467) .b8 104
(EngineCore_DP0 pid=296467) .b8 47
(EngineCore_DP0 pid=296467) .b8 115
(EngineCore_DP0 pid=296467) .b8 108
(EngineCore_DP0 pid=296467) .b8 105
(EngineCore_DP0 pid=296467) .b8 100
(EngineCore_DP0 pid=296467) .b8 101
(EngineCore_DP0 pid=296467) .b8 115
(EngineCore_DP0 pid=296467) .b8 112
(EngineCore_DP0 pid=296467) .b8 97
(EngineCore_DP0 pid=296467) .b8 114
(EngineCore_DP0 pid=296467) .b8 115
(EngineCore_DP0 pid=296467) .b8 101
(EngineCore_DP0 pid=296467) .b8 47
(EngineCore_DP0 pid=296467) .b8 99
(EngineCore_DP0 pid=296467) .b8 115
(EngineCore_DP0 pid=296467) .b8 114
(EngineCore_DP0 pid=296467) .b8 99
(EngineCore_DP0 pid=296467) .b8 47
(EngineCore_DP0 pid=296467) .b8 113
(EngineCore_DP0 pid=296467) .b8 117
(EngineCore_DP0 pid=296467) .b8 97
(EngineCore_DP0 pid=296467) .b8 110
(EngineCore_DP0 pid=296467) .b8 116
(EngineCore_DP0 pid=296467) .b8 95
(EngineCore_DP0 pid=296467) .b8 111
(EngineCore_DP0 pid=296467) .b8 110
(EngineCore_DP0 pid=296467) .b8 108
(EngineCore_DP0 pid=296467) .b8 121
(EngineCore_DP0 pid=296467) .b8 95
(EngineCore_DP0 pid=296467) .b8 116
(EngineCore_DP0 pid=296467) .b8 114
(EngineCore_DP0 pid=296467) .b8 105
(EngineCore_DP0 pid=296467) .b8 116
(EngineCore_DP0 pid=296467) .b8 111
(EngineCore_DP0 pid=296467) .b8 110
(EngineCore_DP0 pid=296467) .b8 47
(EngineCore_DP0 pid=296467) .b8 98
(EngineCore_DP0 pid=296467) .b8 117
(EngineCore_DP0 pid=296467) .b8 105
(EngineCore_DP0 pid=296467) .b8 108
(EngineCore_DP0 pid=296467) .b8 100
(EngineCore_DP0 pid=296467) .b8 47
(EngineCore_DP0 pid=296467) .b8 71
(EngineCore_DP0 pid=296467) .b8 66
(EngineCore_DP0 pid=296467) .b8 49
(EngineCore_DP0 pid=296467) .b8 48
(EngineCore_DP0 pid=296467) .b8 95
(EngineCore_DP0 pid=296467) .b8 99
(EngineCore_DP0 pid=296467) .b8 99
(EngineCore_DP0 pid=296467) .b8 49
(EngineCore_DP0 pid=296467) .b8 50
(EngineCore_DP0 pid=296467) .b8 49
(EngineCore_DP0 pid=296467) .b8 95
(EngineCore_DP0 pid=296467) .b8 112
(EngineCore_DP0 pid=296467) .b8 121
(EngineCore_DP0 pid=296467) .b8 51
(EngineCore_DP0 pid=296467) .b8 49
(EngineCore_DP0 pid=296467) .b8 50
(EngineCore_DP0 pid=296467) .b8 95
(EngineCore_DP0 pid=296467) .b8 99
(EngineCore_DP0 pid=296467) .b8 117
(EngineCore_DP0 pid=296467) .b8 49
(EngineCore_DP0 pid=296467) .b8 50
(EngineCore_DP0 pid=296467) .b8 57
(EngineCore_DP0 pid=296467) .b8 95
(EngineCore_DP0 pid=296467) .b8 97
(EngineCore_DP0 pid=296467) .b8 97
(EngineCore_DP0 pid=296467) .b8 114
(EngineCore_DP0 pid=296467) .b8 99
(EngineCore_DP0 pid=296467) .b8 104
(EngineCore_DP0 pid=296467) .b8 54
(EngineCore_DP0 pid=296467) .b8 52
(EngineCore_DP0 pid=296467) .b8 0
(EngineCore_DP0 pid=296467) .b8 2                                   // Abbrev [2] 0x91:0x19 DW_TAG_subprogram
(EngineCore_DP0 pid=296467) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=296467) .b8 113
(EngineCore_DP0 pid=296467) .b8 117
(EngineCore_DP0 pid=296467) .b8 97
(EngineCore_DP0 pid=296467) .b8 110
(EngineCore_DP0 pid=296467) .b8 116
(EngineCore_DP0 pid=296467) .b8 95
(EngineCore_DP0 pid=296467) .b8 111
(EngineCore_DP0 pid=296467) .b8 110
(EngineCore_DP0 pid=296467) .b8 108
(EngineCore_DP0 pid=296467) .b8 121
(EngineCore_DP0 pid=296467) .b8 95
(EngineCore_DP0 pid=296467) .b8 102
(EngineCore_DP0 pid=296467) .b8 112
(EngineCore_DP0 pid=296467) .b8 56
(EngineCore_DP0 pid=296467) .b8 95
(EngineCore_DP0 pid=296467) .b8 107
(EngineCore_DP0 pid=296467) .b8 101
(EngineCore_DP0 pid=296467) .b8 114
(EngineCore_DP0 pid=296467) .b8 110
(EngineCore_DP0 pid=296467) .b8 101
(EngineCore_DP0 pid=296467) .b8 108
(EngineCore_DP0 pid=296467) .b8 0
(EngineCore_DP0 pid=296467) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=296467) .b8 3                                   // Abbrev [3] 0xaa:0x2e DW_TAG_subprogram
(EngineCore_DP0 pid=296467) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=296467) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=296467) .b32 145                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=296467) .b8 4                                   // Abbrev [4] 0xbf:0x18 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=296467) .b32 145                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=296467) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=296467) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=296467) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=296467) .b8 94                                  // DW_AT_call_line
(EngineCore_DP0 pid=296467) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=296467) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=296467) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=296467) 	}
(EngineCore_DP0 pid=296467) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) ================================================================
(EngineCore_DP0 pid=296467) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpwjw7gma8.ptx', '-o', '/tmp/tmpwjw7gma8.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866] 
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866] 
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 698, in apply_weights
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 483, in apply
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 181, in cuBLASLt_FP8_linear
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     qinput, scale_a_pad = quant_only_fp8_kernel(input, model_name)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 245, in quant_only_fp8_kernel
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return torch.ops.slidesparse.quant_only_fp8(input, model_name)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 500, in _quant_only_fp8_impl
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return fn(input)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]            ^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py", line 131, in quant_only_fp8_triton
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     _quant_only_fp8_kernel[(M,)](
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866] 
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpwjw7gma8.ptx -o /tmp/tmpwjw7gma8.ptx.o
(EngineCore_DP0 pid=296467) ERROR 01-25 18:45:18 [core.py:866] 

STDERR:
[2026-01-25 18:45:04] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:45:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:45:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:45:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:45:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:45:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:45:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:45:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:45:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:45:07] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:45:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:45:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:45:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:45:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:45:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:45:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:45:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:45:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=296467) [2026-01-25 18:45:08] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=296467) [2026-01-25 18:45:08] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=296467) [2026-01-25 18:45:08] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=296467) [2026-01-25 18:45:08] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=296467) [2026-01-25 18:45:08] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=296467) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=296467) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.33s/it]
(EngineCore_DP0 pid=296467) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.33s/it]
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) Process EngineCore_DP0:
(EngineCore_DP0 pid=296467) Traceback (most recent call last):
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=296467)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=296467)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=296467)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=296467) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpwjw7gma8.ptx', '-o', '/tmp/tmpwjw7gma8.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) Traceback (most recent call last):
(EngineCore_DP0 pid=296467)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=296467)     self.run()
(EngineCore_DP0 pid=296467)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=296467)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=296467)     raise e
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=296467)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=296467)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=296467)     super().__init__(
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=296467)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=296467)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=296467)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=296467)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=296467)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=296467)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=296467)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=296467)     return func(*args, **kwargs)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=296467)     return func(*args, **kwargs)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=296467)     self.model_runner.profile_run()
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=296467)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=296467)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=296467)     return func(*args, **kwargs)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=296467)     outputs = self.model(
(EngineCore_DP0 pid=296467)               ^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296467)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296467)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=296467)     model_output = self.model(
(EngineCore_DP0 pid=296467)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=296467)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=296467)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=296467)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296467)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296467)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=296467)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=296467)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296467)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296467)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=296467)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=296467)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296467)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296467)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=296467)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=296467)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=296467)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 698, in apply_weights
(EngineCore_DP0 pid=296467)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 483, in apply
(EngineCore_DP0 pid=296467)     return self._linear_fn(
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 181, in cuBLASLt_FP8_linear
(EngineCore_DP0 pid=296467)     qinput, scale_a_pad = quant_only_fp8_kernel(input, model_name)
(EngineCore_DP0 pid=296467)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/slidesparse/core/kernels.py", line 245, in quant_only_fp8_kernel
(EngineCore_DP0 pid=296467)     return torch.ops.slidesparse.quant_only_fp8(input, model_name)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=296467)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/slidesparse/core/kernels.py", line 500, in _quant_only_fp8_impl
(EngineCore_DP0 pid=296467)     return fn(input)
(EngineCore_DP0 pid=296467)            ^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py", line 131, in quant_only_fp8_triton
(EngineCore_DP0 pid=296467)     _quant_only_fp8_kernel[(M,)](
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=296467)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=296467)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=296467)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=296467)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=296467)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=296467)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=296467)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=296467)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=296467)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=296467)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296467)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=296467)     raise PTXASError(error)
(EngineCore_DP0 pid=296467) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=296467) `ptxas` stderr:
(EngineCore_DP0 pid=296467) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=296467) 
(EngineCore_DP0 pid=296467) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpwjw7gma8.ptx -o /tmp/tmpwjw7gma8.ptx.o
(EngineCore_DP0 pid=296467) 
[rank0]:[W125 18:45:18.796180926 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=128 ==========
Time: 2026-01-25 18:45:20
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:45:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:45:23 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) ================================================================
(EngineCore_DP0 pid=296925) Internal Triton PTX codegen error
(EngineCore_DP0 pid=296925) `ptxas` stderr:
(EngineCore_DP0 pid=296925) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp4odzhdb5.ptx -o /tmp/tmp4odzhdb5.ptx.o
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) //
(EngineCore_DP0 pid=296925) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=296925) //
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) .version 8.7
(EngineCore_DP0 pid=296925) .target sm_121a
(EngineCore_DP0 pid=296925) .address_size 64
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) 	// .globl	_quant_only_fp8_kernel  // -- Begin function _quant_only_fp8_kernel
(EngineCore_DP0 pid=296925) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=296925)                                         // @_quant_only_fp8_kernel
(EngineCore_DP0 pid=296925) .visible .entry _quant_only_fp8_kernel(
(EngineCore_DP0 pid=296925) 	.param .u64 .ptr .global .align 1 _quant_only_fp8_kernel_param_0,
(EngineCore_DP0 pid=296925) 	.param .u64 .ptr .global .align 1 _quant_only_fp8_kernel_param_1,
(EngineCore_DP0 pid=296925) 	.param .u64 .ptr .global .align 1 _quant_only_fp8_kernel_param_2,
(EngineCore_DP0 pid=296925) 	.param .u32 _quant_only_fp8_kernel_param_3,
(EngineCore_DP0 pid=296925) 	.param .u32 _quant_only_fp8_kernel_param_4,
(EngineCore_DP0 pid=296925) 	.param .u32 _quant_only_fp8_kernel_param_5,
(EngineCore_DP0 pid=296925) 	.param .u64 .ptr .global .align 1 _quant_only_fp8_kernel_param_6,
(EngineCore_DP0 pid=296925) 	.param .u64 .ptr .global .align 1 _quant_only_fp8_kernel_param_7
(EngineCore_DP0 pid=296925) )
(EngineCore_DP0 pid=296925) .reqntid 256
(EngineCore_DP0 pid=296925) {
(EngineCore_DP0 pid=296925) 	.reg .pred 	%p<10>;
(EngineCore_DP0 pid=296925) 	.reg .b16 	%rs<72>;
(EngineCore_DP0 pid=296925) 	.reg .b32 	%r<125>;
(EngineCore_DP0 pid=296925) 	.reg .b64 	%rd<14>;
(EngineCore_DP0 pid=296925) 	.loc	1 72 0                          // quant_only_tuned_Llama3.2-1B.py:72:0
(EngineCore_DP0 pid=296925) $L__func_begin0:
(EngineCore_DP0 pid=296925) 	.loc	1 72 0                          // quant_only_tuned_Llama3.2-1B.py:72:0
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) // %bb.0:
(EngineCore_DP0 pid=296925) 	ld.param.b64 	%rd7, [_quant_only_fp8_kernel_param_0];
(EngineCore_DP0 pid=296925) 	ld.param.b64 	%rd8, [_quant_only_fp8_kernel_param_1];
(EngineCore_DP0 pid=296925) $L__tmp0:
(EngineCore_DP0 pid=296925) 	.loc	1 79 24                         // quant_only_tuned_Llama3.2-1B.py:79:24
(EngineCore_DP0 pid=296925) 	mov.u32 	%r60, %ctaid.x;
(EngineCore_DP0 pid=296925) 	ld.param.b32 	%r61, [_quant_only_fp8_kernel_param_4];
(EngineCore_DP0 pid=296925) 	.loc	1 84 30                         // quant_only_tuned_Llama3.2-1B.py:84:30
(EngineCore_DP0 pid=296925) 	mul.lo.s32 	%r62, %r61, %r60;
(EngineCore_DP0 pid=296925) 	ld.param.b64 	%rd9, [_quant_only_fp8_kernel_param_2];
(EngineCore_DP0 pid=296925) 	.loc	1 84 24                         // quant_only_tuned_Llama3.2-1B.py:84:24
(EngineCore_DP0 pid=296925) 	mad.wide.s32 	%rd10, %r62, 2, %rd7;
(EngineCore_DP0 pid=296925) 	ld.param.b32 	%r63, [_quant_only_fp8_kernel_param_5];
(EngineCore_DP0 pid=296925) 	.loc	1 85 34                         // quant_only_tuned_Llama3.2-1B.py:85:34
(EngineCore_DP0 pid=296925) 	mul.lo.s32 	%r64, %r63, %r60;
(EngineCore_DP0 pid=296925) 	.loc	1 85 28                         // quant_only_tuned_Llama3.2-1B.py:85:28
(EngineCore_DP0 pid=296925) 	cvt.s64.s32 	%rd11, %r64;
(EngineCore_DP0 pid=296925) 	add.s64 	%rd12, %rd8, %rd11;
(EngineCore_DP0 pid=296925) 	.loc	1 91 40                         // quant_only_tuned_Llama3.2-1B.py:91:40
(EngineCore_DP0 pid=296925) 	mov.u32 	%r65, %tid.x;
(EngineCore_DP0 pid=296925) 	and.b32 	%r66, %r65, 31;
(EngineCore_DP0 pid=296925) 	shl.b32 	%r67, %r65, 4;
(EngineCore_DP0 pid=296925) 	and.b32 	%r68, %r67, 4080;
(EngineCore_DP0 pid=296925) 	.loc	1 92 26                         // quant_only_tuned_Llama3.2-1B.py:92:26
(EngineCore_DP0 pid=296925) 	setp.lt.u32 	%p1, %r68, 2048;
(EngineCore_DP0 pid=296925) 	.loc	1 93 36                         // quant_only_tuned_Llama3.2-1B.py:93:36
(EngineCore_DP0 pid=296925) 	cvt.u64.u32 	%rd13, %r68;
(EngineCore_DP0 pid=296925) 	mad.wide.u32 	%rd1, %r68, 2, %rd10;
(EngineCore_DP0 pid=296925) 	add.s64 	%rd2, %rd1, 16;
(EngineCore_DP0 pid=296925) 	mov.b32 	%r5, 0;
(EngineCore_DP0 pid=296925) 	.loc	1 93 24                         // quant_only_tuned_Llama3.2-1B.py:93:24
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	mov.u32 %r1, %r5;
(EngineCore_DP0 pid=296925) 	mov.u32 %r2, %r5;
(EngineCore_DP0 pid=296925) 	mov.u32 %r3, %r5;
(EngineCore_DP0 pid=296925) 	mov.u32 %r4, %r5;
(EngineCore_DP0 pid=296925) 	@%p1 ld.global.v4.b32 { %r1, %r2, %r3, %r4 }, [ %rd1 + 0 ];
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs9, %rs10}, %r1;
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs11, %rs12}, %r2;
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs13, %rs14}, %r3;
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs15, %rs16}, %r4;
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	mov.u32 %r9, %r5;
(EngineCore_DP0 pid=296925) 	mov.u32 %r10, %r5;
(EngineCore_DP0 pid=296925) 	mov.u32 %r11, %r5;
(EngineCore_DP0 pid=296925) 	mov.u32 %r12, %r5;
(EngineCore_DP0 pid=296925) 	@%p1 ld.global.v4.b32 { %r9, %r10, %r11, %r12 }, [ %rd2 + 0 ];
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs17, %rs18}, %r9;
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs19, %rs20}, %r10;
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs21, %rs22}, %r11;
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs23, %rs24}, %r12;
(EngineCore_DP0 pid=296925) 	.loc	1 94 50                         // quant_only_tuned_Llama3.2-1B.py:94:50
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs25, %rs9;
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs26, %rs10;
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs27, %rs11;
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs28, %rs12;
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs29, %rs13;
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs30, %rs14;
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs31, %rs15;
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs32, %rs16;
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs33, %rs17;
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs34, %rs18;
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs35, %rs19;
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs36, %rs20;
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs37, %rs21;
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs38, %rs22;
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs39, %rs23;
(EngineCore_DP0 pid=296925) 	abs.bf16 	%rs40, %rs24;
(EngineCore_DP0 pid=296925) $L__tmp1:
(EngineCore_DP0 pid=296925) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	max.bf16 	%rs41, %rs25, %rs26;
(EngineCore_DP0 pid=296925) 	max.bf16 	%rs42, %rs41, %rs27;
(EngineCore_DP0 pid=296925) 	max.bf16 	%rs43, %rs42, %rs28;
(EngineCore_DP0 pid=296925) 	max.bf16 	%rs44, %rs43, %rs29;
(EngineCore_DP0 pid=296925) 	max.bf16 	%rs45, %rs44, %rs30;
(EngineCore_DP0 pid=296925) 	max.bf16 	%rs46, %rs45, %rs31;
(EngineCore_DP0 pid=296925) 	max.bf16 	%rs47, %rs46, %rs32;
(EngineCore_DP0 pid=296925) 	max.bf16 	%rs48, %rs47, %rs33;
(EngineCore_DP0 pid=296925) 	max.bf16 	%rs49, %rs48, %rs34;
(EngineCore_DP0 pid=296925) 	max.bf16 	%rs50, %rs49, %rs35;
(EngineCore_DP0 pid=296925) 	max.bf16 	%rs51, %rs50, %rs36;
(EngineCore_DP0 pid=296925) 	max.bf16 	%rs52, %rs51, %rs37;
(EngineCore_DP0 pid=296925) 	max.bf16 	%rs53, %rs52, %rs38;
(EngineCore_DP0 pid=296925) 	max.bf16 	%rs54, %rs53, %rs39;
(EngineCore_DP0 pid=296925) 	max.bf16 	%rs55, %rs54, %rs40;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r69, %rs55;
(EngineCore_DP0 pid=296925) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	shfl.sync.bfly.b32 	%r70, %r69, 16, 31, -1;
(EngineCore_DP0 pid=296925) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	max.f32 	%r71, %r69, %r70;
(EngineCore_DP0 pid=296925) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	shfl.sync.bfly.b32 	%r72, %r71, 8, 31, -1;
(EngineCore_DP0 pid=296925) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	max.f32 	%r73, %r71, %r72;
(EngineCore_DP0 pid=296925) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	shfl.sync.bfly.b32 	%r74, %r73, 4, 31, -1;
(EngineCore_DP0 pid=296925) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	max.f32 	%r75, %r73, %r74;
(EngineCore_DP0 pid=296925) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	shfl.sync.bfly.b32 	%r76, %r75, 2, 31, -1;
(EngineCore_DP0 pid=296925) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	max.f32 	%r77, %r75, %r76;
(EngineCore_DP0 pid=296925) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	shfl.sync.bfly.b32 	%r78, %r77, 1, 31, -1;
(EngineCore_DP0 pid=296925) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	max.f32 	%r18, %r77, %r78;
(EngineCore_DP0 pid=296925) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	setp.eq.b32 	%p3, %r66, 0;
(EngineCore_DP0 pid=296925) 	shr.u32 	%r79, %r65, 3;
(EngineCore_DP0 pid=296925) 	and.b32 	%r80, %r79, 28;
(EngineCore_DP0 pid=296925) 	mov.b32 	%r81, global_smem;
(EngineCore_DP0 pid=296925) 	add.s32 	%r17, %r81, %r80;
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	@%p3 st.shared.b32 [ %r17 + 0 ], %r18;
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	bar.sync 	0;
(EngineCore_DP0 pid=296925) 	setp.lt.u32 	%p4, %r65, 8;
(EngineCore_DP0 pid=296925) 	shl.b32 	%r82, %r65, 2;
(EngineCore_DP0 pid=296925) 	add.s32 	%r20, %r81, %r82;
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	@%p4 ld.shared.b32 %r19, [ %r20 + 0 ];
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	shfl.sync.bfly.b32 	%r83, %r19, 4, 31, -1;
(EngineCore_DP0 pid=296925) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	max.f32 	%r84, %r19, %r83;
(EngineCore_DP0 pid=296925) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	shfl.sync.bfly.b32 	%r85, %r84, 2, 31, -1;
(EngineCore_DP0 pid=296925) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	max.f32 	%r86, %r84, %r85;
(EngineCore_DP0 pid=296925) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	shfl.sync.bfly.b32 	%r87, %r86, 1, 31, -1;
(EngineCore_DP0 pid=296925) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	max.f32 	%r22, %r86, %r87;
(EngineCore_DP0 pid=296925) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=296925) 	setp.eq.b32 	%p5, %r65, 0;
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	@%p5 st.shared.b32 [ %r20 + 0 ], %r22;
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	bar.sync 	0;
(EngineCore_DP0 pid=296925) 	ld.shared.b32 	%r88, [global_smem];
(EngineCore_DP0 pid=296925) $L__tmp2:
(EngineCore_DP0 pid=296925) 	.loc	1 97 32                         // quant_only_tuned_Llama3.2-1B.py:97:32
(EngineCore_DP0 pid=296925) 	max.f32 	%r89, %r88, 0f2B8CBCCC;
(EngineCore_DP0 pid=296925) 	mov.b32 	%r90, 0f43E00000;
(EngineCore_DP0 pid=296925) 	.loc	1 98 32                         // quant_only_tuned_Llama3.2-1B.py:98:32
(EngineCore_DP0 pid=296925) 	div.full.f32 	%r91, %r89, %r90;
(EngineCore_DP0 pid=296925) 	.loc	1 98 41                         // quant_only_tuned_Llama3.2-1B.py:98:41
(EngineCore_DP0 pid=296925) 	max.f32 	%r23, %r91, 0f36924925;
(EngineCore_DP0 pid=296925) 	.loc	1 99 26                         // quant_only_tuned_Llama3.2-1B.py:99:26
(EngineCore_DP0 pid=296925) 	div.full.f32 	%r92, %r90, %r89;
(EngineCore_DP0 pid=296925) 	.loc	1 101 25                        // quant_only_tuned_Llama3.2-1B.py:101:25
(EngineCore_DP0 pid=296925) 	mad.wide.u32 	%rd3, %r60, 4, %rd9;
(EngineCore_DP0 pid=296925) 	.loc	1 101 30                        // quant_only_tuned_Llama3.2-1B.py:101:30
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	@%p5 st.global.b32 [ %rd3 + 0 ], { %r23 };
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	.loc	1 107 24                        // quant_only_tuned_Llama3.2-1B.py:107:24
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	mov.u32 %r24, %r5;
(EngineCore_DP0 pid=296925) 	mov.u32 %r25, %r5;
(EngineCore_DP0 pid=296925) 	mov.u32 %r26, %r5;
(EngineCore_DP0 pid=296925) 	mov.u32 %r27, %r5;
(EngineCore_DP0 pid=296925) 	@%p1 ld.global.v4.b32 { %r24, %r25, %r26, %r27 }, [ %rd1 + 0 ];
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs56, %rs57}, %r24;
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs58, %rs59}, %r25;
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs60, %rs61}, %r26;
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs62, %rs63}, %r27;
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	mov.u32 %r32, %r5;
(EngineCore_DP0 pid=296925) 	mov.u32 %r33, %r5;
(EngineCore_DP0 pid=296925) 	mov.u32 %r34, %r5;
(EngineCore_DP0 pid=296925) 	mov.u32 %r35, %r5;
(EngineCore_DP0 pid=296925) 	@%p1 ld.global.v4.b32 { %r32, %r33, %r34, %r35 }, [ %rd2 + 0 ];
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs64, %rs65}, %r32;
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs66, %rs67}, %r33;
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs68, %rs69}, %r34;
(EngineCore_DP0 pid=296925) 	mov.b32 	{%rs70, %rs71}, %r35;
(EngineCore_DP0 pid=296925) 	.loc	1 107 71                        // quant_only_tuned_Llama3.2-1B.py:107:71
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r93, %rs56;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r94, %rs57;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r95, %rs58;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r96, %rs59;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r97, %rs60;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r98, %rs61;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r99, %rs62;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r100, %rs63;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r101, %rs64;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r102, %rs65;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r103, %rs66;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r104, %rs67;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r105, %rs68;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r106, %rs69;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r107, %rs70;
(EngineCore_DP0 pid=296925) 	cvt.f32.bf16 	%r108, %rs71;
(EngineCore_DP0 pid=296925) 	.loc	1 108 33                        // quant_only_tuned_Llama3.2-1B.py:108:33
(EngineCore_DP0 pid=296925) 	mul.f32 	%r109, %r92, %r93;
(EngineCore_DP0 pid=296925) 	mul.f32 	%r110, %r92, %r94;
(EngineCore_DP0 pid=296925) 	mul.f32 	%r111, %r92, %r95;
(EngineCore_DP0 pid=296925) 	mul.f32 	%r112, %r92, %r96;
(EngineCore_DP0 pid=296925) 	mul.f32 	%r113, %r92, %r97;
(EngineCore_DP0 pid=296925) 	mul.f32 	%r114, %r92, %r98;
(EngineCore_DP0 pid=296925) 	mul.f32 	%r115, %r92, %r99;
(EngineCore_DP0 pid=296925) 	mul.f32 	%r116, %r92, %r100;
(EngineCore_DP0 pid=296925) 	mul.f32 	%r117, %r92, %r101;
(EngineCore_DP0 pid=296925) 	mul.f32 	%r118, %r92, %r102;
(EngineCore_DP0 pid=296925) 	mul.f32 	%r119, %r92, %r103;
(EngineCore_DP0 pid=296925) 	mul.f32 	%r120, %r92, %r104;
(EngineCore_DP0 pid=296925) 	mul.f32 	%r121, %r92, %r105;
(EngineCore_DP0 pid=296925) 	mul.f32 	%r122, %r92, %r106;
(EngineCore_DP0 pid=296925) 	mul.f32 	%r123, %r92, %r107;
(EngineCore_DP0 pid=296925) 	mul.f32 	%r124, %r92, %r108;
(EngineCore_DP0 pid=296925) 	.loc	1 108 54                        // quant_only_tuned_Llama3.2-1B.py:108:54
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r40, %r109, %r90;
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r41, %r110, %r90;
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r42, %r111, %r90;
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r43, %r112, %r90;
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r44, %r113, %r90;
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r45, %r114, %r90;
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r46, %r115, %r90;
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r47, %r116, %r90;
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r48, %r117, %r90;
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r49, %r118, %r90;
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r50, %r119, %r90;
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r51, %r120, %r90;
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r52, %r121, %r90;
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r53, %r122, %r90;
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r54, %r123, %r90;
(EngineCore_DP0 pid=296925) 	min.xorsign.abs.f32 	%r55, %r124, %r90;
(EngineCore_DP0 pid=296925) 	.loc	1 109 31                        // quant_only_tuned_Llama3.2-1B.py:109:31
(EngineCore_DP0 pid=296925) 	add.s64 	%rd6, %rd12, %rd13;
(EngineCore_DP0 pid=296925) 	.loc	1 109 48                        // quant_only_tuned_Llama3.2-1B.py:109:48
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	cvt.rn.satfinite.e4m3x2.f32  %rs1, %r41, %r40; 
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	cvt.rn.satfinite.e4m3x2.f32  %rs2, %r43, %r42; 
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	cvt.rn.satfinite.e4m3x2.f32  %rs3, %r45, %r44; 
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	cvt.rn.satfinite.e4m3x2.f32  %rs4, %r47, %r46; 
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	cvt.rn.satfinite.e4m3x2.f32  %rs5, %r49, %r48; 
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	cvt.rn.satfinite.e4m3x2.f32  %rs6, %r51, %r50; 
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	cvt.rn.satfinite.e4m3x2.f32  %rs7, %r53, %r52; 
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	cvt.rn.satfinite.e4m3x2.f32  %rs8, %r55, %r54; 
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	.loc	1 109 39                        // quant_only_tuned_Llama3.2-1B.py:109:39
(EngineCore_DP0 pid=296925) 	mov.b32 	%r56, {%rs1, %rs2};
(EngineCore_DP0 pid=296925) 	mov.b32 	%r57, {%rs3, %rs4};
(EngineCore_DP0 pid=296925) 	mov.b32 	%r58, {%rs5, %rs6};
(EngineCore_DP0 pid=296925) 	mov.b32 	%r59, {%rs7, %rs8};
(EngineCore_DP0 pid=296925) 	// begin inline asm
(EngineCore_DP0 pid=296925) 	@%p1 st.global.v4.b32 [ %rd6 + 0 ], { %r56, %r57, %r58, %r59 };
(EngineCore_DP0 pid=296925) 	// end inline asm
(EngineCore_DP0 pid=296925) 	.loc	1 104 4                         // quant_only_tuned_Llama3.2-1B.py:104:4
(EngineCore_DP0 pid=296925) 	ret;
(EngineCore_DP0 pid=296925) $L__tmp3:
(EngineCore_DP0 pid=296925) $L__func_end0:
(EngineCore_DP0 pid=296925)                                         // -- End function
(EngineCore_DP0 pid=296925) }
(EngineCore_DP0 pid=296925) 	.file	1 "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=296925) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=296925) 	.section	.debug_abbrev
(EngineCore_DP0 pid=296925) 	{
(EngineCore_DP0 pid=296925) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=296925) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=296925) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=296925) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=296925) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=296925) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=296925) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=296925) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=296925) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=296925) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=296925) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=296925) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=296925) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=296925) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=296925) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=296925) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=296925) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=296925) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=296925) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=296925) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=296925) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=296925) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=296925) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=296925) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=296925) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=296925) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=296925) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=296925) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=296925) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=296925) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=296925) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=296925) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=296925) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=296925) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=296925) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=296925) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=296925) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=296925) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=296925) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=296925) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=296925) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=296925) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=296925) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=296925) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=296925) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=296925) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=296925) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=296925) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=296925) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=296925) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=296925) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=296925) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=296925) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=296925) 	}
(EngineCore_DP0 pid=296925) 	.section	.debug_info
(EngineCore_DP0 pid=296925) 	{
(EngineCore_DP0 pid=296925) .b32 213                                // Length of Unit
(EngineCore_DP0 pid=296925) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=296925) .b8 0
(EngineCore_DP0 pid=296925) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=296925) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=296925) .b8 1                                   // Abbrev [1] 0xb:0xce DW_TAG_compile_unit
(EngineCore_DP0 pid=296925) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=296925) .b8 114
(EngineCore_DP0 pid=296925) .b8 105
(EngineCore_DP0 pid=296925) .b8 116
(EngineCore_DP0 pid=296925) .b8 111
(EngineCore_DP0 pid=296925) .b8 110
(EngineCore_DP0 pid=296925) .b8 0
(EngineCore_DP0 pid=296925) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=296925) .b8 0
(EngineCore_DP0 pid=296925) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=296925) .b8 117
(EngineCore_DP0 pid=296925) .b8 97
(EngineCore_DP0 pid=296925) .b8 110
(EngineCore_DP0 pid=296925) .b8 116
(EngineCore_DP0 pid=296925) .b8 95
(EngineCore_DP0 pid=296925) .b8 111
(EngineCore_DP0 pid=296925) .b8 110
(EngineCore_DP0 pid=296925) .b8 108
(EngineCore_DP0 pid=296925) .b8 121
(EngineCore_DP0 pid=296925) .b8 95
(EngineCore_DP0 pid=296925) .b8 116
(EngineCore_DP0 pid=296925) .b8 117
(EngineCore_DP0 pid=296925) .b8 110
(EngineCore_DP0 pid=296925) .b8 101
(EngineCore_DP0 pid=296925) .b8 100
(EngineCore_DP0 pid=296925) .b8 95
(EngineCore_DP0 pid=296925) .b8 76
(EngineCore_DP0 pid=296925) .b8 108
(EngineCore_DP0 pid=296925) .b8 97
(EngineCore_DP0 pid=296925) .b8 109
(EngineCore_DP0 pid=296925) .b8 97
(EngineCore_DP0 pid=296925) .b8 51
(EngineCore_DP0 pid=296925) .b8 46
(EngineCore_DP0 pid=296925) .b8 50
(EngineCore_DP0 pid=296925) .b8 45
(EngineCore_DP0 pid=296925) .b8 49
(EngineCore_DP0 pid=296925) .b8 66
(EngineCore_DP0 pid=296925) .b8 46
(EngineCore_DP0 pid=296925) .b8 112
(EngineCore_DP0 pid=296925) .b8 121
(EngineCore_DP0 pid=296925) .b8 0
(EngineCore_DP0 pid=296925) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=296925) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=296925) .b8 114
(EngineCore_DP0 pid=296925) .b8 111
(EngineCore_DP0 pid=296925) .b8 111
(EngineCore_DP0 pid=296925) .b8 116
(EngineCore_DP0 pid=296925) .b8 47
(EngineCore_DP0 pid=296925) .b8 118
(EngineCore_DP0 pid=296925) .b8 108
(EngineCore_DP0 pid=296925) .b8 108
(EngineCore_DP0 pid=296925) .b8 109
(EngineCore_DP0 pid=296925) .b8 98
(EngineCore_DP0 pid=296925) .b8 101
(EngineCore_DP0 pid=296925) .b8 110
(EngineCore_DP0 pid=296925) .b8 99
(EngineCore_DP0 pid=296925) .b8 104
(EngineCore_DP0 pid=296925) .b8 47
(EngineCore_DP0 pid=296925) .b8 115
(EngineCore_DP0 pid=296925) .b8 108
(EngineCore_DP0 pid=296925) .b8 105
(EngineCore_DP0 pid=296925) .b8 100
(EngineCore_DP0 pid=296925) .b8 101
(EngineCore_DP0 pid=296925) .b8 115
(EngineCore_DP0 pid=296925) .b8 112
(EngineCore_DP0 pid=296925) .b8 97
(EngineCore_DP0 pid=296925) .b8 114
(EngineCore_DP0 pid=296925) .b8 115
(EngineCore_DP0 pid=296925) .b8 101
(EngineCore_DP0 pid=296925) .b8 47
(EngineCore_DP0 pid=296925) .b8 99
(EngineCore_DP0 pid=296925) .b8 115
(EngineCore_DP0 pid=296925) .b8 114
(EngineCore_DP0 pid=296925) .b8 99
(EngineCore_DP0 pid=296925) .b8 47
(EngineCore_DP0 pid=296925) .b8 113
(EngineCore_DP0 pid=296925) .b8 117
(EngineCore_DP0 pid=296925) .b8 97
(EngineCore_DP0 pid=296925) .b8 110
(EngineCore_DP0 pid=296925) .b8 116
(EngineCore_DP0 pid=296925) .b8 95
(EngineCore_DP0 pid=296925) .b8 111
(EngineCore_DP0 pid=296925) .b8 110
(EngineCore_DP0 pid=296925) .b8 108
(EngineCore_DP0 pid=296925) .b8 121
(EngineCore_DP0 pid=296925) .b8 95
(EngineCore_DP0 pid=296925) .b8 116
(EngineCore_DP0 pid=296925) .b8 114
(EngineCore_DP0 pid=296925) .b8 105
(EngineCore_DP0 pid=296925) .b8 116
(EngineCore_DP0 pid=296925) .b8 111
(EngineCore_DP0 pid=296925) .b8 110
(EngineCore_DP0 pid=296925) .b8 47
(EngineCore_DP0 pid=296925) .b8 98
(EngineCore_DP0 pid=296925) .b8 117
(EngineCore_DP0 pid=296925) .b8 105
(EngineCore_DP0 pid=296925) .b8 108
(EngineCore_DP0 pid=296925) .b8 100
(EngineCore_DP0 pid=296925) .b8 47
(EngineCore_DP0 pid=296925) .b8 71
(EngineCore_DP0 pid=296925) .b8 66
(EngineCore_DP0 pid=296925) .b8 49
(EngineCore_DP0 pid=296925) .b8 48
(EngineCore_DP0 pid=296925) .b8 95
(EngineCore_DP0 pid=296925) .b8 99
(EngineCore_DP0 pid=296925) .b8 99
(EngineCore_DP0 pid=296925) .b8 49
(EngineCore_DP0 pid=296925) .b8 50
(EngineCore_DP0 pid=296925) .b8 49
(EngineCore_DP0 pid=296925) .b8 95
(EngineCore_DP0 pid=296925) .b8 112
(EngineCore_DP0 pid=296925) .b8 121
(EngineCore_DP0 pid=296925) .b8 51
(EngineCore_DP0 pid=296925) .b8 49
(EngineCore_DP0 pid=296925) .b8 50
(EngineCore_DP0 pid=296925) .b8 95
(EngineCore_DP0 pid=296925) .b8 99
(EngineCore_DP0 pid=296925) .b8 117
(EngineCore_DP0 pid=296925) .b8 49
(EngineCore_DP0 pid=296925) .b8 50
(EngineCore_DP0 pid=296925) .b8 57
(EngineCore_DP0 pid=296925) .b8 95
(EngineCore_DP0 pid=296925) .b8 97
(EngineCore_DP0 pid=296925) .b8 97
(EngineCore_DP0 pid=296925) .b8 114
(EngineCore_DP0 pid=296925) .b8 99
(EngineCore_DP0 pid=296925) .b8 104
(EngineCore_DP0 pid=296925) .b8 54
(EngineCore_DP0 pid=296925) .b8 52
(EngineCore_DP0 pid=296925) .b8 0
(EngineCore_DP0 pid=296925) .b8 2                                   // Abbrev [2] 0x91:0x19 DW_TAG_subprogram
(EngineCore_DP0 pid=296925) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=296925) .b8 113
(EngineCore_DP0 pid=296925) .b8 117
(EngineCore_DP0 pid=296925) .b8 97
(EngineCore_DP0 pid=296925) .b8 110
(EngineCore_DP0 pid=296925) .b8 116
(EngineCore_DP0 pid=296925) .b8 95
(EngineCore_DP0 pid=296925) .b8 111
(EngineCore_DP0 pid=296925) .b8 110
(EngineCore_DP0 pid=296925) .b8 108
(EngineCore_DP0 pid=296925) .b8 121
(EngineCore_DP0 pid=296925) .b8 95
(EngineCore_DP0 pid=296925) .b8 102
(EngineCore_DP0 pid=296925) .b8 112
(EngineCore_DP0 pid=296925) .b8 56
(EngineCore_DP0 pid=296925) .b8 95
(EngineCore_DP0 pid=296925) .b8 107
(EngineCore_DP0 pid=296925) .b8 101
(EngineCore_DP0 pid=296925) .b8 114
(EngineCore_DP0 pid=296925) .b8 110
(EngineCore_DP0 pid=296925) .b8 101
(EngineCore_DP0 pid=296925) .b8 108
(EngineCore_DP0 pid=296925) .b8 0
(EngineCore_DP0 pid=296925) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=296925) .b8 3                                   // Abbrev [3] 0xaa:0x2e DW_TAG_subprogram
(EngineCore_DP0 pid=296925) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=296925) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=296925) .b32 145                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=296925) .b8 4                                   // Abbrev [4] 0xbf:0x18 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=296925) .b32 145                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=296925) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=296925) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=296925) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=296925) .b8 94                                  // DW_AT_call_line
(EngineCore_DP0 pid=296925) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=296925) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=296925) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=296925) 	}
(EngineCore_DP0 pid=296925) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) ================================================================
(EngineCore_DP0 pid=296925) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmp4odzhdb5.ptx', '-o', '/tmp/tmp4odzhdb5.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866] 
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866] 
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 698, in apply_weights
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 483, in apply
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 181, in cuBLASLt_FP8_linear
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     qinput, scale_a_pad = quant_only_fp8_kernel(input, model_name)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 245, in quant_only_fp8_kernel
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return torch.ops.slidesparse.quant_only_fp8(input, model_name)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 500, in _quant_only_fp8_impl
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return fn(input)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]            ^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py", line 131, in quant_only_fp8_triton
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     _quant_only_fp8_kernel[(M,)](
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866] 
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp4odzhdb5.ptx -o /tmp/tmp4odzhdb5.ptx.o
(EngineCore_DP0 pid=296925) ERROR 01-25 18:45:37 [core.py:866] 

STDERR:
[2026-01-25 18:45:23] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:45:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:45:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:45:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:45:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:45:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:45:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:45:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:45:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:45:27] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:45:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:45:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:45:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:45:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:45:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:45:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:45:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:45:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=296925) [2026-01-25 18:45:28] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=296925) [2026-01-25 18:45:28] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=296925) [2026-01-25 18:45:28] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=296925) [2026-01-25 18:45:28] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=296925) [2026-01-25 18:45:28] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=296925) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=296925) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.31s/it]
(EngineCore_DP0 pid=296925) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.31s/it]
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) Process EngineCore_DP0:
(EngineCore_DP0 pid=296925) Traceback (most recent call last):
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=296925)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=296925)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=296925)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=296925) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmp4odzhdb5.ptx', '-o', '/tmp/tmp4odzhdb5.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) Traceback (most recent call last):
(EngineCore_DP0 pid=296925)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=296925)     self.run()
(EngineCore_DP0 pid=296925)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=296925)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=296925)     raise e
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=296925)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=296925)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=296925)     super().__init__(
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=296925)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=296925)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=296925)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=296925)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=296925)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=296925)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=296925)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=296925)     return func(*args, **kwargs)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=296925)     return func(*args, **kwargs)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=296925)     self.model_runner.profile_run()
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=296925)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=296925)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=296925)     return func(*args, **kwargs)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=296925)     outputs = self.model(
(EngineCore_DP0 pid=296925)               ^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296925)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296925)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=296925)     model_output = self.model(
(EngineCore_DP0 pid=296925)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=296925)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=296925)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=296925)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296925)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296925)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=296925)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=296925)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296925)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296925)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=296925)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=296925)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=296925)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=296925)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=296925)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=296925)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=296925)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 698, in apply_weights
(EngineCore_DP0 pid=296925)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 483, in apply
(EngineCore_DP0 pid=296925)     return self._linear_fn(
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 181, in cuBLASLt_FP8_linear
(EngineCore_DP0 pid=296925)     qinput, scale_a_pad = quant_only_fp8_kernel(input, model_name)
(EngineCore_DP0 pid=296925)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/slidesparse/core/kernels.py", line 245, in quant_only_fp8_kernel
(EngineCore_DP0 pid=296925)     return torch.ops.slidesparse.quant_only_fp8(input, model_name)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=296925)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/slidesparse/core/kernels.py", line 500, in _quant_only_fp8_impl
(EngineCore_DP0 pid=296925)     return fn(input)
(EngineCore_DP0 pid=296925)            ^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py", line 131, in quant_only_fp8_triton
(EngineCore_DP0 pid=296925)     _quant_only_fp8_kernel[(M,)](
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=296925)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=296925)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=296925)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=296925)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=296925)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=296925)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=296925)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=296925)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=296925)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=296925)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=296925)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=296925)     raise PTXASError(error)
(EngineCore_DP0 pid=296925) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=296925) `ptxas` stderr:
(EngineCore_DP0 pid=296925) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=296925) 
(EngineCore_DP0 pid=296925) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp4odzhdb5.ptx -o /tmp/tmp4odzhdb5.ptx.o
(EngineCore_DP0 pid=296925) 
[rank0]:[W125 18:45:37.138422902 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=128

========== M=256 ==========
Time: 2026-01-25 18:45:39
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:45:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:45:43 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) ================================================================
(EngineCore_DP0 pid=297388) Internal Triton PTX codegen error
(EngineCore_DP0 pid=297388) `ptxas` stderr:
(EngineCore_DP0 pid=297388) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpaqk5a3y6.ptx -o /tmp/tmpaqk5a3y6.ptx.o
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) //
(EngineCore_DP0 pid=297388) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=297388) //
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) .version 8.7
(EngineCore_DP0 pid=297388) .target sm_121a
(EngineCore_DP0 pid=297388) .address_size 64
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) 	// .globl	_quant_only_fp8_kernel  // -- Begin function _quant_only_fp8_kernel
(EngineCore_DP0 pid=297388) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=297388)                                         // @_quant_only_fp8_kernel
(EngineCore_DP0 pid=297388) .visible .entry _quant_only_fp8_kernel(
(EngineCore_DP0 pid=297388) 	.param .u64 .ptr .global .align 1 _quant_only_fp8_kernel_param_0,
(EngineCore_DP0 pid=297388) 	.param .u64 .ptr .global .align 1 _quant_only_fp8_kernel_param_1,
(EngineCore_DP0 pid=297388) 	.param .u64 .ptr .global .align 1 _quant_only_fp8_kernel_param_2,
(EngineCore_DP0 pid=297388) 	.param .u32 _quant_only_fp8_kernel_param_3,
(EngineCore_DP0 pid=297388) 	.param .u32 _quant_only_fp8_kernel_param_4,
(EngineCore_DP0 pid=297388) 	.param .u32 _quant_only_fp8_kernel_param_5,
(EngineCore_DP0 pid=297388) 	.param .u64 .ptr .global .align 1 _quant_only_fp8_kernel_param_6,
(EngineCore_DP0 pid=297388) 	.param .u64 .ptr .global .align 1 _quant_only_fp8_kernel_param_7
(EngineCore_DP0 pid=297388) )
(EngineCore_DP0 pid=297388) .reqntid 256
(EngineCore_DP0 pid=297388) {
(EngineCore_DP0 pid=297388) 	.reg .pred 	%p<10>;
(EngineCore_DP0 pid=297388) 	.reg .b16 	%rs<72>;
(EngineCore_DP0 pid=297388) 	.reg .b32 	%r<125>;
(EngineCore_DP0 pid=297388) 	.reg .b64 	%rd<14>;
(EngineCore_DP0 pid=297388) 	.loc	1 72 0                          // quant_only_tuned_Llama3.2-1B.py:72:0
(EngineCore_DP0 pid=297388) $L__func_begin0:
(EngineCore_DP0 pid=297388) 	.loc	1 72 0                          // quant_only_tuned_Llama3.2-1B.py:72:0
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) // %bb.0:
(EngineCore_DP0 pid=297388) 	ld.param.b64 	%rd7, [_quant_only_fp8_kernel_param_0];
(EngineCore_DP0 pid=297388) 	ld.param.b64 	%rd8, [_quant_only_fp8_kernel_param_1];
(EngineCore_DP0 pid=297388) $L__tmp0:
(EngineCore_DP0 pid=297388) 	.loc	1 79 24                         // quant_only_tuned_Llama3.2-1B.py:79:24
(EngineCore_DP0 pid=297388) 	mov.u32 	%r60, %ctaid.x;
(EngineCore_DP0 pid=297388) 	ld.param.b32 	%r61, [_quant_only_fp8_kernel_param_4];
(EngineCore_DP0 pid=297388) 	.loc	1 84 30                         // quant_only_tuned_Llama3.2-1B.py:84:30
(EngineCore_DP0 pid=297388) 	mul.lo.s32 	%r62, %r61, %r60;
(EngineCore_DP0 pid=297388) 	ld.param.b64 	%rd9, [_quant_only_fp8_kernel_param_2];
(EngineCore_DP0 pid=297388) 	.loc	1 84 24                         // quant_only_tuned_Llama3.2-1B.py:84:24
(EngineCore_DP0 pid=297388) 	mad.wide.s32 	%rd10, %r62, 2, %rd7;
(EngineCore_DP0 pid=297388) 	ld.param.b32 	%r63, [_quant_only_fp8_kernel_param_5];
(EngineCore_DP0 pid=297388) 	.loc	1 85 34                         // quant_only_tuned_Llama3.2-1B.py:85:34
(EngineCore_DP0 pid=297388) 	mul.lo.s32 	%r64, %r63, %r60;
(EngineCore_DP0 pid=297388) 	.loc	1 85 28                         // quant_only_tuned_Llama3.2-1B.py:85:28
(EngineCore_DP0 pid=297388) 	cvt.s64.s32 	%rd11, %r64;
(EngineCore_DP0 pid=297388) 	add.s64 	%rd12, %rd8, %rd11;
(EngineCore_DP0 pid=297388) 	.loc	1 91 40                         // quant_only_tuned_Llama3.2-1B.py:91:40
(EngineCore_DP0 pid=297388) 	mov.u32 	%r65, %tid.x;
(EngineCore_DP0 pid=297388) 	and.b32 	%r66, %r65, 31;
(EngineCore_DP0 pid=297388) 	shl.b32 	%r67, %r65, 4;
(EngineCore_DP0 pid=297388) 	and.b32 	%r68, %r67, 4080;
(EngineCore_DP0 pid=297388) 	.loc	1 92 26                         // quant_only_tuned_Llama3.2-1B.py:92:26
(EngineCore_DP0 pid=297388) 	setp.lt.u32 	%p1, %r68, 2048;
(EngineCore_DP0 pid=297388) 	.loc	1 93 36                         // quant_only_tuned_Llama3.2-1B.py:93:36
(EngineCore_DP0 pid=297388) 	cvt.u64.u32 	%rd13, %r68;
(EngineCore_DP0 pid=297388) 	mad.wide.u32 	%rd1, %r68, 2, %rd10;
(EngineCore_DP0 pid=297388) 	add.s64 	%rd2, %rd1, 16;
(EngineCore_DP0 pid=297388) 	mov.b32 	%r5, 0;
(EngineCore_DP0 pid=297388) 	.loc	1 93 24                         // quant_only_tuned_Llama3.2-1B.py:93:24
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	mov.u32 %r1, %r5;
(EngineCore_DP0 pid=297388) 	mov.u32 %r2, %r5;
(EngineCore_DP0 pid=297388) 	mov.u32 %r3, %r5;
(EngineCore_DP0 pid=297388) 	mov.u32 %r4, %r5;
(EngineCore_DP0 pid=297388) 	@%p1 ld.global.v4.b32 { %r1, %r2, %r3, %r4 }, [ %rd1 + 0 ];
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs9, %rs10}, %r1;
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs11, %rs12}, %r2;
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs13, %rs14}, %r3;
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs15, %rs16}, %r4;
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	mov.u32 %r9, %r5;
(EngineCore_DP0 pid=297388) 	mov.u32 %r10, %r5;
(EngineCore_DP0 pid=297388) 	mov.u32 %r11, %r5;
(EngineCore_DP0 pid=297388) 	mov.u32 %r12, %r5;
(EngineCore_DP0 pid=297388) 	@%p1 ld.global.v4.b32 { %r9, %r10, %r11, %r12 }, [ %rd2 + 0 ];
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs17, %rs18}, %r9;
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs19, %rs20}, %r10;
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs21, %rs22}, %r11;
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs23, %rs24}, %r12;
(EngineCore_DP0 pid=297388) 	.loc	1 94 50                         // quant_only_tuned_Llama3.2-1B.py:94:50
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs25, %rs9;
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs26, %rs10;
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs27, %rs11;
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs28, %rs12;
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs29, %rs13;
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs30, %rs14;
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs31, %rs15;
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs32, %rs16;
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs33, %rs17;
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs34, %rs18;
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs35, %rs19;
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs36, %rs20;
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs37, %rs21;
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs38, %rs22;
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs39, %rs23;
(EngineCore_DP0 pid=297388) 	abs.bf16 	%rs40, %rs24;
(EngineCore_DP0 pid=297388) $L__tmp1:
(EngineCore_DP0 pid=297388) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	max.bf16 	%rs41, %rs25, %rs26;
(EngineCore_DP0 pid=297388) 	max.bf16 	%rs42, %rs41, %rs27;
(EngineCore_DP0 pid=297388) 	max.bf16 	%rs43, %rs42, %rs28;
(EngineCore_DP0 pid=297388) 	max.bf16 	%rs44, %rs43, %rs29;
(EngineCore_DP0 pid=297388) 	max.bf16 	%rs45, %rs44, %rs30;
(EngineCore_DP0 pid=297388) 	max.bf16 	%rs46, %rs45, %rs31;
(EngineCore_DP0 pid=297388) 	max.bf16 	%rs47, %rs46, %rs32;
(EngineCore_DP0 pid=297388) 	max.bf16 	%rs48, %rs47, %rs33;
(EngineCore_DP0 pid=297388) 	max.bf16 	%rs49, %rs48, %rs34;
(EngineCore_DP0 pid=297388) 	max.bf16 	%rs50, %rs49, %rs35;
(EngineCore_DP0 pid=297388) 	max.bf16 	%rs51, %rs50, %rs36;
(EngineCore_DP0 pid=297388) 	max.bf16 	%rs52, %rs51, %rs37;
(EngineCore_DP0 pid=297388) 	max.bf16 	%rs53, %rs52, %rs38;
(EngineCore_DP0 pid=297388) 	max.bf16 	%rs54, %rs53, %rs39;
(EngineCore_DP0 pid=297388) 	max.bf16 	%rs55, %rs54, %rs40;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r69, %rs55;
(EngineCore_DP0 pid=297388) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	shfl.sync.bfly.b32 	%r70, %r69, 16, 31, -1;
(EngineCore_DP0 pid=297388) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	max.f32 	%r71, %r69, %r70;
(EngineCore_DP0 pid=297388) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	shfl.sync.bfly.b32 	%r72, %r71, 8, 31, -1;
(EngineCore_DP0 pid=297388) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	max.f32 	%r73, %r71, %r72;
(EngineCore_DP0 pid=297388) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	shfl.sync.bfly.b32 	%r74, %r73, 4, 31, -1;
(EngineCore_DP0 pid=297388) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	max.f32 	%r75, %r73, %r74;
(EngineCore_DP0 pid=297388) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	shfl.sync.bfly.b32 	%r76, %r75, 2, 31, -1;
(EngineCore_DP0 pid=297388) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	max.f32 	%r77, %r75, %r76;
(EngineCore_DP0 pid=297388) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	shfl.sync.bfly.b32 	%r78, %r77, 1, 31, -1;
(EngineCore_DP0 pid=297388) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	max.f32 	%r18, %r77, %r78;
(EngineCore_DP0 pid=297388) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	setp.eq.b32 	%p3, %r66, 0;
(EngineCore_DP0 pid=297388) 	shr.u32 	%r79, %r65, 3;
(EngineCore_DP0 pid=297388) 	and.b32 	%r80, %r79, 28;
(EngineCore_DP0 pid=297388) 	mov.b32 	%r81, global_smem;
(EngineCore_DP0 pid=297388) 	add.s32 	%r17, %r81, %r80;
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	@%p3 st.shared.b32 [ %r17 + 0 ], %r18;
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	bar.sync 	0;
(EngineCore_DP0 pid=297388) 	setp.lt.u32 	%p4, %r65, 8;
(EngineCore_DP0 pid=297388) 	shl.b32 	%r82, %r65, 2;
(EngineCore_DP0 pid=297388) 	add.s32 	%r20, %r81, %r82;
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	@%p4 ld.shared.b32 %r19, [ %r20 + 0 ];
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	shfl.sync.bfly.b32 	%r83, %r19, 4, 31, -1;
(EngineCore_DP0 pid=297388) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	max.f32 	%r84, %r19, %r83;
(EngineCore_DP0 pid=297388) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	shfl.sync.bfly.b32 	%r85, %r84, 2, 31, -1;
(EngineCore_DP0 pid=297388) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	max.f32 	%r86, %r84, %r85;
(EngineCore_DP0 pid=297388) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	shfl.sync.bfly.b32 	%r87, %r86, 1, 31, -1;
(EngineCore_DP0 pid=297388) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	max.f32 	%r22, %r86, %r87;
(EngineCore_DP0 pid=297388) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:94:43 ]
(EngineCore_DP0 pid=297388) 	setp.eq.b32 	%p5, %r65, 0;
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	@%p5 st.shared.b32 [ %r20 + 0 ], %r22;
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	bar.sync 	0;
(EngineCore_DP0 pid=297388) 	ld.shared.b32 	%r88, [global_smem];
(EngineCore_DP0 pid=297388) $L__tmp2:
(EngineCore_DP0 pid=297388) 	.loc	1 97 32                         // quant_only_tuned_Llama3.2-1B.py:97:32
(EngineCore_DP0 pid=297388) 	max.f32 	%r89, %r88, 0f2B8CBCCC;
(EngineCore_DP0 pid=297388) 	mov.b32 	%r90, 0f43E00000;
(EngineCore_DP0 pid=297388) 	.loc	1 98 32                         // quant_only_tuned_Llama3.2-1B.py:98:32
(EngineCore_DP0 pid=297388) 	div.full.f32 	%r91, %r89, %r90;
(EngineCore_DP0 pid=297388) 	.loc	1 98 41                         // quant_only_tuned_Llama3.2-1B.py:98:41
(EngineCore_DP0 pid=297388) 	max.f32 	%r23, %r91, 0f36924925;
(EngineCore_DP0 pid=297388) 	.loc	1 99 26                         // quant_only_tuned_Llama3.2-1B.py:99:26
(EngineCore_DP0 pid=297388) 	div.full.f32 	%r92, %r90, %r89;
(EngineCore_DP0 pid=297388) 	.loc	1 101 25                        // quant_only_tuned_Llama3.2-1B.py:101:25
(EngineCore_DP0 pid=297388) 	mad.wide.u32 	%rd3, %r60, 4, %rd9;
(EngineCore_DP0 pid=297388) 	.loc	1 101 30                        // quant_only_tuned_Llama3.2-1B.py:101:30
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	@%p5 st.global.b32 [ %rd3 + 0 ], { %r23 };
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	.loc	1 107 24                        // quant_only_tuned_Llama3.2-1B.py:107:24
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	mov.u32 %r24, %r5;
(EngineCore_DP0 pid=297388) 	mov.u32 %r25, %r5;
(EngineCore_DP0 pid=297388) 	mov.u32 %r26, %r5;
(EngineCore_DP0 pid=297388) 	mov.u32 %r27, %r5;
(EngineCore_DP0 pid=297388) 	@%p1 ld.global.v4.b32 { %r24, %r25, %r26, %r27 }, [ %rd1 + 0 ];
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs56, %rs57}, %r24;
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs58, %rs59}, %r25;
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs60, %rs61}, %r26;
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs62, %rs63}, %r27;
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	mov.u32 %r32, %r5;
(EngineCore_DP0 pid=297388) 	mov.u32 %r33, %r5;
(EngineCore_DP0 pid=297388) 	mov.u32 %r34, %r5;
(EngineCore_DP0 pid=297388) 	mov.u32 %r35, %r5;
(EngineCore_DP0 pid=297388) 	@%p1 ld.global.v4.b32 { %r32, %r33, %r34, %r35 }, [ %rd2 + 0 ];
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs64, %rs65}, %r32;
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs66, %rs67}, %r33;
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs68, %rs69}, %r34;
(EngineCore_DP0 pid=297388) 	mov.b32 	{%rs70, %rs71}, %r35;
(EngineCore_DP0 pid=297388) 	.loc	1 107 71                        // quant_only_tuned_Llama3.2-1B.py:107:71
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r93, %rs56;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r94, %rs57;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r95, %rs58;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r96, %rs59;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r97, %rs60;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r98, %rs61;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r99, %rs62;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r100, %rs63;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r101, %rs64;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r102, %rs65;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r103, %rs66;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r104, %rs67;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r105, %rs68;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r106, %rs69;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r107, %rs70;
(EngineCore_DP0 pid=297388) 	cvt.f32.bf16 	%r108, %rs71;
(EngineCore_DP0 pid=297388) 	.loc	1 108 33                        // quant_only_tuned_Llama3.2-1B.py:108:33
(EngineCore_DP0 pid=297388) 	mul.f32 	%r109, %r92, %r93;
(EngineCore_DP0 pid=297388) 	mul.f32 	%r110, %r92, %r94;
(EngineCore_DP0 pid=297388) 	mul.f32 	%r111, %r92, %r95;
(EngineCore_DP0 pid=297388) 	mul.f32 	%r112, %r92, %r96;
(EngineCore_DP0 pid=297388) 	mul.f32 	%r113, %r92, %r97;
(EngineCore_DP0 pid=297388) 	mul.f32 	%r114, %r92, %r98;
(EngineCore_DP0 pid=297388) 	mul.f32 	%r115, %r92, %r99;
(EngineCore_DP0 pid=297388) 	mul.f32 	%r116, %r92, %r100;
(EngineCore_DP0 pid=297388) 	mul.f32 	%r117, %r92, %r101;
(EngineCore_DP0 pid=297388) 	mul.f32 	%r118, %r92, %r102;
(EngineCore_DP0 pid=297388) 	mul.f32 	%r119, %r92, %r103;
(EngineCore_DP0 pid=297388) 	mul.f32 	%r120, %r92, %r104;
(EngineCore_DP0 pid=297388) 	mul.f32 	%r121, %r92, %r105;
(EngineCore_DP0 pid=297388) 	mul.f32 	%r122, %r92, %r106;
(EngineCore_DP0 pid=297388) 	mul.f32 	%r123, %r92, %r107;
(EngineCore_DP0 pid=297388) 	mul.f32 	%r124, %r92, %r108;
(EngineCore_DP0 pid=297388) 	.loc	1 108 54                        // quant_only_tuned_Llama3.2-1B.py:108:54
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r40, %r109, %r90;
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r41, %r110, %r90;
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r42, %r111, %r90;
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r43, %r112, %r90;
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r44, %r113, %r90;
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r45, %r114, %r90;
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r46, %r115, %r90;
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r47, %r116, %r90;
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r48, %r117, %r90;
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r49, %r118, %r90;
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r50, %r119, %r90;
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r51, %r120, %r90;
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r52, %r121, %r90;
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r53, %r122, %r90;
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r54, %r123, %r90;
(EngineCore_DP0 pid=297388) 	min.xorsign.abs.f32 	%r55, %r124, %r90;
(EngineCore_DP0 pid=297388) 	.loc	1 109 31                        // quant_only_tuned_Llama3.2-1B.py:109:31
(EngineCore_DP0 pid=297388) 	add.s64 	%rd6, %rd12, %rd13;
(EngineCore_DP0 pid=297388) 	.loc	1 109 48                        // quant_only_tuned_Llama3.2-1B.py:109:48
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	cvt.rn.satfinite.e4m3x2.f32  %rs1, %r41, %r40; 
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	cvt.rn.satfinite.e4m3x2.f32  %rs2, %r43, %r42; 
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	cvt.rn.satfinite.e4m3x2.f32  %rs3, %r45, %r44; 
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	cvt.rn.satfinite.e4m3x2.f32  %rs4, %r47, %r46; 
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	cvt.rn.satfinite.e4m3x2.f32  %rs5, %r49, %r48; 
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	cvt.rn.satfinite.e4m3x2.f32  %rs6, %r51, %r50; 
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	cvt.rn.satfinite.e4m3x2.f32  %rs7, %r53, %r52; 
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	cvt.rn.satfinite.e4m3x2.f32  %rs8, %r55, %r54; 
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	.loc	1 109 39                        // quant_only_tuned_Llama3.2-1B.py:109:39
(EngineCore_DP0 pid=297388) 	mov.b32 	%r56, {%rs1, %rs2};
(EngineCore_DP0 pid=297388) 	mov.b32 	%r57, {%rs3, %rs4};
(EngineCore_DP0 pid=297388) 	mov.b32 	%r58, {%rs5, %rs6};
(EngineCore_DP0 pid=297388) 	mov.b32 	%r59, {%rs7, %rs8};
(EngineCore_DP0 pid=297388) 	// begin inline asm
(EngineCore_DP0 pid=297388) 	@%p1 st.global.v4.b32 [ %rd6 + 0 ], { %r56, %r57, %r58, %r59 };
(EngineCore_DP0 pid=297388) 	// end inline asm
(EngineCore_DP0 pid=297388) 	.loc	1 104 4                         // quant_only_tuned_Llama3.2-1B.py:104:4
(EngineCore_DP0 pid=297388) 	ret;
(EngineCore_DP0 pid=297388) $L__tmp3:
(EngineCore_DP0 pid=297388) $L__func_end0:
(EngineCore_DP0 pid=297388)                                         // -- End function
(EngineCore_DP0 pid=297388) }
(EngineCore_DP0 pid=297388) 	.file	1 "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=297388) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=297388) 	.section	.debug_abbrev
(EngineCore_DP0 pid=297388) 	{
(EngineCore_DP0 pid=297388) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=297388) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=297388) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=297388) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=297388) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=297388) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=297388) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=297388) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=297388) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=297388) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=297388) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=297388) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=297388) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=297388) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=297388) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=297388) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=297388) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=297388) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=297388) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=297388) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=297388) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=297388) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=297388) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=297388) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=297388) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=297388) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=297388) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=297388) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=297388) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=297388) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=297388) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=297388) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=297388) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=297388) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=297388) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=297388) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=297388) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=297388) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=297388) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=297388) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=297388) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=297388) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=297388) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=297388) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=297388) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=297388) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=297388) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=297388) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=297388) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=297388) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=297388) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=297388) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=297388) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=297388) 	}
(EngineCore_DP0 pid=297388) 	.section	.debug_info
(EngineCore_DP0 pid=297388) 	{
(EngineCore_DP0 pid=297388) .b32 213                                // Length of Unit
(EngineCore_DP0 pid=297388) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=297388) .b8 0
(EngineCore_DP0 pid=297388) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=297388) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=297388) .b8 1                                   // Abbrev [1] 0xb:0xce DW_TAG_compile_unit
(EngineCore_DP0 pid=297388) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=297388) .b8 114
(EngineCore_DP0 pid=297388) .b8 105
(EngineCore_DP0 pid=297388) .b8 116
(EngineCore_DP0 pid=297388) .b8 111
(EngineCore_DP0 pid=297388) .b8 110
(EngineCore_DP0 pid=297388) .b8 0
(EngineCore_DP0 pid=297388) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=297388) .b8 0
(EngineCore_DP0 pid=297388) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=297388) .b8 117
(EngineCore_DP0 pid=297388) .b8 97
(EngineCore_DP0 pid=297388) .b8 110
(EngineCore_DP0 pid=297388) .b8 116
(EngineCore_DP0 pid=297388) .b8 95
(EngineCore_DP0 pid=297388) .b8 111
(EngineCore_DP0 pid=297388) .b8 110
(EngineCore_DP0 pid=297388) .b8 108
(EngineCore_DP0 pid=297388) .b8 121
(EngineCore_DP0 pid=297388) .b8 95
(EngineCore_DP0 pid=297388) .b8 116
(EngineCore_DP0 pid=297388) .b8 117
(EngineCore_DP0 pid=297388) .b8 110
(EngineCore_DP0 pid=297388) .b8 101
(EngineCore_DP0 pid=297388) .b8 100
(EngineCore_DP0 pid=297388) .b8 95
(EngineCore_DP0 pid=297388) .b8 76
(EngineCore_DP0 pid=297388) .b8 108
(EngineCore_DP0 pid=297388) .b8 97
(EngineCore_DP0 pid=297388) .b8 109
(EngineCore_DP0 pid=297388) .b8 97
(EngineCore_DP0 pid=297388) .b8 51
(EngineCore_DP0 pid=297388) .b8 46
(EngineCore_DP0 pid=297388) .b8 50
(EngineCore_DP0 pid=297388) .b8 45
(EngineCore_DP0 pid=297388) .b8 49
(EngineCore_DP0 pid=297388) .b8 66
(EngineCore_DP0 pid=297388) .b8 46
(EngineCore_DP0 pid=297388) .b8 112
(EngineCore_DP0 pid=297388) .b8 121
(EngineCore_DP0 pid=297388) .b8 0
(EngineCore_DP0 pid=297388) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=297388) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=297388) .b8 114
(EngineCore_DP0 pid=297388) .b8 111
(EngineCore_DP0 pid=297388) .b8 111
(EngineCore_DP0 pid=297388) .b8 116
(EngineCore_DP0 pid=297388) .b8 47
(EngineCore_DP0 pid=297388) .b8 118
(EngineCore_DP0 pid=297388) .b8 108
(EngineCore_DP0 pid=297388) .b8 108
(EngineCore_DP0 pid=297388) .b8 109
(EngineCore_DP0 pid=297388) .b8 98
(EngineCore_DP0 pid=297388) .b8 101
(EngineCore_DP0 pid=297388) .b8 110
(EngineCore_DP0 pid=297388) .b8 99
(EngineCore_DP0 pid=297388) .b8 104
(EngineCore_DP0 pid=297388) .b8 47
(EngineCore_DP0 pid=297388) .b8 115
(EngineCore_DP0 pid=297388) .b8 108
(EngineCore_DP0 pid=297388) .b8 105
(EngineCore_DP0 pid=297388) .b8 100
(EngineCore_DP0 pid=297388) .b8 101
(EngineCore_DP0 pid=297388) .b8 115
(EngineCore_DP0 pid=297388) .b8 112
(EngineCore_DP0 pid=297388) .b8 97
(EngineCore_DP0 pid=297388) .b8 114
(EngineCore_DP0 pid=297388) .b8 115
(EngineCore_DP0 pid=297388) .b8 101
(EngineCore_DP0 pid=297388) .b8 47
(EngineCore_DP0 pid=297388) .b8 99
(EngineCore_DP0 pid=297388) .b8 115
(EngineCore_DP0 pid=297388) .b8 114
(EngineCore_DP0 pid=297388) .b8 99
(EngineCore_DP0 pid=297388) .b8 47
(EngineCore_DP0 pid=297388) .b8 113
(EngineCore_DP0 pid=297388) .b8 117
(EngineCore_DP0 pid=297388) .b8 97
(EngineCore_DP0 pid=297388) .b8 110
(EngineCore_DP0 pid=297388) .b8 116
(EngineCore_DP0 pid=297388) .b8 95
(EngineCore_DP0 pid=297388) .b8 111
(EngineCore_DP0 pid=297388) .b8 110
(EngineCore_DP0 pid=297388) .b8 108
(EngineCore_DP0 pid=297388) .b8 121
(EngineCore_DP0 pid=297388) .b8 95
(EngineCore_DP0 pid=297388) .b8 116
(EngineCore_DP0 pid=297388) .b8 114
(EngineCore_DP0 pid=297388) .b8 105
(EngineCore_DP0 pid=297388) .b8 116
(EngineCore_DP0 pid=297388) .b8 111
(EngineCore_DP0 pid=297388) .b8 110
(EngineCore_DP0 pid=297388) .b8 47
(EngineCore_DP0 pid=297388) .b8 98
(EngineCore_DP0 pid=297388) .b8 117
(EngineCore_DP0 pid=297388) .b8 105
(EngineCore_DP0 pid=297388) .b8 108
(EngineCore_DP0 pid=297388) .b8 100
(EngineCore_DP0 pid=297388) .b8 47
(EngineCore_DP0 pid=297388) .b8 71
(EngineCore_DP0 pid=297388) .b8 66
(EngineCore_DP0 pid=297388) .b8 49
(EngineCore_DP0 pid=297388) .b8 48
(EngineCore_DP0 pid=297388) .b8 95
(EngineCore_DP0 pid=297388) .b8 99
(EngineCore_DP0 pid=297388) .b8 99
(EngineCore_DP0 pid=297388) .b8 49
(EngineCore_DP0 pid=297388) .b8 50
(EngineCore_DP0 pid=297388) .b8 49
(EngineCore_DP0 pid=297388) .b8 95
(EngineCore_DP0 pid=297388) .b8 112
(EngineCore_DP0 pid=297388) .b8 121
(EngineCore_DP0 pid=297388) .b8 51
(EngineCore_DP0 pid=297388) .b8 49
(EngineCore_DP0 pid=297388) .b8 50
(EngineCore_DP0 pid=297388) .b8 95
(EngineCore_DP0 pid=297388) .b8 99
(EngineCore_DP0 pid=297388) .b8 117
(EngineCore_DP0 pid=297388) .b8 49
(EngineCore_DP0 pid=297388) .b8 50
(EngineCore_DP0 pid=297388) .b8 57
(EngineCore_DP0 pid=297388) .b8 95
(EngineCore_DP0 pid=297388) .b8 97
(EngineCore_DP0 pid=297388) .b8 97
(EngineCore_DP0 pid=297388) .b8 114
(EngineCore_DP0 pid=297388) .b8 99
(EngineCore_DP0 pid=297388) .b8 104
(EngineCore_DP0 pid=297388) .b8 54
(EngineCore_DP0 pid=297388) .b8 52
(EngineCore_DP0 pid=297388) .b8 0
(EngineCore_DP0 pid=297388) .b8 2                                   // Abbrev [2] 0x91:0x19 DW_TAG_subprogram
(EngineCore_DP0 pid=297388) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=297388) .b8 113
(EngineCore_DP0 pid=297388) .b8 117
(EngineCore_DP0 pid=297388) .b8 97
(EngineCore_DP0 pid=297388) .b8 110
(EngineCore_DP0 pid=297388) .b8 116
(EngineCore_DP0 pid=297388) .b8 95
(EngineCore_DP0 pid=297388) .b8 111
(EngineCore_DP0 pid=297388) .b8 110
(EngineCore_DP0 pid=297388) .b8 108
(EngineCore_DP0 pid=297388) .b8 121
(EngineCore_DP0 pid=297388) .b8 95
(EngineCore_DP0 pid=297388) .b8 102
(EngineCore_DP0 pid=297388) .b8 112
(EngineCore_DP0 pid=297388) .b8 56
(EngineCore_DP0 pid=297388) .b8 95
(EngineCore_DP0 pid=297388) .b8 107
(EngineCore_DP0 pid=297388) .b8 101
(EngineCore_DP0 pid=297388) .b8 114
(EngineCore_DP0 pid=297388) .b8 110
(EngineCore_DP0 pid=297388) .b8 101
(EngineCore_DP0 pid=297388) .b8 108
(EngineCore_DP0 pid=297388) .b8 0
(EngineCore_DP0 pid=297388) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=297388) .b8 3                                   // Abbrev [3] 0xaa:0x2e DW_TAG_subprogram
(EngineCore_DP0 pid=297388) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=297388) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=297388) .b32 145                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=297388) .b8 4                                   // Abbrev [4] 0xbf:0x18 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=297388) .b32 145                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=297388) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=297388) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=297388) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=297388) .b8 94                                  // DW_AT_call_line
(EngineCore_DP0 pid=297388) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=297388) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=297388) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=297388) 	}
(EngineCore_DP0 pid=297388) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) ================================================================
(EngineCore_DP0 pid=297388) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpaqk5a3y6.ptx', '-o', '/tmp/tmpaqk5a3y6.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866] 
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866] 
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 698, in apply_weights
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 483, in apply
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 181, in cuBLASLt_FP8_linear
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     qinput, scale_a_pad = quant_only_fp8_kernel(input, model_name)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 245, in quant_only_fp8_kernel
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return torch.ops.slidesparse.quant_only_fp8(input, model_name)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 500, in _quant_only_fp8_impl
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return fn(input)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]            ^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py", line 131, in quant_only_fp8_triton
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     _quant_only_fp8_kernel[(M,)](
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866] 
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpaqk5a3y6.ptx -o /tmp/tmpaqk5a3y6.ptx.o
(EngineCore_DP0 pid=297388) ERROR 01-25 18:45:56 [core.py:866] 

STDERR:
[2026-01-25 18:45:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:45:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:45:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:45:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:45:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:45:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:45:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:45:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:45:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:45:46] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:45:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:45:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:45:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:45:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:45:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:45:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:45:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:45:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:45:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=297388) [2026-01-25 18:45:47] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=297388) [2026-01-25 18:45:47] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=297388) [2026-01-25 18:45:47] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=297388) [2026-01-25 18:45:47] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=297388) [2026-01-25 18:45:47] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=297388) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=297388) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.37s/it]
(EngineCore_DP0 pid=297388) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.37s/it]
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) Process EngineCore_DP0:
(EngineCore_DP0 pid=297388) Traceback (most recent call last):
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=297388)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=297388)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=297388)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=297388) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpaqk5a3y6.ptx', '-o', '/tmp/tmpaqk5a3y6.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) Traceback (most recent call last):
(EngineCore_DP0 pid=297388)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=297388)     self.run()
(EngineCore_DP0 pid=297388)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=297388)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=297388)     raise e
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=297388)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=297388)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=297388)     super().__init__(
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=297388)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=297388)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=297388)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=297388)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=297388)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=297388)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=297388)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=297388)     return func(*args, **kwargs)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=297388)     return func(*args, **kwargs)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=297388)     self.model_runner.profile_run()
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=297388)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=297388)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=297388)     return func(*args, **kwargs)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=297388)     outputs = self.model(
(EngineCore_DP0 pid=297388)               ^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=297388)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=297388)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=297388)     model_output = self.model(
(EngineCore_DP0 pid=297388)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=297388)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=297388)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=297388)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=297388)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=297388)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=297388)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=297388)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=297388)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=297388)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=297388)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=297388)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=297388)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=297388)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=297388)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=297388)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=297388)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 698, in apply_weights
(EngineCore_DP0 pid=297388)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 483, in apply
(EngineCore_DP0 pid=297388)     return self._linear_fn(
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 181, in cuBLASLt_FP8_linear
(EngineCore_DP0 pid=297388)     qinput, scale_a_pad = quant_only_fp8_kernel(input, model_name)
(EngineCore_DP0 pid=297388)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/slidesparse/core/kernels.py", line 245, in quant_only_fp8_kernel
(EngineCore_DP0 pid=297388)     return torch.ops.slidesparse.quant_only_fp8(input, model_name)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=297388)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/slidesparse/core/kernels.py", line 500, in _quant_only_fp8_impl
(EngineCore_DP0 pid=297388)     return fn(input)
(EngineCore_DP0 pid=297388)            ^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py", line 131, in quant_only_fp8_triton
(EngineCore_DP0 pid=297388)     _quant_only_fp8_kernel[(M,)](
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=297388)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=297388)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=297388)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=297388)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=297388)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=297388)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=297388)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=297388)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=297388)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=297388)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=297388)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=297388)     raise PTXASError(error)
(EngineCore_DP0 pid=297388) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=297388) `ptxas` stderr:
(EngineCore_DP0 pid=297388) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=297388) 
(EngineCore_DP0 pid=297388) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpaqk5a3y6.ptx -o /tmp/tmpaqk5a3y6.ptx.o
(EngineCore_DP0 pid=297388) 
[rank0]:[W125 18:45:57.481725766 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=256

========== M=16 ==========
Time: 2026-01-26 02:34:17
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-FP8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:34:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:34:21 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=750200) WARNING 01-26 02:34:40 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.29 requests/s, 1439.94 total tokens/s, 1355.24 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096

STDERR:
[2026-01-26 02:34:21] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:34:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:34:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:34:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:34:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:34:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:34:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:34:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:34:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:34:24] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:34:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:34:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:34:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:34:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:34:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:34:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:34:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:34:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=750200) [2026-01-26 02:34:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=750200) [2026-01-26 02:34:25] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=750200) [2026-01-26 02:34:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=750200) [2026-01-26 02:34:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=750200) [2026-01-26 02:34:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=750200) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=750200) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.11s/it]
(EngineCore_DP0 pid=750200) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.11s/it]
(EngineCore_DP0 pid=750200) 
(EngineCore_DP0 pid=750200) 2026-01-26 02:34:39,418 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=750200) 2026-01-26 02:34:39,425 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|| 16/16 [00:00<00:00, 8548.90it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|         | 1/16 [00:03<00:45,  3.01s/it, est. speed input: 5.32 toks/s, output: 85.16 toks/s]
Processed prompts: 100%|| 16/16 [00:03<00:00,  3.01s/it, est. speed input: 84.78 toks/s, output: 1356.43 toks/s]
Processed prompts: 100%|| 16/16 [00:03<00:00,  5.30it/s, est. speed input: 84.78 toks/s, output: 1356.43 toks/s]
[rank0]:[W126 02:34:43.467201009 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 02:34:45
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:34:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:34:49 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=750796) WARNING 01-26 02:35:07 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 23.88 requests/s, 6495.61 total tokens/s, 6113.52 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 02:34:48] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:34:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:34:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:34:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:34:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:34:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:34:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:34:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:34:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:34:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:34:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:34:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:34:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:34:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:34:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:34:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:34:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:34:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:34:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=750796) [2026-01-26 02:34:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=750796) [2026-01-26 02:34:53] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=750796) [2026-01-26 02:34:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=750796) [2026-01-26 02:34:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=750796) [2026-01-26 02:34:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=750796) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=750796) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.17s/it]
(EngineCore_DP0 pid=750796) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.17s/it]
(EngineCore_DP0 pid=750796) 
(EngineCore_DP0 pid=750796) 2026-01-26 02:35:06,934 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=750796) 2026-01-26 02:35:06,941 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 12901.83it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:05<10:49,  5.12s/it, est. speed input: 3.13 toks/s, output: 50.04 toks/s]
Processed prompts:  59%|    | 76/128 [00:05<00:02, 20.48it/s, est. speed input: 232.47 toks/s, output: 3719.49 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 38.81it/s, est. speed input: 382.87 toks/s, output: 6125.97 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 38.81it/s, est. speed input: 382.87 toks/s, output: 6125.97 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 23.93it/s, est. speed input: 382.87 toks/s, output: 6125.97 toks/s]
[rank0]:[W126 02:35:13.463621634 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 02:35:15
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:35:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:35:19 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=751409) WARNING 01-26 02:35:37 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 30.09 requests/s, 8184.16 total tokens/s, 7702.73 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 02:35:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:35:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:35:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:35:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:35:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:35:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:35:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:35:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:35:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:35:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:35:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:35:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:35:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:35:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:35:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:35:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:35:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:35:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:35:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:35:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:35:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:35:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:35:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:35:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:35:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:35:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:35:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:35:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=751409) [2026-01-26 02:35:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=751409) [2026-01-26 02:35:23] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=751409) [2026-01-26 02:35:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=751409) [2026-01-26 02:35:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=751409) [2026-01-26 02:35:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=751409) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=751409) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.14s/it]
(EngineCore_DP0 pid=751409) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.14s/it]
(EngineCore_DP0 pid=751409) 
(EngineCore_DP0 pid=751409) 2026-01-26 02:35:37,106 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=751409) 2026-01-26 02:35:37,114 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 12620.08it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<32:13,  7.58s/it, est. speed input: 2.11 toks/s, output: 33.77 toks/s]
Processed prompts:  14%|        | 36/256 [00:07<00:33,  6.60it/s, est. speed input: 74.98 toks/s, output: 1199.61 toks/s]
Processed prompts:  34%|      | 88/256 [00:07<00:08, 19.77it/s, est. speed input: 180.73 toks/s, output: 2891.64 toks/s]
Processed prompts:  54%|    | 137/256 [00:07<00:03, 36.23it/s, est. speed input: 277.30 toks/s, output: 4436.85 toks/s]
Processed prompts:  70%|   | 179/256 [00:08<00:01, 54.39it/s, est. speed input: 357.42 toks/s, output: 5718.63 toks/s]
Processed prompts:  85%| | 217/256 [00:08<00:00, 73.51it/s, est. speed input: 426.09 toks/s, output: 6817.47 toks/s]
Processed prompts:  98%|| 252/256 [00:08<00:00, 86.32it/s, est. speed input: 480.72 toks/s, output: 7691.55 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 86.32it/s, est. speed input: 482.63 toks/s, output: 7722.02 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 30.16it/s, est. speed input: 482.63 toks/s, output: 7722.02 toks/s]
[rank0]:[W126 02:35:46.693636773 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:01:10
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:01:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:01:13 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2841531) WARNING 01-27 17:01:32 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.26 requests/s, 4423.25 total tokens/s, 4163.06 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:01:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:01:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:01:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:01:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:01:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:01:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:01:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:01:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:01:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:01:16] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:01:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:01:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:01:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:01:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:01:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:01:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:01:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:01:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2841531) [2026-01-27 17:01:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2841531) [2026-01-27 17:01:18] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2841531) [2026-01-27 17:01:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2841531) [2026-01-27 17:01:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2841531) [2026-01-27 17:01:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2841531) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2841531) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.15s/it]
(EngineCore_DP0 pid=2841531) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.15s/it]
(EngineCore_DP0 pid=2841531) 
(EngineCore_DP0 pid=2841531) 2026-01-27 17:01:31,781 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2841531) 2026-01-27 17:01:31,793 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 12091.14it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:03<04:04,  3.88s/it, est. speed input: 4.13 toks/s, output: 66.04 toks/s]
Processed prompts: 100%|| 64/64 [00:03<00:00,  3.88s/it, est. speed input: 260.60 toks/s, output: 4169.57 toks/s]
Processed prompts: 100%|| 64/64 [00:03<00:00, 16.29it/s, est. speed input: 260.60 toks/s, output: 4169.57 toks/s]
[rank0]:[W127 17:01:36.567362451 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:01:39
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:01:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:01:42 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2842129) WARNING 01-27 17:02:01 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 24.44 requests/s, 6646.68 total tokens/s, 6255.70 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:01:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:01:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:01:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:01:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:01:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:01:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:01:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:01:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:01:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:01:45] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:01:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:01:45] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:01:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:01:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:01:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:01:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:01:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:01:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:01:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2842129) [2026-01-27 17:01:46] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2842129) [2026-01-27 17:01:46] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2842129) [2026-01-27 17:01:46] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2842129) [2026-01-27 17:01:46] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2842129) [2026-01-27 17:01:46] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2842129) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2842129) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.02s/it]
(EngineCore_DP0 pid=2842129) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.02s/it]
(EngineCore_DP0 pid=2842129) 
(EngineCore_DP0 pid=2842129) 2026-01-27 17:02:00,542 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2842129) 2026-01-27 17:02:00,550 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 12790.86it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:04<10:32,  4.98s/it, est. speed input: 3.21 toks/s, output: 51.41 toks/s]
Processed prompts:  59%|    | 76/128 [00:05<00:02, 21.06it/s, est. speed input: 238.96 toks/s, output: 3823.35 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 39.55it/s, est. speed input: 391.81 toks/s, output: 6269.00 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 39.55it/s, est. speed input: 391.81 toks/s, output: 6269.00 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 24.49it/s, est. speed input: 391.81 toks/s, output: 6269.00 toks/s]
[rank0]:[W127 17:02:06.646176693 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:02:08
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:02:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:02:12 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2842742) WARNING 01-27 17:02:30 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 30.16 requests/s, 8204.19 total tokens/s, 7721.59 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:02:12] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:02:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:02:12] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:02:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:02:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:02:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:02:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:02:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:02:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:02:15] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:02:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:02:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:02:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:02:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:02:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:02:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:02:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:02:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2842742) [2026-01-27 17:02:16] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2842742) [2026-01-27 17:02:16] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2842742) [2026-01-27 17:02:16] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2842742) [2026-01-27 17:02:16] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2842742) [2026-01-27 17:02:16] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2842742) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2842742) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.95s/it]
(EngineCore_DP0 pid=2842742) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.95s/it]
(EngineCore_DP0 pid=2842742) 
(EngineCore_DP0 pid=2842742) 2026-01-27 17:02:30,220 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2842742) 2026-01-27 17:02:30,230 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 12441.25it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<32:21,  7.61s/it, est. speed input: 2.10 toks/s, output: 33.62 toks/s]
Processed prompts:  19%|        | 48/256 [00:07<00:23,  8.74it/s, est. speed input: 99.19 toks/s, output: 1587.02 toks/s]
Processed prompts:  38%|      | 98/256 [00:07<00:07, 21.22it/s, est. speed input: 199.71 toks/s, output: 3195.31 toks/s]
Processed prompts:  56%|    | 144/256 [00:07<00:03, 36.49it/s, est. speed input: 289.41 toks/s, output: 4630.59 toks/s]
Processed prompts:  71%|  | 183/256 [00:08<00:01, 53.17it/s, est. speed input: 362.98 toks/s, output: 5807.73 toks/s]
Processed prompts:  86%| | 220/256 [00:08<00:00, 71.90it/s, est. speed input: 429.44 toks/s, output: 6870.96 toks/s]
Processed prompts:  99%|| 254/256 [00:08<00:00, 83.85it/s, est. speed input: 481.40 toks/s, output: 7702.37 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 83.85it/s, est. speed input: 483.82 toks/s, output: 7741.15 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 30.24it/s, est. speed input: 483.82 toks/s, output: 7741.15 toks/s]
[rank0]:[W127 17:02:39.608190481 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:02:41
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:02:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:02:45 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2843382) WARNING 01-27 17:03:04 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 31.79 requests/s, 8647.29 total tokens/s, 8138.63 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:02:45] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:02:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:02:45] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:02:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:02:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:02:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:02:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:02:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:02:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:02:48] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:02:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:02:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:02:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:02:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:02:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:02:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:02:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:02:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:02:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2843382) [2026-01-27 17:02:49] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2843382) [2026-01-27 17:02:49] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2843382) [2026-01-27 17:02:49] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2843382) [2026-01-27 17:02:49] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2843382) [2026-01-27 17:02:49] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2843382) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2843382) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.22s/it]
(EngineCore_DP0 pid=2843382) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.22s/it]
(EngineCore_DP0 pid=2843382) 
(EngineCore_DP0 pid=2843382) 2026-01-27 17:03:03,416 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2843382) 2026-01-27 17:03:03,428 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 13572.17it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:12<1:45:54, 12.43s/it, est. speed input: 1.29 toks/s, output: 20.59 toks/s]
Processed prompts:   1%|          | 3/512 [00:12<27:40,  3.26s/it, est. speed input: 3.83 toks/s, output: 61.24 toks/s]  
Processed prompts:  12%|        | 63/512 [00:12<00:46,  9.74it/s, est. speed input: 79.20 toks/s, output: 1267.16 toks/s]
Processed prompts:  23%|       | 117/512 [00:12<00:18, 21.15it/s, est. speed input: 145.32 toks/s, output: 2325.10 toks/s]
Processed prompts:  36%|      | 185/512 [00:13<00:08, 40.71it/s, est. speed input: 227.62 toks/s, output: 3641.91 toks/s]
Processed prompts:  47%|     | 242/512 [00:13<00:04, 62.22it/s, est. speed input: 295.33 toks/s, output: 4725.21 toks/s]
Processed prompts:  59%|    | 302/512 [00:13<00:02, 90.59it/s, est. speed input: 365.05 toks/s, output: 5840.71 toks/s]
Processed prompts:  68%|   | 348/512 [00:13<00:01, 116.37it/s, est. speed input: 417.18 toks/s, output: 6674.83 toks/s]
Processed prompts:  77%|  | 392/512 [00:13<00:00, 145.03it/s, est. speed input: 466.08 toks/s, output: 7457.25 toks/s]
Processed prompts:  85%| | 435/512 [00:13<00:00, 172.87it/s, est. speed input: 512.40 toks/s, output: 8198.46 toks/s]
Processed prompts:  93%|| 475/512 [00:13<00:00, 182.71it/s, est. speed input: 551.98 toks/s, output: 8831.74 toks/s]
Processed prompts: 100%|| 510/512 [00:16<00:00, 48.45it/s, est. speed input: 508.22 toks/s, output: 8131.45 toks/s] 
Processed prompts: 100%|| 512/512 [00:16<00:00, 48.45it/s, est. speed input: 509.89 toks/s, output: 8158.24 toks/s]
Processed prompts: 100%|| 512/512 [00:16<00:00, 31.87it/s, est. speed input: 509.89 toks/s, output: 8158.24 toks/s]
[rank0]:[W127 17:03:20.484827520 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:33:44
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-FP8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Llama3.2-3B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:33:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:33:47 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2876337) WARNING 01-27 17:34:15 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.75 requests/s, 2107.83 total tokens/s, 1983.84 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:33:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:33:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:33:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:33:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:33:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:33:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:33:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:33:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:33:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:33:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:33:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:33:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:33:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:33:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:33:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:33:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:33:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:33:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2876337) [2026-01-27 17:33:52] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2876337) [2026-01-27 17:33:52] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2876337) [2026-01-27 17:33:52] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2876337) [2026-01-27 17:33:52] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2876337) [2026-01-27 17:33:52] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2876337) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2876337) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.09s/it]
(EngineCore_DP0 pid=2876337) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.09s/it]
(EngineCore_DP0 pid=2876337) 
(EngineCore_DP0 pid=2876337) 2026-01-27 17:34:14,563 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2876337) 2026-01-27 17:34:14,583 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 11840.48it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:08<08:30,  8.11s/it, est. speed input: 1.97 toks/s, output: 31.56 toks/s]
Processed prompts:  78%|  | 50/64 [00:08<00:01,  8.57it/s, est. speed input: 97.20 toks/s, output: 1555.17 toks/s]
Processed prompts: 100%|| 64/64 [00:08<00:00,  8.57it/s, est. speed input: 124.09 toks/s, output: 1985.36 toks/s]
Processed prompts: 100%|| 64/64 [00:08<00:00,  7.76it/s, est. speed input: 124.09 toks/s, output: 1985.36 toks/s]
[rank0]:[W127 17:34:23.696517299 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:34:26
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-FP8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Llama3.2-3B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:34:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:34:29 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2877092) WARNING 01-27 17:34:55 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 11.58 requests/s, 3149.69 total tokens/s, 2964.41 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:34:29] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:34:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:34:29] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:34:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:34:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:34:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:34:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:34:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:34:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:34:33] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:34:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:34:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:34:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:34:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:34:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:34:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:34:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:34:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2877092) [2026-01-27 17:34:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2877092) [2026-01-27 17:34:34] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2877092) [2026-01-27 17:34:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2877092) [2026-01-27 17:34:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2877092) [2026-01-27 17:34:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2877092) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2877092) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.53s/it]
(EngineCore_DP0 pid=2877092) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.53s/it]
(EngineCore_DP0 pid=2877092) 
(EngineCore_DP0 pid=2877092) 2026-01-27 17:34:55,335 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2877092) 2026-01-27 17:34:55,350 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 12303.96it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:10<22:33, 10.66s/it, est. speed input: 1.50 toks/s, output: 24.02 toks/s]
Processed prompts:  26%|       | 33/128 [00:10<00:22,  4.30it/s, est. speed input: 48.92 toks/s, output: 782.68 toks/s]
Processed prompts:  59%|    | 75/128 [00:10<00:04, 11.95it/s, est. speed input: 110.04 toks/s, output: 1760.65 toks/s]
Processed prompts:  92%|| 118/128 [00:11<00:00, 22.59it/s, est. speed input: 171.34 toks/s, output: 2741.40 toks/s]
Processed prompts: 100%|| 128/128 [00:11<00:00, 22.59it/s, est. speed input: 185.47 toks/s, output: 2967.49 toks/s]
Processed prompts: 100%|| 128/128 [00:11<00:00, 11.59it/s, est. speed input: 185.47 toks/s, output: 2967.49 toks/s]
[rank0]:[W127 17:35:07.255895819 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:35:09
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-FP8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Llama3.2-3B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:35:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:35:13 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2877886) WARNING 01-27 17:35:39 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.78 requests/s, 4020.63 total tokens/s, 3784.12 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:35:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:35:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:35:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:35:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:35:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:35:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:35:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:35:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:35:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:35:16] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:35:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:35:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:35:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:35:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:35:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:35:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:35:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:35:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2877886) [2026-01-27 17:35:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2877886) [2026-01-27 17:35:17] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2877886) [2026-01-27 17:35:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2877886) [2026-01-27 17:35:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2877886) [2026-01-27 17:35:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2877886) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2877886) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.42s/it]
(EngineCore_DP0 pid=2877886) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.42s/it]
(EngineCore_DP0 pid=2877886) 
(EngineCore_DP0 pid=2877886) 2026-01-27 17:35:38,555 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2877886) 2026-01-27 17:35:38,570 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 4458.62it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:15<1:05:04, 15.31s/it, est. speed input: 1.04 toks/s, output: 16.72 toks/s]
Processed prompts:   7%|         | 18/256 [00:15<02:26,  1.63it/s, est. speed input: 18.63 toks/s, output: 298.01 toks/s]
Processed prompts:  19%|        | 48/256 [00:15<00:37,  5.47it/s, est. speed input: 49.24 toks/s, output: 787.84 toks/s]
Processed prompts:  29%|       | 75/256 [00:15<00:17, 10.22it/s, est. speed input: 76.33 toks/s, output: 1221.29 toks/s]
Processed prompts:  38%|      | 98/256 [00:15<00:10, 15.69it/s, est. speed input: 99.02 toks/s, output: 1584.29 toks/s]
Processed prompts:  46%|     | 118/256 [00:15<00:06, 22.02it/s, est. speed input: 118.45 toks/s, output: 1895.24 toks/s]
Processed prompts:  56%|    | 144/256 [00:16<00:03, 32.53it/s, est. speed input: 143.26 toks/s, output: 2292.18 toks/s]
Processed prompts:  65%|   | 166/256 [00:16<00:02, 43.30it/s, est. speed input: 163.83 toks/s, output: 2621.26 toks/s]
Processed prompts:  72%|  | 185/256 [00:16<00:01, 54.31it/s, est. speed input: 181.29 toks/s, output: 2900.59 toks/s]
Processed prompts:  79%|  | 203/256 [00:16<00:00, 63.98it/s, est. speed input: 197.15 toks/s, output: 3154.44 toks/s]
Processed prompts:  86%| | 220/256 [00:16<00:00, 71.84it/s, est. speed input: 211.64 toks/s, output: 3386.28 toks/s]
Processed prompts:  92%|| 235/256 [00:16<00:00, 75.50it/s, est. speed input: 223.80 toks/s, output: 3580.84 toks/s]
Processed prompts:  97%|| 248/256 [00:17<00:00, 67.96it/s, est. speed input: 232.69 toks/s, output: 3722.99 toks/s]
Processed prompts: 100%|| 256/256 [00:17<00:00, 67.96it/s, est. speed input: 237.31 toks/s, output: 3796.99 toks/s]
Processed prompts: 100%|| 256/256 [00:17<00:00, 14.83it/s, est. speed input: 237.31 toks/s, output: 3796.99 toks/s]
[rank0]:[W127 17:35:56.803425052 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:35:59
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-FP8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Llama3.2-3B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:36:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:36:02 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2878764) WARNING 01-27 17:36:28 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.43 requests/s, 4196.29 total tokens/s, 3949.45 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:36:02] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:36:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:36:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:36:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:36:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:36:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:36:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:36:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:36:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:36:06] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:36:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:36:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:36:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:36:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:36:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:36:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:36:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:36:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2878764) [2026-01-27 17:36:07] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2878764) [2026-01-27 17:36:07] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2878764) [2026-01-27 17:36:07] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2878764) [2026-01-27 17:36:07] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2878764) [2026-01-27 17:36:07] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2878764) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2878764) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.31s/it]
(EngineCore_DP0 pid=2878764) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.31s/it]
(EngineCore_DP0 pid=2878764) 
(EngineCore_DP0 pid=2878764) 2026-01-27 17:36:28,075 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2878764) 2026-01-27 17:36:28,092 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 13749.97it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:24<3:30:30, 24.72s/it, est. speed input: 0.65 toks/s, output: 10.36 toks/s]
Processed prompts:   1%|          | 3/512 [00:24<55:09,  6.50s/it, est. speed input: 1.92 toks/s, output: 30.75 toks/s]  
Processed prompts:   7%|         | 34/512 [00:25<03:02,  2.62it/s, est. speed input: 21.57 toks/s, output: 345.20 toks/s]
Processed prompts:  12%|        | 63/512 [00:25<01:17,  5.78it/s, est. speed input: 39.62 toks/s, output: 633.98 toks/s]
Processed prompts:  18%|        | 91/512 [00:25<00:42,  9.90it/s, est. speed input: 56.76 toks/s, output: 908.20 toks/s]
Processed prompts:  23%|       | 117/512 [00:25<00:26, 14.94it/s, est. speed input: 72.41 toks/s, output: 1158.62 toks/s]
Processed prompts:  32%|      | 164/512 [00:26<00:12, 27.48it/s, est. speed input: 100.76 toks/s, output: 1612.11 toks/s]
Processed prompts:  40%|      | 205/512 [00:26<00:07, 41.17it/s, est. speed input: 125.10 toks/s, output: 2001.60 toks/s]
Processed prompts:  47%|     | 242/512 [00:26<00:04, 56.35it/s, est. speed input: 146.79 toks/s, output: 2348.68 toks/s]
Processed prompts:  53%|    | 273/512 [00:26<00:03, 70.95it/s, est. speed input: 164.67 toks/s, output: 2634.78 toks/s]
Processed prompts:  59%|    | 302/512 [00:26<00:02, 87.33it/s, est. speed input: 181.29 toks/s, output: 2900.64 toks/s]
Processed prompts:  64%|   | 327/512 [00:26<00:01, 102.26it/s, est. speed input: 195.42 toks/s, output: 3126.65 toks/s]
Processed prompts:  68%|   | 348/512 [00:26<00:01, 114.35it/s, est. speed input: 207.10 toks/s, output: 3313.61 toks/s]
Processed prompts:  72%|  | 369/512 [00:26<00:01, 128.21it/s, est. speed input: 218.75 toks/s, output: 3500.05 toks/s]
Processed prompts:  77%|  | 392/512 [00:27<00:00, 135.70it/s, est. speed input: 231.15 toks/s, output: 3698.46 toks/s]
Processed prompts:  80%|  | 412/512 [00:27<00:00, 138.57it/s, est. speed input: 241.74 toks/s, output: 3867.83 toks/s]
Processed prompts:  84%| | 430/512 [00:27<00:00, 142.04it/s, est. speed input: 251.22 toks/s, output: 4019.54 toks/s]
Processed prompts:  87%| | 447/512 [00:27<00:00, 135.14it/s, est. speed input: 259.79 toks/s, output: 4156.61 toks/s]
Processed prompts:  90%| | 463/512 [00:27<00:00, 122.52it/s, est. speed input: 267.47 toks/s, output: 4279.49 toks/s]
Processed prompts:  93%|| 477/512 [00:27<00:00, 107.89it/s, est. speed input: 273.79 toks/s, output: 4380.61 toks/s]
Processed prompts:  96%|| 489/512 [00:28<00:00, 81.48it/s, est. speed input: 277.99 toks/s, output: 4447.76 toks/s] 
Processed prompts:  97%|| 499/512 [00:33<00:01,  8.72it/s, est. speed input: 241.36 toks/s, output: 3861.74 toks/s]
Processed prompts: 100%|| 512/512 [00:33<00:00,  8.72it/s, est. speed input: 247.13 toks/s, output: 3954.01 toks/s]
Processed prompts: 100%|| 512/512 [00:33<00:00, 15.45it/s, est. speed input: 247.13 toks/s, output: 3954.01 toks/s]
[rank0]:[W127 17:37:02.255581764 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 18:25:57
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Qwen2.5-7B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:26:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:26:00 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2927366) WARNING 01-27 18:26:53 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.63 requests/s, 1260.26 total tokens/s, 1186.12 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 18:26:00] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:26:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:26:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:26:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:26:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:26:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:26:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:26:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:26:04] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:26:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:26:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:26:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:26:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:26:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:26:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:26:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2927366) [2026-01-27 18:26:05] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2927366) [2026-01-27 18:26:05] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2927366) [2026-01-27 18:26:05] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2927366) [2026-01-27 18:26:05] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2927366) [2026-01-27 18:26:05] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2927366) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2927366) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:20<00:20, 20.71s/it]
(EngineCore_DP0 pid=2927366) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:41<00:00, 20.81s/it]
(EngineCore_DP0 pid=2927366) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:41<00:00, 20.79s/it]
(EngineCore_DP0 pid=2927366) 
(EngineCore_DP0 pid=2927366) 2026-01-27 18:26:53,283 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2927366) 2026-01-27 18:26:53,312 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 10150.32it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:13<14:14, 13.57s/it, est. speed input: 1.18 toks/s, output: 18.87 toks/s]
Processed prompts:  28%|       | 18/64 [00:13<00:24,  1.84it/s, est. speed input: 21.07 toks/s, output: 337.10 toks/s]
Processed prompts:  97%|| 62/64 [00:13<00:00,  8.22it/s, est. speed input: 71.85 toks/s, output: 1149.67 toks/s]
Processed prompts: 100%|| 64/64 [00:13<00:00,  8.22it/s, est. speed input: 74.17 toks/s, output: 1186.75 toks/s]
Processed prompts: 100%|| 64/64 [00:13<00:00,  4.64it/s, est. speed input: 74.17 toks/s, output: 1186.75 toks/s]
[rank0]:[W127 18:27:07.984744198 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 18:27:10
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Qwen2.5-7B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:27:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:27:16 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2928610) WARNING 01-27 18:28:06 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.75 requests/s, 2107.52 total tokens/s, 1983.54 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 18:27:15] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:27:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:27:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:27:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:27:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:27:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:27:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:27:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:27:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:27:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:27:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:27:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:27:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:27:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:27:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:27:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2928610) [2026-01-27 18:27:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2928610) [2026-01-27 18:27:20] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2928610) [2026-01-27 18:27:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2928610) [2026-01-27 18:27:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2928610) [2026-01-27 18:27:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2928610) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2928610) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.87s/it]
(EngineCore_DP0 pid=2928610) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 20.00s/it]
(EngineCore_DP0 pid=2928610) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.98s/it]
(EngineCore_DP0 pid=2928610) 
(EngineCore_DP0 pid=2928610) 2026-01-27 18:28:06,219 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2928610) 2026-01-27 18:28:06,252 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 4325.03it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:15<33:37, 15.89s/it, est. speed input: 1.01 toks/s, output: 16.11 toks/s]
Processed prompts:  14%|        | 18/128 [00:16<01:10,  1.57it/s, est. speed input: 17.97 toks/s, output: 287.45 toks/s]
Processed prompts:  38%|      | 48/128 [00:16<00:15,  5.30it/s, est. speed input: 47.57 toks/s, output: 761.08 toks/s]
Processed prompts:  59%|    | 75/128 [00:16<00:05,  9.93it/s, est. speed input: 73.84 toks/s, output: 1181.36 toks/s]
Processed prompts:  84%| | 108/128 [00:16<00:01, 17.53it/s, est. speed input: 105.35 toks/s, output: 1685.64 toks/s]
Processed prompts: 100%|| 128/128 [00:16<00:00, 17.53it/s, est. speed input: 124.21 toks/s, output: 1987.28 toks/s]
Processed prompts: 100%|| 128/128 [00:16<00:00,  7.76it/s, est. speed input: 124.21 toks/s, output: 1987.28 toks/s]
[rank0]:[W127 18:28:23.681694240 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 18:28:26
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Qwen2.5-7B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:28:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:28:31 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2929842) WARNING 01-27 18:29:21 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.69 requests/s, 2908.43 total tokens/s, 2737.34 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 18:28:31] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:28:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:28:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:28:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:28:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:28:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:28:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:28:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:28:34] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:28:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:28:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:28:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:28:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:28:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:28:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:28:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2929842) [2026-01-27 18:28:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2929842) [2026-01-27 18:28:35] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2929842) [2026-01-27 18:28:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2929842) [2026-01-27 18:28:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2929842) [2026-01-27 18:28:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2929842) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2929842) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.67s/it]
(EngineCore_DP0 pid=2929842) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.70s/it]
(EngineCore_DP0 pid=2929842) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.70s/it]
(EngineCore_DP0 pid=2929842) 
(EngineCore_DP0 pid=2929842) 2026-01-27 18:29:20,834 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2929842) 2026-01-27 18:29:20,863 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 11346.86it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:20<1:29:12, 20.99s/it, est. speed input: 0.76 toks/s, output: 12.20 toks/s]
Processed prompts:   8%|         | 20/256 [00:21<02:58,  1.32it/s, est. speed input: 15.12 toks/s, output: 241.88 toks/s]
Processed prompts:  19%|        | 49/256 [00:21<00:51,  4.05it/s, est. speed input: 36.76 toks/s, output: 588.09 toks/s]
Processed prompts:  30%|       | 76/256 [00:21<00:23,  7.54it/s, est. speed input: 56.60 toks/s, output: 905.59 toks/s]
Processed prompts:  39%|      | 99/256 [00:21<00:13, 11.58it/s, est. speed input: 73.25 toks/s, output: 1171.97 toks/s]
Processed prompts:  46%|     | 119/256 [00:21<00:08, 16.27it/s, est. speed input: 87.50 toks/s, output: 1400.01 toks/s]
Processed prompts:  54%|    | 137/256 [00:21<00:05, 21.77it/s, est. speed input: 100.13 toks/s, output: 1602.14 toks/s]
Processed prompts:  60%|    | 153/256 [00:22<00:03, 27.99it/s, est. speed input: 111.21 toks/s, output: 1779.32 toks/s]
Processed prompts:  66%|   | 168/256 [00:22<00:02, 35.28it/s, est. speed input: 121.47 toks/s, output: 1943.58 toks/s]
Processed prompts:  71%|   | 182/256 [00:22<00:01, 43.35it/s, est. speed input: 130.93 toks/s, output: 2094.88 toks/s]
Processed prompts:  77%|  | 196/256 [00:22<00:01, 49.82it/s, est. speed input: 139.94 toks/s, output: 2239.10 toks/s]
Processed prompts:  81%| | 208/256 [00:22<00:00, 54.45it/s, est. speed input: 147.47 toks/s, output: 2359.49 toks/s]
Processed prompts:  86%| | 219/256 [00:22<00:00, 54.56it/s, est. speed input: 153.90 toks/s, output: 2462.41 toks/s]
Processed prompts:  89%| | 229/256 [00:22<00:00, 57.55it/s, est. speed input: 159.92 toks/s, output: 2558.67 toks/s]
Processed prompts:  93%|| 238/256 [00:23<00:00, 51.54it/s, est. speed input: 164.53 toks/s, output: 2632.44 toks/s]
Processed prompts:  96%|| 246/256 [00:23<00:00, 44.26it/s, est. speed input: 168.13 toks/s, output: 2690.08 toks/s]
Processed prompts:  98%|| 252/256 [00:23<00:00, 37.16it/s, est. speed input: 170.30 toks/s, output: 2724.74 toks/s]
Processed prompts: 100%|| 256/256 [00:23<00:00, 37.16it/s, est. speed input: 171.25 toks/s, output: 2740.03 toks/s]
Processed prompts: 100%|| 256/256 [00:23<00:00, 10.70it/s, est. speed input: 171.25 toks/s, output: 2740.03 toks/s]
[rank0]:[W127 18:29:45.680684690 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 18:29:47
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Qwen2.5-7B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:29:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:29:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2931135) WARNING 01-27 18:30:42 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.83 requests/s, 2944.70 total tokens/s, 2771.49 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 18:29:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:29:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:29:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:29:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:29:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:29:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:29:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:29:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:29:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:29:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:29:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:29:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:29:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:29:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:29:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:29:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2931135) [2026-01-27 18:29:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2931135) [2026-01-27 18:29:55] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2931135) [2026-01-27 18:29:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2931135) [2026-01-27 18:29:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2931135) [2026-01-27 18:29:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2931135) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2931135) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.94s/it]
(EngineCore_DP0 pid=2931135) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.97s/it]
(EngineCore_DP0 pid=2931135) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.97s/it]
(EngineCore_DP0 pid=2931135) 
(EngineCore_DP0 pid=2931135) 2026-01-27 18:30:41,612 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2931135) 2026-01-27 18:30:41,633 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 12075.23it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:33<4:45:33, 33.53s/it, est. speed input: 0.48 toks/s, output: 7.63 toks/s]
Processed prompts:   1%|          | 5/512 [00:33<42:48,  5.07s/it, est. speed input: 2.36 toks/s, output: 37.80 toks/s] 
Processed prompts:   7%|         | 36/512 [00:34<03:55,  2.02it/s, est. speed input: 16.85 toks/s, output: 269.63 toks/s]
Processed prompts:  13%|        | 65/512 [00:34<01:42,  4.35it/s, est. speed input: 30.16 toks/s, output: 482.63 toks/s]
Processed prompts:  18%|        | 93/512 [00:34<00:56,  7.38it/s, est. speed input: 42.80 toks/s, output: 684.79 toks/s]
Processed prompts:  23%|       | 119/512 [00:35<00:35, 11.11it/s, est. speed input: 54.34 toks/s, output: 869.46 toks/s]
Processed prompts:  28%|       | 143/512 [00:35<00:23, 16.04it/s, est. speed input: 65.10 toks/s, output: 1041.54 toks/s]
Processed prompts:  32%|      | 166/512 [00:35<00:15, 22.42it/s, est. speed input: 75.34 toks/s, output: 1205.42 toks/s]
Processed prompts:  37%|      | 187/512 [00:35<00:10, 30.09it/s, est. speed input: 84.63 toks/s, output: 1354.00 toks/s]
Processed prompts:  44%|     | 226/512 [00:35<00:06, 47.55it/s, est. speed input: 101.72 toks/s, output: 1627.51 toks/s]
Processed prompts:  51%|     | 259/512 [00:35<00:03, 63.59it/s, est. speed input: 115.97 toks/s, output: 1855.58 toks/s]
Processed prompts:  56%|    | 288/512 [00:35<00:02, 78.61it/s, est. speed input: 128.36 toks/s, output: 2053.79 toks/s]
Processed prompts:  62%|   | 315/512 [00:36<00:02, 92.61it/s, est. speed input: 139.77 toks/s, output: 2236.25 toks/s]
Processed prompts:  66%|   | 338/512 [00:36<00:01, 103.43it/s, est. speed input: 149.36 toks/s, output: 2389.75 toks/s]
Processed prompts:  70%|   | 358/512 [00:36<00:01, 110.79it/s, est. speed input: 157.59 toks/s, output: 2521.37 toks/s]
Processed prompts:  73%|  | 376/512 [00:36<00:01, 115.35it/s, est. speed input: 164.89 toks/s, output: 2638.31 toks/s]
Processed prompts:  77%|  | 392/512 [00:36<00:01, 117.37it/s, est. speed input: 171.31 toks/s, output: 2740.93 toks/s]
Processed prompts:  79%|  | 407/512 [00:36<00:00, 118.77it/s, est. speed input: 177.28 toks/s, output: 2836.40 toks/s]
Processed prompts:  82%| | 421/512 [00:36<00:00, 119.48it/s, est. speed input: 182.80 toks/s, output: 2924.82 toks/s]
Processed prompts:  85%| | 435/512 [00:37<00:00, 106.36it/s, est. speed input: 187.99 toks/s, output: 3007.90 toks/s]
Processed prompts:  87%| | 447/512 [00:37<00:00, 96.42it/s, est. speed input: 192.34 toks/s, output: 3077.44 toks/s] 
Processed prompts:  89%| | 458/512 [00:37<00:00, 88.73it/s, est. speed input: 196.25 toks/s, output: 3140.03 toks/s]
Processed prompts:  91%|| 468/512 [00:37<00:00, 75.13it/s, est. speed input: 199.47 toks/s, output: 3191.56 toks/s]
Processed prompts:  93%|| 477/512 [00:37<00:00, 65.35it/s, est. speed input: 202.25 toks/s, output: 3235.97 toks/s]
Processed prompts:  95%|| 485/512 [00:38<00:00, 50.46it/s, est. speed input: 204.12 toks/s, output: 3265.95 toks/s]
Processed prompts:  96%|| 491/512 [00:38<00:00, 39.72it/s, est. speed input: 205.12 toks/s, output: 3281.96 toks/s]
Processed prompts:  97%|| 496/512 [00:38<00:00, 34.58it/s, est. speed input: 206.00 toks/s, output: 3296.05 toks/s]
Processed prompts:  98%|| 500/512 [00:47<00:04,  2.47it/s, est. speed input: 169.97 toks/s, output: 2719.49 toks/s]
Processed prompts: 100%|| 512/512 [00:47<00:00,  4.24it/s, est. speed input: 173.38 toks/s, output: 2774.05 toks/s]
Processed prompts: 100%|| 512/512 [00:47<00:00,  4.24it/s, est. speed input: 173.38 toks/s, output: 2774.05 toks/s]
Processed prompts: 100%|| 512/512 [00:47<00:00, 10.84it/s, est. speed input: 173.38 toks/s, output: 2774.05 toks/s]
[rank0]:[W127 18:31:29.892280924 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 20:01:41
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-FP8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Qwen2.5-14B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:01:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:01:45 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3026336) WARNING 01-27 20:03:21 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.39 requests/s, 648.84 total tokens/s, 610.67 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 20:01:45] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:01:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:01:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:01:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:01:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:01:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:01:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:01:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:01:48] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:01:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:01:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:01:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:01:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:01:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:01:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:01:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3026336) [2026-01-27 20:01:50] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3026336) [2026-01-27 20:01:50] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3026336) [2026-01-27 20:01:50] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3026336) [2026-01-27 20:01:50] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3026336) [2026-01-27 20:01:50] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3026336) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3026336) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.40s/it]
(EngineCore_DP0 pid=3026336) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:33<00:36, 18.42s/it]
(EngineCore_DP0 pid=3026336) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:55<00:19, 19.74s/it]
(EngineCore_DP0 pid=3026336) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:22<00:00, 22.61s/it]
(EngineCore_DP0 pid=3026336) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:22<00:00, 20.54s/it]
(EngineCore_DP0 pid=3026336) 
(EngineCore_DP0 pid=3026336) 2026-01-27 20:03:20,355 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3026336) 2026-01-27 20:03:20,405 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|         | 1/64 [00:00<00:08,  7.02it/s]
Adding requests:  11%|         | 7/64 [00:00<00:01, 31.67it/s]
Adding requests:  38%|      | 24/64 [00:00<00:00, 87.29it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 157.55it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:25<27:09, 25.87s/it, est. speed input: 0.62 toks/s, output: 9.89 toks/s]
Processed prompts:  28%|       | 18/64 [00:26<00:47,  1.03s/it, est. speed input: 11.05 toks/s, output: 176.80 toks/s]
Processed prompts:  75%|  | 48/64 [00:26<00:04,  3.26it/s, est. speed input: 29.26 toks/s, output: 468.14 toks/s]
Processed prompts: 100%|| 64/64 [00:26<00:00,  3.26it/s, est. speed input: 38.89 toks/s, output: 622.21 toks/s]
Processed prompts: 100%|| 64/64 [00:26<00:00,  2.43it/s, est. speed input: 38.89 toks/s, output: 622.21 toks/s]
[rank0]:[W127 20:03:48.392420629 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 20:04:01
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-FP8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Qwen2.5-14B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:04:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:04:07 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3028489) WARNING 01-27 20:05:42 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.05 requests/s, 1101.25 total tokens/s, 1036.47 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 20:04:07] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:04:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:04:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:04:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:04:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:04:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:04:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:04:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:04:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:04:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:04:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:04:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:04:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:04:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:04:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:04:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3028489) [2026-01-27 20:04:11] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3028489) [2026-01-27 20:04:11] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3028489) [2026-01-27 20:04:11] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3028489) [2026-01-27 20:04:11] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3028489) [2026-01-27 20:04:11] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3028489) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3028489) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:24,  8.31s/it]
(EngineCore_DP0 pid=3028489) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.20s/it]
(EngineCore_DP0 pid=3028489) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.26s/it]
(EngineCore_DP0 pid=3028489) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.83s/it]
(EngineCore_DP0 pid=3028489) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.86s/it]
(EngineCore_DP0 pid=3028489) 
(EngineCore_DP0 pid=3028489) 2026-01-27 20:05:41,830 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3028489) 2026-01-27 20:05:41,867 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 3972.41it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:30<1:04:22, 30.41s/it, est. speed input: 0.53 toks/s, output: 8.42 toks/s]
Processed prompts:   2%|         | 2/128 [00:30<26:27, 12.60s/it, est. speed input: 1.05 toks/s, output: 16.76 toks/s] 
Processed prompts:  14%|        | 18/128 [00:30<01:36,  1.14it/s, est. speed input: 9.39 toks/s, output: 150.22 toks/s]
Processed prompts:  26%|       | 33/128 [00:30<00:37,  2.53it/s, est. speed input: 17.15 toks/s, output: 274.35 toks/s]
Processed prompts:  38%|      | 48/128 [00:30<00:17,  4.46it/s, est. speed input: 24.85 toks/s, output: 397.61 toks/s]
Processed prompts:  48%|     | 62/128 [00:31<00:09,  6.95it/s, est. speed input: 31.99 toks/s, output: 511.78 toks/s]
Processed prompts:  59%|    | 75/128 [00:31<00:05, 10.09it/s, est. speed input: 38.56 toks/s, output: 617.00 toks/s]
Processed prompts:  69%|   | 88/128 [00:31<00:02, 14.31it/s, est. speed input: 45.10 toks/s, output: 721.60 toks/s]
Processed prompts:  84%| | 108/128 [00:31<00:00, 22.24it/s, est. speed input: 55.01 toks/s, output: 880.20 toks/s]
Processed prompts:  99%|| 127/128 [00:31<00:00, 31.22it/s, est. speed input: 64.34 toks/s, output: 1029.48 toks/s]
Processed prompts: 100%|| 128/128 [00:31<00:00, 31.22it/s, est. speed input: 64.85 toks/s, output: 1037.58 toks/s]
Processed prompts: 100%|| 128/128 [00:31<00:00,  4.05it/s, est. speed input: 64.85 toks/s, output: 1037.58 toks/s]
[rank0]:[W127 20:06:14.500307975 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 20:06:17
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-FP8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Qwen2.5-14B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:06:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:06:23 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3030558) WARNING 01-27 20:08:05 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.45 requests/s, 1481.11 total tokens/s, 1393.99 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 20:06:23] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:06:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:06:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:06:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:06:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:06:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:06:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:06:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:06:26] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:06:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:06:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:06:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:06:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:06:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:06:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:06:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3030558) [2026-01-27 20:06:27] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3030558) [2026-01-27 20:06:27] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3030558) [2026-01-27 20:06:27] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3030558) [2026-01-27 20:06:27] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3030558) [2026-01-27 20:06:27] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3030558) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3030558) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.45s/it]
(EngineCore_DP0 pid=3030558) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.16s/it]
(EngineCore_DP0 pid=3030558) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.13s/it]
(EngineCore_DP0 pid=3030558) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.82s/it]
(EngineCore_DP0 pid=3030558) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.83s/it]
(EngineCore_DP0 pid=3030558) 
(EngineCore_DP0 pid=3030558) 2026-01-27 20:07:58,370 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3030558) 2026-01-27 20:07:58,602 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:53,  4.78it/s]
Adding requests:   1%|          | 3/256 [00:00<00:26,  9.42it/s]
Adding requests:   2%|         | 5/256 [00:00<00:19, 12.64it/s]
Adding requests:   3%|         | 8/256 [00:00<00:14, 17.00it/s]
Adding requests:   5%|         | 12/256 [00:00<00:10, 22.29it/s]
Adding requests:   7%|         | 18/256 [00:00<00:07, 31.80it/s]
Adding requests:  10%|         | 26/256 [00:00<00:05, 45.11it/s]
Adding requests:  14%|        | 36/256 [00:01<00:03, 60.35it/s]
Adding requests:  20%|        | 50/256 [00:01<00:02, 81.25it/s]
Adding requests:  25%|       | 65/256 [00:01<00:01, 100.07it/s]
Adding requests:  31%|      | 80/256 [00:01<00:01, 114.05it/s]
Adding requests:  38%|      | 98/256 [00:01<00:01, 132.41it/s]
Adding requests:  48%|     | 122/256 [00:01<00:00, 163.46it/s]
Adding requests:  57%|    | 145/256 [00:01<00:00, 182.95it/s]
Adding requests:  64%|   | 164/256 [00:01<00:00, 125.93it/s]
Adding requests:  77%|  | 196/256 [00:01<00:00, 168.87it/s]
Adding requests:  92%|| 236/256 [00:02<00:00, 224.18it/s]
Adding requests: 100%|| 256/256 [00:02<00:00, 119.14it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:38<2:41:35, 38.02s/it, est. speed input: 0.42 toks/s, output: 6.73 toks/s]
Processed prompts:   1%|          | 2/256 [00:38<1:06:42, 15.76s/it, est. speed input: 0.84 toks/s, output: 13.41 toks/s]
Processed prompts:   2%|         | 4/256 [00:38<24:55,  5.93s/it, est. speed input: 1.67 toks/s, output: 26.69 toks/s]  
Processed prompts:   2%|         | 6/256 [00:38<13:13,  3.18s/it, est. speed input: 2.49 toks/s, output: 39.83 toks/s]
Processed prompts:   3%|         | 8/256 [00:38<07:58,  1.93s/it, est. speed input: 3.30 toks/s, output: 52.88 toks/s]
Processed prompts:   4%|         | 11/256 [00:38<04:20,  1.06s/it, est. speed input: 4.52 toks/s, output: 72.38 toks/s]
Processed prompts:   5%|         | 14/256 [00:39<02:40,  1.51it/s, est. speed input: 5.73 toks/s, output: 91.70 toks/s]
Processed prompts:   8%|         | 20/256 [00:39<01:16,  3.07it/s, est. speed input: 8.15 toks/s, output: 130.40 toks/s]
Processed prompts:  11%|         | 28/256 [00:39<00:39,  5.81it/s, est. speed input: 11.36 toks/s, output: 181.82 toks/s]
Processed prompts:  15%|        | 38/256 [00:39<00:21, 10.14it/s, est. speed input: 15.36 toks/s, output: 245.69 toks/s]
Processed prompts:  20%|        | 52/256 [00:39<00:11, 17.70it/s, est. speed input: 20.92 toks/s, output: 334.78 toks/s]
Processed prompts:  25%|       | 65/256 [00:39<00:07, 25.79it/s, est. speed input: 26.06 toks/s, output: 416.94 toks/s]
Processed prompts:  30%|       | 78/256 [00:40<00:05, 34.37it/s, est. speed input: 31.15 toks/s, output: 498.41 toks/s]
Processed prompts:  35%|      | 90/256 [00:40<00:03, 42.36it/s, est. speed input: 35.81 toks/s, output: 573.03 toks/s]
Processed prompts:  39%|      | 101/256 [00:40<00:03, 48.35it/s, est. speed input: 40.04 toks/s, output: 640.66 toks/s]
Processed prompts:  43%|     | 111/256 [00:40<00:02, 54.11it/s, est. speed input: 43.87 toks/s, output: 701.87 toks/s]
Processed prompts:  47%|     | 121/256 [00:40<00:02, 58.12it/s, est. speed input: 47.65 toks/s, output: 762.46 toks/s]
Processed prompts:  51%|     | 130/256 [00:40<00:02, 60.92it/s, est. speed input: 51.04 toks/s, output: 816.60 toks/s]
Processed prompts:  54%|    | 138/256 [00:40<00:01, 61.67it/s, est. speed input: 54.01 toks/s, output: 864.20 toks/s]
Processed prompts:  57%|    | 146/256 [00:41<00:01, 62.38it/s, est. speed input: 56.97 toks/s, output: 911.53 toks/s]
Processed prompts:  60%|    | 153/256 [00:41<00:01, 60.00it/s, est. speed input: 59.51 toks/s, output: 952.21 toks/s]
Processed prompts:  62%|   | 160/256 [00:41<00:01, 61.17it/s, est. speed input: 62.07 toks/s, output: 993.17 toks/s]
Processed prompts:  65%|   | 167/256 [00:41<00:01, 60.77it/s, est. speed input: 64.60 toks/s, output: 1033.68 toks/s]
Processed prompts:  68%|   | 174/256 [00:41<00:01, 61.14it/s, est. speed input: 67.13 toks/s, output: 1074.08 toks/s]
Processed prompts:  71%|   | 181/256 [00:41<00:01, 61.09it/s, est. speed input: 69.64 toks/s, output: 1114.21 toks/s]
Processed prompts:  73%|  | 188/256 [00:41<00:01, 61.39it/s, est. speed input: 72.14 toks/s, output: 1154.17 toks/s]
Processed prompts:  76%|  | 195/256 [00:41<00:01, 48.83it/s, est. speed input: 74.44 toks/s, output: 1191.03 toks/s]
Processed prompts:  79%|  | 201/256 [00:42<00:01, 50.65it/s, est. speed input: 76.54 toks/s, output: 1224.57 toks/s]
Processed prompts:  81%|  | 207/256 [00:42<00:01, 42.04it/s, est. speed input: 78.43 toks/s, output: 1254.95 toks/s]
Processed prompts:  83%| | 212/256 [00:42<00:01, 43.75it/s, est. speed input: 80.14 toks/s, output: 1282.22 toks/s]
Processed prompts:  85%| | 217/256 [00:42<00:01, 36.63it/s, est. speed input: 81.64 toks/s, output: 1306.30 toks/s]
Processed prompts:  87%| | 222/256 [00:42<00:01, 32.53it/s, est. speed input: 83.13 toks/s, output: 1330.14 toks/s]
Processed prompts:  89%| | 228/256 [00:42<00:00, 32.41it/s, est. speed input: 85.01 toks/s, output: 1360.15 toks/s]
Processed prompts:  91%| | 232/256 [00:43<00:00, 28.99it/s, est. speed input: 86.13 toks/s, output: 1378.04 toks/s]
Processed prompts:  92%|| 236/256 [00:43<00:00, 26.84it/s, est. speed input: 87.24 toks/s, output: 1395.89 toks/s]
Processed prompts:  94%|| 240/256 [00:43<00:00, 25.52it/s, est. speed input: 88.36 toks/s, output: 1413.72 toks/s]
Processed prompts:  95%|| 243/256 [00:43<00:00, 23.12it/s, est. speed input: 89.11 toks/s, output: 1425.71 toks/s]
Processed prompts:  96%|| 246/256 [00:43<00:00, 18.56it/s, est. speed input: 89.67 toks/s, output: 1434.65 toks/s]
Processed prompts:  97%|| 249/256 [00:44<00:00, 16.12it/s, est. speed input: 90.23 toks/s, output: 1443.67 toks/s]
Processed prompts:  98%|| 251/256 [00:44<00:00, 15.05it/s, est. speed input: 90.61 toks/s, output: 1449.70 toks/s]
Processed prompts:  99%|| 253/256 [00:44<00:00, 14.27it/s, est. speed input: 90.99 toks/s, output: 1455.79 toks/s]
Processed prompts: 100%|| 255/256 [00:44<00:00, 13.69it/s, est. speed input: 91.37 toks/s, output: 1461.87 toks/s]
Processed prompts: 100%|| 256/256 [00:44<00:00, 13.69it/s, est. speed input: 91.51 toks/s, output: 1464.20 toks/s]
Processed prompts: 100%|| 256/256 [00:44<00:00,  5.72it/s, est. speed input: 91.51 toks/s, output: 1464.20 toks/s]
[rank0]:[W127 20:08:54.069755230 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 20:09:10
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-FP8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/json/Qwen2.5-14B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:09:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:09:13 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3033132) WARNING 01-27 20:10:49 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.63 requests/s, 1532.21 total tokens/s, 1442.08 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 20:09:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:09:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:09:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:09:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:09:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:09:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:09:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:09:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:09:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:09:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:09:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:09:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:09:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:09:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:09:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:09:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3033132) [2026-01-27 20:09:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3033132) [2026-01-27 20:09:18] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3033132) [2026-01-27 20:09:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3033132) [2026-01-27 20:09:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3033132) [2026-01-27 20:09:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3033132) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3033132) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.43s/it]
(EngineCore_DP0 pid=3033132) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:34<00:38, 19.05s/it]
(EngineCore_DP0 pid=3033132) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.08s/it]
(EngineCore_DP0 pid=3033132) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.81s/it]
(EngineCore_DP0 pid=3033132) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.80s/it]
(EngineCore_DP0 pid=3033132) 
(EngineCore_DP0 pid=3033132) 2026-01-27 20:10:48,168 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3033132) 2026-01-27 20:10:48,309 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   1%|         | 7/512 [00:00<00:07, 66.51it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 2778.16it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [01:03<8:59:45, 63.38s/it, est. speed input: 0.25 toks/s, output: 4.04 toks/s]
Processed prompts:   0%|          | 2/512 [01:04<3:45:31, 26.53s/it, est. speed input: 0.50 toks/s, output: 7.99 toks/s]
Processed prompts:   6%|         | 33/512 [01:04<08:00,  1.00s/it, est. speed input: 8.15 toks/s, output: 130.34 toks/s]
Processed prompts:  12%|        | 63/512 [01:05<03:18,  2.26it/s, est. speed input: 15.39 toks/s, output: 246.28 toks/s]
Processed prompts:  18%|        | 91/512 [01:06<01:49,  3.83it/s, est. speed input: 22.02 toks/s, output: 352.33 toks/s]
Processed prompts:  23%|       | 117/512 [01:06<01:08,  5.74it/s, est. speed input: 28.06 toks/s, output: 448.96 toks/s]
Processed prompts:  28%|       | 141/512 [01:06<00:44,  8.27it/s, est. speed input: 33.70 toks/s, output: 539.19 toks/s]
Processed prompts:  32%|      | 164/512 [01:07<00:30, 11.54it/s, est. speed input: 39.07 toks/s, output: 625.07 toks/s]
Processed prompts:  36%|      | 185/512 [01:07<00:21, 15.44it/s, est. speed input: 43.93 toks/s, output: 702.86 toks/s]
Processed prompts:  40%|      | 205/512 [01:07<00:15, 20.20it/s, est. speed input: 48.53 toks/s, output: 776.45 toks/s]
Processed prompts:  44%|     | 224/512 [01:07<00:11, 25.76it/s, est. speed input: 52.87 toks/s, output: 845.86 toks/s]
Processed prompts:  47%|     | 242/512 [01:07<00:08, 32.13it/s, est. speed input: 56.96 toks/s, output: 911.30 toks/s]
Processed prompts:  50%|     | 258/512 [01:08<00:06, 38.05it/s, est. speed input: 60.55 toks/s, output: 968.76 toks/s]
Processed prompts:  53%|    | 273/512 [01:08<00:05, 43.77it/s, est. speed input: 63.89 toks/s, output: 1022.19 toks/s]
Processed prompts:  56%|    | 288/512 [01:08<00:04, 50.79it/s, est. speed input: 67.23 toks/s, output: 1075.75 toks/s]
Processed prompts:  59%|    | 302/512 [01:08<00:03, 57.00it/s, est. speed input: 70.34 toks/s, output: 1125.39 toks/s]
Processed prompts:  62%|   | 315/512 [01:08<00:03, 61.67it/s, est. speed input: 73.19 toks/s, output: 1171.09 toks/s]
Processed prompts:  64%|   | 327/512 [01:09<00:02, 65.66it/s, est. speed input: 75.82 toks/s, output: 1213.09 toks/s]
Processed prompts:  66%|   | 338/512 [01:09<00:02, 67.81it/s, est. speed input: 78.20 toks/s, output: 1251.24 toks/s]
Processed prompts:  68%|   | 348/512 [01:09<00:02, 68.65it/s, est. speed input: 80.35 toks/s, output: 1285.66 toks/s]
Processed prompts:  70%|   | 358/512 [01:09<00:02, 69.27it/s, est. speed input: 82.50 toks/s, output: 1319.92 toks/s]
Processed prompts:  72%|  | 367/512 [01:09<00:02, 68.35it/s, est. speed input: 84.40 toks/s, output: 1350.44 toks/s]
Processed prompts:  73%|  | 376/512 [01:09<00:01, 68.47it/s, est. speed input: 86.31 toks/s, output: 1380.96 toks/s]
Processed prompts:  75%|  | 384/512 [01:09<00:01, 66.06it/s, est. speed input: 87.98 toks/s, output: 1407.64 toks/s]
Processed prompts:  77%|  | 392/512 [01:09<00:01, 63.11it/s, est. speed input: 89.63 toks/s, output: 1434.04 toks/s]
Processed prompts:  78%|  | 399/512 [01:10<00:01, 61.20it/s, est. speed input: 91.07 toks/s, output: 1457.05 toks/s]
Processed prompts:  79%|  | 406/512 [01:10<00:01, 60.26it/s, est. speed input: 92.50 toks/s, output: 1480.05 toks/s]
Processed prompts:  81%|  | 413/512 [01:10<00:01, 59.73it/s, est. speed input: 93.94 toks/s, output: 1503.00 toks/s]
Processed prompts:  82%| | 420/512 [01:10<00:01, 59.97it/s, est. speed input: 95.37 toks/s, output: 1525.97 toks/s]
Processed prompts:  83%| | 427/512 [01:10<00:01, 60.60it/s, est. speed input: 96.81 toks/s, output: 1548.93 toks/s]
Processed prompts:  85%| | 434/512 [01:10<00:01, 47.74it/s, est. speed input: 98.09 toks/s, output: 1569.37 toks/s]
Processed prompts:  86%| | 440/512 [01:10<00:01, 49.10it/s, est. speed input: 99.28 toks/s, output: 1588.54 toks/s]
Processed prompts:  87%| | 446/512 [01:11<00:01, 40.88it/s, est. speed input: 100.34 toks/s, output: 1605.39 toks/s]
Processed prompts:  88%| | 451/512 [01:11<00:01, 42.25it/s, est. speed input: 101.31 toks/s, output: 1620.97 toks/s]
Processed prompts:  89%| | 456/512 [01:11<00:01, 35.33it/s, est. speed input: 102.14 toks/s, output: 1634.17 toks/s]
Processed prompts:  90%| | 460/512 [01:11<00:01, 36.24it/s, est. speed input: 102.89 toks/s, output: 1646.19 toks/s]
Processed prompts:  91%| | 464/512 [01:11<00:01, 36.91it/s, est. speed input: 103.63 toks/s, output: 1658.13 toks/s]
Processed prompts:  91%|| 468/512 [01:11<00:01, 30.50it/s, est. speed input: 104.24 toks/s, output: 1667.86 toks/s]
Processed prompts:  92%|| 472/512 [01:12<00:01, 26.87it/s, est. speed input: 104.85 toks/s, output: 1677.53 toks/s]
Processed prompts:  93%|| 476/512 [01:12<00:01, 24.97it/s, est. speed input: 105.46 toks/s, output: 1687.31 toks/s]
Processed prompts:  94%|| 480/512 [01:12<00:01, 23.83it/s, est. speed input: 106.07 toks/s, output: 1697.09 toks/s]
Processed prompts:  94%|| 483/512 [01:12<00:01, 21.53it/s, est. speed input: 106.46 toks/s, output: 1703.39 toks/s]
Processed prompts:  95%|| 486/512 [01:12<00:01, 17.40it/s, est. speed input: 106.72 toks/s, output: 1707.58 toks/s]
Processed prompts:  95%|| 488/512 [01:13<00:01, 15.61it/s, est. speed input: 106.89 toks/s, output: 1710.30 toks/s]
Processed prompts:  96%|| 490/512 [01:13<00:01, 14.41it/s, est. speed input: 107.07 toks/s, output: 1713.15 toks/s]
Processed prompts:  96%|| 492/512 [01:13<00:01, 13.55it/s, est. speed input: 107.25 toks/s, output: 1716.01 toks/s]
Processed prompts:  96%|| 494/512 [01:13<00:01, 12.95it/s, est. speed input: 107.43 toks/s, output: 1718.90 toks/s]
Processed prompts:  97%|| 496/512 [01:13<00:01, 12.52it/s, est. speed input: 107.61 toks/s, output: 1721.77 toks/s]
Processed prompts:  97%|| 497/512 [01:29<00:01, 12.52it/s, est. speed input: 107.70 toks/s, output: 1723.25 toks/s]
Processed prompts:  97%|| 498/512 [01:30<00:32,  2.33s/it, est. speed input: 88.26 toks/s, output: 1412.15 toks/s] 
Processed prompts:  98%|| 501/512 [01:30<00:16,  1.49s/it, est. speed input: 88.62 toks/s, output: 1417.98 toks/s]
Processed prompts: 100%|| 511/512 [01:30<00:00,  1.82it/s, est. speed input: 90.22 toks/s, output: 1443.46 toks/s]
Processed prompts: 100%|| 512/512 [01:30<00:00,  1.82it/s, est. speed input: 90.39 toks/s, output: 1446.28 toks/s]
Processed prompts: 100%|| 512/512 [01:30<00:00,  5.65it/s, est. speed input: 90.39 toks/s, output: 1446.28 toks/s]
[rank0]:[W127 20:12:21.034587998 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

