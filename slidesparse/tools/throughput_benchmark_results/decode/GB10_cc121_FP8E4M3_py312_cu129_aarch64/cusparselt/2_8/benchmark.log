
========== M=64 ==========
Time: 2026-01-27 17:07:58
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-1B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:08:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:08:01 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2849490) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2849490) WARNING 01-27 17:08:23 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.29 requests/s, 4159.39 total tokens/s, 3914.72 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:08:01] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:08:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:08:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:08:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:08:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:08:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:08:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:08:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:08:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:08:05] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:08:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:08:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:08:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:08:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:08:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:08:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:08:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:08:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2849490) [2026-01-27 17:08:06] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2849490) [2026-01-27 17:08:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2849490) [2026-01-27 17:08:06] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2849490) [2026-01-27 17:08:06] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2849490) [2026-01-27 17:08:06] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2849490) [2026-01-27 17:08:06] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2849490) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2849490) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.79s/it]
(EngineCore_DP0 pid=2849490) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.79s/it]
(EngineCore_DP0 pid=2849490) 
(EngineCore_DP0 pid=2849490) [2026-01-27 17:08:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2849490) [2026-01-27 17:08:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=2849490) [2026-01-27 17:08:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=2849490) [2026-01-27 17:08:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=2849490) [2026-01-27 17:08:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2849490) [2026-01-27 17:08:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=2849490) [2026-01-27 17:08:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=2849490) [2026-01-27 17:08:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=2849490) 2026-01-27 17:08:22,893 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2849490) 2026-01-27 17:08:22,901 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 4240.02it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:04<04:15,  4.06s/it, est. speed input: 3.94 toks/s, output: 63.11 toks/s]
Processed prompts:  97%|█████████▋| 62/64 [00:04<00:00, 20.90it/s, est. speed input: 237.96 toks/s, output: 3807.27 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:04<00:00, 20.90it/s, est. speed input: 245.63 toks/s, output: 3930.01 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:04<00:00, 15.35it/s, est. speed input: 245.63 toks/s, output: 3930.01 toks/s]
[rank0]:[W127 17:08:27.964168760 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:08:30
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-1B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:08:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:08:34 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2850119) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2850119) WARNING 01-27 17:08:55 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 23.56 requests/s, 6408.35 total tokens/s, 6031.39 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:08:33] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:08:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:08:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:08:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:08:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:08:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:08:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:08:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:08:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:08:37] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:08:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:08:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:08:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:08:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:08:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:08:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:08:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:08:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:08:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2850119) [2026-01-27 17:08:38] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2850119) [2026-01-27 17:08:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2850119) [2026-01-27 17:08:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2850119) [2026-01-27 17:08:38] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2850119) [2026-01-27 17:08:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2850119) [2026-01-27 17:08:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2850119) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2850119) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.86s/it]
(EngineCore_DP0 pid=2850119) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.86s/it]
(EngineCore_DP0 pid=2850119) 
(EngineCore_DP0 pid=2850119) [2026-01-27 17:08:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2850119) [2026-01-27 17:08:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=2850119) [2026-01-27 17:08:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=2850119) [2026-01-27 17:08:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=2850119) [2026-01-27 17:08:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2850119) [2026-01-27 17:08:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=2850119) [2026-01-27 17:08:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=2850119) [2026-01-27 17:08:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=2850119) 2026-01-27 17:08:55,088 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2850119) 2026-01-27 17:08:55,096 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 13245.93it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:05<10:47,  5.10s/it, est. speed input: 3.14 toks/s, output: 50.24 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:05<00:10,  8.86it/s, est. speed input: 101.13 toks/s, output: 1618.06 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:05<00:02, 24.17it/s, est. speed input: 225.51 toks/s, output: 3608.15 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:05<00:00, 49.19it/s, est. speed input: 374.75 toks/s, output: 5995.92 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 49.19it/s, est. speed input: 377.69 toks/s, output: 6043.04 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.61it/s, est. speed input: 377.69 toks/s, output: 6043.04 toks/s]
[rank0]:[W127 17:09:01.483067483 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:09:03
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-1B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:09:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:09:07 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2850789) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2850789) WARNING 01-27 17:09:29 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 29.19 requests/s, 7938.54 total tokens/s, 7471.57 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:09:07] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:09:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:09:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:09:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:09:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:09:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:09:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:09:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:09:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:09:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:09:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:09:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:09:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:09:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:09:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:09:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:09:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:09:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2850789) [2026-01-27 17:09:11] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2850789) [2026-01-27 17:09:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2850789) [2026-01-27 17:09:12] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2850789) [2026-01-27 17:09:12] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2850789) [2026-01-27 17:09:12] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2850789) [2026-01-27 17:09:12] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2850789) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2850789) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.74s/it]
(EngineCore_DP0 pid=2850789) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.74s/it]
(EngineCore_DP0 pid=2850789) 
(EngineCore_DP0 pid=2850789) [2026-01-27 17:09:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2850789) [2026-01-27 17:09:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=2850789) [2026-01-27 17:09:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=2850789) [2026-01-27 17:09:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=2850789) [2026-01-27 17:09:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2850789) [2026-01-27 17:09:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=2850789) [2026-01-27 17:09:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=2850789) [2026-01-27 17:09:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=2850789) 2026-01-27 17:09:28,548 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2850789) 2026-01-27 17:09:28,555 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13279.68it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<31:47,  7.48s/it, est. speed input: 2.14 toks/s, output: 34.23 toks/s]
Processed prompts:   8%|▊         | 21/256 [00:07<01:00,  3.88it/s, est. speed input: 44.31 toks/s, output: 708.93 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:07<00:18, 11.32it/s, est. speed input: 104.08 toks/s, output: 1665.30 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:07<00:06, 24.37it/s, est. speed input: 180.38 toks/s, output: 2886.00 toks/s]
Processed prompts:  46%|████▋     | 119/256 [00:07<00:03, 37.90it/s, est. speed input: 240.34 toks/s, output: 3845.46 toks/s]
Processed prompts:  57%|█████▋    | 145/256 [00:08<00:02, 51.23it/s, est. speed input: 288.18 toks/s, output: 4610.88 toks/s]
Processed prompts:  70%|██████▉   | 179/256 [00:08<00:01, 73.19it/s, est. speed input: 350.04 toks/s, output: 5600.59 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:08<00:00, 92.84it/s, est. speed input: 397.64 toks/s, output: 6362.23 toks/s]
Processed prompts:  90%|█████████ | 231/256 [00:08<00:00, 105.72it/s, est. speed input: 437.81 toks/s, output: 7004.88 toks/s]
Processed prompts:  99%|█████████▉| 253/256 [00:08<00:00, 107.37it/s, est. speed input: 468.63 toks/s, output: 7498.00 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 107.37it/s, est. speed input: 468.05 toks/s, output: 7488.86 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 29.25it/s, est. speed input: 468.05 toks/s, output: 7488.86 toks/s] 
[rank0]:[W127 17:09:38.300850474 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:09:40
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-1B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:09:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:09:44 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2851476) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2851476) WARNING 01-27 17:10:05 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 30.18 requests/s, 8210.27 total tokens/s, 7727.31 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:09:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:09:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:09:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:09:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:09:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:09:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:09:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:09:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:09:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:09:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:09:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:09:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:09:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:09:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:09:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:09:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:09:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:09:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:09:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2851476) [2026-01-27 17:09:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2851476) [2026-01-27 17:09:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2851476) [2026-01-27 17:09:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2851476) [2026-01-27 17:09:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2851476) [2026-01-27 17:09:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2851476) [2026-01-27 17:09:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2851476) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2851476) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.58s/it]
(EngineCore_DP0 pid=2851476) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.58s/it]
(EngineCore_DP0 pid=2851476) 
(EngineCore_DP0 pid=2851476) [2026-01-27 17:09:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2851476) [2026-01-27 17:09:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=2851476) [2026-01-27 17:09:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=2851476) [2026-01-27 17:09:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=2851476) [2026-01-27 17:09:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2851476) [2026-01-27 17:09:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=2851476) [2026-01-27 17:09:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=2851476) [2026-01-27 17:09:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=2851476) 2026-01-27 17:10:05,201 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2851476) 2026-01-27 17:10:05,209 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  28%|██▊       | 142/512 [00:00<00:00, 1415.59it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3439.35it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:12<1:47:54, 12.67s/it, est. speed input: 1.26 toks/s, output: 20.20 toks/s]
Processed prompts:   0%|          | 2/512 [00:12<44:52,  5.28s/it, est. speed input: 2.50 toks/s, output: 40.07 toks/s]  
Processed prompts:  12%|█▏        | 63/512 [00:12<00:46,  9.61it/s, est. speed input: 77.74 toks/s, output: 1243.91 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:13<00:19, 20.73it/s, est. speed input: 142.46 toks/s, output: 2279.43 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:13<00:10, 33.72it/s, est. speed input: 197.80 toks/s, output: 3164.86 toks/s]
Processed prompts:  40%|████      | 205/512 [00:13<00:06, 48.34it/s, est. speed input: 244.96 toks/s, output: 3919.37 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:13<00:04, 64.73it/s, est. speed input: 286.49 toks/s, output: 4583.81 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:13<00:02, 80.29it/s, est. speed input: 320.11 toks/s, output: 5121.77 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:13<00:01, 107.81it/s, est. speed input: 365.86 toks/s, output: 5853.83 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:13<00:01, 129.25it/s, est. speed input: 400.64 toks/s, output: 6410.20 toks/s]
Processed prompts:  74%|███████▎  | 377/512 [00:14<00:00, 148.66it/s, est. speed input: 430.60 toks/s, output: 6889.58 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:14<00:00, 162.73it/s, est. speed input: 459.38 toks/s, output: 7350.10 toks/s]
Processed prompts:  85%|████████▍ | 433/512 [00:14<00:00, 171.10it/s, est. speed input: 485.30 toks/s, output: 7764.72 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:14<00:00, 178.60it/s, est. speed input: 508.95 toks/s, output: 8143.23 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:14<00:00, 161.30it/s, est. speed input: 528.67 toks/s, output: 8458.64 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:16<00:00, 33.28it/s, est. speed input: 480.40 toks/s, output: 7686.32 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:16<00:00, 33.28it/s, est. speed input: 489.15 toks/s, output: 7826.33 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:16<00:00, 30.57it/s, est. speed input: 489.15 toks/s, output: 7826.33 toks/s]
[rank0]:[W127 17:10:23.200251744 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:43:58
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-3B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:44:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:44:02 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2887131) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2887131) WARNING 01-27 17:44:37 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.95 requests/s, 2163.66 total tokens/s, 2036.38 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:44:02] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:44:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:44:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:44:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:44:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:44:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:44:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:44:05] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:44:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:44:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:44:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:44:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:44:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:44:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:06] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:06] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:06] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:06] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:06] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2887131) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2887131) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:23<00:00, 23.70s/it]
(EngineCore_DP0 pid=2887131) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:23<00:00, 23.70s/it]
(EngineCore_DP0 pid=2887131) 
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=2887131) 2026-01-27 17:44:36,702 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2887131) 2026-01-27 17:44:36,716 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 11860.36it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:07<08:13,  7.84s/it, est. speed input: 2.04 toks/s, output: 32.67 toks/s]
Processed prompts:  28%|██▊       | 18/64 [00:07<00:14,  3.17it/s, est. speed input: 36.26 toks/s, output: 580.14 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:08<00:00,  3.17it/s, est. speed input: 127.37 toks/s, output: 2037.98 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:08<00:00,  7.96it/s, est. speed input: 127.37 toks/s, output: 2037.98 toks/s]
[rank0]:[W127 17:44:45.616577639 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:44:47
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-3B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:44:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:44:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2888018) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2888018) WARNING 01-27 17:45:25 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 12.11 requests/s, 3295.03 total tokens/s, 3101.21 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:44:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:44:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:44:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:44:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:44:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:44:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:44:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:44:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:44:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:44:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:44:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:44:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:44:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:44:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2888018) [2026-01-27 17:44:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2888018) [2026-01-27 17:44:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2888018) [2026-01-27 17:44:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2888018) [2026-01-27 17:44:56] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2888018) [2026-01-27 17:44:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2888018) [2026-01-27 17:44:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2888018) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2888018) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:23<00:00, 23.03s/it]
(EngineCore_DP0 pid=2888018) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:23<00:00, 23.04s/it]
(EngineCore_DP0 pid=2888018) 
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=2888018) 2026-01-27 17:45:25,060 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2888018) 2026-01-27 17:45:25,071 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4676.66it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:10<21:14, 10.03s/it, est. speed input: 1.59 toks/s, output: 25.51 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:10<00:44,  2.48it/s, est. speed input: 28.37 toks/s, output: 453.93 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:10<00:09,  8.31it/s, est. speed input: 74.91 toks/s, output: 1198.61 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:10<00:03, 15.39it/s, est. speed input: 115.76 toks/s, output: 1852.21 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:10<00:01, 23.77it/s, est. speed input: 151.35 toks/s, output: 2421.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:10<00:00, 23.77it/s, est. speed input: 194.36 toks/s, output: 3109.74 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:10<00:00, 12.15it/s, est. speed input: 194.36 toks/s, output: 3109.74 toks/s]
[rank0]:[W127 17:45:36.623849432 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:45:38
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-3B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:45:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:45:42 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2888909) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2888909) WARNING 01-27 17:46:16 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.16 requests/s, 4122.33 total tokens/s, 3879.84 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:45:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:45:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:45:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:45:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:45:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:45:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:45:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:45:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:45:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:45:45] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:45:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:45:45] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:45:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:45:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:45:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:45:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:45:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:45:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2888909) [2026-01-27 17:45:46] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2888909) [2026-01-27 17:45:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2888909) [2026-01-27 17:45:46] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2888909) [2026-01-27 17:45:46] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2888909) [2026-01-27 17:45:46] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2888909) [2026-01-27 17:45:46] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2888909) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2888909) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:22<00:00, 22.83s/it]
(EngineCore_DP0 pid=2888909) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:22<00:00, 22.83s/it]
(EngineCore_DP0 pid=2888909) 
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=2888909) 2026-01-27 17:46:15,721 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2888909) 2026-01-27 17:46:15,731 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 4298.25it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:14<1:02:22, 14.67s/it, est. speed input: 1.09 toks/s, output: 17.44 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:14<02:20,  1.70it/s, est. speed input: 19.41 toks/s, output: 310.52 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:14<00:36,  5.68it/s, est. speed input: 51.21 toks/s, output: 819.42 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:15<00:17, 10.53it/s, est. speed input: 79.19 toks/s, output: 1267.03 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:15<00:09, 15.97it/s, est. speed input: 102.40 toks/s, output: 1638.47 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:15<00:06, 22.27it/s, est. speed input: 122.34 toks/s, output: 1957.45 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:15<00:04, 29.51it/s, est. speed input: 139.94 toks/s, output: 2238.97 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:15<00:02, 37.57it/s, est. speed input: 155.33 toks/s, output: 2485.36 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:15<00:01, 47.49it/s, est. speed input: 170.56 toks/s, output: 2728.90 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:15<00:01, 57.17it/s, est. speed input: 185.21 toks/s, output: 2963.33 toks/s]
Processed prompts:  78%|███████▊  | 199/256 [00:16<00:00, 66.63it/s, est. speed input: 198.72 toks/s, output: 3179.54 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:16<00:00, 72.77it/s, est. speed input: 210.81 toks/s, output: 3372.91 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:16<00:00, 77.71it/s, est. speed input: 221.82 toks/s, output: 3549.06 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:16<00:00, 78.26it/s, est. speed input: 231.46 toks/s, output: 3703.37 toks/s]
Processed prompts:  97%|█████████▋| 249/256 [00:16<00:00, 69.78it/s, est. speed input: 239.13 toks/s, output: 3826.09 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:16<00:00, 69.78it/s, est. speed input: 243.37 toks/s, output: 3893.92 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:16<00:00, 15.21it/s, est. speed input: 243.37 toks/s, output: 3893.92 toks/s]
[rank0]:[W127 17:46:33.503380054 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:46:35
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-3B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:46:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:46:39 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2889877) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2889877) WARNING 01-27 17:47:13 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.34 requests/s, 4172.46 total tokens/s, 3927.02 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:46:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:46:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:46:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:46:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:46:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:46:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:46:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:46:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:46:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:46:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:46:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:46:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:46:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:46:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:46:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:46:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:46:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:46:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2889877) [2026-01-27 17:46:43] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2889877) [2026-01-27 17:46:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2889877) [2026-01-27 17:46:43] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2889877) [2026-01-27 17:46:43] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2889877) [2026-01-27 17:46:43] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2889877) [2026-01-27 17:46:43] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2889877) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2889877) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:22<00:00, 22.79s/it]
(EngineCore_DP0 pid=2889877) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:22<00:00, 22.79s/it]
(EngineCore_DP0 pid=2889877) 
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=2889877) 2026-01-27 17:47:12,631 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2889877) 2026-01-27 17:47:12,642 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 13692.11it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:24<3:31:29, 24.83s/it, est. speed input: 0.64 toks/s, output: 10.31 toks/s]
Processed prompts:   1%|          | 5/512 [00:25<31:41,  3.75s/it, est. speed input: 3.19 toks/s, output: 51.04 toks/s]  
Processed prompts:   7%|▋         | 36/512 [00:25<02:54,  2.73it/s, est. speed input: 22.75 toks/s, output: 363.99 toks/s]
Processed prompts:  13%|█▎        | 65/512 [00:25<01:16,  5.86it/s, est. speed input: 40.70 toks/s, output: 651.24 toks/s]
Processed prompts:  18%|█▊        | 93/512 [00:25<00:42,  9.96it/s, est. speed input: 57.76 toks/s, output: 924.12 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:25<00:26, 14.92it/s, est. speed input: 73.28 toks/s, output: 1172.47 toks/s]
Processed prompts:  28%|██▊       | 143/512 [00:26<00:17, 21.31it/s, est. speed input: 87.66 toks/s, output: 1402.48 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:26<00:11, 29.40it/s, est. speed input: 101.32 toks/s, output: 1621.08 toks/s]
Processed prompts:  37%|███▋      | 187/512 [00:26<00:08, 38.79it/s, est. speed input: 113.66 toks/s, output: 1818.53 toks/s]
Processed prompts:  40%|████      | 207/512 [00:26<00:06, 49.81it/s, est. speed input: 125.30 toks/s, output: 2004.76 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:26<00:04, 62.13it/s, est. speed input: 136.25 toks/s, output: 2179.96 toks/s]
Processed prompts:  48%|████▊     | 244/512 [00:26<00:03, 75.54it/s, est. speed input: 146.54 toks/s, output: 2344.72 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:26<00:02, 94.93it/s, est. speed input: 163.39 toks/s, output: 2614.31 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:26<00:01, 114.86it/s, est. speed input: 179.12 toks/s, output: 2865.87 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:27<00:01, 125.32it/s, est. speed input: 192.81 toks/s, output: 3085.03 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:27<00:01, 133.08it/s, est. speed input: 204.21 toks/s, output: 3267.37 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [00:27<00:01, 137.59it/s, est. speed input: 214.38 toks/s, output: 3430.09 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [00:27<00:00, 140.56it/s, est. speed input: 223.39 toks/s, output: 3574.23 toks/s]
Processed prompts:  78%|███████▊  | 400/512 [00:27<00:00, 139.31it/s, est. speed input: 231.70 toks/s, output: 3707.26 toks/s]
Processed prompts:  81%|████████▏ | 416/512 [00:27<00:00, 143.20it/s, est. speed input: 240.08 toks/s, output: 3841.23 toks/s]
Processed prompts:  84%|████████▍ | 432/512 [00:27<00:00, 135.92it/s, est. speed input: 248.11 toks/s, output: 3969.83 toks/s]
Processed prompts:  87%|████████▋ | 447/512 [00:28<00:00, 120.96it/s, est. speed input: 255.25 toks/s, output: 4084.02 toks/s]
Processed prompts:  90%|████████▉ | 460/512 [00:28<00:00, 115.84it/s, est. speed input: 261.49 toks/s, output: 4183.89 toks/s]
Processed prompts:  92%|█████████▏| 473/512 [00:28<00:00, 103.60it/s, est. speed input: 267.34 toks/s, output: 4277.40 toks/s]
Processed prompts:  95%|█████████▍| 484/512 [00:28<00:00, 84.32it/s, est. speed input: 271.57 toks/s, output: 4345.10 toks/s] 
Processed prompts:  96%|█████████▋| 494/512 [00:28<00:00, 67.47it/s, est. speed input: 274.84 toks/s, output: 4397.41 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:33<00:01,  7.62it/s, est. speed input: 241.69 toks/s, output: 3867.06 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:33<00:00, 10.21it/s, est. speed input: 245.72 toks/s, output: 3931.55 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:33<00:00, 10.21it/s, est. speed input: 245.72 toks/s, output: 3931.55 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:33<00:00, 15.36it/s, est. speed input: 245.72 toks/s, output: 3931.55 toks/s]
[rank0]:[W127 17:47:46.942740286 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 18:43:29
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:43:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:43:32 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2947027) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2947027) WARNING 01-27 18:44:46 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.88 requests/s, 1327.10 total tokens/s, 1249.03 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 18:43:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:43:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:43:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:43:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:43:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:43:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:43:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:43:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:43:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:43:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:43:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:43:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:43:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:43:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:43:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:43:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2947027) [2026-01-27 18:43:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2947027) [2026-01-27 18:43:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2947027) [2026-01-27 18:43:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2947027) [2026-01-27 18:43:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2947027) [2026-01-27 18:43:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2947027) [2026-01-27 18:43:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2947027) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2947027) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.55s/it]
(EngineCore_DP0 pid=2947027) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 30.83s/it]
(EngineCore_DP0 pid=2947027) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 30.34s/it]
(EngineCore_DP0 pid=2947027) 
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=2947027) 2026-01-27 18:44:46,182 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2947027) 2026-01-27 18:44:46,196 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2674.81it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:12<13:26, 12.80s/it, est. speed input: 1.25 toks/s, output: 20.01 toks/s]
Processed prompts:  28%|██▊       | 18/64 [00:12<00:23,  1.95it/s, est. speed input: 22.29 toks/s, output: 356.71 toks/s]
Processed prompts:  75%|███████▌  | 48/64 [00:13<00:02,  6.55it/s, est. speed input: 58.94 toks/s, output: 943.04 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:13<00:00,  6.55it/s, est. speed input: 78.35 toks/s, output: 1253.66 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:13<00:00,  4.90it/s, est. speed input: 78.35 toks/s, output: 1253.66 toks/s]
[rank0]:[W127 18:45:00.244919407 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 18:45:03
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:45:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:45:09 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2948829) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2948829) WARNING 01-27 18:46:22 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 8.04 requests/s, 2186.81 total tokens/s, 2058.17 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 18:45:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:45:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:45:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:45:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:45:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:45:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:45:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:45:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:45:12] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:45:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:45:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:45:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:45:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:45:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:45:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:45:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2948829) [2026-01-27 18:45:13] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2948829) [2026-01-27 18:45:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2948829) [2026-01-27 18:45:13] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2948829) [2026-01-27 18:45:13] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2948829) [2026-01-27 18:45:13] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2948829) [2026-01-27 18:45:13] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2948829) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2948829) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.96s/it]
(EngineCore_DP0 pid=2948829) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 30.78s/it]
(EngineCore_DP0 pid=2948829) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 30.36s/it]
(EngineCore_DP0 pid=2948829) 
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=2948829) 2026-01-27 18:46:21,435 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2948829) 2026-01-27 18:46:21,449 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 10539.28it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:15<32:18, 15.26s/it, est. speed input: 1.05 toks/s, output: 16.77 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:15<00:47,  2.19it/s, est. speed input: 24.96 toks/s, output: 399.29 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:15<00:12,  5.90it/s, est. speed input: 54.56 toks/s, output: 872.98 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:15<00:04, 10.29it/s, est. speed input: 79.66 toks/s, output: 1274.56 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:15<00:01, 15.69it/s, est. speed input: 102.26 toks/s, output: 1636.21 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:15<00:00, 21.99it/s, est. speed input: 121.70 toks/s, output: 1947.23 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:15<00:00, 21.99it/s, est. speed input: 128.74 toks/s, output: 2059.86 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:15<00:00,  8.05it/s, est. speed input: 128.74 toks/s, output: 2059.86 toks/s]
[rank0]:[W127 18:46:38.472964834 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 18:46:40
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:46:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:46:44 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2950611) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2950611) WARNING 01-27 18:47:57 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.94 requests/s, 2975.69 total tokens/s, 2800.64 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 18:46:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:46:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:46:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:46:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:46:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:46:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:46:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:46:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:46:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:46:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:46:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:46:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:46:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:46:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:46:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:46:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2950611) [2026-01-27 18:46:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2950611) [2026-01-27 18:46:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2950611) [2026-01-27 18:46:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2950611) [2026-01-27 18:46:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2950611) [2026-01-27 18:46:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2950611) [2026-01-27 18:46:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2950611) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2950611) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.62s/it]
(EngineCore_DP0 pid=2950611) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 30.73s/it]
(EngineCore_DP0 pid=2950611) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 30.26s/it]
(EngineCore_DP0 pid=2950611) 
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=2950611) 2026-01-27 18:47:56,004 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2950611) 2026-01-27 18:47:56,022 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 4289.80it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:20<1:26:09, 20.27s/it, est. speed input: 0.79 toks/s, output: 12.63 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:20<03:13,  1.23it/s, est. speed input: 14.07 toks/s, output: 225.06 toks/s]
Processed prompts:  13%|█▎        | 33/256 [00:20<01:22,  2.69it/s, est. speed input: 25.66 toks/s, output: 410.48 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:20<00:29,  6.60it/s, est. speed input: 47.81 toks/s, output: 764.88 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:20<00:15, 11.08it/s, est. speed input: 66.47 toks/s, output: 1063.52 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:21<00:09, 16.04it/s, est. speed input: 81.87 toks/s, output: 1309.98 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:21<00:05, 21.80it/s, est. speed input: 95.58 toks/s, output: 1529.28 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:21<00:03, 28.16it/s, est. speed input: 107.61 toks/s, output: 1721.75 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:21<00:02, 34.92it/s, est. speed input: 118.06 toks/s, output: 1888.97 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:21<00:02, 41.49it/s, est. speed input: 126.94 toks/s, output: 2031.11 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:21<00:01, 48.10it/s, est. speed input: 135.02 toks/s, output: 2160.25 toks/s]
Processed prompts:  76%|███████▌  | 195/256 [00:21<00:01, 54.13it/s, est. speed input: 142.28 toks/s, output: 2276.53 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:22<00:00, 56.72it/s, est. speed input: 149.17 toks/s, output: 2386.75 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:22<00:00, 57.77it/s, est. speed input: 155.27 toks/s, output: 2484.24 toks/s]
Processed prompts:  88%|████████▊ | 225/256 [00:22<00:00, 57.77it/s, est. speed input: 160.61 toks/s, output: 2569.77 toks/s]
Processed prompts:  91%|█████████ | 233/256 [00:22<00:00, 58.41it/s, est. speed input: 165.35 toks/s, output: 2645.55 toks/s]
Processed prompts:  94%|█████████▍| 241/256 [00:22<00:00, 55.24it/s, est. speed input: 169.76 toks/s, output: 2716.13 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:23<00:00, 42.28it/s, est. speed input: 172.50 toks/s, output: 2759.93 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:23<00:00, 36.41it/s, est. speed input: 174.82 toks/s, output: 2797.20 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:23<00:00, 36.41it/s, est. speed input: 175.50 toks/s, output: 2807.99 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:23<00:00, 10.97it/s, est. speed input: 175.50 toks/s, output: 2807.99 toks/s]
[rank0]:[W127 18:48:20.712594917 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 18:48:23
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:48:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:48:28 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2952546) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2952546) WARNING 01-27 18:49:41 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.84 requests/s, 2949.31 total tokens/s, 2775.82 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 18:48:28] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:48:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:48:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:48:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:48:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:48:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:48:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:48:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:48:31] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:48:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:48:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:48:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:48:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:48:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:48:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:48:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2952546) [2026-01-27 18:48:33] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2952546) [2026-01-27 18:48:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2952546) [2026-01-27 18:48:33] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2952546) [2026-01-27 18:48:33] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2952546) [2026-01-27 18:48:33] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2952546) [2026-01-27 18:48:33] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2952546) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2952546) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.99s/it]
(EngineCore_DP0 pid=2952546) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:01<00:00, 30.96s/it]
(EngineCore_DP0 pid=2952546) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:01<00:00, 30.51s/it]
(EngineCore_DP0 pid=2952546) 
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=2952546) 2026-01-27 18:49:40,822 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2952546) 2026-01-27 18:49:40,840 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 11854.14it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:33<4:47:34, 33.77s/it, est. speed input: 0.47 toks/s, output: 7.58 toks/s]
Processed prompts:   1%|          | 3/512 [00:34<1:15:18,  8.88s/it, est. speed input: 1.41 toks/s, output: 22.52 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:34<04:08,  1.92it/s, est. speed input: 15.81 toks/s, output: 252.94 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:34<01:45,  4.24it/s, est. speed input: 29.03 toks/s, output: 464.49 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:35<00:58,  7.25it/s, est. speed input: 41.59 toks/s, output: 665.42 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:35<00:36, 10.92it/s, est. speed input: 53.03 toks/s, output: 848.42 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:35<00:23, 15.73it/s, est. speed input: 63.66 toks/s, output: 1018.60 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:35<00:15, 21.85it/s, est. speed input: 73.77 toks/s, output: 1180.35 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:35<00:11, 29.11it/s, est. speed input: 82.93 toks/s, output: 1326.82 toks/s]
Processed prompts:  40%|████      | 205/512 [00:35<00:08, 37.56it/s, est. speed input: 91.54 toks/s, output: 1464.67 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:35<00:06, 47.75it/s, est. speed input: 99.71 toks/s, output: 1595.33 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:36<00:04, 59.02it/s, est. speed input: 107.39 toks/s, output: 1718.16 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:36<00:03, 68.37it/s, est. speed input: 114.08 toks/s, output: 1825.35 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:36<00:03, 78.44it/s, est. speed input: 120.35 toks/s, output: 1925.66 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:36<00:02, 99.65it/s, est. speed input: 132.48 toks/s, output: 2119.72 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:36<00:01, 107.70it/s, est. speed input: 142.69 toks/s, output: 2282.97 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:36<00:01, 112.48it/s, est. speed input: 151.16 toks/s, output: 2418.62 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [00:36<00:01, 114.88it/s, est. speed input: 158.74 toks/s, output: 2539.92 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [00:37<00:01, 115.35it/s, est. speed input: 165.45 toks/s, output: 2647.15 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [00:37<00:01, 110.12it/s, est. speed input: 171.19 toks/s, output: 2739.10 toks/s]
Processed prompts:  80%|████████  | 412/512 [00:37<00:00, 106.25it/s, est. speed input: 176.13 toks/s, output: 2818.03 toks/s]
Processed prompts:  83%|████████▎ | 424/512 [00:37<00:00, 102.20it/s, est. speed input: 180.62 toks/s, output: 2889.92 toks/s]
Processed prompts:  85%|████████▍ | 435/512 [00:37<00:00, 94.46it/s, est. speed input: 184.59 toks/s, output: 2953.48 toks/s] 
Processed prompts:  87%|████████▋ | 445/512 [00:37<00:00, 95.02it/s, est. speed input: 188.32 toks/s, output: 3013.13 toks/s]
Processed prompts:  89%|████████▉ | 455/512 [00:37<00:00, 79.82it/s, est. speed input: 191.61 toks/s, output: 3065.82 toks/s]
Processed prompts:  91%|█████████ | 464/512 [00:38<00:00, 75.06it/s, est. speed input: 194.68 toks/s, output: 3114.81 toks/s]
Processed prompts:  92%|█████████▏| 472/512 [00:38<00:00, 61.29it/s, est. speed input: 196.96 toks/s, output: 3151.33 toks/s]
Processed prompts:  94%|█████████▎| 479/512 [00:38<00:00, 59.04it/s, est. speed input: 199.19 toks/s, output: 3186.99 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:38<00:00, 45.60it/s, est. speed input: 200.74 toks/s, output: 3211.80 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [00:38<00:00, 37.30it/s, est. speed input: 201.87 toks/s, output: 3229.92 toks/s]
Processed prompts:  97%|█████████▋| 497/512 [00:39<00:00, 32.24it/s, est. speed input: 202.72 toks/s, output: 3243.49 toks/s]
Processed prompts:  98%|█████████▊| 501/512 [00:47<00:04,  2.41it/s, est. speed input: 170.21 toks/s, output: 2723.35 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:47<00:00,  2.41it/s, est. speed input: 173.65 toks/s, output: 2778.47 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:47<00:00, 10.85it/s, est. speed input: 173.65 toks/s, output: 2778.47 toks/s]
[rank0]:[W127 18:50:29.244544942 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 20:35:33
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:35:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:35:39 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3057156) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3057156) WARNING 01-27 20:37:59 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.57 requests/s, 698.27 total tokens/s, 657.20 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 20:35:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:35:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:35:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:35:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:35:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:35:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:35:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:35:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:35:43] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:35:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:35:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:35:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:35:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:35:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:35:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:35:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3057156) [2026-01-27 20:35:44] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3057156) [2026-01-27 20:35:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3057156) [2026-01-27 20:35:44] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3057156) [2026-01-27 20:35:44] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3057156) [2026-01-27 20:35:44] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3057156) [2026-01-27 20:35:44] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3057156) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3057156) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.56s/it]
(EngineCore_DP0 pid=3057156) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:47<00:52, 26.24s/it]
(EngineCore_DP0 pid=3057156) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:18<00:28, 28.36s/it]
(EngineCore_DP0 pid=3057156) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 33.30s/it]
(EngineCore_DP0 pid=3057156) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 29.73s/it]
(EngineCore_DP0 pid=3057156) 
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=3057156) 2026-01-27 20:37:53,214 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3057156) 2026-01-27 20:37:53,455 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 1/64 [00:00<00:10,  5.79it/s]
Adding requests:   5%|▍         | 3/64 [00:00<00:05, 11.58it/s]
Adding requests:  11%|█         | 7/64 [00:00<00:02, 20.40it/s]
Adding requests:  19%|█▉        | 12/64 [00:00<00:01, 27.32it/s]
Adding requests:  30%|██▉       | 19/64 [00:00<00:01, 39.71it/s]
Adding requests:  47%|████▋     | 30/64 [00:00<00:00, 60.41it/s]
Adding requests:  69%|██████▉   | 44/64 [00:00<00:00, 82.64it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 115.30it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 66.71it/s] 

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:23<24:32, 23.38s/it, est. speed input: 0.68 toks/s, output: 10.95 toks/s]
Processed prompts:  11%|█         | 7/64 [00:23<02:20,  2.47s/it, est. speed input: 4.75 toks/s, output: 75.99 toks/s]
Processed prompts:  36%|███▌      | 23/64 [00:23<00:23,  1.77it/s, est. speed input: 15.54 toks/s, output: 248.64 toks/s]
Processed prompts:  59%|█████▉    | 38/64 [00:23<00:07,  3.55it/s, est. speed input: 25.56 toks/s, output: 408.88 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:23<00:00,  3.55it/s, est. speed input: 42.88 toks/s, output: 686.11 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:23<00:00,  2.68it/s, est. speed input: 42.88 toks/s, output: 686.11 toks/s]
[rank0]:[W127 20:38:25.793077436 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 20:38:41
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:38:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:38:47 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3059956) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3059956) WARNING 01-27 20:41:02 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.28 requests/s, 1163.48 total tokens/s, 1095.04 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 20:38:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:38:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:38:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:38:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:38:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:38:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:38:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:38:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:38:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:38:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:38:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:38:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:38:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:38:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:38:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:38:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3059956) [2026-01-27 20:38:51] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3059956) [2026-01-27 20:38:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3059956) [2026-01-27 20:38:51] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3059956) [2026-01-27 20:38:51] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3059956) [2026-01-27 20:38:51] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3059956) [2026-01-27 20:38:51] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3059956) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3059956) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.54s/it]
(EngineCore_DP0 pid=3059956) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:48<00:54, 27.03s/it]
(EngineCore_DP0 pid=3059956) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:19<00:28, 28.81s/it]
(EngineCore_DP0 pid=3059956) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 33.38s/it]
(EngineCore_DP0 pid=3059956) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 29.96s/it]
(EngineCore_DP0 pid=3059956) 
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=3059956) 2026-01-27 20:41:00,837 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3059956) 2026-01-27 20:41:00,909 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   1%|          | 1/128 [00:00<00:17,  7.20it/s]
Adding requests:   5%|▍         | 6/128 [00:00<00:04, 28.79it/s]
Adding requests:  11%|█         | 14/128 [00:00<00:02, 49.70it/s]
Adding requests:  35%|███▌      | 45/128 [00:00<00:00, 145.90it/s]
Adding requests:  73%|███████▎  | 94/128 [00:00<00:00, 264.25it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 218.30it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:28<59:16, 28.00s/it, est. speed input: 0.57 toks/s, output: 9.14 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:28<24:21, 11.60s/it, est. speed input: 1.14 toks/s, output: 18.20 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:28<01:29,  1.23it/s, est. speed input: 10.19 toks/s, output: 163.03 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:28<00:34,  2.73it/s, est. speed input: 18.59 toks/s, output: 297.44 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:28<00:16,  4.82it/s, est. speed input: 26.94 toks/s, output: 431.02 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:28<00:08,  7.48it/s, est. speed input: 34.65 toks/s, output: 554.37 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:28<00:04, 10.79it/s, est. speed input: 41.74 toks/s, output: 667.76 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:28<00:02, 14.78it/s, est. speed input: 48.21 toks/s, output: 771.38 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:28<00:01, 19.49it/s, est. speed input: 54.10 toks/s, output: 865.56 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:29<00:00, 30.33it/s, est. speed input: 64.78 toks/s, output: 1036.41 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:29<00:00, 30.33it/s, est. speed input: 70.08 toks/s, output: 1121.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:29<00:00,  4.38it/s, est. speed input: 70.08 toks/s, output: 1121.25 toks/s]
[rank0]:[W127 20:41:32.880713627 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 20:41:48
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:41:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:41:55 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3062837) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3062837) WARNING 01-27 20:44:16 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.84 requests/s, 1587.47 total tokens/s, 1494.09 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 20:41:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:41:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:41:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:41:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:41:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:41:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:41:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:41:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:41:58] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:41:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:41:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:41:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:41:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:41:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:41:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:41:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3062837) [2026-01-27 20:41:59] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3062837) [2026-01-27 20:41:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3062837) [2026-01-27 20:41:59] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3062837) [2026-01-27 20:41:59] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3062837) [2026-01-27 20:41:59] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3062837) [2026-01-27 20:41:59] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3062837) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3062837) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.61s/it]
(EngineCore_DP0 pid=3062837) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:48<00:53, 26.73s/it]
(EngineCore_DP0 pid=3062837) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:18<00:28, 28.59s/it]
(EngineCore_DP0 pid=3062837) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 33.44s/it]
(EngineCore_DP0 pid=3062837) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 29.93s/it]
(EngineCore_DP0 pid=3062837) 
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=3062837) 2026-01-27 20:44:09,259 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3062837) 2026-01-27 20:44:09,318 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:37,  6.89it/s]
Adding requests:   2%|▏         | 4/256 [00:00<00:13, 18.08it/s]
Adding requests:   4%|▎         | 9/256 [00:00<00:07, 31.11it/s]
Adding requests:   7%|▋         | 17/256 [00:00<00:04, 48.36it/s]
Adding requests:  13%|█▎        | 33/256 [00:00<00:02, 85.22it/s]
Adding requests:  23%|██▎       | 60/256 [00:00<00:01, 143.87it/s]
Adding requests:  38%|███▊      | 98/256 [00:00<00:00, 217.61it/s]
Adding requests:  52%|█████▏    | 134/256 [00:00<00:00, 261.03it/s]
Adding requests:  64%|██████▍   | 164/256 [00:00<00:00, 270.72it/s]
Adding requests:  75%|███████▌  | 192/256 [00:01<00:00, 211.33it/s]
Adding requests:  93%|█████████▎| 237/256 [00:01<00:00, 270.48it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 196.68it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:36<2:36:36, 36.85s/it, est. speed input: 0.43 toks/s, output: 6.95 toks/s]
Processed prompts:   1%|          | 2/256 [00:37<1:04:38, 15.27s/it, est. speed input: 0.86 toks/s, output: 13.83 toks/s]
Processed prompts:   5%|▌         | 14/256 [00:37<05:36,  1.39s/it, est. speed input: 6.03 toks/s, output: 96.41 toks/s] 
Processed prompts:  12%|█▏        | 30/256 [00:37<01:56,  1.94it/s, est. speed input: 12.85 toks/s, output: 205.61 toks/s]
Processed prompts:  18%|█▊        | 45/256 [00:37<00:59,  3.52it/s, est. speed input: 19.18 toks/s, output: 306.94 toks/s]
Processed prompts:  23%|██▎       | 59/256 [00:37<00:35,  5.54it/s, est. speed input: 25.05 toks/s, output: 400.82 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:37<00:22,  8.02it/s, est. speed input: 30.43 toks/s, output: 486.80 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:38<00:15, 11.04it/s, est. speed input: 35.35 toks/s, output: 565.52 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:38<00:10, 14.67it/s, est. speed input: 39.83 toks/s, output: 637.30 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:38<00:07, 19.03it/s, est. speed input: 44.26 toks/s, output: 708.18 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:38<00:05, 23.68it/s, est. speed input: 48.25 toks/s, output: 771.98 toks/s]
Processed prompts:  49%|████▉     | 125/256 [00:38<00:04, 28.73it/s, est. speed input: 51.83 toks/s, output: 829.21 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:38<00:03, 32.98it/s, est. speed input: 55.32 toks/s, output: 885.15 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:38<00:03, 37.84it/s, est. speed input: 58.44 toks/s, output: 935.08 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:39<00:02, 41.93it/s, est. speed input: 61.52 toks/s, output: 984.36 toks/s]
Processed prompts:  61%|██████▏   | 157/256 [00:39<00:02, 45.80it/s, est. speed input: 64.21 toks/s, output: 1027.37 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:39<00:01, 47.61it/s, est. speed input: 66.85 toks/s, output: 1069.60 toks/s]
Processed prompts:  67%|██████▋   | 171/256 [00:39<00:01, 51.53it/s, est. speed input: 69.52 toks/s, output: 1112.25 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:39<00:01, 55.04it/s, est. speed input: 72.17 toks/s, output: 1154.70 toks/s]
Processed prompts:  72%|███████▏  | 185/256 [00:39<00:01, 55.48it/s, est. speed input: 74.77 toks/s, output: 1196.36 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:39<00:01, 47.27it/s, est. speed input: 77.21 toks/s, output: 1235.34 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:39<00:01, 45.58it/s, est. speed input: 79.33 toks/s, output: 1269.32 toks/s]
Processed prompts:  80%|████████  | 205/256 [00:40<00:01, 42.55it/s, est. speed input: 81.75 toks/s, output: 1307.99 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:40<00:01, 43.04it/s, est. speed input: 83.51 toks/s, output: 1336.16 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:40<00:01, 39.67it/s, est. speed input: 85.51 toks/s, output: 1368.20 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:40<00:00, 37.88it/s, est. speed input: 87.51 toks/s, output: 1400.12 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:40<00:00, 37.67it/s, est. speed input: 88.85 toks/s, output: 1421.56 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:40<00:00, 33.15it/s, est. speed input: 90.05 toks/s, output: 1440.80 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:41<00:00, 30.07it/s, est. speed input: 91.24 toks/s, output: 1459.80 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:41<00:00, 28.40it/s, est. speed input: 92.43 toks/s, output: 1478.90 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:41<00:00, 27.31it/s, est. speed input: 93.62 toks/s, output: 1497.89 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:41<00:00, 21.52it/s, est. speed input: 94.23 toks/s, output: 1507.72 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:41<00:00, 18.40it/s, est. speed input: 94.85 toks/s, output: 1517.58 toks/s]
Processed prompts:  98%|█████████▊| 251/256 [00:42<00:00, 16.45it/s, est. speed input: 95.45 toks/s, output: 1527.26 toks/s]
Processed prompts:  99%|█████████▉| 253/256 [00:42<00:00, 15.66it/s, est. speed input: 95.87 toks/s, output: 1533.88 toks/s]
Processed prompts: 100%|█████████▉| 255/256 [00:42<00:00, 15.01it/s, est. speed input: 96.28 toks/s, output: 1540.42 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:42<00:00, 15.01it/s, est. speed input: 96.42 toks/s, output: 1542.68 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:42<00:00,  6.03it/s, est. speed input: 96.42 toks/s, output: 1542.68 toks/s]
[rank0]:[W127 20:45:01.588261787 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 20:45:15
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:45:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:45:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3065999) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3065999) WARNING 01-27 20:47:40 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.71 requests/s, 1553.37 total tokens/s, 1462.00 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 20:45:21] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:45:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:45:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:45:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:45:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:45:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:45:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:45:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:45:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:45:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:45:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:45:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:45:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:45:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:45:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:45:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3065999) [2026-01-27 20:45:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3065999) [2026-01-27 20:45:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3065999) [2026-01-27 20:45:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3065999) [2026-01-27 20:45:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3065999) [2026-01-27 20:45:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3065999) [2026-01-27 20:45:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3065999) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3065999) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.54s/it]
(EngineCore_DP0 pid=3065999) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:49<00:54, 27.32s/it]
(EngineCore_DP0 pid=3065999) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:20<00:29, 29.06s/it]
(EngineCore_DP0 pid=3065999) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:00<00:00, 33.44s/it]
(EngineCore_DP0 pid=3065999) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:00<00:00, 30.08s/it]
(EngineCore_DP0 pid=3065999) 
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=3065999) 2026-01-27 20:47:36,325 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3065999) 2026-01-27 20:47:36,413 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/512 [00:00<01:15,  6.78it/s]
Adding requests:   1%|          | 3/512 [00:00<00:38, 13.15it/s]
Adding requests:   1%|▏         | 7/512 [00:00<00:21, 23.47it/s]
Adding requests:   2%|▏         | 12/512 [00:00<00:15, 33.01it/s]
Adding requests:   4%|▍         | 20/512 [00:00<00:10, 47.71it/s]
Adding requests:   6%|▌         | 30/512 [00:00<00:07, 64.16it/s]
Adding requests:   9%|▊         | 44/512 [00:00<00:05, 86.85it/s]
Adding requests:  15%|█▍        | 76/512 [00:00<00:02, 157.50it/s]
Adding requests:  20%|██        | 103/512 [00:00<00:02, 188.01it/s]
Adding requests:  29%|██▊       | 146/512 [00:01<00:01, 257.86it/s]
Adding requests:  39%|███▊      | 198/512 [00:01<00:00, 333.83it/s]
Adding requests:  52%|█████▏    | 264/512 [00:01<00:00, 429.81it/s]
Adding requests:  72%|███████▏  | 370/512 [00:01<00:00, 617.14it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 350.38it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [01:00<8:37:48, 60.80s/it, est. speed input: 0.26 toks/s, output: 4.21 toks/s]
Processed prompts:   0%|          | 2/512 [01:01<3:36:18, 25.45s/it, est. speed input: 0.52 toks/s, output: 8.33 toks/s]
Processed prompts:   2%|▏         | 11/512 [01:02<25:24,  3.04s/it, est. speed input: 2.83 toks/s, output: 45.25 toks/s]
Processed prompts:   4%|▎         | 19/512 [01:02<12:09,  1.48s/it, est. speed input: 4.83 toks/s, output: 77.30 toks/s]
Processed prompts:   6%|▌         | 30/512 [01:03<06:05,  1.32it/s, est. speed input: 7.55 toks/s, output: 120.73 toks/s]
Processed prompts:  10%|▉         | 49/512 [01:04<02:44,  2.81it/s, est. speed input: 12.19 toks/s, output: 195.11 toks/s]
Processed prompts:  15%|█▌        | 78/512 [01:04<01:14,  5.79it/s, est. speed input: 19.22 toks/s, output: 307.52 toks/s]
Processed prompts:  21%|██        | 105/512 [01:05<00:44,  9.12it/s, est. speed input: 25.63 toks/s, output: 410.02 toks/s]
Processed prompts:  25%|██▌       | 130/512 [01:05<00:28, 13.53it/s, est. speed input: 31.61 toks/s, output: 505.75 toks/s]
Processed prompts:  30%|██▉       | 153/512 [01:06<00:19, 18.66it/s, est. speed input: 37.07 toks/s, output: 593.07 toks/s]
Processed prompts:  34%|███▍      | 175/512 [01:06<00:13, 24.63it/s, est. speed input: 42.24 toks/s, output: 675.89 toks/s]
Processed prompts:  38%|███▊      | 196/512 [01:06<00:10, 31.55it/s, est. speed input: 47.16 toks/s, output: 754.56 toks/s]
Processed prompts:  42%|████▏     | 215/512 [01:06<00:07, 38.17it/s, est. speed input: 51.56 toks/s, output: 824.94 toks/s]
Processed prompts:  46%|████▌     | 233/512 [01:06<00:06, 45.37it/s, est. speed input: 55.71 toks/s, output: 891.40 toks/s]
Processed prompts:  49%|████▉     | 250/512 [01:07<00:05, 51.94it/s, est. speed input: 59.60 toks/s, output: 953.62 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [01:07<00:04, 57.91it/s, est. speed input: 63.24 toks/s, output: 1011.81 toks/s]
Processed prompts:  55%|█████▍    | 281/512 [01:07<00:03, 62.34it/s, est. speed input: 66.62 toks/s, output: 1065.88 toks/s]
Processed prompts:  58%|█████▊    | 295/512 [01:07<00:03, 65.57it/s, est. speed input: 69.75 toks/s, output: 1115.98 toks/s]
Processed prompts:  60%|██████    | 308/512 [01:07<00:02, 69.72it/s, est. speed input: 72.66 toks/s, output: 1162.55 toks/s]
Processed prompts:  62%|██████▎   | 320/512 [01:07<00:02, 72.67it/s, est. speed input: 75.33 toks/s, output: 1205.27 toks/s]
Processed prompts:  65%|██████▍   | 332/512 [01:08<00:02, 72.94it/s, est. speed input: 77.97 toks/s, output: 1247.48 toks/s]
Processed prompts:  67%|██████▋   | 343/512 [01:08<00:02, 71.95it/s, est. speed input: 80.36 toks/s, output: 1285.82 toks/s]
Processed prompts:  69%|██████▉   | 353/512 [01:08<00:02, 71.23it/s, est. speed input: 82.53 toks/s, output: 1320.51 toks/s]
Processed prompts:  71%|███████   | 363/512 [01:08<00:02, 71.99it/s, est. speed input: 84.70 toks/s, output: 1355.25 toks/s]
Processed prompts:  73%|███████▎  | 372/512 [01:08<00:01, 72.09it/s, est. speed input: 86.65 toks/s, output: 1386.33 toks/s]
Processed prompts:  74%|███████▍  | 380/512 [01:08<00:01, 70.15it/s, est. speed input: 88.35 toks/s, output: 1413.60 toks/s]
Processed prompts:  76%|███████▌  | 388/512 [01:08<00:01, 63.36it/s, est. speed input: 90.00 toks/s, output: 1439.99 toks/s]
Processed prompts:  77%|███████▋  | 395/512 [01:09<00:01, 62.78it/s, est. speed input: 91.47 toks/s, output: 1463.53 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [01:09<00:01, 59.11it/s, est. speed input: 92.91 toks/s, output: 1486.49 toks/s]
Processed prompts:  80%|███████▉  | 408/512 [01:09<00:01, 57.79it/s, est. speed input: 94.14 toks/s, output: 1506.26 toks/s]
Processed prompts:  81%|████████  | 414/512 [01:09<00:01, 57.00it/s, est. speed input: 95.38 toks/s, output: 1526.00 toks/s]
Processed prompts:  82%|████████▏ | 420/512 [01:09<00:01, 53.69it/s, est. speed input: 96.58 toks/s, output: 1545.24 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [01:09<00:01, 54.23it/s, est. speed input: 97.81 toks/s, output: 1564.89 toks/s]
Processed prompts:  84%|████████▍ | 432/512 [01:09<00:01, 55.17it/s, est. speed input: 99.04 toks/s, output: 1584.57 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [01:09<00:01, 52.96it/s, est. speed input: 100.23 toks/s, output: 1603.71 toks/s]
Processed prompts:  87%|████████▋ | 444/512 [01:10<00:01, 43.42it/s, est. speed input: 101.32 toks/s, output: 1621.07 toks/s]
Processed prompts:  88%|████████▊ | 451/512 [01:10<00:01, 38.81it/s, est. speed input: 102.59 toks/s, output: 1641.48 toks/s]
Processed prompts:  89%|████████▉ | 457/512 [01:10<00:01, 36.56it/s, est. speed input: 103.68 toks/s, output: 1658.89 toks/s]
Processed prompts:  90%|█████████ | 463/512 [01:10<00:01, 35.24it/s, est. speed input: 104.77 toks/s, output: 1676.27 toks/s]
Processed prompts:  91%|█████████ | 467/512 [01:10<00:01, 35.46it/s, est. speed input: 105.51 toks/s, output: 1688.12 toks/s]
Processed prompts:  92%|█████████▏| 471/512 [01:10<00:01, 31.37it/s, est. speed input: 106.15 toks/s, output: 1698.38 toks/s]
Processed prompts:  93%|█████████▎| 475/512 [01:11<00:01, 28.86it/s, est. speed input: 106.79 toks/s, output: 1708.67 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [01:11<00:01, 25.39it/s, est. speed input: 107.21 toks/s, output: 1715.35 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [01:11<00:01, 24.00it/s, est. speed input: 107.82 toks/s, output: 1725.13 toks/s]
Processed prompts:  95%|█████████▍| 485/512 [01:11<00:01, 19.35it/s, est. speed input: 108.11 toks/s, output: 1729.77 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [01:12<00:01, 16.89it/s, est. speed input: 108.41 toks/s, output: 1734.55 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [01:12<00:01, 15.82it/s, est. speed input: 108.61 toks/s, output: 1737.81 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [01:12<00:01, 14.96it/s, est. speed input: 108.82 toks/s, output: 1741.04 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [01:12<00:01, 14.36it/s, est. speed input: 109.02 toks/s, output: 1744.33 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [01:12<00:01, 13.98it/s, est. speed input: 109.23 toks/s, output: 1747.67 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [01:27<00:28,  2.04s/it, est. speed input: 90.92 toks/s, output: 1454.64 toks/s] 
Processed prompts:  98%|█████████▊| 500/512 [01:27<00:18,  1.50s/it, est. speed input: 91.11 toks/s, output: 1457.75 toks/s]
Processed prompts:  98%|█████████▊| 503/512 [01:27<00:08,  1.04it/s, est. speed input: 91.49 toks/s, output: 1463.87 toks/s]
Processed prompts: 100%|█████████▉| 511/512 [01:28<00:00,  2.43it/s, est. speed input: 92.78 toks/s, output: 1484.55 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:28<00:00,  2.43it/s, est. speed input: 92.97 toks/s, output: 1487.45 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:28<00:00,  5.81it/s, est. speed input: 92.97 toks/s, output: 1487.45 toks/s]
[rank0]:[W127 20:49:11.711035338 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-28 10:09:20
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/BitNet-2B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:09:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:09:24 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3789549) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3789549) WARNING 01-28 10:09:52 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 9.25 requests/s, 2516.51 total tokens/s, 2368.48 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-28 10:09:24] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:09:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:09:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:09:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:09:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:09:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:09:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:09:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:09:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:09:27] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:09:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:09:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:09:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:09:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:09:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:09:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:09:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:09:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:09:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:28] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:28] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:28] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:28] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:28] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3789549) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3789549) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.70s/it]
(EngineCore_DP0 pid=3789549) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.70s/it]
(EngineCore_DP0 pid=3789549) 
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3789549) [2026-01-28 10:09:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3789549) 2026-01-28 10:09:51,699 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3789549) 2026-01-28 10:09:51,717 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 12072.65it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:06<07:04,  6.74s/it, est. speed input: 2.37 toks/s, output: 37.99 toks/s]
Processed prompts:  52%|█████▏    | 33/64 [00:06<00:04,  6.74it/s, est. speed input: 76.82 toks/s, output: 1229.11 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:06<00:00,  6.74it/s, est. speed input: 148.16 toks/s, output: 2370.55 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:06<00:00,  9.26it/s, est. speed input: 148.16 toks/s, output: 2370.55 toks/s]
[rank0]:[W128 10:09:59.361733638 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-28 10:10:01
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/BitNet-2B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:10:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:10:05 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3790299) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3790299) WARNING 01-28 10:10:32 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.25 requests/s, 3876.97 total tokens/s, 3648.91 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-28 10:10:05] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:10:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:10:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:10:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:10:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:10:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:10:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:10:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:10:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:10:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:10:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:10:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:10:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:10:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:10:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:10:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:10:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:10:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:09] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:09] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:09] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:09] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:09] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3790299) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3790299) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.48s/it]
(EngineCore_DP0 pid=3790299) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.48s/it]
(EngineCore_DP0 pid=3790299) 
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3790299) [2026-01-28 10:10:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3790299) 2026-01-28 10:10:32,113 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3790299) 2026-01-28 10:10:32,129 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3306.41it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:08<18:00,  8.51s/it, est. speed input: 1.88 toks/s, output: 30.10 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:08<00:17,  5.36it/s, est. speed input: 60.99 toks/s, output: 975.77 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:08<00:03, 14.73it/s, est. speed input: 136.52 toks/s, output: 2184.28 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:08<00:00, 24.54it/s, est. speed input: 194.12 toks/s, output: 3105.91 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 24.54it/s, est. speed input: 229.08 toks/s, output: 3665.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.32it/s, est. speed input: 229.08 toks/s, output: 3665.30 toks/s]
[rank0]:[W128 10:10:41.971377723 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-28 10:10:44
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/BitNet-2B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:10:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:10:47 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3791075) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3791075) WARNING 01-28 10:11:15 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.44 requests/s, 5014.59 total tokens/s, 4719.62 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-28 10:10:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:10:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:10:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:10:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:10:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:10:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:10:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:10:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:10:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:10:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:10:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:10:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:10:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:10:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:10:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:10:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:10:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:10:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:10:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3791075) [2026-01-28 10:10:51] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3791075) [2026-01-28 10:10:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3791075) [2026-01-28 10:10:51] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3791075) [2026-01-28 10:10:51] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3791075) [2026-01-28 10:10:51] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3791075) [2026-01-28 10:10:51] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3791075) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3791075) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.37s/it]
(EngineCore_DP0 pid=3791075) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.37s/it]
(EngineCore_DP0 pid=3791075) 
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3791075) [2026-01-28 10:11:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3791075) 2026-01-28 10:11:14,430 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3791075) 2026-01-28 10:11:14,445 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13724.92it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:12<51:11, 12.05s/it, est. speed input: 1.33 toks/s, output: 21.25 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:12<01:55,  2.07it/s, est. speed input: 23.64 toks/s, output: 378.23 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:12<00:30,  6.91it/s, est. speed input: 62.35 toks/s, output: 997.61 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:12<00:14, 12.78it/s, est. speed input: 96.31 toks/s, output: 1541.02 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:12<00:08, 19.36it/s, est. speed input: 124.52 toks/s, output: 1992.30 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:12<00:05, 26.99it/s, est. speed input: 148.75 toks/s, output: 2379.98 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:12<00:03, 36.26it/s, est. speed input: 171.34 toks/s, output: 2741.48 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:12<00:01, 49.23it/s, est. speed input: 196.97 toks/s, output: 3151.46 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:13<00:01, 60.74it/s, est. speed input: 218.25 toks/s, output: 3491.99 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:13<00:00, 74.14it/s, est. speed input: 238.38 toks/s, output: 3814.08 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:13<00:00, 82.36it/s, est. speed input: 257.18 toks/s, output: 4114.86 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:13<00:00, 86.43it/s, est. speed input: 273.12 toks/s, output: 4369.97 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:13<00:00, 86.29it/s, est. speed input: 286.29 toks/s, output: 4580.56 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 86.29it/s, est. speed input: 295.39 toks/s, output: 4726.27 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:13<00:00, 18.46it/s, est. speed input: 295.39 toks/s, output: 4726.27 toks/s]
[rank0]:[W128 10:11:29.149188521 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-28 10:11:31
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/json/BitNet-2B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 10:11:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 10:11:34 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3791939) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3791939) WARNING 01-28 10:12:02 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.67 requests/s, 5078.57 total tokens/s, 4779.83 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-28 10:11:34] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:11:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:11:34] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:11:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:34] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:34] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:11:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:11:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:11:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:11:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:11:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 10:11:38] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 10:11:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 10:11:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 10:11:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 10:11:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 10:11:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 10:11:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 10:11:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 10:11:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 10:11:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:39] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:39] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:39] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:39] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3791939) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3791939) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.55s/it]
(EngineCore_DP0 pid=3791939) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.55s/it]
(EngineCore_DP0 pid=3791939) 
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3791939) [2026-01-28 10:11:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3791939) 2026-01-28 10:12:01,797 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3791939) 2026-01-28 10:12:01,813 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 14502.77it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:20<2:52:45, 20.29s/it, est. speed input: 0.79 toks/s, output: 12.62 toks/s]
Processed prompts:   1%|          | 3/512 [00:20<45:19,  5.34s/it, est. speed input: 2.34 toks/s, output: 37.44 toks/s]  
Processed prompts:   7%|▋         | 34/512 [00:20<02:29,  3.19it/s, est. speed input: 26.25 toks/s, output: 420.04 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:20<01:03,  7.02it/s, est. speed input: 48.18 toks/s, output: 770.96 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:21<00:35, 11.98it/s, est. speed input: 68.98 toks/s, output: 1103.68 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:21<00:22, 17.92it/s, est. speed input: 87.85 toks/s, output: 1405.55 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:21<00:10, 32.57it/s, est. speed input: 122.02 toks/s, output: 1952.37 toks/s]
Processed prompts:  40%|████      | 205/512 [00:21<00:06, 48.05it/s, est. speed input: 151.24 toks/s, output: 2419.85 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:21<00:04, 64.67it/s, est. speed input: 177.19 toks/s, output: 2835.01 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:22<00:03, 79.57it/s, est. speed input: 198.43 toks/s, output: 3174.88 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:22<00:02, 97.46it/s, est. speed input: 218.32 toks/s, output: 3493.16 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:22<00:01, 110.31it/s, est. speed input: 234.93 toks/s, output: 3758.91 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:22<00:01, 122.36it/s, est. speed input: 248.80 toks/s, output: 3980.73 toks/s]
Processed prompts:  72%|███████▏  | 368/512 [00:22<00:01, 134.67it/s, est. speed input: 261.91 toks/s, output: 4190.56 toks/s]
Processed prompts:  76%|███████▌  | 388/512 [00:22<00:00, 145.72it/s, est. speed input: 274.87 toks/s, output: 4397.89 toks/s]
Processed prompts:  80%|███████▉  | 408/512 [00:22<00:00, 141.69it/s, est. speed input: 287.11 toks/s, output: 4593.69 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:22<00:00, 145.19it/s, est. speed input: 298.26 toks/s, output: 4772.21 toks/s]
Processed prompts:  87%|████████▋ | 444/512 [00:22<00:00, 141.30it/s, est. speed input: 309.02 toks/s, output: 4944.26 toks/s]
Processed prompts:  90%|████████▉ | 460/512 [00:23<00:00, 129.88it/s, est. speed input: 318.04 toks/s, output: 5088.68 toks/s]
Processed prompts:  93%|█████████▎| 475/512 [00:23<00:00, 117.69it/s, est. speed input: 326.12 toks/s, output: 5217.93 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [00:23<00:00, 96.44it/s, est. speed input: 332.02 toks/s, output: 5312.34 toks/s] 
Processed prompts:  97%|█████████▋| 499/512 [00:27<00:01, 11.66it/s, est. speed input: 292.11 toks/s, output: 4673.73 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:27<00:00, 11.66it/s, est. speed input: 299.14 toks/s, output: 4786.18 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:27<00:00, 18.70it/s, est. speed input: 299.14 toks/s, output: 4786.18 toks/s]
[rank0]:[W128 10:12:30.069320132 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

