
========== M=16 ==========
Time: 2026-01-25 18:51:05
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-FP8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:51:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:51:08 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=304951) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) ================================================================
(EngineCore_DP0 pid=304951) Internal Triton PTX codegen error
(EngineCore_DP0 pid=304951) `ptxas` stderr:
(EngineCore_DP0 pid=304951) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp1ob1fkpx.ptx -o /tmp/tmp1ob1fkpx.ptx.o
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) //
(EngineCore_DP0 pid=304951) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=304951) //
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) .version 8.7
(EngineCore_DP0 pid=304951) .target sm_121a
(EngineCore_DP0 pid=304951) .address_size 64
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) 	// .globl	_quant_slide_fp8_kernel // -- Begin function _quant_slide_fp8_kernel
(EngineCore_DP0 pid=304951) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=304951)                                         // @_quant_slide_fp8_kernel
(EngineCore_DP0 pid=304951) .visible .entry _quant_slide_fp8_kernel(
(EngineCore_DP0 pid=304951) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_0,
(EngineCore_DP0 pid=304951) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_1,
(EngineCore_DP0 pid=304951) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_2,
(EngineCore_DP0 pid=304951) 	.param .u32 _quant_slide_fp8_kernel_param_3,
(EngineCore_DP0 pid=304951) 	.param .u32 _quant_slide_fp8_kernel_param_4,
(EngineCore_DP0 pid=304951) 	.param .u32 _quant_slide_fp8_kernel_param_5,
(EngineCore_DP0 pid=304951) 	.param .u32 _quant_slide_fp8_kernel_param_6,
(EngineCore_DP0 pid=304951) 	.param .u32 _quant_slide_fp8_kernel_param_7,
(EngineCore_DP0 pid=304951) 	.param .u32 _quant_slide_fp8_kernel_param_8,
(EngineCore_DP0 pid=304951) 	.param .u32 _quant_slide_fp8_kernel_param_9,
(EngineCore_DP0 pid=304951) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_10,
(EngineCore_DP0 pid=304951) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_11
(EngineCore_DP0 pid=304951) )
(EngineCore_DP0 pid=304951) .reqntid 1024
(EngineCore_DP0 pid=304951) {
(EngineCore_DP0 pid=304951) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=304951) 	.reg .b16 	%rs<25>;
(EngineCore_DP0 pid=304951) 	.reg .b32 	%r<115>;
(EngineCore_DP0 pid=304951) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=304951) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=304951) $L__func_begin0:
(EngineCore_DP0 pid=304951) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) // %bb.0:
(EngineCore_DP0 pid=304951) 	ld.param.b32 	%r19, [_quant_slide_fp8_kernel_param_7];
(EngineCore_DP0 pid=304951) 	ld.param.b32 	%r18, [_quant_slide_fp8_kernel_param_5];
(EngineCore_DP0 pid=304951) 	ld.param.b32 	%r17, [_quant_slide_fp8_kernel_param_4];
(EngineCore_DP0 pid=304951) 	ld.param.b64 	%rd3, [_quant_slide_fp8_kernel_param_2];
(EngineCore_DP0 pid=304951) 	ld.param.b64 	%rd4, [_quant_slide_fp8_kernel_param_0];
(EngineCore_DP0 pid=304951) $L__tmp0:
(EngineCore_DP0 pid=304951) 	.loc	1 177 24                        // quant_slide_tuned_Llama3.2-1B.py:177:24
(EngineCore_DP0 pid=304951) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=304951) 	ld.param.b32 	%r21, [_quant_slide_fp8_kernel_param_8];
(EngineCore_DP0 pid=304951) 	.loc	1 182 26                        // quant_slide_tuned_Llama3.2-1B.py:182:26
(EngineCore_DP0 pid=304951) 	mul.lo.s32 	%r22, %r21, %r1;
(EngineCore_DP0 pid=304951) 	.loc	1 182 20                        // quant_slide_tuned_Llama3.2-1B.py:182:20
(EngineCore_DP0 pid=304951) 	mad.wide.s32 	%rd1, %r22, 2, %rd4;
(EngineCore_DP0 pid=304951) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=304951) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=304951) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=304951) 	setp.lt.s32 	%p1, %r18, 1;
(EngineCore_DP0 pid=304951) 	mov.b32 	%r113, 0f2B8CBCCC;
(EngineCore_DP0 pid=304951) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=304951) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=304951) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=304951) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=304951) 	shl.b32 	%r3, %r2, 2;
(EngineCore_DP0 pid=304951) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=304951) 	shr.u32 	%r31, %r2, 3;
(EngineCore_DP0 pid=304951) 	and.b32 	%r32, %r31, 124;
(EngineCore_DP0 pid=304951) 	mov.b32 	%r33, global_smem;
(EngineCore_DP0 pid=304951) 	add.s32 	%r39, %r33, %r32;
(EngineCore_DP0 pid=304951) 	add.s32 	%r42, %r33, %r3;
(EngineCore_DP0 pid=304951) 	mov.b32 	%r37, 0;
(EngineCore_DP0 pid=304951) 	mov.b32 	%r111, 0f00000000;
(EngineCore_DP0 pid=304951) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=304951) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=304951) 	mov.b32 	%r112, %r37;
(EngineCore_DP0 pid=304951) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=304951) 	.loc	1 189 22                        // quant_slide_tuned_Llama3.2-1B.py:189:22
(EngineCore_DP0 pid=304951) 	add.s32 	%r45, %r3, %r112;
(EngineCore_DP0 pid=304951) 	setp.lt.s32 	%p2, %r45, %r17;
(EngineCore_DP0 pid=304951) 	.loc	1 190 29                        // quant_slide_tuned_Llama3.2-1B.py:190:29
(EngineCore_DP0 pid=304951) 	mad.wide.s32 	%rd6, %r45, 2, %rd1;
(EngineCore_DP0 pid=304951) 	.loc	1 190 21                        // quant_slide_tuned_Llama3.2-1B.py:190:21
(EngineCore_DP0 pid=304951) 	// begin inline asm
(EngineCore_DP0 pid=304951) 	mov.u32 %r35, %r37;
(EngineCore_DP0 pid=304951) 	mov.u32 %r36, %r37;
(EngineCore_DP0 pid=304951) 	@%p2 ld.global.v2.b32 { %r35, %r36 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=304951) 	// end inline asm
(EngineCore_DP0 pid=304951) 	mov.b32 	{%rs1, %rs2}, %r35;
(EngineCore_DP0 pid=304951) 	mov.b32 	{%rs3, %rs4}, %r36;
(EngineCore_DP0 pid=304951) 	.loc	1 191 50                        // quant_slide_tuned_Llama3.2-1B.py:191:50
(EngineCore_DP0 pid=304951) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=304951) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=304951) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=304951) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=304951) $L__tmp1:
(EngineCore_DP0 pid=304951) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	bar.sync 	0;
(EngineCore_DP0 pid=304951) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=304951) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=304951) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=304951) 	cvt.f32.bf16 	%r46, %rs11;
(EngineCore_DP0 pid=304951) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	shfl.sync.bfly.b32 	%r47, %r46, 16, 31, -1;
(EngineCore_DP0 pid=304951) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	max.f32 	%r48, %r46, %r47;
(EngineCore_DP0 pid=304951) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	shfl.sync.bfly.b32 	%r49, %r48, 8, 31, -1;
(EngineCore_DP0 pid=304951) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	max.f32 	%r50, %r48, %r49;
(EngineCore_DP0 pid=304951) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	shfl.sync.bfly.b32 	%r51, %r50, 4, 31, -1;
(EngineCore_DP0 pid=304951) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	max.f32 	%r52, %r50, %r51;
(EngineCore_DP0 pid=304951) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	shfl.sync.bfly.b32 	%r53, %r52, 2, 31, -1;
(EngineCore_DP0 pid=304951) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	max.f32 	%r54, %r52, %r53;
(EngineCore_DP0 pid=304951) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	shfl.sync.bfly.b32 	%r55, %r54, 1, 31, -1;
(EngineCore_DP0 pid=304951) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	max.f32 	%r40, %r54, %r55;
(EngineCore_DP0 pid=304951) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	// begin inline asm
(EngineCore_DP0 pid=304951) 	@%p3 st.shared.b32 [ %r39 + 0 ], %r40;
(EngineCore_DP0 pid=304951) 	// end inline asm
(EngineCore_DP0 pid=304951) 	bar.sync 	0;
(EngineCore_DP0 pid=304951) 	// begin inline asm
(EngineCore_DP0 pid=304951) 	@%p4 ld.shared.b32 %r41, [ %r42 + 0 ];
(EngineCore_DP0 pid=304951) 	// end inline asm
(EngineCore_DP0 pid=304951) 	shfl.sync.bfly.b32 	%r56, %r41, 16, 31, -1;
(EngineCore_DP0 pid=304951) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	max.f32 	%r57, %r41, %r56;
(EngineCore_DP0 pid=304951) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	shfl.sync.bfly.b32 	%r58, %r57, 8, 31, -1;
(EngineCore_DP0 pid=304951) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	max.f32 	%r59, %r57, %r58;
(EngineCore_DP0 pid=304951) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	shfl.sync.bfly.b32 	%r60, %r59, 4, 31, -1;
(EngineCore_DP0 pid=304951) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	max.f32 	%r61, %r59, %r60;
(EngineCore_DP0 pid=304951) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	shfl.sync.bfly.b32 	%r62, %r61, 2, 31, -1;
(EngineCore_DP0 pid=304951) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	max.f32 	%r63, %r61, %r62;
(EngineCore_DP0 pid=304951) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	shfl.sync.bfly.b32 	%r64, %r63, 1, 31, -1;
(EngineCore_DP0 pid=304951) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	max.f32 	%r44, %r63, %r64;
(EngineCore_DP0 pid=304951) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=304951) 	// begin inline asm
(EngineCore_DP0 pid=304951) 	@%p19 st.shared.b32 [ %r42 + 0 ], %r44;
(EngineCore_DP0 pid=304951) 	// end inline asm
(EngineCore_DP0 pid=304951) 	bar.sync 	0;
(EngineCore_DP0 pid=304951) 	ld.shared.b32 	%r65, [global_smem];
(EngineCore_DP0 pid=304951) $L__tmp2:
(EngineCore_DP0 pid=304951) 	.loc	1 191 36                        // quant_slide_tuned_Llama3.2-1B.py:191:36
(EngineCore_DP0 pid=304951) 	max.f32 	%r111, %r111, %r65;
(EngineCore_DP0 pid=304951) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=304951) 	add.s32 	%r112, %r112, 4096;
(EngineCore_DP0 pid=304951) 	setp.lt.s32 	%p6, %r112, %r18;
(EngineCore_DP0 pid=304951) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=304951) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=304951) 	.loc	1 193 32                        // quant_slide_tuned_Llama3.2-1B.py:193:32
(EngineCore_DP0 pid=304951) 	max.f32 	%r113, %r111, 0f2B8CBCCC;
(EngineCore_DP0 pid=304951) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=304951) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=304951) 	mov.b32 	%r67, 0f43E00000;
(EngineCore_DP0 pid=304951) 	.loc	1 194 32                        // quant_slide_tuned_Llama3.2-1B.py:194:32
(EngineCore_DP0 pid=304951) 	div.full.f32 	%r68, %r113, %r67;
(EngineCore_DP0 pid=304951) 	.loc	1 194 41                        // quant_slide_tuned_Llama3.2-1B.py:194:41
(EngineCore_DP0 pid=304951) 	max.f32 	%r66, %r68, 0f36924925;
(EngineCore_DP0 pid=304951) 	.loc	1 196 25                        // quant_slide_tuned_Llama3.2-1B.py:196:25
(EngineCore_DP0 pid=304951) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=304951) 	.loc	1 196 30                        // quant_slide_tuned_Llama3.2-1B.py:196:30
(EngineCore_DP0 pid=304951) 	// begin inline asm
(EngineCore_DP0 pid=304951) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r66 };
(EngineCore_DP0 pid=304951) 	// end inline asm
(EngineCore_DP0 pid=304951) 	.loc	1 199 29                        // quant_slide_tuned_Llama3.2-1B.py:199:29
(EngineCore_DP0 pid=304951) 	shl.b32 	%r14, %r19, 2;
(EngineCore_DP0 pid=304951) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=304951) 	setp.lt.s32 	%p8, %r14, 1;
(EngineCore_DP0 pid=304951) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=304951) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=304951) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=304951) 	ld.param.b32 	%r23, [_quant_slide_fp8_kernel_param_9];
(EngineCore_DP0 pid=304951) 	shr.s32 	%r24, %r23, 31;
(EngineCore_DP0 pid=304951) 	shr.u32 	%r25, %r24, 30;
(EngineCore_DP0 pid=304951) 	add.s32 	%r26, %r23, %r25;
(EngineCore_DP0 pid=304951) 	shr.s32 	%r27, %r26, 2;
(EngineCore_DP0 pid=304951) 	ld.param.b64 	%rd5, [_quant_slide_fp8_kernel_param_1];
(EngineCore_DP0 pid=304951) 	mul.lo.s32 	%r28, %r27, %r1;
(EngineCore_DP0 pid=304951) 	mad.wide.s32 	%rd2, %r28, 4, %rd5;
(EngineCore_DP0 pid=304951) 	div.full.f32 	%r13, %r67, %r113;
(EngineCore_DP0 pid=304951) 	mov.b32 	%r114, 0;
(EngineCore_DP0 pid=304951) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=304951)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=304951) 	.loc	1 203 30                        // quant_slide_tuned_Llama3.2-1B.py:203:30
(EngineCore_DP0 pid=304951) 	add.s32 	%r80, %r2, %r114;
(EngineCore_DP0 pid=304951) 	setp.lt.s32 	%p13, %r80, %r14;
(EngineCore_DP0 pid=304951) 	.loc	1 206 24                        // quant_slide_tuned_Llama3.2-1B.py:206:24
(EngineCore_DP0 pid=304951) 	shr.s32 	%r81, %r80, 31;
(EngineCore_DP0 pid=304951) 	shr.u32 	%r82, %r81, 30;
(EngineCore_DP0 pid=304951) 	add.s32 	%r83, %r80, %r82;
(EngineCore_DP0 pid=304951) 	shr.s32 	%r84, %r83, 2;
(EngineCore_DP0 pid=304951) 	.loc	1 207 23                        // quant_slide_tuned_Llama3.2-1B.py:207:23
(EngineCore_DP0 pid=304951) 	and.b32 	%r85, %r83, 2147483644;
(EngineCore_DP0 pid=304951) 	sub.s32 	%r86, %r80, %r85;
(EngineCore_DP0 pid=304951) 	.loc	1 208 30                        // quant_slide_tuned_Llama3.2-1B.py:208:30
(EngineCore_DP0 pid=304951) 	shl.b32 	%r87, %r86, 1;
(EngineCore_DP0 pid=304951) 	.loc	1 208 26                        // quant_slide_tuned_Llama3.2-1B.py:208:26
(EngineCore_DP0 pid=304951) 	mad.lo.s32 	%r88, %r84, 10, %r87;
(EngineCore_DP0 pid=304951) 	.loc	1 211 53                        // quant_slide_tuned_Llama3.2-1B.py:211:53
(EngineCore_DP0 pid=304951) 	setp.lt.s32 	%p14, %r88, %r17;
(EngineCore_DP0 pid=304951) 	.loc	1 211 37                        // quant_slide_tuned_Llama3.2-1B.py:211:37
(EngineCore_DP0 pid=304951) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=304951) 	.loc	1 210 29                        // quant_slide_tuned_Llama3.2-1B.py:210:29
(EngineCore_DP0 pid=304951) 	mad.wide.s32 	%rd8, %r88, 2, %rd1;
(EngineCore_DP0 pid=304951) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=304951) 	.loc	1 210 21                        // quant_slide_tuned_Llama3.2-1B.py:210:21
(EngineCore_DP0 pid=304951) 	// begin inline asm
(EngineCore_DP0 pid=304951) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=304951) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=304951) 	// end inline asm
(EngineCore_DP0 pid=304951) 	.loc	1 211 79                        // quant_slide_tuned_Llama3.2-1B.py:211:79
(EngineCore_DP0 pid=304951) 	cvt.f32.bf16 	%r89, %rs12;
(EngineCore_DP0 pid=304951) 	.loc	1 213 48                        // quant_slide_tuned_Llama3.2-1B.py:213:48
(EngineCore_DP0 pid=304951) 	or.b32 	%r90, %r88, 1;
(EngineCore_DP0 pid=304951) 	.loc	1 213 53                        // quant_slide_tuned_Llama3.2-1B.py:213:53
(EngineCore_DP0 pid=304951) 	setp.lt.s32 	%p15, %r90, %r17;
(EngineCore_DP0 pid=304951) 	.loc	1 213 37                        // quant_slide_tuned_Llama3.2-1B.py:213:37
(EngineCore_DP0 pid=304951) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=304951) 	.loc	1 212 39                        // quant_slide_tuned_Llama3.2-1B.py:212:39
(EngineCore_DP0 pid=304951) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=304951) 	.loc	1 212 21                        // quant_slide_tuned_Llama3.2-1B.py:212:21
(EngineCore_DP0 pid=304951) 	// begin inline asm
(EngineCore_DP0 pid=304951) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=304951) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=304951) 	// end inline asm
(EngineCore_DP0 pid=304951) 	.loc	1 213 79                        // quant_slide_tuned_Llama3.2-1B.py:213:79
(EngineCore_DP0 pid=304951) 	cvt.f32.bf16 	%r91, %rs14;
(EngineCore_DP0 pid=304951) 	.loc	1 215 48                        // quant_slide_tuned_Llama3.2-1B.py:215:48
(EngineCore_DP0 pid=304951) 	add.s32 	%r92, %r88, 2;
(EngineCore_DP0 pid=304951) 	.loc	1 215 53                        // quant_slide_tuned_Llama3.2-1B.py:215:53
(EngineCore_DP0 pid=304951) 	setp.lt.s32 	%p16, %r92, %r17;
(EngineCore_DP0 pid=304951) 	.loc	1 215 37                        // quant_slide_tuned_Llama3.2-1B.py:215:37
(EngineCore_DP0 pid=304951) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=304951) 	.loc	1 214 39                        // quant_slide_tuned_Llama3.2-1B.py:214:39
(EngineCore_DP0 pid=304951) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=304951) 	.loc	1 214 21                        // quant_slide_tuned_Llama3.2-1B.py:214:21
(EngineCore_DP0 pid=304951) 	// begin inline asm
(EngineCore_DP0 pid=304951) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=304951) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=304951) 	// end inline asm
(EngineCore_DP0 pid=304951) 	.loc	1 215 79                        // quant_slide_tuned_Llama3.2-1B.py:215:79
(EngineCore_DP0 pid=304951) 	cvt.f32.bf16 	%r93, %rs16;
(EngineCore_DP0 pid=304951) 	.loc	1 217 48                        // quant_slide_tuned_Llama3.2-1B.py:217:48
(EngineCore_DP0 pid=304951) 	add.s32 	%r94, %r88, 3;
(EngineCore_DP0 pid=304951) 	.loc	1 217 53                        // quant_slide_tuned_Llama3.2-1B.py:217:53
(EngineCore_DP0 pid=304951) 	setp.lt.s32 	%p17, %r94, %r17;
(EngineCore_DP0 pid=304951) 	.loc	1 217 37                        // quant_slide_tuned_Llama3.2-1B.py:217:37
(EngineCore_DP0 pid=304951) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=304951) 	.loc	1 216 39                        // quant_slide_tuned_Llama3.2-1B.py:216:39
(EngineCore_DP0 pid=304951) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=304951) 	.loc	1 216 21                        // quant_slide_tuned_Llama3.2-1B.py:216:21
(EngineCore_DP0 pid=304951) 	// begin inline asm
(EngineCore_DP0 pid=304951) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=304951) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=304951) 	// end inline asm
(EngineCore_DP0 pid=304951) 	.loc	1 217 79                        // quant_slide_tuned_Llama3.2-1B.py:217:79
(EngineCore_DP0 pid=304951) 	cvt.f32.bf16 	%r95, %rs18;
(EngineCore_DP0 pid=304951) 	.loc	1 219 27                        // quant_slide_tuned_Llama3.2-1B.py:219:27
(EngineCore_DP0 pid=304951) 	mul.f32 	%r96, %r13, %r89;
(EngineCore_DP0 pid=304951) 	mov.b32 	%r97, 0f43E00000;
(EngineCore_DP0 pid=304951) 	.loc	1 219 48                        // quant_slide_tuned_Llama3.2-1B.py:219:48
(EngineCore_DP0 pid=304951) 	min.xorsign.abs.f32 	%r70, %r96, %r97;
(EngineCore_DP0 pid=304951) 	.loc	1 219 60                        // quant_slide_tuned_Llama3.2-1B.py:219:60
(EngineCore_DP0 pid=304951) 	// begin inline asm
(EngineCore_DP0 pid=304951) 	cvt.rn.satfinite.e4m3x2.f32  %rs20, %r71, %r70; 
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) 	// end inline asm
(EngineCore_DP0 pid=304951) 	.loc	1 220 27                        // quant_slide_tuned_Llama3.2-1B.py:220:27
(EngineCore_DP0 pid=304951) 	mul.f32 	%r98, %r13, %r91;
(EngineCore_DP0 pid=304951) 	.loc	1 220 48                        // quant_slide_tuned_Llama3.2-1B.py:220:48
(EngineCore_DP0 pid=304951) 	min.xorsign.abs.f32 	%r72, %r98, %r97;
(EngineCore_DP0 pid=304951) 	.loc	1 220 60                        // quant_slide_tuned_Llama3.2-1B.py:220:60
(EngineCore_DP0 pid=304951) 	// begin inline asm
(EngineCore_DP0 pid=304951) 	cvt.rn.satfinite.e4m3x2.f32  %rs21, %r73, %r72; 
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) 	// end inline asm
(EngineCore_DP0 pid=304951) 	.loc	1 221 27                        // quant_slide_tuned_Llama3.2-1B.py:221:27
(EngineCore_DP0 pid=304951) 	mul.f32 	%r99, %r13, %r93;
(EngineCore_DP0 pid=304951) 	.loc	1 221 48                        // quant_slide_tuned_Llama3.2-1B.py:221:48
(EngineCore_DP0 pid=304951) 	min.xorsign.abs.f32 	%r74, %r99, %r97;
(EngineCore_DP0 pid=304951) 	.loc	1 221 60                        // quant_slide_tuned_Llama3.2-1B.py:221:60
(EngineCore_DP0 pid=304951) 	// begin inline asm
(EngineCore_DP0 pid=304951) 	cvt.rn.satfinite.e4m3x2.f32  %rs22, %r75, %r74; 
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) 	// end inline asm
(EngineCore_DP0 pid=304951) 	.loc	1 222 27                        // quant_slide_tuned_Llama3.2-1B.py:222:27
(EngineCore_DP0 pid=304951) 	mul.f32 	%r100, %r13, %r95;
(EngineCore_DP0 pid=304951) 	.loc	1 222 48                        // quant_slide_tuned_Llama3.2-1B.py:222:48
(EngineCore_DP0 pid=304951) 	min.xorsign.abs.f32 	%r76, %r100, %r97;
(EngineCore_DP0 pid=304951) 	.loc	1 222 60                        // quant_slide_tuned_Llama3.2-1B.py:222:60
(EngineCore_DP0 pid=304951) 	// begin inline asm
(EngineCore_DP0 pid=304951) 	cvt.rn.satfinite.e4m3x2.f32  %rs23, %r77, %r76; 
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) 	// end inline asm
(EngineCore_DP0 pid=304951) 	.loc	1 224 45                        // quant_slide_tuned_Llama3.2-1B.py:224:45
(EngineCore_DP0 pid=304951) 	cvt.u32.u16 	%r101, %rs20;
(EngineCore_DP0 pid=304951) 	and.b32 	%r102, %r101, 255;
(EngineCore_DP0 pid=304951) 	.loc	1 226 45                        // quant_slide_tuned_Llama3.2-1B.py:226:45
(EngineCore_DP0 pid=304951) 	cvt.u32.u16 	%r103, %rs22;
(EngineCore_DP0 pid=304951) 	and.b32 	%r104, %r103, 255;
(EngineCore_DP0 pid=304951) 	.loc	1 227 45                        // quant_slide_tuned_Llama3.2-1B.py:227:45
(EngineCore_DP0 pid=304951) 	cvt.u32.u16 	%r105, %rs23;
(EngineCore_DP0 pid=304951) 	.loc	1 229 30                        // quant_slide_tuned_Llama3.2-1B.py:229:30
(EngineCore_DP0 pid=304951) 	and.b16 	%rs24, %rs21, 255;
(EngineCore_DP0 pid=304951) 	mul.wide.u16 	%r106, %rs24, 256;
(EngineCore_DP0 pid=304951) 	.loc	1 229 24                        // quant_slide_tuned_Llama3.2-1B.py:229:24
(EngineCore_DP0 pid=304951) 	or.b32 	%r107, %r106, %r102;
(EngineCore_DP0 pid=304951) 	.loc	1 229 42                        // quant_slide_tuned_Llama3.2-1B.py:229:42
(EngineCore_DP0 pid=304951) 	shl.b32 	%r108, %r104, 16;
(EngineCore_DP0 pid=304951) 	.loc	1 229 36                        // quant_slide_tuned_Llama3.2-1B.py:229:36
(EngineCore_DP0 pid=304951) 	or.b32 	%r109, %r107, %r108;
(EngineCore_DP0 pid=304951) 	.loc	1 229 55                        // quant_slide_tuned_Llama3.2-1B.py:229:55
(EngineCore_DP0 pid=304951) 	shl.b32 	%r110, %r105, 24;
(EngineCore_DP0 pid=304951) 	.loc	1 229 49                        // quant_slide_tuned_Llama3.2-1B.py:229:49
(EngineCore_DP0 pid=304951) 	or.b32 	%r78, %r109, %r110;
(EngineCore_DP0 pid=304951) 	.loc	1 230 29                        // quant_slide_tuned_Llama3.2-1B.py:230:29
(EngineCore_DP0 pid=304951) 	mad.wide.s32 	%rd12, %r80, 4, %rd2;
(EngineCore_DP0 pid=304951) 	.loc	1 230 39                        // quant_slide_tuned_Llama3.2-1B.py:230:39
(EngineCore_DP0 pid=304951) 	// begin inline asm
(EngineCore_DP0 pid=304951) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r78 };
(EngineCore_DP0 pid=304951) 	// end inline asm
(EngineCore_DP0 pid=304951) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=304951) 	add.s32 	%r114, %r114, 1024;
(EngineCore_DP0 pid=304951) 	setp.lt.s32 	%p18, %r114, %r14;
(EngineCore_DP0 pid=304951) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=304951) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=304951) 	.loc	1 201 4                         // quant_slide_tuned_Llama3.2-1B.py:201:4
(EngineCore_DP0 pid=304951) 	ret;
(EngineCore_DP0 pid=304951) $L__tmp3:
(EngineCore_DP0 pid=304951) $L__func_end0:
(EngineCore_DP0 pid=304951)                                         // -- End function
(EngineCore_DP0 pid=304951) }
(EngineCore_DP0 pid=304951) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=304951) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=304951) 	.section	.debug_abbrev
(EngineCore_DP0 pid=304951) 	{
(EngineCore_DP0 pid=304951) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=304951) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=304951) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=304951) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=304951) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=304951) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=304951) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=304951) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=304951) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=304951) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=304951) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=304951) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=304951) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=304951) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=304951) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=304951) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=304951) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=304951) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=304951) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=304951) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=304951) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=304951) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=304951) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=304951) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=304951) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=304951) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=304951) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=304951) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=304951) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=304951) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=304951) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=304951) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=304951) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=304951) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=304951) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=304951) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=304951) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=304951) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=304951) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=304951) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=304951) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=304951) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=304951) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=304951) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=304951) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=304951) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=304951) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=304951) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=304951) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=304951) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=304951) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=304951) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=304951) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=304951) 	}
(EngineCore_DP0 pid=304951) 	.section	.debug_info
(EngineCore_DP0 pid=304951) 	{
(EngineCore_DP0 pid=304951) .b32 222                                // Length of Unit
(EngineCore_DP0 pid=304951) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=304951) .b8 0
(EngineCore_DP0 pid=304951) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=304951) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=304951) .b8 1                                   // Abbrev [1] 0xb:0xd7 DW_TAG_compile_unit
(EngineCore_DP0 pid=304951) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=304951) .b8 114
(EngineCore_DP0 pid=304951) .b8 105
(EngineCore_DP0 pid=304951) .b8 116
(EngineCore_DP0 pid=304951) .b8 111
(EngineCore_DP0 pid=304951) .b8 110
(EngineCore_DP0 pid=304951) .b8 0
(EngineCore_DP0 pid=304951) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=304951) .b8 0
(EngineCore_DP0 pid=304951) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=304951) .b8 117
(EngineCore_DP0 pid=304951) .b8 97
(EngineCore_DP0 pid=304951) .b8 110
(EngineCore_DP0 pid=304951) .b8 116
(EngineCore_DP0 pid=304951) .b8 95
(EngineCore_DP0 pid=304951) .b8 115
(EngineCore_DP0 pid=304951) .b8 108
(EngineCore_DP0 pid=304951) .b8 105
(EngineCore_DP0 pid=304951) .b8 100
(EngineCore_DP0 pid=304951) .b8 101
(EngineCore_DP0 pid=304951) .b8 95
(EngineCore_DP0 pid=304951) .b8 116
(EngineCore_DP0 pid=304951) .b8 117
(EngineCore_DP0 pid=304951) .b8 110
(EngineCore_DP0 pid=304951) .b8 101
(EngineCore_DP0 pid=304951) .b8 100
(EngineCore_DP0 pid=304951) .b8 95
(EngineCore_DP0 pid=304951) .b8 76
(EngineCore_DP0 pid=304951) .b8 108
(EngineCore_DP0 pid=304951) .b8 97
(EngineCore_DP0 pid=304951) .b8 109
(EngineCore_DP0 pid=304951) .b8 97
(EngineCore_DP0 pid=304951) .b8 51
(EngineCore_DP0 pid=304951) .b8 46
(EngineCore_DP0 pid=304951) .b8 50
(EngineCore_DP0 pid=304951) .b8 45
(EngineCore_DP0 pid=304951) .b8 49
(EngineCore_DP0 pid=304951) .b8 66
(EngineCore_DP0 pid=304951) .b8 46
(EngineCore_DP0 pid=304951) .b8 112
(EngineCore_DP0 pid=304951) .b8 121
(EngineCore_DP0 pid=304951) .b8 0
(EngineCore_DP0 pid=304951) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=304951) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=304951) .b8 114
(EngineCore_DP0 pid=304951) .b8 111
(EngineCore_DP0 pid=304951) .b8 111
(EngineCore_DP0 pid=304951) .b8 116
(EngineCore_DP0 pid=304951) .b8 47
(EngineCore_DP0 pid=304951) .b8 118
(EngineCore_DP0 pid=304951) .b8 108
(EngineCore_DP0 pid=304951) .b8 108
(EngineCore_DP0 pid=304951) .b8 109
(EngineCore_DP0 pid=304951) .b8 98
(EngineCore_DP0 pid=304951) .b8 101
(EngineCore_DP0 pid=304951) .b8 110
(EngineCore_DP0 pid=304951) .b8 99
(EngineCore_DP0 pid=304951) .b8 104
(EngineCore_DP0 pid=304951) .b8 47
(EngineCore_DP0 pid=304951) .b8 115
(EngineCore_DP0 pid=304951) .b8 108
(EngineCore_DP0 pid=304951) .b8 105
(EngineCore_DP0 pid=304951) .b8 100
(EngineCore_DP0 pid=304951) .b8 101
(EngineCore_DP0 pid=304951) .b8 115
(EngineCore_DP0 pid=304951) .b8 112
(EngineCore_DP0 pid=304951) .b8 97
(EngineCore_DP0 pid=304951) .b8 114
(EngineCore_DP0 pid=304951) .b8 115
(EngineCore_DP0 pid=304951) .b8 101
(EngineCore_DP0 pid=304951) .b8 47
(EngineCore_DP0 pid=304951) .b8 99
(EngineCore_DP0 pid=304951) .b8 115
(EngineCore_DP0 pid=304951) .b8 114
(EngineCore_DP0 pid=304951) .b8 99
(EngineCore_DP0 pid=304951) .b8 47
(EngineCore_DP0 pid=304951) .b8 102
(EngineCore_DP0 pid=304951) .b8 117
(EngineCore_DP0 pid=304951) .b8 115
(EngineCore_DP0 pid=304951) .b8 101
(EngineCore_DP0 pid=304951) .b8 100
(EngineCore_DP0 pid=304951) .b8 95
(EngineCore_DP0 pid=304951) .b8 113
(EngineCore_DP0 pid=304951) .b8 117
(EngineCore_DP0 pid=304951) .b8 97
(EngineCore_DP0 pid=304951) .b8 110
(EngineCore_DP0 pid=304951) .b8 116
(EngineCore_DP0 pid=304951) .b8 95
(EngineCore_DP0 pid=304951) .b8 115
(EngineCore_DP0 pid=304951) .b8 108
(EngineCore_DP0 pid=304951) .b8 105
(EngineCore_DP0 pid=304951) .b8 100
(EngineCore_DP0 pid=304951) .b8 101
(EngineCore_DP0 pid=304951) .b8 95
(EngineCore_DP0 pid=304951) .b8 116
(EngineCore_DP0 pid=304951) .b8 114
(EngineCore_DP0 pid=304951) .b8 105
(EngineCore_DP0 pid=304951) .b8 116
(EngineCore_DP0 pid=304951) .b8 111
(EngineCore_DP0 pid=304951) .b8 110
(EngineCore_DP0 pid=304951) .b8 47
(EngineCore_DP0 pid=304951) .b8 98
(EngineCore_DP0 pid=304951) .b8 117
(EngineCore_DP0 pid=304951) .b8 105
(EngineCore_DP0 pid=304951) .b8 108
(EngineCore_DP0 pid=304951) .b8 100
(EngineCore_DP0 pid=304951) .b8 47
(EngineCore_DP0 pid=304951) .b8 71
(EngineCore_DP0 pid=304951) .b8 66
(EngineCore_DP0 pid=304951) .b8 49
(EngineCore_DP0 pid=304951) .b8 48
(EngineCore_DP0 pid=304951) .b8 95
(EngineCore_DP0 pid=304951) .b8 99
(EngineCore_DP0 pid=304951) .b8 99
(EngineCore_DP0 pid=304951) .b8 49
(EngineCore_DP0 pid=304951) .b8 50
(EngineCore_DP0 pid=304951) .b8 49
(EngineCore_DP0 pid=304951) .b8 95
(EngineCore_DP0 pid=304951) .b8 112
(EngineCore_DP0 pid=304951) .b8 121
(EngineCore_DP0 pid=304951) .b8 51
(EngineCore_DP0 pid=304951) .b8 49
(EngineCore_DP0 pid=304951) .b8 50
(EngineCore_DP0 pid=304951) .b8 95
(EngineCore_DP0 pid=304951) .b8 99
(EngineCore_DP0 pid=304951) .b8 117
(EngineCore_DP0 pid=304951) .b8 49
(EngineCore_DP0 pid=304951) .b8 50
(EngineCore_DP0 pid=304951) .b8 57
(EngineCore_DP0 pid=304951) .b8 95
(EngineCore_DP0 pid=304951) .b8 97
(EngineCore_DP0 pid=304951) .b8 97
(EngineCore_DP0 pid=304951) .b8 114
(EngineCore_DP0 pid=304951) .b8 99
(EngineCore_DP0 pid=304951) .b8 104
(EngineCore_DP0 pid=304951) .b8 54
(EngineCore_DP0 pid=304951) .b8 52
(EngineCore_DP0 pid=304951) .b8 0
(EngineCore_DP0 pid=304951) .b8 2                                   // Abbrev [2] 0x99:0x1a DW_TAG_subprogram
(EngineCore_DP0 pid=304951) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=304951) .b8 113
(EngineCore_DP0 pid=304951) .b8 117
(EngineCore_DP0 pid=304951) .b8 97
(EngineCore_DP0 pid=304951) .b8 110
(EngineCore_DP0 pid=304951) .b8 116
(EngineCore_DP0 pid=304951) .b8 95
(EngineCore_DP0 pid=304951) .b8 115
(EngineCore_DP0 pid=304951) .b8 108
(EngineCore_DP0 pid=304951) .b8 105
(EngineCore_DP0 pid=304951) .b8 100
(EngineCore_DP0 pid=304951) .b8 101
(EngineCore_DP0 pid=304951) .b8 95
(EngineCore_DP0 pid=304951) .b8 102
(EngineCore_DP0 pid=304951) .b8 112
(EngineCore_DP0 pid=304951) .b8 56
(EngineCore_DP0 pid=304951) .b8 95
(EngineCore_DP0 pid=304951) .b8 107
(EngineCore_DP0 pid=304951) .b8 101
(EngineCore_DP0 pid=304951) .b8 114
(EngineCore_DP0 pid=304951) .b8 110
(EngineCore_DP0 pid=304951) .b8 101
(EngineCore_DP0 pid=304951) .b8 108
(EngineCore_DP0 pid=304951) .b8 0
(EngineCore_DP0 pid=304951) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=304951) .b8 3                                   // Abbrev [3] 0xb3:0x2e DW_TAG_subprogram
(EngineCore_DP0 pid=304951) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=304951) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=304951) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=304951) .b8 4                                   // Abbrev [4] 0xc8:0x18 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=304951) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=304951) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=304951) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=304951) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=304951) .b8 191                                 // DW_AT_call_line
(EngineCore_DP0 pid=304951) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=304951) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=304951) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=304951) 	}
(EngineCore_DP0 pid=304951) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) ================================================================
(EngineCore_DP0 pid=304951) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmp1ob1fkpx.ptx', '-o', '/tmp/tmp1ob1fkpx.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866] 
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866] 
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866] 
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp1ob1fkpx.ptx -o /tmp/tmp1ob1fkpx.ptx.o
(EngineCore_DP0 pid=304951) ERROR 01-25 18:51:25 [core.py:866] 

STDERR:
[2026-01-25 18:51:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:51:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:51:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:51:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:51:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:51:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:51:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:51:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:51:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:51:12] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:51:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:51:12] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:51:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:51:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:51:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:51:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:51:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:51:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=304951) [2026-01-25 18:51:13] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=304951) [2026-01-25 18:51:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=304951) [2026-01-25 18:51:13] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=304951) [2026-01-25 18:51:13] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=304951) [2026-01-25 18:51:13] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=304951) [2026-01-25 18:51:13] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=304951) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=304951) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.12s/it]
(EngineCore_DP0 pid=304951) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.12s/it]
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) [2026-01-25 18:51:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=304951) [2026-01-25 18:51:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=304951) [2026-01-25 18:51:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=304951) [2026-01-25 18:51:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=304951) [2026-01-25 18:51:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=304951) [2026-01-25 18:51:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=304951) [2026-01-25 18:51:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=304951) [2026-01-25 18:51:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=304951) Process EngineCore_DP0:
(EngineCore_DP0 pid=304951) Traceback (most recent call last):
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=304951)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=304951)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=304951)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=304951) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmp1ob1fkpx.ptx', '-o', '/tmp/tmp1ob1fkpx.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) Traceback (most recent call last):
(EngineCore_DP0 pid=304951)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=304951)     self.run()
(EngineCore_DP0 pid=304951)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=304951)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=304951)     raise e
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=304951)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=304951)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=304951)     super().__init__(
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=304951)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=304951)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=304951)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=304951)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=304951)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=304951)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=304951)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=304951)     return func(*args, **kwargs)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=304951)     return func(*args, **kwargs)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=304951)     self.model_runner.profile_run()
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=304951)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=304951)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=304951)     return func(*args, **kwargs)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=304951)     outputs = self.model(
(EngineCore_DP0 pid=304951)               ^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=304951)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=304951)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=304951)     model_output = self.model(
(EngineCore_DP0 pid=304951)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=304951)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=304951)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=304951)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=304951)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=304951)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=304951)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=304951)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=304951)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=304951)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=304951)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=304951)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=304951)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=304951)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=304951)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=304951)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=304951)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=304951)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=304951)     return self._linear_fn(
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=304951)     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=304951)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=304951)     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=304951)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=304951)     return fn(input, L)
(EngineCore_DP0 pid=304951)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=304951)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=304951)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=304951)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=304951)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=304951)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=304951)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=304951)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=304951)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=304951)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=304951)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=304951)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=304951)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=304951)     raise PTXASError(error)
(EngineCore_DP0 pid=304951) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=304951) `ptxas` stderr:
(EngineCore_DP0 pid=304951) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=304951) 
(EngineCore_DP0 pid=304951) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp1ob1fkpx.ptx -o /tmp/tmp1ob1fkpx.ptx.o
(EngineCore_DP0 pid=304951) 
[rank0]:[W125 18:51:25.036332485 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=128 ==========
Time: 2026-01-25 18:51:27
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:51:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:51:31 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=305456) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) ================================================================
(EngineCore_DP0 pid=305456) Internal Triton PTX codegen error
(EngineCore_DP0 pid=305456) `ptxas` stderr:
(EngineCore_DP0 pid=305456) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpsfqvi4sr.ptx -o /tmp/tmpsfqvi4sr.ptx.o
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) //
(EngineCore_DP0 pid=305456) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=305456) //
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) .version 8.7
(EngineCore_DP0 pid=305456) .target sm_121a
(EngineCore_DP0 pid=305456) .address_size 64
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) 	// .globl	_quant_slide_fp8_kernel // -- Begin function _quant_slide_fp8_kernel
(EngineCore_DP0 pid=305456) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=305456)                                         // @_quant_slide_fp8_kernel
(EngineCore_DP0 pid=305456) .visible .entry _quant_slide_fp8_kernel(
(EngineCore_DP0 pid=305456) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_0,
(EngineCore_DP0 pid=305456) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_1,
(EngineCore_DP0 pid=305456) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_2,
(EngineCore_DP0 pid=305456) 	.param .u32 _quant_slide_fp8_kernel_param_3,
(EngineCore_DP0 pid=305456) 	.param .u32 _quant_slide_fp8_kernel_param_4,
(EngineCore_DP0 pid=305456) 	.param .u32 _quant_slide_fp8_kernel_param_5,
(EngineCore_DP0 pid=305456) 	.param .u32 _quant_slide_fp8_kernel_param_6,
(EngineCore_DP0 pid=305456) 	.param .u32 _quant_slide_fp8_kernel_param_7,
(EngineCore_DP0 pid=305456) 	.param .u32 _quant_slide_fp8_kernel_param_8,
(EngineCore_DP0 pid=305456) 	.param .u32 _quant_slide_fp8_kernel_param_9,
(EngineCore_DP0 pid=305456) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_10,
(EngineCore_DP0 pid=305456) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_11
(EngineCore_DP0 pid=305456) )
(EngineCore_DP0 pid=305456) .reqntid 1024
(EngineCore_DP0 pid=305456) {
(EngineCore_DP0 pid=305456) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=305456) 	.reg .b16 	%rs<25>;
(EngineCore_DP0 pid=305456) 	.reg .b32 	%r<115>;
(EngineCore_DP0 pid=305456) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=305456) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=305456) $L__func_begin0:
(EngineCore_DP0 pid=305456) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) // %bb.0:
(EngineCore_DP0 pid=305456) 	ld.param.b32 	%r19, [_quant_slide_fp8_kernel_param_7];
(EngineCore_DP0 pid=305456) 	ld.param.b32 	%r18, [_quant_slide_fp8_kernel_param_5];
(EngineCore_DP0 pid=305456) 	ld.param.b32 	%r17, [_quant_slide_fp8_kernel_param_4];
(EngineCore_DP0 pid=305456) 	ld.param.b64 	%rd3, [_quant_slide_fp8_kernel_param_2];
(EngineCore_DP0 pid=305456) 	ld.param.b64 	%rd4, [_quant_slide_fp8_kernel_param_0];
(EngineCore_DP0 pid=305456) $L__tmp0:
(EngineCore_DP0 pid=305456) 	.loc	1 177 24                        // quant_slide_tuned_Llama3.2-1B.py:177:24
(EngineCore_DP0 pid=305456) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=305456) 	ld.param.b32 	%r21, [_quant_slide_fp8_kernel_param_8];
(EngineCore_DP0 pid=305456) 	.loc	1 182 26                        // quant_slide_tuned_Llama3.2-1B.py:182:26
(EngineCore_DP0 pid=305456) 	mul.lo.s32 	%r22, %r21, %r1;
(EngineCore_DP0 pid=305456) 	.loc	1 182 20                        // quant_slide_tuned_Llama3.2-1B.py:182:20
(EngineCore_DP0 pid=305456) 	mad.wide.s32 	%rd1, %r22, 2, %rd4;
(EngineCore_DP0 pid=305456) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=305456) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=305456) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=305456) 	setp.lt.s32 	%p1, %r18, 1;
(EngineCore_DP0 pid=305456) 	mov.b32 	%r113, 0f2B8CBCCC;
(EngineCore_DP0 pid=305456) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=305456) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=305456) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=305456) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=305456) 	shl.b32 	%r3, %r2, 2;
(EngineCore_DP0 pid=305456) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=305456) 	shr.u32 	%r31, %r2, 3;
(EngineCore_DP0 pid=305456) 	and.b32 	%r32, %r31, 124;
(EngineCore_DP0 pid=305456) 	mov.b32 	%r33, global_smem;
(EngineCore_DP0 pid=305456) 	add.s32 	%r39, %r33, %r32;
(EngineCore_DP0 pid=305456) 	add.s32 	%r42, %r33, %r3;
(EngineCore_DP0 pid=305456) 	mov.b32 	%r37, 0;
(EngineCore_DP0 pid=305456) 	mov.b32 	%r111, 0f00000000;
(EngineCore_DP0 pid=305456) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=305456) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=305456) 	mov.b32 	%r112, %r37;
(EngineCore_DP0 pid=305456) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=305456) 	.loc	1 189 22                        // quant_slide_tuned_Llama3.2-1B.py:189:22
(EngineCore_DP0 pid=305456) 	add.s32 	%r45, %r3, %r112;
(EngineCore_DP0 pid=305456) 	setp.lt.s32 	%p2, %r45, %r17;
(EngineCore_DP0 pid=305456) 	.loc	1 190 29                        // quant_slide_tuned_Llama3.2-1B.py:190:29
(EngineCore_DP0 pid=305456) 	mad.wide.s32 	%rd6, %r45, 2, %rd1;
(EngineCore_DP0 pid=305456) 	.loc	1 190 21                        // quant_slide_tuned_Llama3.2-1B.py:190:21
(EngineCore_DP0 pid=305456) 	// begin inline asm
(EngineCore_DP0 pid=305456) 	mov.u32 %r35, %r37;
(EngineCore_DP0 pid=305456) 	mov.u32 %r36, %r37;
(EngineCore_DP0 pid=305456) 	@%p2 ld.global.v2.b32 { %r35, %r36 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=305456) 	// end inline asm
(EngineCore_DP0 pid=305456) 	mov.b32 	{%rs1, %rs2}, %r35;
(EngineCore_DP0 pid=305456) 	mov.b32 	{%rs3, %rs4}, %r36;
(EngineCore_DP0 pid=305456) 	.loc	1 191 50                        // quant_slide_tuned_Llama3.2-1B.py:191:50
(EngineCore_DP0 pid=305456) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=305456) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=305456) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=305456) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=305456) $L__tmp1:
(EngineCore_DP0 pid=305456) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	bar.sync 	0;
(EngineCore_DP0 pid=305456) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=305456) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=305456) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=305456) 	cvt.f32.bf16 	%r46, %rs11;
(EngineCore_DP0 pid=305456) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	shfl.sync.bfly.b32 	%r47, %r46, 16, 31, -1;
(EngineCore_DP0 pid=305456) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	max.f32 	%r48, %r46, %r47;
(EngineCore_DP0 pid=305456) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	shfl.sync.bfly.b32 	%r49, %r48, 8, 31, -1;
(EngineCore_DP0 pid=305456) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	max.f32 	%r50, %r48, %r49;
(EngineCore_DP0 pid=305456) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	shfl.sync.bfly.b32 	%r51, %r50, 4, 31, -1;
(EngineCore_DP0 pid=305456) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	max.f32 	%r52, %r50, %r51;
(EngineCore_DP0 pid=305456) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	shfl.sync.bfly.b32 	%r53, %r52, 2, 31, -1;
(EngineCore_DP0 pid=305456) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	max.f32 	%r54, %r52, %r53;
(EngineCore_DP0 pid=305456) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	shfl.sync.bfly.b32 	%r55, %r54, 1, 31, -1;
(EngineCore_DP0 pid=305456) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	max.f32 	%r40, %r54, %r55;
(EngineCore_DP0 pid=305456) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	// begin inline asm
(EngineCore_DP0 pid=305456) 	@%p3 st.shared.b32 [ %r39 + 0 ], %r40;
(EngineCore_DP0 pid=305456) 	// end inline asm
(EngineCore_DP0 pid=305456) 	bar.sync 	0;
(EngineCore_DP0 pid=305456) 	// begin inline asm
(EngineCore_DP0 pid=305456) 	@%p4 ld.shared.b32 %r41, [ %r42 + 0 ];
(EngineCore_DP0 pid=305456) 	// end inline asm
(EngineCore_DP0 pid=305456) 	shfl.sync.bfly.b32 	%r56, %r41, 16, 31, -1;
(EngineCore_DP0 pid=305456) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	max.f32 	%r57, %r41, %r56;
(EngineCore_DP0 pid=305456) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	shfl.sync.bfly.b32 	%r58, %r57, 8, 31, -1;
(EngineCore_DP0 pid=305456) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	max.f32 	%r59, %r57, %r58;
(EngineCore_DP0 pid=305456) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	shfl.sync.bfly.b32 	%r60, %r59, 4, 31, -1;
(EngineCore_DP0 pid=305456) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	max.f32 	%r61, %r59, %r60;
(EngineCore_DP0 pid=305456) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	shfl.sync.bfly.b32 	%r62, %r61, 2, 31, -1;
(EngineCore_DP0 pid=305456) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	max.f32 	%r63, %r61, %r62;
(EngineCore_DP0 pid=305456) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	shfl.sync.bfly.b32 	%r64, %r63, 1, 31, -1;
(EngineCore_DP0 pid=305456) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	max.f32 	%r44, %r63, %r64;
(EngineCore_DP0 pid=305456) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305456) 	// begin inline asm
(EngineCore_DP0 pid=305456) 	@%p19 st.shared.b32 [ %r42 + 0 ], %r44;
(EngineCore_DP0 pid=305456) 	// end inline asm
(EngineCore_DP0 pid=305456) 	bar.sync 	0;
(EngineCore_DP0 pid=305456) 	ld.shared.b32 	%r65, [global_smem];
(EngineCore_DP0 pid=305456) $L__tmp2:
(EngineCore_DP0 pid=305456) 	.loc	1 191 36                        // quant_slide_tuned_Llama3.2-1B.py:191:36
(EngineCore_DP0 pid=305456) 	max.f32 	%r111, %r111, %r65;
(EngineCore_DP0 pid=305456) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=305456) 	add.s32 	%r112, %r112, 4096;
(EngineCore_DP0 pid=305456) 	setp.lt.s32 	%p6, %r112, %r18;
(EngineCore_DP0 pid=305456) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=305456) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=305456) 	.loc	1 193 32                        // quant_slide_tuned_Llama3.2-1B.py:193:32
(EngineCore_DP0 pid=305456) 	max.f32 	%r113, %r111, 0f2B8CBCCC;
(EngineCore_DP0 pid=305456) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=305456) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=305456) 	mov.b32 	%r67, 0f43E00000;
(EngineCore_DP0 pid=305456) 	.loc	1 194 32                        // quant_slide_tuned_Llama3.2-1B.py:194:32
(EngineCore_DP0 pid=305456) 	div.full.f32 	%r68, %r113, %r67;
(EngineCore_DP0 pid=305456) 	.loc	1 194 41                        // quant_slide_tuned_Llama3.2-1B.py:194:41
(EngineCore_DP0 pid=305456) 	max.f32 	%r66, %r68, 0f36924925;
(EngineCore_DP0 pid=305456) 	.loc	1 196 25                        // quant_slide_tuned_Llama3.2-1B.py:196:25
(EngineCore_DP0 pid=305456) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=305456) 	.loc	1 196 30                        // quant_slide_tuned_Llama3.2-1B.py:196:30
(EngineCore_DP0 pid=305456) 	// begin inline asm
(EngineCore_DP0 pid=305456) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r66 };
(EngineCore_DP0 pid=305456) 	// end inline asm
(EngineCore_DP0 pid=305456) 	.loc	1 199 29                        // quant_slide_tuned_Llama3.2-1B.py:199:29
(EngineCore_DP0 pid=305456) 	shl.b32 	%r14, %r19, 2;
(EngineCore_DP0 pid=305456) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=305456) 	setp.lt.s32 	%p8, %r14, 1;
(EngineCore_DP0 pid=305456) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=305456) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=305456) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=305456) 	ld.param.b32 	%r23, [_quant_slide_fp8_kernel_param_9];
(EngineCore_DP0 pid=305456) 	shr.s32 	%r24, %r23, 31;
(EngineCore_DP0 pid=305456) 	shr.u32 	%r25, %r24, 30;
(EngineCore_DP0 pid=305456) 	add.s32 	%r26, %r23, %r25;
(EngineCore_DP0 pid=305456) 	shr.s32 	%r27, %r26, 2;
(EngineCore_DP0 pid=305456) 	ld.param.b64 	%rd5, [_quant_slide_fp8_kernel_param_1];
(EngineCore_DP0 pid=305456) 	mul.lo.s32 	%r28, %r27, %r1;
(EngineCore_DP0 pid=305456) 	mad.wide.s32 	%rd2, %r28, 4, %rd5;
(EngineCore_DP0 pid=305456) 	div.full.f32 	%r13, %r67, %r113;
(EngineCore_DP0 pid=305456) 	mov.b32 	%r114, 0;
(EngineCore_DP0 pid=305456) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=305456)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=305456) 	.loc	1 203 30                        // quant_slide_tuned_Llama3.2-1B.py:203:30
(EngineCore_DP0 pid=305456) 	add.s32 	%r80, %r2, %r114;
(EngineCore_DP0 pid=305456) 	setp.lt.s32 	%p13, %r80, %r14;
(EngineCore_DP0 pid=305456) 	.loc	1 206 24                        // quant_slide_tuned_Llama3.2-1B.py:206:24
(EngineCore_DP0 pid=305456) 	shr.s32 	%r81, %r80, 31;
(EngineCore_DP0 pid=305456) 	shr.u32 	%r82, %r81, 30;
(EngineCore_DP0 pid=305456) 	add.s32 	%r83, %r80, %r82;
(EngineCore_DP0 pid=305456) 	shr.s32 	%r84, %r83, 2;
(EngineCore_DP0 pid=305456) 	.loc	1 207 23                        // quant_slide_tuned_Llama3.2-1B.py:207:23
(EngineCore_DP0 pid=305456) 	and.b32 	%r85, %r83, 2147483644;
(EngineCore_DP0 pid=305456) 	sub.s32 	%r86, %r80, %r85;
(EngineCore_DP0 pid=305456) 	.loc	1 208 30                        // quant_slide_tuned_Llama3.2-1B.py:208:30
(EngineCore_DP0 pid=305456) 	shl.b32 	%r87, %r86, 1;
(EngineCore_DP0 pid=305456) 	.loc	1 208 26                        // quant_slide_tuned_Llama3.2-1B.py:208:26
(EngineCore_DP0 pid=305456) 	mad.lo.s32 	%r88, %r84, 10, %r87;
(EngineCore_DP0 pid=305456) 	.loc	1 211 53                        // quant_slide_tuned_Llama3.2-1B.py:211:53
(EngineCore_DP0 pid=305456) 	setp.lt.s32 	%p14, %r88, %r17;
(EngineCore_DP0 pid=305456) 	.loc	1 211 37                        // quant_slide_tuned_Llama3.2-1B.py:211:37
(EngineCore_DP0 pid=305456) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=305456) 	.loc	1 210 29                        // quant_slide_tuned_Llama3.2-1B.py:210:29
(EngineCore_DP0 pid=305456) 	mad.wide.s32 	%rd8, %r88, 2, %rd1;
(EngineCore_DP0 pid=305456) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=305456) 	.loc	1 210 21                        // quant_slide_tuned_Llama3.2-1B.py:210:21
(EngineCore_DP0 pid=305456) 	// begin inline asm
(EngineCore_DP0 pid=305456) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=305456) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=305456) 	// end inline asm
(EngineCore_DP0 pid=305456) 	.loc	1 211 79                        // quant_slide_tuned_Llama3.2-1B.py:211:79
(EngineCore_DP0 pid=305456) 	cvt.f32.bf16 	%r89, %rs12;
(EngineCore_DP0 pid=305456) 	.loc	1 213 48                        // quant_slide_tuned_Llama3.2-1B.py:213:48
(EngineCore_DP0 pid=305456) 	or.b32 	%r90, %r88, 1;
(EngineCore_DP0 pid=305456) 	.loc	1 213 53                        // quant_slide_tuned_Llama3.2-1B.py:213:53
(EngineCore_DP0 pid=305456) 	setp.lt.s32 	%p15, %r90, %r17;
(EngineCore_DP0 pid=305456) 	.loc	1 213 37                        // quant_slide_tuned_Llama3.2-1B.py:213:37
(EngineCore_DP0 pid=305456) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=305456) 	.loc	1 212 39                        // quant_slide_tuned_Llama3.2-1B.py:212:39
(EngineCore_DP0 pid=305456) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=305456) 	.loc	1 212 21                        // quant_slide_tuned_Llama3.2-1B.py:212:21
(EngineCore_DP0 pid=305456) 	// begin inline asm
(EngineCore_DP0 pid=305456) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=305456) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=305456) 	// end inline asm
(EngineCore_DP0 pid=305456) 	.loc	1 213 79                        // quant_slide_tuned_Llama3.2-1B.py:213:79
(EngineCore_DP0 pid=305456) 	cvt.f32.bf16 	%r91, %rs14;
(EngineCore_DP0 pid=305456) 	.loc	1 215 48                        // quant_slide_tuned_Llama3.2-1B.py:215:48
(EngineCore_DP0 pid=305456) 	add.s32 	%r92, %r88, 2;
(EngineCore_DP0 pid=305456) 	.loc	1 215 53                        // quant_slide_tuned_Llama3.2-1B.py:215:53
(EngineCore_DP0 pid=305456) 	setp.lt.s32 	%p16, %r92, %r17;
(EngineCore_DP0 pid=305456) 	.loc	1 215 37                        // quant_slide_tuned_Llama3.2-1B.py:215:37
(EngineCore_DP0 pid=305456) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=305456) 	.loc	1 214 39                        // quant_slide_tuned_Llama3.2-1B.py:214:39
(EngineCore_DP0 pid=305456) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=305456) 	.loc	1 214 21                        // quant_slide_tuned_Llama3.2-1B.py:214:21
(EngineCore_DP0 pid=305456) 	// begin inline asm
(EngineCore_DP0 pid=305456) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=305456) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=305456) 	// end inline asm
(EngineCore_DP0 pid=305456) 	.loc	1 215 79                        // quant_slide_tuned_Llama3.2-1B.py:215:79
(EngineCore_DP0 pid=305456) 	cvt.f32.bf16 	%r93, %rs16;
(EngineCore_DP0 pid=305456) 	.loc	1 217 48                        // quant_slide_tuned_Llama3.2-1B.py:217:48
(EngineCore_DP0 pid=305456) 	add.s32 	%r94, %r88, 3;
(EngineCore_DP0 pid=305456) 	.loc	1 217 53                        // quant_slide_tuned_Llama3.2-1B.py:217:53
(EngineCore_DP0 pid=305456) 	setp.lt.s32 	%p17, %r94, %r17;
(EngineCore_DP0 pid=305456) 	.loc	1 217 37                        // quant_slide_tuned_Llama3.2-1B.py:217:37
(EngineCore_DP0 pid=305456) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=305456) 	.loc	1 216 39                        // quant_slide_tuned_Llama3.2-1B.py:216:39
(EngineCore_DP0 pid=305456) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=305456) 	.loc	1 216 21                        // quant_slide_tuned_Llama3.2-1B.py:216:21
(EngineCore_DP0 pid=305456) 	// begin inline asm
(EngineCore_DP0 pid=305456) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=305456) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=305456) 	// end inline asm
(EngineCore_DP0 pid=305456) 	.loc	1 217 79                        // quant_slide_tuned_Llama3.2-1B.py:217:79
(EngineCore_DP0 pid=305456) 	cvt.f32.bf16 	%r95, %rs18;
(EngineCore_DP0 pid=305456) 	.loc	1 219 27                        // quant_slide_tuned_Llama3.2-1B.py:219:27
(EngineCore_DP0 pid=305456) 	mul.f32 	%r96, %r13, %r89;
(EngineCore_DP0 pid=305456) 	mov.b32 	%r97, 0f43E00000;
(EngineCore_DP0 pid=305456) 	.loc	1 219 48                        // quant_slide_tuned_Llama3.2-1B.py:219:48
(EngineCore_DP0 pid=305456) 	min.xorsign.abs.f32 	%r70, %r96, %r97;
(EngineCore_DP0 pid=305456) 	.loc	1 219 60                        // quant_slide_tuned_Llama3.2-1B.py:219:60
(EngineCore_DP0 pid=305456) 	// begin inline asm
(EngineCore_DP0 pid=305456) 	cvt.rn.satfinite.e4m3x2.f32  %rs20, %r71, %r70; 
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) 	// end inline asm
(EngineCore_DP0 pid=305456) 	.loc	1 220 27                        // quant_slide_tuned_Llama3.2-1B.py:220:27
(EngineCore_DP0 pid=305456) 	mul.f32 	%r98, %r13, %r91;
(EngineCore_DP0 pid=305456) 	.loc	1 220 48                        // quant_slide_tuned_Llama3.2-1B.py:220:48
(EngineCore_DP0 pid=305456) 	min.xorsign.abs.f32 	%r72, %r98, %r97;
(EngineCore_DP0 pid=305456) 	.loc	1 220 60                        // quant_slide_tuned_Llama3.2-1B.py:220:60
(EngineCore_DP0 pid=305456) 	// begin inline asm
(EngineCore_DP0 pid=305456) 	cvt.rn.satfinite.e4m3x2.f32  %rs21, %r73, %r72; 
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) 	// end inline asm
(EngineCore_DP0 pid=305456) 	.loc	1 221 27                        // quant_slide_tuned_Llama3.2-1B.py:221:27
(EngineCore_DP0 pid=305456) 	mul.f32 	%r99, %r13, %r93;
(EngineCore_DP0 pid=305456) 	.loc	1 221 48                        // quant_slide_tuned_Llama3.2-1B.py:221:48
(EngineCore_DP0 pid=305456) 	min.xorsign.abs.f32 	%r74, %r99, %r97;
(EngineCore_DP0 pid=305456) 	.loc	1 221 60                        // quant_slide_tuned_Llama3.2-1B.py:221:60
(EngineCore_DP0 pid=305456) 	// begin inline asm
(EngineCore_DP0 pid=305456) 	cvt.rn.satfinite.e4m3x2.f32  %rs22, %r75, %r74; 
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) 	// end inline asm
(EngineCore_DP0 pid=305456) 	.loc	1 222 27                        // quant_slide_tuned_Llama3.2-1B.py:222:27
(EngineCore_DP0 pid=305456) 	mul.f32 	%r100, %r13, %r95;
(EngineCore_DP0 pid=305456) 	.loc	1 222 48                        // quant_slide_tuned_Llama3.2-1B.py:222:48
(EngineCore_DP0 pid=305456) 	min.xorsign.abs.f32 	%r76, %r100, %r97;
(EngineCore_DP0 pid=305456) 	.loc	1 222 60                        // quant_slide_tuned_Llama3.2-1B.py:222:60
(EngineCore_DP0 pid=305456) 	// begin inline asm
(EngineCore_DP0 pid=305456) 	cvt.rn.satfinite.e4m3x2.f32  %rs23, %r77, %r76; 
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) 	// end inline asm
(EngineCore_DP0 pid=305456) 	.loc	1 224 45                        // quant_slide_tuned_Llama3.2-1B.py:224:45
(EngineCore_DP0 pid=305456) 	cvt.u32.u16 	%r101, %rs20;
(EngineCore_DP0 pid=305456) 	and.b32 	%r102, %r101, 255;
(EngineCore_DP0 pid=305456) 	.loc	1 226 45                        // quant_slide_tuned_Llama3.2-1B.py:226:45
(EngineCore_DP0 pid=305456) 	cvt.u32.u16 	%r103, %rs22;
(EngineCore_DP0 pid=305456) 	and.b32 	%r104, %r103, 255;
(EngineCore_DP0 pid=305456) 	.loc	1 227 45                        // quant_slide_tuned_Llama3.2-1B.py:227:45
(EngineCore_DP0 pid=305456) 	cvt.u32.u16 	%r105, %rs23;
(EngineCore_DP0 pid=305456) 	.loc	1 229 30                        // quant_slide_tuned_Llama3.2-1B.py:229:30
(EngineCore_DP0 pid=305456) 	and.b16 	%rs24, %rs21, 255;
(EngineCore_DP0 pid=305456) 	mul.wide.u16 	%r106, %rs24, 256;
(EngineCore_DP0 pid=305456) 	.loc	1 229 24                        // quant_slide_tuned_Llama3.2-1B.py:229:24
(EngineCore_DP0 pid=305456) 	or.b32 	%r107, %r106, %r102;
(EngineCore_DP0 pid=305456) 	.loc	1 229 42                        // quant_slide_tuned_Llama3.2-1B.py:229:42
(EngineCore_DP0 pid=305456) 	shl.b32 	%r108, %r104, 16;
(EngineCore_DP0 pid=305456) 	.loc	1 229 36                        // quant_slide_tuned_Llama3.2-1B.py:229:36
(EngineCore_DP0 pid=305456) 	or.b32 	%r109, %r107, %r108;
(EngineCore_DP0 pid=305456) 	.loc	1 229 55                        // quant_slide_tuned_Llama3.2-1B.py:229:55
(EngineCore_DP0 pid=305456) 	shl.b32 	%r110, %r105, 24;
(EngineCore_DP0 pid=305456) 	.loc	1 229 49                        // quant_slide_tuned_Llama3.2-1B.py:229:49
(EngineCore_DP0 pid=305456) 	or.b32 	%r78, %r109, %r110;
(EngineCore_DP0 pid=305456) 	.loc	1 230 29                        // quant_slide_tuned_Llama3.2-1B.py:230:29
(EngineCore_DP0 pid=305456) 	mad.wide.s32 	%rd12, %r80, 4, %rd2;
(EngineCore_DP0 pid=305456) 	.loc	1 230 39                        // quant_slide_tuned_Llama3.2-1B.py:230:39
(EngineCore_DP0 pid=305456) 	// begin inline asm
(EngineCore_DP0 pid=305456) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r78 };
(EngineCore_DP0 pid=305456) 	// end inline asm
(EngineCore_DP0 pid=305456) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=305456) 	add.s32 	%r114, %r114, 1024;
(EngineCore_DP0 pid=305456) 	setp.lt.s32 	%p18, %r114, %r14;
(EngineCore_DP0 pid=305456) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=305456) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=305456) 	.loc	1 201 4                         // quant_slide_tuned_Llama3.2-1B.py:201:4
(EngineCore_DP0 pid=305456) 	ret;
(EngineCore_DP0 pid=305456) $L__tmp3:
(EngineCore_DP0 pid=305456) $L__func_end0:
(EngineCore_DP0 pid=305456)                                         // -- End function
(EngineCore_DP0 pid=305456) }
(EngineCore_DP0 pid=305456) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=305456) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=305456) 	.section	.debug_abbrev
(EngineCore_DP0 pid=305456) 	{
(EngineCore_DP0 pid=305456) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=305456) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=305456) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=305456) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=305456) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=305456) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=305456) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=305456) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=305456) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=305456) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=305456) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=305456) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=305456) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=305456) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=305456) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=305456) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=305456) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=305456) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=305456) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=305456) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=305456) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=305456) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=305456) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=305456) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=305456) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=305456) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=305456) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=305456) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=305456) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=305456) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=305456) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=305456) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=305456) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=305456) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=305456) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=305456) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=305456) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=305456) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=305456) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=305456) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=305456) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=305456) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=305456) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=305456) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=305456) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=305456) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=305456) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=305456) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=305456) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=305456) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=305456) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=305456) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=305456) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=305456) 	}
(EngineCore_DP0 pid=305456) 	.section	.debug_info
(EngineCore_DP0 pid=305456) 	{
(EngineCore_DP0 pid=305456) .b32 222                                // Length of Unit
(EngineCore_DP0 pid=305456) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=305456) .b8 0
(EngineCore_DP0 pid=305456) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=305456) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=305456) .b8 1                                   // Abbrev [1] 0xb:0xd7 DW_TAG_compile_unit
(EngineCore_DP0 pid=305456) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=305456) .b8 114
(EngineCore_DP0 pid=305456) .b8 105
(EngineCore_DP0 pid=305456) .b8 116
(EngineCore_DP0 pid=305456) .b8 111
(EngineCore_DP0 pid=305456) .b8 110
(EngineCore_DP0 pid=305456) .b8 0
(EngineCore_DP0 pid=305456) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=305456) .b8 0
(EngineCore_DP0 pid=305456) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=305456) .b8 117
(EngineCore_DP0 pid=305456) .b8 97
(EngineCore_DP0 pid=305456) .b8 110
(EngineCore_DP0 pid=305456) .b8 116
(EngineCore_DP0 pid=305456) .b8 95
(EngineCore_DP0 pid=305456) .b8 115
(EngineCore_DP0 pid=305456) .b8 108
(EngineCore_DP0 pid=305456) .b8 105
(EngineCore_DP0 pid=305456) .b8 100
(EngineCore_DP0 pid=305456) .b8 101
(EngineCore_DP0 pid=305456) .b8 95
(EngineCore_DP0 pid=305456) .b8 116
(EngineCore_DP0 pid=305456) .b8 117
(EngineCore_DP0 pid=305456) .b8 110
(EngineCore_DP0 pid=305456) .b8 101
(EngineCore_DP0 pid=305456) .b8 100
(EngineCore_DP0 pid=305456) .b8 95
(EngineCore_DP0 pid=305456) .b8 76
(EngineCore_DP0 pid=305456) .b8 108
(EngineCore_DP0 pid=305456) .b8 97
(EngineCore_DP0 pid=305456) .b8 109
(EngineCore_DP0 pid=305456) .b8 97
(EngineCore_DP0 pid=305456) .b8 51
(EngineCore_DP0 pid=305456) .b8 46
(EngineCore_DP0 pid=305456) .b8 50
(EngineCore_DP0 pid=305456) .b8 45
(EngineCore_DP0 pid=305456) .b8 49
(EngineCore_DP0 pid=305456) .b8 66
(EngineCore_DP0 pid=305456) .b8 46
(EngineCore_DP0 pid=305456) .b8 112
(EngineCore_DP0 pid=305456) .b8 121
(EngineCore_DP0 pid=305456) .b8 0
(EngineCore_DP0 pid=305456) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=305456) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=305456) .b8 114
(EngineCore_DP0 pid=305456) .b8 111
(EngineCore_DP0 pid=305456) .b8 111
(EngineCore_DP0 pid=305456) .b8 116
(EngineCore_DP0 pid=305456) .b8 47
(EngineCore_DP0 pid=305456) .b8 118
(EngineCore_DP0 pid=305456) .b8 108
(EngineCore_DP0 pid=305456) .b8 108
(EngineCore_DP0 pid=305456) .b8 109
(EngineCore_DP0 pid=305456) .b8 98
(EngineCore_DP0 pid=305456) .b8 101
(EngineCore_DP0 pid=305456) .b8 110
(EngineCore_DP0 pid=305456) .b8 99
(EngineCore_DP0 pid=305456) .b8 104
(EngineCore_DP0 pid=305456) .b8 47
(EngineCore_DP0 pid=305456) .b8 115
(EngineCore_DP0 pid=305456) .b8 108
(EngineCore_DP0 pid=305456) .b8 105
(EngineCore_DP0 pid=305456) .b8 100
(EngineCore_DP0 pid=305456) .b8 101
(EngineCore_DP0 pid=305456) .b8 115
(EngineCore_DP0 pid=305456) .b8 112
(EngineCore_DP0 pid=305456) .b8 97
(EngineCore_DP0 pid=305456) .b8 114
(EngineCore_DP0 pid=305456) .b8 115
(EngineCore_DP0 pid=305456) .b8 101
(EngineCore_DP0 pid=305456) .b8 47
(EngineCore_DP0 pid=305456) .b8 99
(EngineCore_DP0 pid=305456) .b8 115
(EngineCore_DP0 pid=305456) .b8 114
(EngineCore_DP0 pid=305456) .b8 99
(EngineCore_DP0 pid=305456) .b8 47
(EngineCore_DP0 pid=305456) .b8 102
(EngineCore_DP0 pid=305456) .b8 117
(EngineCore_DP0 pid=305456) .b8 115
(EngineCore_DP0 pid=305456) .b8 101
(EngineCore_DP0 pid=305456) .b8 100
(EngineCore_DP0 pid=305456) .b8 95
(EngineCore_DP0 pid=305456) .b8 113
(EngineCore_DP0 pid=305456) .b8 117
(EngineCore_DP0 pid=305456) .b8 97
(EngineCore_DP0 pid=305456) .b8 110
(EngineCore_DP0 pid=305456) .b8 116
(EngineCore_DP0 pid=305456) .b8 95
(EngineCore_DP0 pid=305456) .b8 115
(EngineCore_DP0 pid=305456) .b8 108
(EngineCore_DP0 pid=305456) .b8 105
(EngineCore_DP0 pid=305456) .b8 100
(EngineCore_DP0 pid=305456) .b8 101
(EngineCore_DP0 pid=305456) .b8 95
(EngineCore_DP0 pid=305456) .b8 116
(EngineCore_DP0 pid=305456) .b8 114
(EngineCore_DP0 pid=305456) .b8 105
(EngineCore_DP0 pid=305456) .b8 116
(EngineCore_DP0 pid=305456) .b8 111
(EngineCore_DP0 pid=305456) .b8 110
(EngineCore_DP0 pid=305456) .b8 47
(EngineCore_DP0 pid=305456) .b8 98
(EngineCore_DP0 pid=305456) .b8 117
(EngineCore_DP0 pid=305456) .b8 105
(EngineCore_DP0 pid=305456) .b8 108
(EngineCore_DP0 pid=305456) .b8 100
(EngineCore_DP0 pid=305456) .b8 47
(EngineCore_DP0 pid=305456) .b8 71
(EngineCore_DP0 pid=305456) .b8 66
(EngineCore_DP0 pid=305456) .b8 49
(EngineCore_DP0 pid=305456) .b8 48
(EngineCore_DP0 pid=305456) .b8 95
(EngineCore_DP0 pid=305456) .b8 99
(EngineCore_DP0 pid=305456) .b8 99
(EngineCore_DP0 pid=305456) .b8 49
(EngineCore_DP0 pid=305456) .b8 50
(EngineCore_DP0 pid=305456) .b8 49
(EngineCore_DP0 pid=305456) .b8 95
(EngineCore_DP0 pid=305456) .b8 112
(EngineCore_DP0 pid=305456) .b8 121
(EngineCore_DP0 pid=305456) .b8 51
(EngineCore_DP0 pid=305456) .b8 49
(EngineCore_DP0 pid=305456) .b8 50
(EngineCore_DP0 pid=305456) .b8 95
(EngineCore_DP0 pid=305456) .b8 99
(EngineCore_DP0 pid=305456) .b8 117
(EngineCore_DP0 pid=305456) .b8 49
(EngineCore_DP0 pid=305456) .b8 50
(EngineCore_DP0 pid=305456) .b8 57
(EngineCore_DP0 pid=305456) .b8 95
(EngineCore_DP0 pid=305456) .b8 97
(EngineCore_DP0 pid=305456) .b8 97
(EngineCore_DP0 pid=305456) .b8 114
(EngineCore_DP0 pid=305456) .b8 99
(EngineCore_DP0 pid=305456) .b8 104
(EngineCore_DP0 pid=305456) .b8 54
(EngineCore_DP0 pid=305456) .b8 52
(EngineCore_DP0 pid=305456) .b8 0
(EngineCore_DP0 pid=305456) .b8 2                                   // Abbrev [2] 0x99:0x1a DW_TAG_subprogram
(EngineCore_DP0 pid=305456) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=305456) .b8 113
(EngineCore_DP0 pid=305456) .b8 117
(EngineCore_DP0 pid=305456) .b8 97
(EngineCore_DP0 pid=305456) .b8 110
(EngineCore_DP0 pid=305456) .b8 116
(EngineCore_DP0 pid=305456) .b8 95
(EngineCore_DP0 pid=305456) .b8 115
(EngineCore_DP0 pid=305456) .b8 108
(EngineCore_DP0 pid=305456) .b8 105
(EngineCore_DP0 pid=305456) .b8 100
(EngineCore_DP0 pid=305456) .b8 101
(EngineCore_DP0 pid=305456) .b8 95
(EngineCore_DP0 pid=305456) .b8 102
(EngineCore_DP0 pid=305456) .b8 112
(EngineCore_DP0 pid=305456) .b8 56
(EngineCore_DP0 pid=305456) .b8 95
(EngineCore_DP0 pid=305456) .b8 107
(EngineCore_DP0 pid=305456) .b8 101
(EngineCore_DP0 pid=305456) .b8 114
(EngineCore_DP0 pid=305456) .b8 110
(EngineCore_DP0 pid=305456) .b8 101
(EngineCore_DP0 pid=305456) .b8 108
(EngineCore_DP0 pid=305456) .b8 0
(EngineCore_DP0 pid=305456) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=305456) .b8 3                                   // Abbrev [3] 0xb3:0x2e DW_TAG_subprogram
(EngineCore_DP0 pid=305456) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=305456) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=305456) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=305456) .b8 4                                   // Abbrev [4] 0xc8:0x18 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=305456) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=305456) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=305456) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=305456) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=305456) .b8 191                                 // DW_AT_call_line
(EngineCore_DP0 pid=305456) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=305456) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=305456) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=305456) 	}
(EngineCore_DP0 pid=305456) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) ================================================================
(EngineCore_DP0 pid=305456) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpsfqvi4sr.ptx', '-o', '/tmp/tmpsfqvi4sr.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866] 
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866] 
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866] 
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpsfqvi4sr.ptx -o /tmp/tmpsfqvi4sr.ptx.o
(EngineCore_DP0 pid=305456) ERROR 01-25 18:51:47 [core.py:866] 

STDERR:
[2026-01-25 18:51:30] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:51:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:51:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:51:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:51:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:51:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:51:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:51:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:51:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:51:34] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:51:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:51:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:51:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:51:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:51:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:51:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:51:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:51:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=305456) [2026-01-25 18:51:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=305456) [2026-01-25 18:51:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=305456) [2026-01-25 18:51:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=305456) [2026-01-25 18:51:35] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=305456) [2026-01-25 18:51:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=305456) [2026-01-25 18:51:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=305456) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=305456) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.89s/it]
(EngineCore_DP0 pid=305456) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.89s/it]
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) [2026-01-25 18:51:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=305456) [2026-01-25 18:51:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=305456) [2026-01-25 18:51:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=305456) [2026-01-25 18:51:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=305456) [2026-01-25 18:51:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=305456) [2026-01-25 18:51:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=305456) [2026-01-25 18:51:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=305456) [2026-01-25 18:51:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=305456) Process EngineCore_DP0:
(EngineCore_DP0 pid=305456) Traceback (most recent call last):
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=305456)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=305456)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=305456)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=305456) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpsfqvi4sr.ptx', '-o', '/tmp/tmpsfqvi4sr.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) Traceback (most recent call last):
(EngineCore_DP0 pid=305456)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=305456)     self.run()
(EngineCore_DP0 pid=305456)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=305456)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=305456)     raise e
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=305456)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=305456)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=305456)     super().__init__(
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=305456)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=305456)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=305456)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=305456)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=305456)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=305456)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=305456)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=305456)     return func(*args, **kwargs)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=305456)     return func(*args, **kwargs)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=305456)     self.model_runner.profile_run()
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=305456)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=305456)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=305456)     return func(*args, **kwargs)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=305456)     outputs = self.model(
(EngineCore_DP0 pid=305456)               ^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305456)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305456)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=305456)     model_output = self.model(
(EngineCore_DP0 pid=305456)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=305456)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=305456)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=305456)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305456)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305456)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=305456)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=305456)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305456)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305456)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=305456)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=305456)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305456)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305456)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=305456)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=305456)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=305456)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=305456)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=305456)     return self._linear_fn(
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=305456)     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=305456)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=305456)     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=305456)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=305456)     return fn(input, L)
(EngineCore_DP0 pid=305456)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=305456)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=305456)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=305456)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=305456)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=305456)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=305456)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=305456)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=305456)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=305456)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=305456)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=305456)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305456)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=305456)     raise PTXASError(error)
(EngineCore_DP0 pid=305456) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=305456) `ptxas` stderr:
(EngineCore_DP0 pid=305456) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=305456) 
(EngineCore_DP0 pid=305456) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpsfqvi4sr.ptx -o /tmp/tmpsfqvi4sr.ptx.o
(EngineCore_DP0 pid=305456) 
[rank0]:[W125 18:51:47.093495371 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=128

========== M=256 ==========
Time: 2026-01-25 18:51:49
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:51:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:51:53 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=305949) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) ================================================================
(EngineCore_DP0 pid=305949) Internal Triton PTX codegen error
(EngineCore_DP0 pid=305949) `ptxas` stderr:
(EngineCore_DP0 pid=305949) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpemzzerg4.ptx -o /tmp/tmpemzzerg4.ptx.o
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) //
(EngineCore_DP0 pid=305949) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=305949) //
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) .version 8.7
(EngineCore_DP0 pid=305949) .target sm_121a
(EngineCore_DP0 pid=305949) .address_size 64
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) 	// .globl	_quant_slide_fp8_kernel // -- Begin function _quant_slide_fp8_kernel
(EngineCore_DP0 pid=305949) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=305949)                                         // @_quant_slide_fp8_kernel
(EngineCore_DP0 pid=305949) .visible .entry _quant_slide_fp8_kernel(
(EngineCore_DP0 pid=305949) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_0,
(EngineCore_DP0 pid=305949) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_1,
(EngineCore_DP0 pid=305949) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_2,
(EngineCore_DP0 pid=305949) 	.param .u32 _quant_slide_fp8_kernel_param_3,
(EngineCore_DP0 pid=305949) 	.param .u32 _quant_slide_fp8_kernel_param_4,
(EngineCore_DP0 pid=305949) 	.param .u32 _quant_slide_fp8_kernel_param_5,
(EngineCore_DP0 pid=305949) 	.param .u32 _quant_slide_fp8_kernel_param_6,
(EngineCore_DP0 pid=305949) 	.param .u32 _quant_slide_fp8_kernel_param_7,
(EngineCore_DP0 pid=305949) 	.param .u32 _quant_slide_fp8_kernel_param_8,
(EngineCore_DP0 pid=305949) 	.param .u32 _quant_slide_fp8_kernel_param_9,
(EngineCore_DP0 pid=305949) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_10,
(EngineCore_DP0 pid=305949) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_11
(EngineCore_DP0 pid=305949) )
(EngineCore_DP0 pid=305949) .reqntid 1024
(EngineCore_DP0 pid=305949) {
(EngineCore_DP0 pid=305949) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=305949) 	.reg .b16 	%rs<25>;
(EngineCore_DP0 pid=305949) 	.reg .b32 	%r<115>;
(EngineCore_DP0 pid=305949) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=305949) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=305949) $L__func_begin0:
(EngineCore_DP0 pid=305949) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) // %bb.0:
(EngineCore_DP0 pid=305949) 	ld.param.b32 	%r19, [_quant_slide_fp8_kernel_param_7];
(EngineCore_DP0 pid=305949) 	ld.param.b32 	%r18, [_quant_slide_fp8_kernel_param_5];
(EngineCore_DP0 pid=305949) 	ld.param.b32 	%r17, [_quant_slide_fp8_kernel_param_4];
(EngineCore_DP0 pid=305949) 	ld.param.b64 	%rd3, [_quant_slide_fp8_kernel_param_2];
(EngineCore_DP0 pid=305949) 	ld.param.b64 	%rd4, [_quant_slide_fp8_kernel_param_0];
(EngineCore_DP0 pid=305949) $L__tmp0:
(EngineCore_DP0 pid=305949) 	.loc	1 177 24                        // quant_slide_tuned_Llama3.2-1B.py:177:24
(EngineCore_DP0 pid=305949) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=305949) 	ld.param.b32 	%r21, [_quant_slide_fp8_kernel_param_8];
(EngineCore_DP0 pid=305949) 	.loc	1 182 26                        // quant_slide_tuned_Llama3.2-1B.py:182:26
(EngineCore_DP0 pid=305949) 	mul.lo.s32 	%r22, %r21, %r1;
(EngineCore_DP0 pid=305949) 	.loc	1 182 20                        // quant_slide_tuned_Llama3.2-1B.py:182:20
(EngineCore_DP0 pid=305949) 	mad.wide.s32 	%rd1, %r22, 2, %rd4;
(EngineCore_DP0 pid=305949) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=305949) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=305949) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=305949) 	setp.lt.s32 	%p1, %r18, 1;
(EngineCore_DP0 pid=305949) 	mov.b32 	%r113, 0f2B8CBCCC;
(EngineCore_DP0 pid=305949) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=305949) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=305949) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=305949) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=305949) 	shl.b32 	%r3, %r2, 2;
(EngineCore_DP0 pid=305949) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=305949) 	shr.u32 	%r31, %r2, 3;
(EngineCore_DP0 pid=305949) 	and.b32 	%r32, %r31, 124;
(EngineCore_DP0 pid=305949) 	mov.b32 	%r33, global_smem;
(EngineCore_DP0 pid=305949) 	add.s32 	%r39, %r33, %r32;
(EngineCore_DP0 pid=305949) 	add.s32 	%r42, %r33, %r3;
(EngineCore_DP0 pid=305949) 	mov.b32 	%r37, 0;
(EngineCore_DP0 pid=305949) 	mov.b32 	%r111, 0f00000000;
(EngineCore_DP0 pid=305949) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=305949) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=305949) 	mov.b32 	%r112, %r37;
(EngineCore_DP0 pid=305949) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=305949) 	.loc	1 189 22                        // quant_slide_tuned_Llama3.2-1B.py:189:22
(EngineCore_DP0 pid=305949) 	add.s32 	%r45, %r3, %r112;
(EngineCore_DP0 pid=305949) 	setp.lt.s32 	%p2, %r45, %r17;
(EngineCore_DP0 pid=305949) 	.loc	1 190 29                        // quant_slide_tuned_Llama3.2-1B.py:190:29
(EngineCore_DP0 pid=305949) 	mad.wide.s32 	%rd6, %r45, 2, %rd1;
(EngineCore_DP0 pid=305949) 	.loc	1 190 21                        // quant_slide_tuned_Llama3.2-1B.py:190:21
(EngineCore_DP0 pid=305949) 	// begin inline asm
(EngineCore_DP0 pid=305949) 	mov.u32 %r35, %r37;
(EngineCore_DP0 pid=305949) 	mov.u32 %r36, %r37;
(EngineCore_DP0 pid=305949) 	@%p2 ld.global.v2.b32 { %r35, %r36 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=305949) 	// end inline asm
(EngineCore_DP0 pid=305949) 	mov.b32 	{%rs1, %rs2}, %r35;
(EngineCore_DP0 pid=305949) 	mov.b32 	{%rs3, %rs4}, %r36;
(EngineCore_DP0 pid=305949) 	.loc	1 191 50                        // quant_slide_tuned_Llama3.2-1B.py:191:50
(EngineCore_DP0 pid=305949) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=305949) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=305949) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=305949) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=305949) $L__tmp1:
(EngineCore_DP0 pid=305949) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	bar.sync 	0;
(EngineCore_DP0 pid=305949) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=305949) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=305949) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=305949) 	cvt.f32.bf16 	%r46, %rs11;
(EngineCore_DP0 pid=305949) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	shfl.sync.bfly.b32 	%r47, %r46, 16, 31, -1;
(EngineCore_DP0 pid=305949) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	max.f32 	%r48, %r46, %r47;
(EngineCore_DP0 pid=305949) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	shfl.sync.bfly.b32 	%r49, %r48, 8, 31, -1;
(EngineCore_DP0 pid=305949) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	max.f32 	%r50, %r48, %r49;
(EngineCore_DP0 pid=305949) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	shfl.sync.bfly.b32 	%r51, %r50, 4, 31, -1;
(EngineCore_DP0 pid=305949) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	max.f32 	%r52, %r50, %r51;
(EngineCore_DP0 pid=305949) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	shfl.sync.bfly.b32 	%r53, %r52, 2, 31, -1;
(EngineCore_DP0 pid=305949) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	max.f32 	%r54, %r52, %r53;
(EngineCore_DP0 pid=305949) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	shfl.sync.bfly.b32 	%r55, %r54, 1, 31, -1;
(EngineCore_DP0 pid=305949) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	max.f32 	%r40, %r54, %r55;
(EngineCore_DP0 pid=305949) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	// begin inline asm
(EngineCore_DP0 pid=305949) 	@%p3 st.shared.b32 [ %r39 + 0 ], %r40;
(EngineCore_DP0 pid=305949) 	// end inline asm
(EngineCore_DP0 pid=305949) 	bar.sync 	0;
(EngineCore_DP0 pid=305949) 	// begin inline asm
(EngineCore_DP0 pid=305949) 	@%p4 ld.shared.b32 %r41, [ %r42 + 0 ];
(EngineCore_DP0 pid=305949) 	// end inline asm
(EngineCore_DP0 pid=305949) 	shfl.sync.bfly.b32 	%r56, %r41, 16, 31, -1;
(EngineCore_DP0 pid=305949) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	max.f32 	%r57, %r41, %r56;
(EngineCore_DP0 pid=305949) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	shfl.sync.bfly.b32 	%r58, %r57, 8, 31, -1;
(EngineCore_DP0 pid=305949) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	max.f32 	%r59, %r57, %r58;
(EngineCore_DP0 pid=305949) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	shfl.sync.bfly.b32 	%r60, %r59, 4, 31, -1;
(EngineCore_DP0 pid=305949) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	max.f32 	%r61, %r59, %r60;
(EngineCore_DP0 pid=305949) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	shfl.sync.bfly.b32 	%r62, %r61, 2, 31, -1;
(EngineCore_DP0 pid=305949) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	max.f32 	%r63, %r61, %r62;
(EngineCore_DP0 pid=305949) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	shfl.sync.bfly.b32 	%r64, %r63, 1, 31, -1;
(EngineCore_DP0 pid=305949) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	max.f32 	%r44, %r63, %r64;
(EngineCore_DP0 pid=305949) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=305949) 	// begin inline asm
(EngineCore_DP0 pid=305949) 	@%p19 st.shared.b32 [ %r42 + 0 ], %r44;
(EngineCore_DP0 pid=305949) 	// end inline asm
(EngineCore_DP0 pid=305949) 	bar.sync 	0;
(EngineCore_DP0 pid=305949) 	ld.shared.b32 	%r65, [global_smem];
(EngineCore_DP0 pid=305949) $L__tmp2:
(EngineCore_DP0 pid=305949) 	.loc	1 191 36                        // quant_slide_tuned_Llama3.2-1B.py:191:36
(EngineCore_DP0 pid=305949) 	max.f32 	%r111, %r111, %r65;
(EngineCore_DP0 pid=305949) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=305949) 	add.s32 	%r112, %r112, 4096;
(EngineCore_DP0 pid=305949) 	setp.lt.s32 	%p6, %r112, %r18;
(EngineCore_DP0 pid=305949) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=305949) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=305949) 	.loc	1 193 32                        // quant_slide_tuned_Llama3.2-1B.py:193:32
(EngineCore_DP0 pid=305949) 	max.f32 	%r113, %r111, 0f2B8CBCCC;
(EngineCore_DP0 pid=305949) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=305949) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=305949) 	mov.b32 	%r67, 0f43E00000;
(EngineCore_DP0 pid=305949) 	.loc	1 194 32                        // quant_slide_tuned_Llama3.2-1B.py:194:32
(EngineCore_DP0 pid=305949) 	div.full.f32 	%r68, %r113, %r67;
(EngineCore_DP0 pid=305949) 	.loc	1 194 41                        // quant_slide_tuned_Llama3.2-1B.py:194:41
(EngineCore_DP0 pid=305949) 	max.f32 	%r66, %r68, 0f36924925;
(EngineCore_DP0 pid=305949) 	.loc	1 196 25                        // quant_slide_tuned_Llama3.2-1B.py:196:25
(EngineCore_DP0 pid=305949) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=305949) 	.loc	1 196 30                        // quant_slide_tuned_Llama3.2-1B.py:196:30
(EngineCore_DP0 pid=305949) 	// begin inline asm
(EngineCore_DP0 pid=305949) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r66 };
(EngineCore_DP0 pid=305949) 	// end inline asm
(EngineCore_DP0 pid=305949) 	.loc	1 199 29                        // quant_slide_tuned_Llama3.2-1B.py:199:29
(EngineCore_DP0 pid=305949) 	shl.b32 	%r14, %r19, 2;
(EngineCore_DP0 pid=305949) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=305949) 	setp.lt.s32 	%p8, %r14, 1;
(EngineCore_DP0 pid=305949) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=305949) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=305949) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=305949) 	ld.param.b32 	%r23, [_quant_slide_fp8_kernel_param_9];
(EngineCore_DP0 pid=305949) 	shr.s32 	%r24, %r23, 31;
(EngineCore_DP0 pid=305949) 	shr.u32 	%r25, %r24, 30;
(EngineCore_DP0 pid=305949) 	add.s32 	%r26, %r23, %r25;
(EngineCore_DP0 pid=305949) 	shr.s32 	%r27, %r26, 2;
(EngineCore_DP0 pid=305949) 	ld.param.b64 	%rd5, [_quant_slide_fp8_kernel_param_1];
(EngineCore_DP0 pid=305949) 	mul.lo.s32 	%r28, %r27, %r1;
(EngineCore_DP0 pid=305949) 	mad.wide.s32 	%rd2, %r28, 4, %rd5;
(EngineCore_DP0 pid=305949) 	div.full.f32 	%r13, %r67, %r113;
(EngineCore_DP0 pid=305949) 	mov.b32 	%r114, 0;
(EngineCore_DP0 pid=305949) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=305949)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=305949) 	.loc	1 203 30                        // quant_slide_tuned_Llama3.2-1B.py:203:30
(EngineCore_DP0 pid=305949) 	add.s32 	%r80, %r2, %r114;
(EngineCore_DP0 pid=305949) 	setp.lt.s32 	%p13, %r80, %r14;
(EngineCore_DP0 pid=305949) 	.loc	1 206 24                        // quant_slide_tuned_Llama3.2-1B.py:206:24
(EngineCore_DP0 pid=305949) 	shr.s32 	%r81, %r80, 31;
(EngineCore_DP0 pid=305949) 	shr.u32 	%r82, %r81, 30;
(EngineCore_DP0 pid=305949) 	add.s32 	%r83, %r80, %r82;
(EngineCore_DP0 pid=305949) 	shr.s32 	%r84, %r83, 2;
(EngineCore_DP0 pid=305949) 	.loc	1 207 23                        // quant_slide_tuned_Llama3.2-1B.py:207:23
(EngineCore_DP0 pid=305949) 	and.b32 	%r85, %r83, 2147483644;
(EngineCore_DP0 pid=305949) 	sub.s32 	%r86, %r80, %r85;
(EngineCore_DP0 pid=305949) 	.loc	1 208 30                        // quant_slide_tuned_Llama3.2-1B.py:208:30
(EngineCore_DP0 pid=305949) 	shl.b32 	%r87, %r86, 1;
(EngineCore_DP0 pid=305949) 	.loc	1 208 26                        // quant_slide_tuned_Llama3.2-1B.py:208:26
(EngineCore_DP0 pid=305949) 	mad.lo.s32 	%r88, %r84, 10, %r87;
(EngineCore_DP0 pid=305949) 	.loc	1 211 53                        // quant_slide_tuned_Llama3.2-1B.py:211:53
(EngineCore_DP0 pid=305949) 	setp.lt.s32 	%p14, %r88, %r17;
(EngineCore_DP0 pid=305949) 	.loc	1 211 37                        // quant_slide_tuned_Llama3.2-1B.py:211:37
(EngineCore_DP0 pid=305949) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=305949) 	.loc	1 210 29                        // quant_slide_tuned_Llama3.2-1B.py:210:29
(EngineCore_DP0 pid=305949) 	mad.wide.s32 	%rd8, %r88, 2, %rd1;
(EngineCore_DP0 pid=305949) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=305949) 	.loc	1 210 21                        // quant_slide_tuned_Llama3.2-1B.py:210:21
(EngineCore_DP0 pid=305949) 	// begin inline asm
(EngineCore_DP0 pid=305949) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=305949) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=305949) 	// end inline asm
(EngineCore_DP0 pid=305949) 	.loc	1 211 79                        // quant_slide_tuned_Llama3.2-1B.py:211:79
(EngineCore_DP0 pid=305949) 	cvt.f32.bf16 	%r89, %rs12;
(EngineCore_DP0 pid=305949) 	.loc	1 213 48                        // quant_slide_tuned_Llama3.2-1B.py:213:48
(EngineCore_DP0 pid=305949) 	or.b32 	%r90, %r88, 1;
(EngineCore_DP0 pid=305949) 	.loc	1 213 53                        // quant_slide_tuned_Llama3.2-1B.py:213:53
(EngineCore_DP0 pid=305949) 	setp.lt.s32 	%p15, %r90, %r17;
(EngineCore_DP0 pid=305949) 	.loc	1 213 37                        // quant_slide_tuned_Llama3.2-1B.py:213:37
(EngineCore_DP0 pid=305949) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=305949) 	.loc	1 212 39                        // quant_slide_tuned_Llama3.2-1B.py:212:39
(EngineCore_DP0 pid=305949) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=305949) 	.loc	1 212 21                        // quant_slide_tuned_Llama3.2-1B.py:212:21
(EngineCore_DP0 pid=305949) 	// begin inline asm
(EngineCore_DP0 pid=305949) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=305949) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=305949) 	// end inline asm
(EngineCore_DP0 pid=305949) 	.loc	1 213 79                        // quant_slide_tuned_Llama3.2-1B.py:213:79
(EngineCore_DP0 pid=305949) 	cvt.f32.bf16 	%r91, %rs14;
(EngineCore_DP0 pid=305949) 	.loc	1 215 48                        // quant_slide_tuned_Llama3.2-1B.py:215:48
(EngineCore_DP0 pid=305949) 	add.s32 	%r92, %r88, 2;
(EngineCore_DP0 pid=305949) 	.loc	1 215 53                        // quant_slide_tuned_Llama3.2-1B.py:215:53
(EngineCore_DP0 pid=305949) 	setp.lt.s32 	%p16, %r92, %r17;
(EngineCore_DP0 pid=305949) 	.loc	1 215 37                        // quant_slide_tuned_Llama3.2-1B.py:215:37
(EngineCore_DP0 pid=305949) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=305949) 	.loc	1 214 39                        // quant_slide_tuned_Llama3.2-1B.py:214:39
(EngineCore_DP0 pid=305949) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=305949) 	.loc	1 214 21                        // quant_slide_tuned_Llama3.2-1B.py:214:21
(EngineCore_DP0 pid=305949) 	// begin inline asm
(EngineCore_DP0 pid=305949) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=305949) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=305949) 	// end inline asm
(EngineCore_DP0 pid=305949) 	.loc	1 215 79                        // quant_slide_tuned_Llama3.2-1B.py:215:79
(EngineCore_DP0 pid=305949) 	cvt.f32.bf16 	%r93, %rs16;
(EngineCore_DP0 pid=305949) 	.loc	1 217 48                        // quant_slide_tuned_Llama3.2-1B.py:217:48
(EngineCore_DP0 pid=305949) 	add.s32 	%r94, %r88, 3;
(EngineCore_DP0 pid=305949) 	.loc	1 217 53                        // quant_slide_tuned_Llama3.2-1B.py:217:53
(EngineCore_DP0 pid=305949) 	setp.lt.s32 	%p17, %r94, %r17;
(EngineCore_DP0 pid=305949) 	.loc	1 217 37                        // quant_slide_tuned_Llama3.2-1B.py:217:37
(EngineCore_DP0 pid=305949) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=305949) 	.loc	1 216 39                        // quant_slide_tuned_Llama3.2-1B.py:216:39
(EngineCore_DP0 pid=305949) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=305949) 	.loc	1 216 21                        // quant_slide_tuned_Llama3.2-1B.py:216:21
(EngineCore_DP0 pid=305949) 	// begin inline asm
(EngineCore_DP0 pid=305949) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=305949) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=305949) 	// end inline asm
(EngineCore_DP0 pid=305949) 	.loc	1 217 79                        // quant_slide_tuned_Llama3.2-1B.py:217:79
(EngineCore_DP0 pid=305949) 	cvt.f32.bf16 	%r95, %rs18;
(EngineCore_DP0 pid=305949) 	.loc	1 219 27                        // quant_slide_tuned_Llama3.2-1B.py:219:27
(EngineCore_DP0 pid=305949) 	mul.f32 	%r96, %r13, %r89;
(EngineCore_DP0 pid=305949) 	mov.b32 	%r97, 0f43E00000;
(EngineCore_DP0 pid=305949) 	.loc	1 219 48                        // quant_slide_tuned_Llama3.2-1B.py:219:48
(EngineCore_DP0 pid=305949) 	min.xorsign.abs.f32 	%r70, %r96, %r97;
(EngineCore_DP0 pid=305949) 	.loc	1 219 60                        // quant_slide_tuned_Llama3.2-1B.py:219:60
(EngineCore_DP0 pid=305949) 	// begin inline asm
(EngineCore_DP0 pid=305949) 	cvt.rn.satfinite.e4m3x2.f32  %rs20, %r71, %r70; 
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) 	// end inline asm
(EngineCore_DP0 pid=305949) 	.loc	1 220 27                        // quant_slide_tuned_Llama3.2-1B.py:220:27
(EngineCore_DP0 pid=305949) 	mul.f32 	%r98, %r13, %r91;
(EngineCore_DP0 pid=305949) 	.loc	1 220 48                        // quant_slide_tuned_Llama3.2-1B.py:220:48
(EngineCore_DP0 pid=305949) 	min.xorsign.abs.f32 	%r72, %r98, %r97;
(EngineCore_DP0 pid=305949) 	.loc	1 220 60                        // quant_slide_tuned_Llama3.2-1B.py:220:60
(EngineCore_DP0 pid=305949) 	// begin inline asm
(EngineCore_DP0 pid=305949) 	cvt.rn.satfinite.e4m3x2.f32  %rs21, %r73, %r72; 
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) 	// end inline asm
(EngineCore_DP0 pid=305949) 	.loc	1 221 27                        // quant_slide_tuned_Llama3.2-1B.py:221:27
(EngineCore_DP0 pid=305949) 	mul.f32 	%r99, %r13, %r93;
(EngineCore_DP0 pid=305949) 	.loc	1 221 48                        // quant_slide_tuned_Llama3.2-1B.py:221:48
(EngineCore_DP0 pid=305949) 	min.xorsign.abs.f32 	%r74, %r99, %r97;
(EngineCore_DP0 pid=305949) 	.loc	1 221 60                        // quant_slide_tuned_Llama3.2-1B.py:221:60
(EngineCore_DP0 pid=305949) 	// begin inline asm
(EngineCore_DP0 pid=305949) 	cvt.rn.satfinite.e4m3x2.f32  %rs22, %r75, %r74; 
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) 	// end inline asm
(EngineCore_DP0 pid=305949) 	.loc	1 222 27                        // quant_slide_tuned_Llama3.2-1B.py:222:27
(EngineCore_DP0 pid=305949) 	mul.f32 	%r100, %r13, %r95;
(EngineCore_DP0 pid=305949) 	.loc	1 222 48                        // quant_slide_tuned_Llama3.2-1B.py:222:48
(EngineCore_DP0 pid=305949) 	min.xorsign.abs.f32 	%r76, %r100, %r97;
(EngineCore_DP0 pid=305949) 	.loc	1 222 60                        // quant_slide_tuned_Llama3.2-1B.py:222:60
(EngineCore_DP0 pid=305949) 	// begin inline asm
(EngineCore_DP0 pid=305949) 	cvt.rn.satfinite.e4m3x2.f32  %rs23, %r77, %r76; 
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) 	// end inline asm
(EngineCore_DP0 pid=305949) 	.loc	1 224 45                        // quant_slide_tuned_Llama3.2-1B.py:224:45
(EngineCore_DP0 pid=305949) 	cvt.u32.u16 	%r101, %rs20;
(EngineCore_DP0 pid=305949) 	and.b32 	%r102, %r101, 255;
(EngineCore_DP0 pid=305949) 	.loc	1 226 45                        // quant_slide_tuned_Llama3.2-1B.py:226:45
(EngineCore_DP0 pid=305949) 	cvt.u32.u16 	%r103, %rs22;
(EngineCore_DP0 pid=305949) 	and.b32 	%r104, %r103, 255;
(EngineCore_DP0 pid=305949) 	.loc	1 227 45                        // quant_slide_tuned_Llama3.2-1B.py:227:45
(EngineCore_DP0 pid=305949) 	cvt.u32.u16 	%r105, %rs23;
(EngineCore_DP0 pid=305949) 	.loc	1 229 30                        // quant_slide_tuned_Llama3.2-1B.py:229:30
(EngineCore_DP0 pid=305949) 	and.b16 	%rs24, %rs21, 255;
(EngineCore_DP0 pid=305949) 	mul.wide.u16 	%r106, %rs24, 256;
(EngineCore_DP0 pid=305949) 	.loc	1 229 24                        // quant_slide_tuned_Llama3.2-1B.py:229:24
(EngineCore_DP0 pid=305949) 	or.b32 	%r107, %r106, %r102;
(EngineCore_DP0 pid=305949) 	.loc	1 229 42                        // quant_slide_tuned_Llama3.2-1B.py:229:42
(EngineCore_DP0 pid=305949) 	shl.b32 	%r108, %r104, 16;
(EngineCore_DP0 pid=305949) 	.loc	1 229 36                        // quant_slide_tuned_Llama3.2-1B.py:229:36
(EngineCore_DP0 pid=305949) 	or.b32 	%r109, %r107, %r108;
(EngineCore_DP0 pid=305949) 	.loc	1 229 55                        // quant_slide_tuned_Llama3.2-1B.py:229:55
(EngineCore_DP0 pid=305949) 	shl.b32 	%r110, %r105, 24;
(EngineCore_DP0 pid=305949) 	.loc	1 229 49                        // quant_slide_tuned_Llama3.2-1B.py:229:49
(EngineCore_DP0 pid=305949) 	or.b32 	%r78, %r109, %r110;
(EngineCore_DP0 pid=305949) 	.loc	1 230 29                        // quant_slide_tuned_Llama3.2-1B.py:230:29
(EngineCore_DP0 pid=305949) 	mad.wide.s32 	%rd12, %r80, 4, %rd2;
(EngineCore_DP0 pid=305949) 	.loc	1 230 39                        // quant_slide_tuned_Llama3.2-1B.py:230:39
(EngineCore_DP0 pid=305949) 	// begin inline asm
(EngineCore_DP0 pid=305949) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r78 };
(EngineCore_DP0 pid=305949) 	// end inline asm
(EngineCore_DP0 pid=305949) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=305949) 	add.s32 	%r114, %r114, 1024;
(EngineCore_DP0 pid=305949) 	setp.lt.s32 	%p18, %r114, %r14;
(EngineCore_DP0 pid=305949) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=305949) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=305949) 	.loc	1 201 4                         // quant_slide_tuned_Llama3.2-1B.py:201:4
(EngineCore_DP0 pid=305949) 	ret;
(EngineCore_DP0 pid=305949) $L__tmp3:
(EngineCore_DP0 pid=305949) $L__func_end0:
(EngineCore_DP0 pid=305949)                                         // -- End function
(EngineCore_DP0 pid=305949) }
(EngineCore_DP0 pid=305949) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=305949) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=305949) 	.section	.debug_abbrev
(EngineCore_DP0 pid=305949) 	{
(EngineCore_DP0 pid=305949) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=305949) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=305949) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=305949) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=305949) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=305949) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=305949) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=305949) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=305949) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=305949) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=305949) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=305949) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=305949) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=305949) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=305949) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=305949) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=305949) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=305949) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=305949) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=305949) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=305949) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=305949) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=305949) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=305949) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=305949) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=305949) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=305949) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=305949) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=305949) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=305949) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=305949) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=305949) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=305949) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=305949) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=305949) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=305949) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=305949) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=305949) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=305949) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=305949) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=305949) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=305949) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=305949) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=305949) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=305949) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=305949) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=305949) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=305949) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=305949) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=305949) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=305949) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=305949) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=305949) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=305949) 	}
(EngineCore_DP0 pid=305949) 	.section	.debug_info
(EngineCore_DP0 pid=305949) 	{
(EngineCore_DP0 pid=305949) .b32 222                                // Length of Unit
(EngineCore_DP0 pid=305949) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=305949) .b8 0
(EngineCore_DP0 pid=305949) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=305949) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=305949) .b8 1                                   // Abbrev [1] 0xb:0xd7 DW_TAG_compile_unit
(EngineCore_DP0 pid=305949) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=305949) .b8 114
(EngineCore_DP0 pid=305949) .b8 105
(EngineCore_DP0 pid=305949) .b8 116
(EngineCore_DP0 pid=305949) .b8 111
(EngineCore_DP0 pid=305949) .b8 110
(EngineCore_DP0 pid=305949) .b8 0
(EngineCore_DP0 pid=305949) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=305949) .b8 0
(EngineCore_DP0 pid=305949) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=305949) .b8 117
(EngineCore_DP0 pid=305949) .b8 97
(EngineCore_DP0 pid=305949) .b8 110
(EngineCore_DP0 pid=305949) .b8 116
(EngineCore_DP0 pid=305949) .b8 95
(EngineCore_DP0 pid=305949) .b8 115
(EngineCore_DP0 pid=305949) .b8 108
(EngineCore_DP0 pid=305949) .b8 105
(EngineCore_DP0 pid=305949) .b8 100
(EngineCore_DP0 pid=305949) .b8 101
(EngineCore_DP0 pid=305949) .b8 95
(EngineCore_DP0 pid=305949) .b8 116
(EngineCore_DP0 pid=305949) .b8 117
(EngineCore_DP0 pid=305949) .b8 110
(EngineCore_DP0 pid=305949) .b8 101
(EngineCore_DP0 pid=305949) .b8 100
(EngineCore_DP0 pid=305949) .b8 95
(EngineCore_DP0 pid=305949) .b8 76
(EngineCore_DP0 pid=305949) .b8 108
(EngineCore_DP0 pid=305949) .b8 97
(EngineCore_DP0 pid=305949) .b8 109
(EngineCore_DP0 pid=305949) .b8 97
(EngineCore_DP0 pid=305949) .b8 51
(EngineCore_DP0 pid=305949) .b8 46
(EngineCore_DP0 pid=305949) .b8 50
(EngineCore_DP0 pid=305949) .b8 45
(EngineCore_DP0 pid=305949) .b8 49
(EngineCore_DP0 pid=305949) .b8 66
(EngineCore_DP0 pid=305949) .b8 46
(EngineCore_DP0 pid=305949) .b8 112
(EngineCore_DP0 pid=305949) .b8 121
(EngineCore_DP0 pid=305949) .b8 0
(EngineCore_DP0 pid=305949) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=305949) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=305949) .b8 114
(EngineCore_DP0 pid=305949) .b8 111
(EngineCore_DP0 pid=305949) .b8 111
(EngineCore_DP0 pid=305949) .b8 116
(EngineCore_DP0 pid=305949) .b8 47
(EngineCore_DP0 pid=305949) .b8 118
(EngineCore_DP0 pid=305949) .b8 108
(EngineCore_DP0 pid=305949) .b8 108
(EngineCore_DP0 pid=305949) .b8 109
(EngineCore_DP0 pid=305949) .b8 98
(EngineCore_DP0 pid=305949) .b8 101
(EngineCore_DP0 pid=305949) .b8 110
(EngineCore_DP0 pid=305949) .b8 99
(EngineCore_DP0 pid=305949) .b8 104
(EngineCore_DP0 pid=305949) .b8 47
(EngineCore_DP0 pid=305949) .b8 115
(EngineCore_DP0 pid=305949) .b8 108
(EngineCore_DP0 pid=305949) .b8 105
(EngineCore_DP0 pid=305949) .b8 100
(EngineCore_DP0 pid=305949) .b8 101
(EngineCore_DP0 pid=305949) .b8 115
(EngineCore_DP0 pid=305949) .b8 112
(EngineCore_DP0 pid=305949) .b8 97
(EngineCore_DP0 pid=305949) .b8 114
(EngineCore_DP0 pid=305949) .b8 115
(EngineCore_DP0 pid=305949) .b8 101
(EngineCore_DP0 pid=305949) .b8 47
(EngineCore_DP0 pid=305949) .b8 99
(EngineCore_DP0 pid=305949) .b8 115
(EngineCore_DP0 pid=305949) .b8 114
(EngineCore_DP0 pid=305949) .b8 99
(EngineCore_DP0 pid=305949) .b8 47
(EngineCore_DP0 pid=305949) .b8 102
(EngineCore_DP0 pid=305949) .b8 117
(EngineCore_DP0 pid=305949) .b8 115
(EngineCore_DP0 pid=305949) .b8 101
(EngineCore_DP0 pid=305949) .b8 100
(EngineCore_DP0 pid=305949) .b8 95
(EngineCore_DP0 pid=305949) .b8 113
(EngineCore_DP0 pid=305949) .b8 117
(EngineCore_DP0 pid=305949) .b8 97
(EngineCore_DP0 pid=305949) .b8 110
(EngineCore_DP0 pid=305949) .b8 116
(EngineCore_DP0 pid=305949) .b8 95
(EngineCore_DP0 pid=305949) .b8 115
(EngineCore_DP0 pid=305949) .b8 108
(EngineCore_DP0 pid=305949) .b8 105
(EngineCore_DP0 pid=305949) .b8 100
(EngineCore_DP0 pid=305949) .b8 101
(EngineCore_DP0 pid=305949) .b8 95
(EngineCore_DP0 pid=305949) .b8 116
(EngineCore_DP0 pid=305949) .b8 114
(EngineCore_DP0 pid=305949) .b8 105
(EngineCore_DP0 pid=305949) .b8 116
(EngineCore_DP0 pid=305949) .b8 111
(EngineCore_DP0 pid=305949) .b8 110
(EngineCore_DP0 pid=305949) .b8 47
(EngineCore_DP0 pid=305949) .b8 98
(EngineCore_DP0 pid=305949) .b8 117
(EngineCore_DP0 pid=305949) .b8 105
(EngineCore_DP0 pid=305949) .b8 108
(EngineCore_DP0 pid=305949) .b8 100
(EngineCore_DP0 pid=305949) .b8 47
(EngineCore_DP0 pid=305949) .b8 71
(EngineCore_DP0 pid=305949) .b8 66
(EngineCore_DP0 pid=305949) .b8 49
(EngineCore_DP0 pid=305949) .b8 48
(EngineCore_DP0 pid=305949) .b8 95
(EngineCore_DP0 pid=305949) .b8 99
(EngineCore_DP0 pid=305949) .b8 99
(EngineCore_DP0 pid=305949) .b8 49
(EngineCore_DP0 pid=305949) .b8 50
(EngineCore_DP0 pid=305949) .b8 49
(EngineCore_DP0 pid=305949) .b8 95
(EngineCore_DP0 pid=305949) .b8 112
(EngineCore_DP0 pid=305949) .b8 121
(EngineCore_DP0 pid=305949) .b8 51
(EngineCore_DP0 pid=305949) .b8 49
(EngineCore_DP0 pid=305949) .b8 50
(EngineCore_DP0 pid=305949) .b8 95
(EngineCore_DP0 pid=305949) .b8 99
(EngineCore_DP0 pid=305949) .b8 117
(EngineCore_DP0 pid=305949) .b8 49
(EngineCore_DP0 pid=305949) .b8 50
(EngineCore_DP0 pid=305949) .b8 57
(EngineCore_DP0 pid=305949) .b8 95
(EngineCore_DP0 pid=305949) .b8 97
(EngineCore_DP0 pid=305949) .b8 97
(EngineCore_DP0 pid=305949) .b8 114
(EngineCore_DP0 pid=305949) .b8 99
(EngineCore_DP0 pid=305949) .b8 104
(EngineCore_DP0 pid=305949) .b8 54
(EngineCore_DP0 pid=305949) .b8 52
(EngineCore_DP0 pid=305949) .b8 0
(EngineCore_DP0 pid=305949) .b8 2                                   // Abbrev [2] 0x99:0x1a DW_TAG_subprogram
(EngineCore_DP0 pid=305949) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=305949) .b8 113
(EngineCore_DP0 pid=305949) .b8 117
(EngineCore_DP0 pid=305949) .b8 97
(EngineCore_DP0 pid=305949) .b8 110
(EngineCore_DP0 pid=305949) .b8 116
(EngineCore_DP0 pid=305949) .b8 95
(EngineCore_DP0 pid=305949) .b8 115
(EngineCore_DP0 pid=305949) .b8 108
(EngineCore_DP0 pid=305949) .b8 105
(EngineCore_DP0 pid=305949) .b8 100
(EngineCore_DP0 pid=305949) .b8 101
(EngineCore_DP0 pid=305949) .b8 95
(EngineCore_DP0 pid=305949) .b8 102
(EngineCore_DP0 pid=305949) .b8 112
(EngineCore_DP0 pid=305949) .b8 56
(EngineCore_DP0 pid=305949) .b8 95
(EngineCore_DP0 pid=305949) .b8 107
(EngineCore_DP0 pid=305949) .b8 101
(EngineCore_DP0 pid=305949) .b8 114
(EngineCore_DP0 pid=305949) .b8 110
(EngineCore_DP0 pid=305949) .b8 101
(EngineCore_DP0 pid=305949) .b8 108
(EngineCore_DP0 pid=305949) .b8 0
(EngineCore_DP0 pid=305949) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=305949) .b8 3                                   // Abbrev [3] 0xb3:0x2e DW_TAG_subprogram
(EngineCore_DP0 pid=305949) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=305949) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=305949) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=305949) .b8 4                                   // Abbrev [4] 0xc8:0x18 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=305949) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=305949) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=305949) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=305949) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=305949) .b8 191                                 // DW_AT_call_line
(EngineCore_DP0 pid=305949) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=305949) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=305949) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=305949) 	}
(EngineCore_DP0 pid=305949) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) ================================================================
(EngineCore_DP0 pid=305949) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpemzzerg4.ptx', '-o', '/tmp/tmpemzzerg4.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866] 
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866] 
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866] 
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpemzzerg4.ptx -o /tmp/tmpemzzerg4.ptx.o
(EngineCore_DP0 pid=305949) ERROR 01-25 18:52:09 [core.py:866] 

STDERR:
[2026-01-25 18:51:53] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:51:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:51:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:51:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:51:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:51:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:51:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:51:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:51:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:51:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:51:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:51:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:51:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:51:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:51:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:51:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:51:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:51:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:51:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=305949) [2026-01-25 18:51:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=305949) [2026-01-25 18:51:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=305949) [2026-01-25 18:51:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=305949) [2026-01-25 18:51:57] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=305949) [2026-01-25 18:51:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=305949) [2026-01-25 18:51:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=305949) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=305949) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.95s/it]
(EngineCore_DP0 pid=305949) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.95s/it]
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) [2026-01-25 18:52:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=305949) [2026-01-25 18:52:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=305949) [2026-01-25 18:52:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=305949) [2026-01-25 18:52:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=305949) [2026-01-25 18:52:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=305949) [2026-01-25 18:52:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=305949) [2026-01-25 18:52:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=305949) [2026-01-25 18:52:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=305949) Process EngineCore_DP0:
(EngineCore_DP0 pid=305949) Traceback (most recent call last):
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=305949)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=305949)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=305949)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=305949) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpemzzerg4.ptx', '-o', '/tmp/tmpemzzerg4.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) Traceback (most recent call last):
(EngineCore_DP0 pid=305949)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=305949)     self.run()
(EngineCore_DP0 pid=305949)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=305949)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=305949)     raise e
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=305949)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=305949)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=305949)     super().__init__(
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=305949)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=305949)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=305949)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=305949)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=305949)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=305949)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=305949)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=305949)     return func(*args, **kwargs)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=305949)     return func(*args, **kwargs)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=305949)     self.model_runner.profile_run()
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=305949)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=305949)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=305949)     return func(*args, **kwargs)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=305949)     outputs = self.model(
(EngineCore_DP0 pid=305949)               ^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305949)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305949)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=305949)     model_output = self.model(
(EngineCore_DP0 pid=305949)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=305949)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=305949)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=305949)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305949)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305949)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=305949)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=305949)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305949)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305949)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=305949)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=305949)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=305949)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=305949)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=305949)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=305949)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=305949)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=305949)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=305949)     return self._linear_fn(
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=305949)     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=305949)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=305949)     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=305949)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=305949)     return fn(input, L)
(EngineCore_DP0 pid=305949)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=305949)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=305949)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=305949)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=305949)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=305949)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=305949)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=305949)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=305949)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=305949)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=305949)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=305949)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=305949)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=305949)     raise PTXASError(error)
(EngineCore_DP0 pid=305949) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=305949) `ptxas` stderr:
(EngineCore_DP0 pid=305949) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=305949) 
(EngineCore_DP0 pid=305949) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpemzzerg4.ptx -o /tmp/tmpemzzerg4.ptx.o
(EngineCore_DP0 pid=305949) 
[rank0]:[W125 18:52:10.232566241 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=256

========== M=16 ==========
Time: 2026-01-26 02:43:07
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-FP8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:43:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:43:10 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=761039) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=761039) WARNING 01-26 02:43:32 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.12 requests/s, 1393.72 total tokens/s, 1311.73 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096

STDERR:
[2026-01-26 02:43:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:43:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:43:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:43:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:43:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:43:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:43:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:43:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:43:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:43:14] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:43:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:43:14] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:43:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:43:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:43:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:43:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:43:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:43:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=761039) [2026-01-26 02:43:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=761039) [2026-01-26 02:43:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=761039) [2026-01-26 02:43:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=761039) [2026-01-26 02:43:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=761039) [2026-01-26 02:43:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=761039) [2026-01-26 02:43:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=761039) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=761039) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.90s/it]
(EngineCore_DP0 pid=761039) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.90s/it]
(EngineCore_DP0 pid=761039) 
(EngineCore_DP0 pid=761039) [2026-01-26 02:43:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=761039) [2026-01-26 02:43:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=761039) [2026-01-26 02:43:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=761039) [2026-01-26 02:43:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=761039) [2026-01-26 02:43:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=761039) [2026-01-26 02:43:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=761039) [2026-01-26 02:43:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=761039) [2026-01-26 02:43:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=761039) 2026-01-26 02:43:31,471 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=761039) 2026-01-26 02:43:31,479 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|| 16/16 [00:00<00:00, 8462.66it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|         | 1/16 [00:03<00:46,  3.10s/it, est. speed input: 5.15 toks/s, output: 82.47 toks/s]
Processed prompts: 100%|| 16/16 [00:03<00:00,  3.10s/it, est. speed input: 82.05 toks/s, output: 1312.86 toks/s]
Processed prompts: 100%|| 16/16 [00:03<00:00,  5.13it/s, est. speed input: 82.05 toks/s, output: 1312.86 toks/s]
[rank0]:[W126 02:43:35.622109452 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 02:43:37
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:43:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:43:41 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=761653) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=761653) WARNING 01-26 02:44:02 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 22.15 requests/s, 6023.46 total tokens/s, 5669.14 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 02:43:41] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:43:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:43:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:43:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:43:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:43:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:43:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:43:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:43:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:43:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:43:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:43:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:43:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:43:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:43:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:43:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:43:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:43:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:43:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=761653) [2026-01-26 02:43:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=761653) [2026-01-26 02:43:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=761653) [2026-01-26 02:43:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=761653) [2026-01-26 02:43:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=761653) [2026-01-26 02:43:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=761653) [2026-01-26 02:43:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=761653) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=761653) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.61s/it]
(EngineCore_DP0 pid=761653) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.62s/it]
(EngineCore_DP0 pid=761653) 
(EngineCore_DP0 pid=761653) [2026-01-26 02:43:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=761653) [2026-01-26 02:43:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=761653) [2026-01-26 02:43:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=761653) [2026-01-26 02:43:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=761653) [2026-01-26 02:43:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=761653) [2026-01-26 02:43:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=761653) [2026-01-26 02:43:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=761653) [2026-01-26 02:43:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=761653) 2026-01-26 02:44:01,777 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=761653) 2026-01-26 02:44:01,784 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 12788.43it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:05<11:13,  5.31s/it, est. speed input: 3.02 toks/s, output: 48.25 toks/s]
Processed prompts:   4%|         | 5/128 [00:05<01:40,  1.23it/s, est. speed input: 14.78 toks/s, output: 236.46 toks/s]
Processed prompts:  39%|      | 50/128 [00:05<00:04, 17.22it/s, est. speed input: 144.33 toks/s, output: 2309.33 toks/s]
Processed prompts:  77%|  | 99/128 [00:05<00:00, 39.99it/s, est. speed input: 280.35 toks/s, output: 4485.64 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 55.44it/s, est. speed input: 354.99 toks/s, output: 5679.88 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 55.44it/s, est. speed input: 354.99 toks/s, output: 5679.88 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 22.19it/s, est. speed input: 354.99 toks/s, output: 5679.88 toks/s]
[rank0]:[W126 02:44:08.605789318 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 02:44:10
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:44:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:44:14 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=762308) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=762308) WARNING 01-26 02:44:35 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 27.96 requests/s, 7604.37 total tokens/s, 7157.06 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 02:44:14] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:44:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:44:14] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:44:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:44:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:44:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:44:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:44:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:44:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:44:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:44:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:44:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:44:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:44:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:44:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:44:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:44:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:44:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:44:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:44:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:44:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:44:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:44:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:44:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:44:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:44:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:44:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:44:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=762308) [2026-01-26 02:44:18] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=762308) [2026-01-26 02:44:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=762308) [2026-01-26 02:44:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=762308) [2026-01-26 02:44:18] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=762308) [2026-01-26 02:44:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=762308) [2026-01-26 02:44:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=762308) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=762308) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.76s/it]
(EngineCore_DP0 pid=762308) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.76s/it]
(EngineCore_DP0 pid=762308) 
(EngineCore_DP0 pid=762308) [2026-01-26 02:44:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=762308) [2026-01-26 02:44:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=762308) [2026-01-26 02:44:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=762308) [2026-01-26 02:44:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=762308) [2026-01-26 02:44:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=762308) [2026-01-26 02:44:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=762308) [2026-01-26 02:44:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=762308) [2026-01-26 02:44:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=762308) 2026-01-26 02:44:34,832 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=762308) 2026-01-26 02:44:34,839 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 13217.89it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<33:31,  7.89s/it, est. speed input: 2.03 toks/s, output: 32.46 toks/s]
Processed prompts:  13%|        | 33/256 [00:08<00:38,  5.76it/s, est. speed input: 65.69 toks/s, output: 1050.96 toks/s]
Processed prompts:  29%|       | 75/256 [00:08<00:11, 15.81it/s, est. speed input: 146.82 toks/s, output: 2349.15 toks/s]
Processed prompts:  42%|     | 108/256 [00:08<00:05, 26.16it/s, est. speed input: 208.25 toks/s, output: 3332.02 toks/s]
Processed prompts:  53%|    | 136/256 [00:08<00:03, 37.39it/s, est. speed input: 258.56 toks/s, output: 4137.01 toks/s]
Processed prompts:  65%|   | 166/256 [00:08<00:01, 52.45it/s, est. speed input: 310.92 toks/s, output: 4974.79 toks/s]
Processed prompts:  75%|  | 193/256 [00:08<00:00, 67.60it/s, est. speed input: 355.82 toks/s, output: 5693.16 toks/s]
Processed prompts:  84%| | 216/256 [00:08<00:00, 81.29it/s, est. speed input: 392.42 toks/s, output: 6278.76 toks/s]
Processed prompts:  93%|| 237/256 [00:08<00:00, 93.43it/s, est. speed input: 424.33 toks/s, output: 6789.25 toks/s]
Processed prompts: 100%|| 256/256 [00:09<00:00, 93.43it/s, est. speed input: 448.33 toks/s, output: 7173.20 toks/s]
Processed prompts: 100%|| 256/256 [00:09<00:00, 28.02it/s, est. speed input: 448.33 toks/s, output: 7173.20 toks/s]
[rank0]:[W126 02:44:44.079079143 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:10:29
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:10:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:10:32 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2852343) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2852343) WARNING 01-27 17:10:54 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.92 requests/s, 4056.88 total tokens/s, 3818.24 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:10:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:10:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:10:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:10:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:10:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:10:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:10:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:10:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:10:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:10:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:10:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:10:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:10:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:10:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:10:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:10:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:10:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:10:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:10:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:10:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:10:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:10:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:10:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:10:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:10:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:10:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:10:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:10:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2852343) [2026-01-27 17:10:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2852343) [2026-01-27 17:10:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2852343) [2026-01-27 17:10:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2852343) [2026-01-27 17:10:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2852343) [2026-01-27 17:10:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2852343) [2026-01-27 17:10:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2852343) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2852343) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.95s/it]
(EngineCore_DP0 pid=2852343) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.95s/it]
(EngineCore_DP0 pid=2852343) 
(EngineCore_DP0 pid=2852343) [2026-01-27 17:10:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=2852343) [2026-01-27 17:10:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=2852343) [2026-01-27 17:10:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=2852343) [2026-01-27 17:10:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=2852343) [2026-01-27 17:10:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=2852343) [2026-01-27 17:10:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=2852343) [2026-01-27 17:10:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=2852343) [2026-01-27 17:10:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=2852343) 2026-01-27 17:10:54,224 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2852343) 2026-01-27 17:10:54,232 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 11472.58it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:04<04:21,  4.16s/it, est. speed input: 3.85 toks/s, output: 61.59 toks/s]
Processed prompts:  77%|  | 49/64 [00:04<00:00, 16.09it/s, est. speed input: 183.44 toks/s, output: 2935.07 toks/s]
Processed prompts: 100%|| 64/64 [00:04<00:00, 16.09it/s, est. speed input: 238.99 toks/s, output: 3823.84 toks/s]
Processed prompts: 100%|| 64/64 [00:04<00:00, 14.94it/s, est. speed input: 238.99 toks/s, output: 3823.84 toks/s]
[rank0]:[W127 17:10:59.415566372 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:11:01
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:11:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:11:05 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2852989) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2852989) WARNING 01-27 17:11:27 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 22.39 requests/s, 6089.37 total tokens/s, 5731.17 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:11:05] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:11:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:11:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:11:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:11:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:11:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:11:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:11:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:11:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:11:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:11:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:11:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:11:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:11:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:11:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:11:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:11:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:11:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2852989) [2026-01-27 17:11:09] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2852989) [2026-01-27 17:11:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2852989) [2026-01-27 17:11:09] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2852989) [2026-01-27 17:11:09] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2852989) [2026-01-27 17:11:09] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2852989) [2026-01-27 17:11:09] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2852989) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2852989) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.68s/it]
(EngineCore_DP0 pid=2852989) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.68s/it]
(EngineCore_DP0 pid=2852989) 
(EngineCore_DP0 pid=2852989) [2026-01-27 17:11:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=2852989) [2026-01-27 17:11:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=2852989) [2026-01-27 17:11:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=2852989) [2026-01-27 17:11:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=2852989) [2026-01-27 17:11:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=2852989) [2026-01-27 17:11:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=2852989) [2026-01-27 17:11:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=2852989) [2026-01-27 17:11:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=2852989) 2026-01-27 17:11:26,464 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2852989) 2026-01-27 17:11:26,471 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 12494.67it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:05<11:11,  5.29s/it, est. speed input: 3.02 toks/s, output: 48.39 toks/s]
Processed prompts:  27%|       | 35/128 [00:05<00:10,  9.09it/s, est. speed input: 103.66 toks/s, output: 1658.52 toks/s]
Processed prompts:  59%|    | 76/128 [00:05<00:02, 23.55it/s, est. speed input: 220.94 toks/s, output: 3535.06 toks/s]
Processed prompts:  93%|| 119/128 [00:05<00:00, 43.43it/s, est. speed input: 339.66 toks/s, output: 5434.52 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 43.43it/s, est. speed input: 358.93 toks/s, output: 5742.86 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 22.43it/s, est. speed input: 358.93 toks/s, output: 5742.86 toks/s]
[rank0]:[W127 17:11:33.087554895 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:11:36
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:11:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:11:40 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2853668) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2853668) WARNING 01-27 17:12:02 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 27.94 requests/s, 7599.84 total tokens/s, 7152.79 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:11:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:11:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:11:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:11:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:11:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:11:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:11:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:11:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:11:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:11:43] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:11:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:11:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:11:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:11:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:11:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:11:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:11:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:11:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:11:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2853668) [2026-01-27 17:11:44] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2853668) [2026-01-27 17:11:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2853668) [2026-01-27 17:11:44] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2853668) [2026-01-27 17:11:44] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2853668) [2026-01-27 17:11:44] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2853668) [2026-01-27 17:11:44] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2853668) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2853668) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.80s/it]
(EngineCore_DP0 pid=2853668) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.80s/it]
(EngineCore_DP0 pid=2853668) 
(EngineCore_DP0 pid=2853668) [2026-01-27 17:11:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=2853668) [2026-01-27 17:11:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=2853668) [2026-01-27 17:11:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=2853668) [2026-01-27 17:11:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=2853668) [2026-01-27 17:11:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=2853668) [2026-01-27 17:11:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=2853668) [2026-01-27 17:11:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=2853668) [2026-01-27 17:11:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=2853668) 2026-01-27 17:12:01,339 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2853668) 2026-01-27 17:12:01,356 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 13097.13it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<33:07,  7.80s/it, est. speed input: 2.05 toks/s, output: 32.84 toks/s]
Processed prompts:  13%|        | 34/256 [00:07<00:36,  6.01it/s, est. speed input: 68.43 toks/s, output: 1094.89 toks/s]
Processed prompts:  29%|       | 75/256 [00:08<00:11, 15.91it/s, est. speed input: 148.47 toks/s, output: 2375.48 toks/s]
Processed prompts:  42%|     | 108/256 [00:08<00:05, 26.40it/s, est. speed input: 210.66 toks/s, output: 3370.50 toks/s]
Processed prompts:  53%|    | 136/256 [00:08<00:03, 37.76it/s, est. speed input: 261.52 toks/s, output: 4184.31 toks/s]
Processed prompts:  65%|   | 166/256 [00:08<00:01, 52.96it/s, est. speed input: 314.44 toks/s, output: 5031.03 toks/s]
Processed prompts:  75%|  | 193/256 [00:08<00:00, 68.27it/s, est. speed input: 359.84 toks/s, output: 5757.40 toks/s]
Processed prompts:  84%| | 216/256 [00:08<00:00, 80.54it/s, est. speed input: 395.93 toks/s, output: 6334.85 toks/s]
Processed prompts:  93%|| 237/256 [00:08<00:00, 95.44it/s, est. speed input: 429.27 toks/s, output: 6868.37 toks/s]
Processed prompts: 100%|| 256/256 [00:09<00:00, 95.44it/s, est. speed input: 448.05 toks/s, output: 7168.78 toks/s]
Processed prompts: 100%|| 256/256 [00:09<00:00, 28.00it/s, est. speed input: 448.05 toks/s, output: 7168.78 toks/s]
[rank0]:[W127 17:12:11.456746352 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:12:13
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:12:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:12:17 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2854384) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2854384) WARNING 01-27 17:12:39 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 29.22 requests/s, 7948.01 total tokens/s, 7480.48 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:12:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:12:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:12:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:12:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:12:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:12:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:12:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:12:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:12:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:12:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:12:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:12:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:12:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:12:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:12:20] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:12:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:12:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:12:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:12:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:12:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:12:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:12:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:12:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:12:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:12:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:12:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:12:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:12:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2854384) [2026-01-27 17:12:21] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2854384) [2026-01-27 17:12:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2854384) [2026-01-27 17:12:21] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2854384) [2026-01-27 17:12:21] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2854384) [2026-01-27 17:12:21] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2854384) [2026-01-27 17:12:21] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2854384) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2854384) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.59s/it]
(EngineCore_DP0 pid=2854384) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.59s/it]
(EngineCore_DP0 pid=2854384) 
(EngineCore_DP0 pid=2854384) [2026-01-27 17:12:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=2854384) [2026-01-27 17:12:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=2854384) [2026-01-27 17:12:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=2854384) [2026-01-27 17:12:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=2854384) [2026-01-27 17:12:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=2854384) [2026-01-27 17:12:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=2854384) [2026-01-27 17:12:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=2854384) [2026-01-27 17:12:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=2854384) 2026-01-27 17:12:38,466 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2854384) 2026-01-27 17:12:38,483 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 13952.49it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:13<1:50:44, 13.00s/it, est. speed input: 1.23 toks/s, output: 19.69 toks/s]
Processed prompts:   1%|          | 6/512 [00:13<13:37,  1.62s/it, est. speed input: 7.32 toks/s, output: 117.16 toks/s] 
Processed prompts:  13%|        | 66/512 [00:13<00:46,  9.65it/s, est. speed input: 79.42 toks/s, output: 1270.73 toks/s]
Processed prompts:  23%|       | 119/512 [00:13<00:19, 20.30it/s, est. speed input: 141.35 toks/s, output: 2261.61 toks/s]
Processed prompts:  32%|      | 166/512 [00:13<00:10, 32.96it/s, est. speed input: 195.27 toks/s, output: 3124.31 toks/s]
Processed prompts:  40%|      | 207/512 [00:13<00:06, 47.17it/s, est. speed input: 241.19 toks/s, output: 3859.00 toks/s]
Processed prompts:  47%|     | 243/512 [00:13<00:04, 62.94it/s, est. speed input: 280.71 toks/s, output: 4491.29 toks/s]
Processed prompts:  54%|    | 274/512 [00:13<00:03, 78.62it/s, est. speed input: 313.68 toks/s, output: 5018.81 toks/s]
Processed prompts:  62%|   | 315/512 [00:14<00:01, 104.68it/s, est. speed input: 357.17 toks/s, output: 5714.66 toks/s]
Processed prompts:  67%|   | 345/512 [00:14<00:01, 125.74it/s, est. speed input: 388.38 toks/s, output: 6214.10 toks/s]
Processed prompts:  73%|  | 376/512 [00:14<00:00, 143.43it/s, est. speed input: 419.24 toks/s, output: 6707.88 toks/s]
Processed prompts:  79%|  | 404/512 [00:14<00:00, 163.67it/s, est. speed input: 447.23 toks/s, output: 7155.76 toks/s]
Processed prompts:  84%| | 432/512 [00:14<00:00, 171.12it/s, est. speed input: 473.52 toks/s, output: 7576.29 toks/s]
Processed prompts:  89%| | 457/512 [00:14<00:00, 164.66it/s, est. speed input: 495.21 toks/s, output: 7923.41 toks/s]
Processed prompts:  94%|| 479/512 [00:14<00:00, 157.94it/s, est. speed input: 513.60 toks/s, output: 8217.58 toks/s]
Processed prompts:  97%|| 499/512 [00:17<00:00, 29.11it/s, est. speed input: 459.70 toks/s, output: 7355.16 toks/s] 
Processed prompts: 100%|| 512/512 [00:17<00:00, 29.11it/s, est. speed input: 468.54 toks/s, output: 7496.61 toks/s]
Processed prompts: 100%|| 512/512 [00:17<00:00, 29.28it/s, est. speed input: 468.54 toks/s, output: 7496.61 toks/s]
[rank0]:[W127 17:12:56.947560169 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:47:49
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-3B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:47:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:47:52 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2891099) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2891099) WARNING 01-27 17:48:29 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.20 requests/s, 1958.72 total tokens/s, 1843.50 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:47:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:47:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:47:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:47:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:47:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:47:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:47:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:47:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:47:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:47:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:47:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:47:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:47:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:47:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:47:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:47:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:47:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:47:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2891099) [2026-01-27 17:47:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2891099) [2026-01-27 17:47:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2891099) [2026-01-27 17:47:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2891099) [2026-01-27 17:47:57] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2891099) [2026-01-27 17:47:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2891099) [2026-01-27 17:47:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2891099) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2891099) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:25<00:00, 25.31s/it]
(EngineCore_DP0 pid=2891099) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:25<00:00, 25.31s/it]
(EngineCore_DP0 pid=2891099) 
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=2891099) 2026-01-27 17:48:29,150 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2891099) 2026-01-27 17:48:29,177 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 11432.03it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:08<09:06,  8.67s/it, est. speed input: 1.85 toks/s, output: 29.53 toks/s]
Processed prompts:  55%|    | 35/64 [00:08<00:05,  5.59it/s, est. speed input: 63.58 toks/s, output: 1017.32 toks/s]
Processed prompts: 100%|| 64/64 [00:08<00:00,  5.59it/s, est. speed input: 115.30 toks/s, output: 1844.87 toks/s]
Processed prompts: 100%|| 64/64 [00:08<00:00,  7.21it/s, est. speed input: 115.30 toks/s, output: 1844.87 toks/s]
[rank0]:[W127 17:48:38.905178331 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:48:41
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-3B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:48:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:48:44 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2892006) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2892006) WARNING 01-27 17:49:20 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 11.23 requests/s, 3054.20 total tokens/s, 2874.55 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:48:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:48:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:48:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:48:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:48:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:48:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:48:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:48:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:48:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:48:48] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:48:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:48:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:48:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:48:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:48:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:48:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:48:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:48:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2892006) [2026-01-27 17:48:49] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2892006) [2026-01-27 17:48:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2892006) [2026-01-27 17:48:49] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2892006) [2026-01-27 17:48:49] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2892006) [2026-01-27 17:48:49] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2892006) [2026-01-27 17:48:49] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2892006) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2892006) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.42s/it]
(EngineCore_DP0 pid=2892006) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.42s/it]
(EngineCore_DP0 pid=2892006) 
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=2892006) 2026-01-27 17:49:19,641 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2892006) 2026-01-27 17:49:19,652 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 12491.47it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:10<22:59, 10.86s/it, est. speed input: 1.47 toks/s, output: 23.57 toks/s]
Processed prompts:  20%|        | 25/128 [00:10<00:32,  3.20it/s, est. speed input: 36.43 toks/s, output: 582.93 toks/s]
Processed prompts:  42%|     | 54/128 [00:11<00:08,  8.37it/s, est. speed input: 77.80 toks/s, output: 1244.81 toks/s]
Processed prompts:  62%|   | 79/128 [00:11<00:03, 14.44it/s, est. speed input: 112.75 toks/s, output: 1804.05 toks/s]
Processed prompts:  80%|  | 102/128 [00:11<00:01, 21.87it/s, est. speed input: 144.23 toks/s, output: 2307.64 toks/s]
Processed prompts: 100%|| 128/128 [00:11<00:00, 21.87it/s, est. speed input: 179.83 toks/s, output: 2877.34 toks/s]
Processed prompts: 100%|| 128/128 [00:11<00:00, 11.24it/s, est. speed input: 179.83 toks/s, output: 2877.34 toks/s]
[rank0]:[W127 17:49:32.061455539 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:49:34
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-3B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:49:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:49:37 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2892935) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2892935) WARNING 01-27 17:50:13 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.32 requests/s, 3894.99 total tokens/s, 3665.87 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:49:37] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:49:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:49:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:49:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:49:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:49:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:49:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:49:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:49:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:49:41] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:49:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:49:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:49:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:49:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:49:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:49:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:49:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:49:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2892935) [2026-01-27 17:49:42] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2892935) [2026-01-27 17:49:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2892935) [2026-01-27 17:49:42] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2892935) [2026-01-27 17:49:42] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2892935) [2026-01-27 17:49:42] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2892935) [2026-01-27 17:49:42] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2892935) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2892935) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.39s/it]
(EngineCore_DP0 pid=2892935) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.39s/it]
(EngineCore_DP0 pid=2892935) 
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=2892935) 2026-01-27 17:50:12,458 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2892935) 2026-01-27 17:50:12,468 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 12805.97it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:15<1:05:47, 15.48s/it, est. speed input: 1.03 toks/s, output: 16.54 toks/s]
Processed prompts:   7%|         | 19/256 [00:15<02:19,  1.70it/s, est. speed input: 19.42 toks/s, output: 310.72 toks/s]
Processed prompts:  19%|        | 48/256 [00:15<00:38,  5.35it/s, est. speed input: 48.58 toks/s, output: 777.23 toks/s]
Processed prompts:  29%|       | 75/256 [00:15<00:18,  9.95it/s, est. speed input: 75.10 toks/s, output: 1201.61 toks/s]
Processed prompts:  38%|      | 98/256 [00:16<00:10, 15.15it/s, est. speed input: 97.18 toks/s, output: 1554.81 toks/s]
Processed prompts:  46%|     | 118/256 [00:16<00:06, 21.07it/s, est. speed input: 116.03 toks/s, output: 1856.43 toks/s]
Processed prompts:  53%|    | 136/256 [00:16<00:04, 27.93it/s, est. speed input: 132.71 toks/s, output: 2123.38 toks/s]
Processed prompts:  59%|    | 152/256 [00:16<00:02, 35.49it/s, est. speed input: 147.28 toks/s, output: 2356.48 toks/s]
Processed prompts:  65%|   | 167/256 [00:16<00:02, 44.21it/s, est. speed input: 160.77 toks/s, output: 2572.39 toks/s]
Processed prompts:  71%|   | 182/256 [00:16<00:01, 54.61it/s, est. speed input: 174.14 toks/s, output: 2786.19 toks/s]
Processed prompts:  77%|  | 197/256 [00:16<00:00, 63.81it/s, est. speed input: 186.99 toks/s, output: 2991.86 toks/s]
Processed prompts:  82%| | 211/256 [00:17<00:00, 68.72it/s, est. speed input: 198.38 toks/s, output: 3174.01 toks/s]
Processed prompts:  87%| | 223/256 [00:17<00:00, 73.50it/s, est. speed input: 208.08 toks/s, output: 3329.24 toks/s]
Processed prompts:  92%|| 235/256 [00:17<00:00, 74.79it/s, est. speed input: 217.34 toks/s, output: 3477.45 toks/s]
Processed prompts:  96%|| 246/256 [00:17<00:00, 64.86it/s, est. speed input: 224.47 toks/s, output: 3591.45 toks/s]
Processed prompts: 100%|| 255/256 [00:17<00:00, 57.05it/s, est. speed input: 229.78 toks/s, output: 3676.50 toks/s]
Processed prompts: 100%|| 256/256 [00:17<00:00, 57.05it/s, est. speed input: 229.39 toks/s, output: 3670.25 toks/s]
Processed prompts: 100%|| 256/256 [00:17<00:00, 14.34it/s, est. speed input: 229.39 toks/s, output: 3670.25 toks/s]
[rank0]:[W127 17:50:31.214860511 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:50:33
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-3B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:50:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:50:37 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2893955) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2893955) WARNING 01-27 17:51:12 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.73 requests/s, 4007.27 total tokens/s, 3771.55 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:50:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:50:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:50:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:50:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:50:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:50:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:50:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:50:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:50:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:50:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:50:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:50:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:50:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:50:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:50:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:50:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:50:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:50:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2893955) [2026-01-27 17:50:41] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2893955) [2026-01-27 17:50:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2893955) [2026-01-27 17:50:41] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2893955) [2026-01-27 17:50:41] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2893955) [2026-01-27 17:50:41] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2893955) [2026-01-27 17:50:41] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2893955) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2893955) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.64s/it]
(EngineCore_DP0 pid=2893955) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.64s/it]
(EngineCore_DP0 pid=2893955) 
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=2893955) 2026-01-27 17:51:12,104 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2893955) 2026-01-27 17:51:12,115 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 5949.30it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:25<3:36:58, 25.48s/it, est. speed input: 0.63 toks/s, output: 10.05 toks/s]
Processed prompts:   0%|          | 2/512 [00:25<1:30:25, 10.64s/it, est. speed input: 1.24 toks/s, output: 19.90 toks/s]
Processed prompts:   6%|         | 33/512 [00:25<03:12,  2.49it/s, est. speed input: 20.33 toks/s, output: 325.20 toks/s]
Processed prompts:  12%|        | 63/512 [00:26<01:19,  5.67it/s, est. speed input: 38.46 toks/s, output: 615.39 toks/s]
Processed prompts:  18%|        | 91/512 [00:26<00:43,  9.67it/s, est. speed input: 55.11 toks/s, output: 881.74 toks/s]
Processed prompts:  23%|       | 117/512 [00:26<00:27, 14.51it/s, est. speed input: 70.25 toks/s, output: 1124.00 toks/s]
Processed prompts:  28%|       | 141/512 [00:26<00:17, 20.74it/s, est. speed input: 84.27 toks/s, output: 1348.35 toks/s]
Processed prompts:  32%|      | 164/512 [00:26<00:12, 28.59it/s, est. speed input: 97.58 toks/s, output: 1561.33 toks/s]
Processed prompts:  36%|      | 185/512 [00:27<00:08, 37.59it/s, est. speed input: 109.59 toks/s, output: 1753.39 toks/s]
Processed prompts:  40%|      | 205/512 [00:27<00:06, 48.30it/s, est. speed input: 120.94 toks/s, output: 1934.98 toks/s]
Processed prompts:  44%|     | 224/512 [00:27<00:04, 60.46it/s, est. speed input: 131.63 toks/s, output: 2106.06 toks/s]
Processed prompts:  50%|     | 258/512 [00:27<00:03, 82.76it/s, est. speed input: 150.53 toks/s, output: 2408.41 toks/s]
Processed prompts:  56%|    | 288/512 [00:27<00:02, 101.70it/s, est. speed input: 167.02 toks/s, output: 2672.34 toks/s]
Processed prompts:  62%|   | 315/512 [00:27<00:01, 113.80it/s, est. speed input: 181.54 toks/s, output: 2904.64 toks/s]
Processed prompts:  66%|   | 338/512 [00:27<00:01, 120.42it/s, est. speed input: 193.67 toks/s, output: 3098.74 toks/s]
Processed prompts:  70%|   | 358/512 [00:28<00:01, 124.92it/s, est. speed input: 204.09 toks/s, output: 3265.45 toks/s]
Processed prompts:  73%|  | 376/512 [00:28<00:01, 129.31it/s, est. speed input: 213.41 toks/s, output: 3414.54 toks/s]
Processed prompts:  77%|  | 392/512 [00:28<00:00, 130.27it/s, est. speed input: 221.55 toks/s, output: 3544.76 toks/s]
Processed prompts:  79%|  | 407/512 [00:28<00:00, 131.43it/s, est. speed input: 229.13 toks/s, output: 3666.04 toks/s]
Processed prompts:  82%| | 422/512 [00:28<00:00, 133.78it/s, est. speed input: 236.69 toks/s, output: 3786.98 toks/s]
Processed prompts:  85%| | 437/512 [00:28<00:00, 125.17it/s, est. speed input: 243.89 toks/s, output: 3902.32 toks/s]
Processed prompts:  88%| | 451/512 [00:28<00:00, 110.50it/s, est. speed input: 250.24 toks/s, output: 4003.83 toks/s]
Processed prompts:  90%| | 463/512 [00:28<00:00, 103.98it/s, est. speed input: 255.69 toks/s, output: 4091.00 toks/s]
Processed prompts:  93%|| 474/512 [00:29<00:00, 88.66it/s, est. speed input: 260.13 toks/s, output: 4162.13 toks/s] 
Processed prompts:  95%|| 484/512 [00:29<00:00, 75.37it/s, est. speed input: 263.85 toks/s, output: 4221.56 toks/s]
Processed prompts:  96%|| 493/512 [00:29<00:00, 60.65it/s, est. speed input: 266.55 toks/s, output: 4264.80 toks/s]
Processed prompts:  98%|| 500/512 [00:34<00:01,  6.21it/s, est. speed input: 231.27 toks/s, output: 3700.27 toks/s]
Processed prompts: 100%|| 512/512 [00:34<00:00,  6.21it/s, est. speed input: 236.32 toks/s, output: 3781.05 toks/s]
Processed prompts: 100%|| 512/512 [00:34<00:00, 14.77it/s, est. speed input: 236.32 toks/s, output: 3781.05 toks/s]
[rank0]:[W127 17:51:47.800520766 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 18:50:31
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:50:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:50:36 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2954881) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2954881) WARNING 01-27 18:52:03 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.47 requests/s, 1216.15 total tokens/s, 1144.61 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 18:50:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:50:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:50:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:50:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:50:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:50:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:50:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:50:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:50:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:50:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:50:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:50:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:50:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:50:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:50:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:50:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2954881) [2026-01-27 18:50:41] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2954881) [2026-01-27 18:50:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2954881) [2026-01-27 18:50:41] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2954881) [2026-01-27 18:50:41] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2954881) [2026-01-27 18:50:41] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2954881) [2026-01-27 18:50:41] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2954881) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2954881) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.79s/it]
(EngineCore_DP0 pid=2954881) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:05<00:00, 33.35s/it]
(EngineCore_DP0 pid=2954881) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:05<00:00, 32.51s/it]
(EngineCore_DP0 pid=2954881) 
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=2954881) 2026-01-27 18:51:55,239 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2954881) 2026-01-27 18:51:55,569 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|         | 1/64 [00:00<00:13,  4.62it/s]
Adding requests:   5%|         | 3/64 [00:00<00:06,  9.12it/s]
Adding requests:   9%|         | 6/64 [00:00<00:03, 14.81it/s]
Adding requests:  14%|        | 9/64 [00:00<00:02, 18.41it/s]
Adding requests:  20%|        | 13/64 [00:00<00:02, 23.57it/s]
Adding requests:  28%|       | 18/64 [00:00<00:01, 30.79it/s]
Adding requests:  42%|     | 27/64 [00:00<00:00, 46.64it/s]
Adding requests:  61%|    | 39/64 [00:01<00:00, 66.54it/s]
Adding requests:  83%| | 53/64 [00:01<00:00, 85.82it/s]
Adding requests: 100%|| 64/64 [00:01<00:00, 53.42it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:12<12:56, 12.32s/it, est. speed input: 1.30 toks/s, output: 20.78 toks/s]
Processed prompts:   9%|         | 6/64 [00:12<01:29,  1.54s/it, est. speed input: 7.70 toks/s, output: 123.21 toks/s]
Processed prompts:  16%|        | 10/64 [00:12<00:42,  1.28it/s, est. speed input: 12.73 toks/s, output: 203.67 toks/s]
Processed prompts:  22%|       | 14/64 [00:12<00:23,  2.12it/s, est. speed input: 17.60 toks/s, output: 281.66 toks/s]
Processed prompts:  28%|       | 18/64 [00:12<00:14,  3.28it/s, est. speed input: 22.46 toks/s, output: 359.30 toks/s]
Processed prompts:  62%|   | 40/64 [00:12<00:01, 12.15it/s, est. speed input: 49.36 toks/s, output: 789.77 toks/s]
Processed prompts: 100%|| 64/64 [00:13<00:00, 12.15it/s, est. speed input: 78.71 toks/s, output: 1259.44 toks/s]
Processed prompts: 100%|| 64/64 [00:13<00:00,  4.92it/s, est. speed input: 78.71 toks/s, output: 1259.44 toks/s]
[rank0]:[W127 18:52:18.906054925 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 18:52:35
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:52:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:52:40 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2957131) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2957131) WARNING 01-27 18:53:58 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.77 requests/s, 2112.84 total tokens/s, 1988.55 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 18:52:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:52:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:52:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:52:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:52:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:52:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:52:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:52:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:52:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:52:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:52:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:52:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:52:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:52:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:52:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:52:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2957131) [2026-01-27 18:52:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2957131) [2026-01-27 18:52:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2957131) [2026-01-27 18:52:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2957131) [2026-01-27 18:52:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2957131) [2026-01-27 18:52:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2957131) [2026-01-27 18:52:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2957131) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2957131) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:28<00:28, 28.29s/it]
(EngineCore_DP0 pid=2957131) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:05<00:00, 33.28s/it]
(EngineCore_DP0 pid=2957131) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:05<00:00, 32.53s/it]
(EngineCore_DP0 pid=2957131) 
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=2957131) 2026-01-27 18:53:57,747 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2957131) 2026-01-27 18:53:57,763 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 10988.17it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:15<33:07, 15.65s/it, est. speed input: 1.02 toks/s, output: 16.36 toks/s]
Processed prompts:  16%|        | 21/128 [00:15<00:57,  1.86it/s, est. speed input: 21.24 toks/s, output: 339.78 toks/s]
Processed prompts:  39%|      | 50/128 [00:15<00:14,  5.47it/s, est. speed input: 50.06 toks/s, output: 800.95 toks/s]
Processed prompts:  59%|    | 76/128 [00:16<00:05,  9.93it/s, est. speed input: 75.52 toks/s, output: 1208.30 toks/s]
Processed prompts:  77%|  | 99/128 [00:16<00:01, 15.18it/s, est. speed input: 97.53 toks/s, output: 1560.54 toks/s]
Processed prompts:  93%|| 119/128 [00:16<00:00, 21.29it/s, est. speed input: 116.43 toks/s, output: 1862.96 toks/s]
Processed prompts: 100%|| 128/128 [00:16<00:00, 21.29it/s, est. speed input: 124.38 toks/s, output: 1990.11 toks/s]
Processed prompts: 100%|| 128/128 [00:16<00:00,  7.77it/s, est. speed input: 124.38 toks/s, output: 1990.11 toks/s]
[rank0]:[W127 18:54:15.201839868 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 18:54:17
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:54:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:54:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2959027) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2959027) WARNING 01-27 18:55:39 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.44 requests/s, 2838.36 total tokens/s, 2671.40 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 18:54:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:54:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:54:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:54:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:54:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:54:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:54:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:54:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:54:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:54:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:54:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:54:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:54:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:54:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:54:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:54:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2959027) [2026-01-27 18:54:27] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2959027) [2026-01-27 18:54:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2959027) [2026-01-27 18:54:27] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2959027) [2026-01-27 18:54:27] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2959027) [2026-01-27 18:54:27] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2959027) [2026-01-27 18:54:27] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2959027) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2959027) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:28<00:28, 28.36s/it]
(EngineCore_DP0 pid=2959027) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:04<00:00, 32.99s/it]
(EngineCore_DP0 pid=2959027) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:04<00:00, 32.30s/it]
(EngineCore_DP0 pid=2959027) 
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=2959027) 2026-01-27 18:55:38,489 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2959027) 2026-01-27 18:55:38,507 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 11411.37it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:21<1:30:07, 21.21s/it, est. speed input: 0.75 toks/s, output: 12.07 toks/s]
Processed prompts:   2%|         | 4/256 [00:21<16:58,  4.04s/it, est. speed input: 3.00 toks/s, output: 48.04 toks/s]  
Processed prompts:   8%|         | 20/256 [00:21<02:14,  1.76it/s, est. speed input: 14.94 toks/s, output: 238.99 toks/s]
Processed prompts:  14%|        | 35/256 [00:21<00:59,  3.73it/s, est. speed input: 26.01 toks/s, output: 416.16 toks/s]
Processed prompts:  19%|        | 49/256 [00:21<00:33,  6.25it/s, est. speed input: 36.21 toks/s, output: 579.32 toks/s]
Processed prompts:  30%|       | 76/256 [00:21<00:14, 12.79it/s, est. speed input: 55.69 toks/s, output: 890.97 toks/s]
Processed prompts:  39%|      | 99/256 [00:22<00:07, 19.73it/s, est. speed input: 71.90 toks/s, output: 1150.43 toks/s]
Processed prompts:  46%|     | 119/256 [00:22<00:05, 27.33it/s, est. speed input: 85.82 toks/s, output: 1373.10 toks/s]
Processed prompts:  54%|    | 137/256 [00:22<00:03, 35.11it/s, est. speed input: 98.06 toks/s, output: 1568.96 toks/s]
Processed prompts:  60%|    | 153/256 [00:22<00:02, 43.08it/s, est. speed input: 108.79 toks/s, output: 1740.68 toks/s]
Processed prompts:  65%|   | 167/256 [00:22<00:01, 50.50it/s, est. speed input: 118.03 toks/s, output: 1888.40 toks/s]
Processed prompts:  70%|   | 179/256 [00:22<00:01, 56.39it/s, est. speed input: 125.77 toks/s, output: 2012.24 toks/s]
Processed prompts:  74%|  | 190/256 [00:22<00:01, 62.86it/s, est. speed input: 132.85 toks/s, output: 2125.58 toks/s]
Processed prompts:  79%|  | 201/256 [00:23<00:00, 65.89it/s, est. speed input: 139.67 toks/s, output: 2234.65 toks/s]
Processed prompts:  82%| | 211/256 [00:23<00:00, 66.21it/s, est. speed input: 145.67 toks/s, output: 2330.77 toks/s]
Processed prompts:  86%| | 220/256 [00:23<00:00, 65.14it/s, est. speed input: 150.94 toks/s, output: 2415.06 toks/s]
Processed prompts:  89%| | 228/256 [00:23<00:00, 60.82it/s, est. speed input: 155.36 toks/s, output: 2485.82 toks/s]
Processed prompts:  92%|| 236/256 [00:23<00:00, 55.82it/s, est. speed input: 159.60 toks/s, output: 2553.64 toks/s]
Processed prompts:  95%|| 243/256 [00:23<00:00, 49.16it/s, est. speed input: 162.98 toks/s, output: 2607.72 toks/s]
Processed prompts:  97%|| 249/256 [00:24<00:00, 38.90it/s, est. speed input: 165.18 toks/s, output: 2642.83 toks/s]
Processed prompts:  99%|| 254/256 [00:24<00:00, 34.29it/s, est. speed input: 167.04 toks/s, output: 2672.64 toks/s]
Processed prompts: 100%|| 256/256 [00:24<00:00, 34.29it/s, est. speed input: 167.12 toks/s, output: 2673.97 toks/s]
Processed prompts: 100%|| 256/256 [00:24<00:00, 10.45it/s, est. speed input: 167.12 toks/s, output: 2673.97 toks/s]
[rank0]:[W127 18:56:04.209391283 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 18:56:07
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-7B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:56:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:56:13 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2961071) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2961071) WARNING 01-27 18:57:30 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.42 requests/s, 2833.17 total tokens/s, 2666.51 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 18:56:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:56:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:56:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:56:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:56:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:56:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:56:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:56:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:56:16] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:56:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:56:16] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:56:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:56:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:56:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:56:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:56:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2961071) [2026-01-27 18:56:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2961071) [2026-01-27 18:56:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2961071) [2026-01-27 18:56:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2961071) [2026-01-27 18:56:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2961071) [2026-01-27 18:56:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2961071) [2026-01-27 18:56:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2961071) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2961071) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.27s/it]
(EngineCore_DP0 pid=2961071) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:04<00:00, 33.14s/it]
(EngineCore_DP0 pid=2961071) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:04<00:00, 32.26s/it]
(EngineCore_DP0 pid=2961071) 
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=2961071) 2026-01-27 18:57:29,101 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2961071) 2026-01-27 18:57:29,118 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 12130.96it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:34<4:58:04, 35.00s/it, est. speed input: 0.46 toks/s, output: 7.31 toks/s]
Processed prompts:   0%|          | 2/512 [00:35<2:04:11, 14.61s/it, est. speed input: 0.91 toks/s, output: 14.49 toks/s]
Processed prompts:   6%|         | 33/512 [00:35<04:23,  1.82it/s, est. speed input: 14.81 toks/s, output: 236.88 toks/s]
Processed prompts:  12%|        | 63/512 [00:35<01:48,  4.13it/s, est. speed input: 28.02 toks/s, output: 448.35 toks/s]
Processed prompts:  18%|        | 91/512 [00:36<00:59,  7.05it/s, est. speed input: 40.15 toks/s, output: 642.41 toks/s]
Processed prompts:  23%|       | 117/512 [00:36<00:37, 10.60it/s, est. speed input: 51.20 toks/s, output: 819.22 toks/s]
Processed prompts:  28%|       | 141/512 [00:36<00:24, 15.23it/s, est. speed input: 61.46 toks/s, output: 983.35 toks/s]
Processed prompts:  32%|      | 164/512 [00:36<00:16, 21.18it/s, est. speed input: 71.23 toks/s, output: 1139.68 toks/s]
Processed prompts:  36%|      | 185/512 [00:36<00:11, 28.19it/s, est. speed input: 80.07 toks/s, output: 1281.09 toks/s]
Processed prompts:  40%|      | 205/512 [00:37<00:08, 36.68it/s, est. speed input: 88.43 toks/s, output: 1414.86 toks/s]
Processed prompts:  44%|     | 224/512 [00:37<00:06, 46.51it/s, est. speed input: 96.31 toks/s, output: 1540.96 toks/s]
Processed prompts:  47%|     | 242/512 [00:37<00:04, 57.50it/s, est. speed input: 103.73 toks/s, output: 1659.67 toks/s]
Processed prompts:  50%|     | 258/512 [00:37<00:03, 67.30it/s, est. speed input: 110.23 toks/s, output: 1763.72 toks/s]
Processed prompts:  53%|    | 273/512 [00:37<00:03, 76.68it/s, est. speed input: 116.28 toks/s, output: 1860.48 toks/s]
Processed prompts:  59%|    | 302/512 [00:37<00:02, 96.64it/s, est. speed input: 127.99 toks/s, output: 2047.79 toks/s]
Processed prompts:  62%|   | 317/512 [00:37<00:01, 104.58it/s, est. speed input: 133.97 toks/s, output: 2143.54 toks/s]
Processed prompts:  66%|   | 338/512 [00:38<00:01, 105.20it/s, est. speed input: 142.11 toks/s, output: 2273.71 toks/s]
Processed prompts:  70%|   | 358/512 [00:38<00:01, 109.44it/s, est. speed input: 149.86 toks/s, output: 2397.80 toks/s]
Processed prompts:  73%|  | 376/512 [00:38<00:01, 110.77it/s, est. speed input: 156.75 toks/s, output: 2508.03 toks/s]
Processed prompts:  77%|  | 392/512 [00:38<00:01, 108.95it/s, est. speed input: 162.77 toks/s, output: 2604.33 toks/s]
Processed prompts:  79%|  | 406/512 [00:38<00:01, 105.69it/s, est. speed input: 167.95 toks/s, output: 2687.26 toks/s]
Processed prompts:  82%| | 418/512 [00:38<00:00, 99.67it/s, est. speed input: 172.28 toks/s, output: 2756.49 toks/s] 
Processed prompts:  84%| | 429/512 [00:38<00:00, 99.54it/s, est. speed input: 176.31 toks/s, output: 2820.96 toks/s]
Processed prompts:  86%| | 440/512 [00:39<00:00, 92.34it/s, est. speed input: 180.16 toks/s, output: 2882.56 toks/s]
Processed prompts:  88%| | 450/512 [00:39<00:00, 78.04it/s, est. speed input: 183.37 toks/s, output: 2933.87 toks/s]
Processed prompts:  90%| | 459/512 [00:39<00:00, 72.58it/s, est. speed input: 186.31 toks/s, output: 2981.04 toks/s]
Processed prompts:  91%| | 467/512 [00:39<00:00, 71.37it/s, est. speed input: 189.00 toks/s, output: 3023.92 toks/s]
Processed prompts:  93%|| 475/512 [00:39<00:00, 60.62it/s, est. speed input: 191.31 toks/s, output: 3060.88 toks/s]
Processed prompts:  94%|| 482/512 [00:39<00:00, 49.48it/s, est. speed input: 193.03 toks/s, output: 3088.53 toks/s]
Processed prompts:  95%|| 488/512 [00:40<00:00, 38.62it/s, est. speed input: 194.11 toks/s, output: 3105.81 toks/s]
Processed prompts:  96%|| 493/512 [00:40<00:00, 33.65it/s, est. speed input: 195.04 toks/s, output: 3120.58 toks/s]
Processed prompts:  97%|| 497/512 [00:40<00:00, 30.87it/s, est. speed input: 195.78 toks/s, output: 3132.43 toks/s]
Processed prompts:  98%|| 501/512 [00:49<00:05,  2.07it/s, est. speed input: 163.50 toks/s, output: 2615.93 toks/s]
Processed prompts: 100%|| 512/512 [00:49<00:00,  2.07it/s, est. speed input: 166.80 toks/s, output: 2668.87 toks/s]
Processed prompts: 100%|| 512/512 [00:49<00:00, 10.43it/s, est. speed input: 166.80 toks/s, output: 2668.87 toks/s]
[rank0]:[W127 18:58:19.752029078 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 20:49:26
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:49:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:49:33 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3069721) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3069721) WARNING 01-27 20:51:53 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.47 requests/s, 670.60 total tokens/s, 631.15 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 20:49:33] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:49:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:49:33] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:49:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:49:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:49:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:49:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:49:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:49:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:49:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:49:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:49:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:49:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:49:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:49:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:49:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3069721) [2026-01-27 20:49:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3069721) [2026-01-27 20:49:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3069721) [2026-01-27 20:49:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3069721) [2026-01-27 20:49:38] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3069721) [2026-01-27 20:49:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3069721) [2026-01-27 20:49:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3069721) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3069721) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.55s/it]
(EngineCore_DP0 pid=3069721) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:40<00:44, 22.16s/it]
(EngineCore_DP0 pid=3069721) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:16<00:28, 28.47s/it]
(EngineCore_DP0 pid=3069721) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:57<00:00, 33.57s/it]
(EngineCore_DP0 pid=3069721) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:57<00:00, 29.40s/it]
(EngineCore_DP0 pid=3069721) 
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=3069721) 2026-01-27 20:51:45,989 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3069721) 2026-01-27 20:51:46,255 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|         | 1/64 [00:00<00:13,  4.82it/s]
Adding requests:   5%|         | 3/64 [00:00<00:06, 10.10it/s]
Adding requests:   9%|         | 6/64 [00:00<00:03, 16.15it/s]
Adding requests:  16%|        | 10/64 [00:00<00:02, 22.88it/s]
Adding requests:  20%|        | 13/64 [00:00<00:02, 25.01it/s]
Adding requests:  31%|      | 20/64 [00:00<00:01, 38.15it/s]
Adding requests:  48%|     | 31/64 [00:00<00:00, 58.39it/s]
Adding requests:  69%|   | 44/64 [00:00<00:00, 77.87it/s]
Adding requests:  98%|| 63/64 [00:01<00:00, 109.84it/s]
Adding requests: 100%|| 64/64 [00:01<00:00, 58.85it/s] 

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:24<25:28, 24.26s/it, est. speed input: 0.66 toks/s, output: 10.55 toks/s]
Processed prompts:   3%|         | 2/64 [00:24<10:23, 10.05s/it, est. speed input: 1.31 toks/s, output: 21.02 toks/s]
Processed prompts:  11%|         | 7/64 [00:24<01:51,  1.95s/it, est. speed input: 4.58 toks/s, output: 73.26 toks/s]
Processed prompts:  36%|      | 23/64 [00:24<00:17,  2.38it/s, est. speed input: 14.97 toks/s, output: 239.57 toks/s]
Processed prompts:  59%|    | 38/64 [00:24<00:05,  4.79it/s, est. speed input: 24.63 toks/s, output: 394.01 toks/s]
Processed prompts: 100%|| 64/64 [00:24<00:00,  4.79it/s, est. speed input: 41.32 toks/s, output: 661.18 toks/s]
Processed prompts: 100%|| 64/64 [00:24<00:00,  2.58it/s, est. speed input: 41.32 toks/s, output: 661.18 toks/s]
[rank0]:[W127 20:52:20.348357616 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 20:52:35
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:52:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:52:41 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3072525) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3072525) WARNING 01-27 20:54:55 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.07 requests/s, 1106.56 total tokens/s, 1041.47 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 20:52:41] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:52:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:52:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:52:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:52:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:52:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:52:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:52:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:52:45] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:52:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:52:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:52:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:52:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:52:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:52:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:52:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3072525) [2026-01-27 20:52:46] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3072525) [2026-01-27 20:52:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3072525) [2026-01-27 20:52:46] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3072525) [2026-01-27 20:52:46] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3072525) [2026-01-27 20:52:46] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3072525) [2026-01-27 20:52:46] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3072525) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3072525) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.34s/it]
(EngineCore_DP0 pid=3072525) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:41<00:45, 22.75s/it]
(EngineCore_DP0 pid=3072525) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:17<00:28, 28.87s/it]
(EngineCore_DP0 pid=3072525) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 33.97s/it]
(EngineCore_DP0 pid=3072525) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 29.78s/it]
(EngineCore_DP0 pid=3072525) 
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=3072525) 2026-01-27 20:54:54,334 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3072525) 2026-01-27 20:54:54,368 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   1%|          | 1/128 [00:00<00:27,  4.69it/s]
Adding requests:   2%|         | 3/128 [00:00<00:13,  9.29it/s]
Adding requests:   5%|         | 6/128 [00:00<00:07, 15.55it/s]
Adding requests:   7%|         | 9/128 [00:00<00:05, 19.91it/s]
Adding requests:  10%|         | 13/128 [00:00<00:04, 26.06it/s]
Adding requests:  15%|        | 19/128 [00:00<00:03, 36.02it/s]
Adding requests:  23%|       | 29/128 [00:00<00:01, 53.80it/s]
Adding requests:  31%|      | 40/128 [00:00<00:01, 70.10it/s]
Adding requests:  43%|     | 55/128 [00:01<00:00, 92.71it/s]
Adding requests:  53%|    | 68/128 [00:01<00:00, 103.31it/s]
Adding requests:  66%|   | 85/128 [00:01<00:00, 121.82it/s]
Adding requests:  84%| | 107/128 [00:01<00:00, 149.92it/s]
Adding requests: 100%|| 128/128 [00:01<00:00, 87.46it/s] 

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:28<59:52, 28.29s/it, est. speed input: 0.57 toks/s, output: 9.05 toks/s]
Processed prompts:   2%|         | 2/128 [00:28<24:36, 11.72s/it, est. speed input: 1.13 toks/s, output: 18.02 toks/s]
Processed prompts:   5%|         | 6/128 [00:28<05:30,  2.71s/it, est. speed input: 3.36 toks/s, output: 53.81 toks/s]
Processed prompts:   9%|         | 11/128 [00:28<02:15,  1.15s/it, est. speed input: 6.14 toks/s, output: 98.27 toks/s]
Processed prompts:  12%|        | 15/128 [00:28<01:19,  1.41it/s, est. speed input: 8.34 toks/s, output: 133.45 toks/s]
Processed prompts:  16%|        | 20/128 [00:28<00:45,  2.35it/s, est. speed input: 11.06 toks/s, output: 177.02 toks/s]
Processed prompts:  23%|       | 29/128 [00:29<00:21,  4.71it/s, est. speed input: 15.98 toks/s, output: 255.64 toks/s]
Processed prompts:  34%|      | 44/128 [00:29<00:08, 10.06it/s, est. speed input: 24.15 toks/s, output: 386.36 toks/s]
Processed prompts:  45%|     | 58/128 [00:29<00:04, 16.50it/s, est. speed input: 31.71 toks/s, output: 507.38 toks/s]
Processed prompts:  55%|    | 71/128 [00:29<00:02, 23.35it/s, est. speed input: 38.63 toks/s, output: 618.02 toks/s]
Processed prompts:  65%|   | 83/128 [00:29<00:01, 31.06it/s, est. speed input: 44.98 toks/s, output: 719.60 toks/s]
Processed prompts:  82%| | 105/128 [00:29<00:00, 46.23it/s, est. speed input: 56.51 toks/s, output: 904.21 toks/s]
Processed prompts:  97%|| 124/128 [00:29<00:00, 58.65it/s, est. speed input: 66.36 toks/s, output: 1061.77 toks/s]
Processed prompts: 100%|| 128/128 [00:29<00:00, 58.65it/s, est. speed input: 68.50 toks/s, output: 1096.01 toks/s]
Processed prompts: 100%|| 128/128 [00:29<00:00,  4.28it/s, est. speed input: 68.50 toks/s, output: 1096.01 toks/s]
[rank0]:[W127 20:55:27.499528907 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 20:55:42
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:55:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:55:48 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3075302) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3075302) WARNING 01-27 20:58:12 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.57 requests/s, 1514.28 total tokens/s, 1425.21 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 20:55:48] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:55:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:55:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:55:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:55:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:55:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:55:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:55:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:55:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:55:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:55:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:55:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:55:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:55:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:55:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:55:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3075302) [2026-01-27 20:55:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3075302) [2026-01-27 20:55:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3075302) [2026-01-27 20:55:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3075302) [2026-01-27 20:55:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3075302) [2026-01-27 20:55:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3075302) [2026-01-27 20:55:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3075302) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3075302) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.52s/it]
(EngineCore_DP0 pid=3075302) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:41<00:45, 22.77s/it]
(EngineCore_DP0 pid=3075302) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:17<00:28, 28.93s/it]
(EngineCore_DP0 pid=3075302) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 34.09s/it]
(EngineCore_DP0 pid=3075302) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 29.88s/it]
(EngineCore_DP0 pid=3075302) 
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=3075302) 2026-01-27 20:58:03,282 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3075302) 2026-01-27 20:58:03,681 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<01:06,  3.84it/s]
Adding requests:   1%|          | 3/256 [00:00<00:30,  8.27it/s]
Adding requests:   2%|         | 6/256 [00:00<00:18, 13.83it/s]
Adding requests:   4%|         | 9/256 [00:00<00:13, 18.07it/s]
Adding requests:   5%|         | 13/256 [00:00<00:10, 23.89it/s]
Adding requests:   7%|         | 19/256 [00:00<00:07, 33.19it/s]
Adding requests:  11%|         | 28/256 [00:00<00:04, 48.08it/s]
Adding requests:  15%|        | 38/256 [00:01<00:03, 62.46it/s]
Adding requests:  20%|        | 51/256 [00:01<00:02, 81.11it/s]
Adding requests:  26%|       | 67/256 [00:01<00:01, 102.54it/s]
Adding requests:  33%|      | 84/256 [00:01<00:01, 120.65it/s]
Adding requests:  42%|     | 107/256 [00:01<00:00, 151.73it/s]
Adding requests:  52%|    | 133/256 [00:01<00:00, 180.63it/s]
Adding requests:  62%|   | 158/256 [00:01<00:00, 126.75it/s]
Adding requests:  73%|  | 186/256 [00:01<00:00, 157.46it/s]
Adding requests:  84%| | 216/256 [00:02<00:00, 189.63it/s]
Adding requests: 100%|| 256/256 [00:02<00:00, 239.27it/s]
Adding requests: 100%|| 256/256 [00:02<00:00, 116.69it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:37<2:39:08, 37.45s/it, est. speed input: 0.43 toks/s, output: 6.84 toks/s]
Processed prompts:   1%|          | 2/256 [00:37<1:05:41, 15.52s/it, est. speed input: 0.85 toks/s, output: 13.61 toks/s]
Processed prompts:   3%|         | 7/256 [00:37<12:29,  3.01s/it, est. speed input: 2.96 toks/s, output: 47.42 toks/s]  
Processed prompts:   4%|         | 11/256 [00:37<06:28,  1.59s/it, est. speed input: 4.64 toks/s, output: 74.18 toks/s]
Processed prompts:   7%|         | 17/256 [00:38<03:11,  1.25it/s, est. speed input: 7.13 toks/s, output: 114.02 toks/s]
Processed prompts:   9%|         | 24/256 [00:38<01:42,  2.25it/s, est. speed input: 10.02 toks/s, output: 160.31 toks/s]
Processed prompts:  14%|        | 35/256 [00:38<00:50,  4.39it/s, est. speed input: 14.54 toks/s, output: 232.59 toks/s]
Processed prompts:  19%|        | 49/256 [00:38<00:25,  8.07it/s, est. speed input: 20.27 toks/s, output: 324.30 toks/s]
Processed prompts:  25%|       | 63/256 [00:38<00:15, 12.84it/s, est. speed input: 25.96 toks/s, output: 415.35 toks/s]
Processed prompts:  30%|       | 76/256 [00:39<00:09, 18.10it/s, est. speed input: 31.18 toks/s, output: 498.85 toks/s]
Processed prompts:  34%|      | 88/256 [00:39<00:07, 23.67it/s, est. speed input: 35.94 toks/s, output: 575.11 toks/s]
Processed prompts:  39%|      | 99/256 [00:39<00:05, 29.12it/s, est. speed input: 40.27 toks/s, output: 644.25 toks/s]
Processed prompts:  43%|     | 109/256 [00:39<00:04, 34.97it/s, est. speed input: 44.18 toks/s, output: 706.90 toks/s]
Processed prompts:  46%|     | 119/256 [00:39<00:03, 39.63it/s, est. speed input: 48.03 toks/s, output: 768.51 toks/s]
Processed prompts:  50%|     | 128/256 [00:39<00:02, 44.74it/s, est. speed input: 51.50 toks/s, output: 823.93 toks/s]
Processed prompts:  54%|    | 137/256 [00:39<00:02, 49.80it/s, est. speed input: 54.94 toks/s, output: 879.06 toks/s]
Processed prompts:  57%|    | 145/256 [00:40<00:02, 53.20it/s, est. speed input: 57.97 toks/s, output: 927.58 toks/s]
Processed prompts:  60%|    | 153/256 [00:40<00:01, 56.18it/s, est. speed input: 60.99 toks/s, output: 975.81 toks/s]
Processed prompts:  62%|   | 160/256 [00:40<00:01, 57.22it/s, est. speed input: 63.60 toks/s, output: 1017.52 toks/s]
Processed prompts:  65%|   | 167/256 [00:40<00:01, 58.54it/s, est. speed input: 66.19 toks/s, output: 1059.09 toks/s]
Processed prompts:  68%|   | 174/256 [00:40<00:01, 58.36it/s, est. speed input: 68.76 toks/s, output: 1100.19 toks/s]
Processed prompts:  71%|   | 181/256 [00:40<00:01, 60.99it/s, est. speed input: 71.35 toks/s, output: 1141.58 toks/s]
Processed prompts:  73%|  | 188/256 [00:40<00:01, 61.85it/s, est. speed input: 73.91 toks/s, output: 1182.55 toks/s]
Processed prompts:  76%|  | 195/256 [00:40<00:01, 46.00it/s, est. speed input: 76.20 toks/s, output: 1219.18 toks/s]
Processed prompts:  79%|  | 202/256 [00:41<00:01, 42.30it/s, est. speed input: 78.56 toks/s, output: 1256.88 toks/s]
Processed prompts:  81%|  | 207/256 [00:41<00:01, 43.29it/s, est. speed input: 80.29 toks/s, output: 1284.66 toks/s]
Processed prompts:  83%| | 212/256 [00:41<00:01, 42.84it/s, est. speed input: 81.99 toks/s, output: 1311.87 toks/s]
Processed prompts:  85%| | 217/256 [00:41<00:01, 37.06it/s, est. speed input: 83.55 toks/s, output: 1336.78 toks/s]
Processed prompts:  87%| | 222/256 [00:41<00:01, 33.64it/s, est. speed input: 85.09 toks/s, output: 1361.52 toks/s]
Processed prompts:  88%| | 226/256 [00:41<00:00, 34.28it/s, est. speed input: 86.40 toks/s, output: 1382.42 toks/s]
Processed prompts:  90%| | 230/256 [00:42<00:00, 30.48it/s, est. speed input: 87.57 toks/s, output: 1401.05 toks/s]
Processed prompts:  91%|| 234/256 [00:42<00:00, 28.09it/s, est. speed input: 88.72 toks/s, output: 1419.57 toks/s]
Processed prompts:  93%|| 238/256 [00:42<00:00, 26.78it/s, est. speed input: 89.88 toks/s, output: 1438.12 toks/s]
Processed prompts:  95%|| 242/256 [00:42<00:00, 25.99it/s, est. speed input: 91.04 toks/s, output: 1456.60 toks/s]
Processed prompts:  96%|| 245/256 [00:42<00:00, 20.62it/s, est. speed input: 91.64 toks/s, output: 1466.20 toks/s]
Processed prompts:  97%|| 248/256 [00:43<00:00, 17.83it/s, est. speed input: 92.25 toks/s, output: 1475.99 toks/s]
Processed prompts:  98%|| 250/256 [00:43<00:00, 16.50it/s, est. speed input: 92.65 toks/s, output: 1482.40 toks/s]
Processed prompts:  98%|| 252/256 [00:43<00:00, 15.19it/s, est. speed input: 93.02 toks/s, output: 1488.40 toks/s]
Processed prompts:  99%|| 254/256 [00:43<00:00, 14.57it/s, est. speed input: 93.43 toks/s, output: 1494.84 toks/s]
Processed prompts: 100%|| 256/256 [00:43<00:00, 13.42it/s, est. speed input: 93.77 toks/s, output: 1500.27 toks/s]
Processed prompts: 100%|| 256/256 [00:43<00:00, 13.42it/s, est. speed input: 93.77 toks/s, output: 1500.27 toks/s]
Processed prompts: 100%|| 256/256 [00:43<00:00,  5.86it/s, est. speed input: 93.77 toks/s, output: 1500.27 toks/s]
[rank0]:[W127 20:59:00.096393281 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 20:59:16
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-14B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:59:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:59:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3078475) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3078475) WARNING 01-27 21:01:36 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.61 requests/s, 1525.73 total tokens/s, 1435.98 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 20:59:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:59:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:59:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:59:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:59:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:59:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:59:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:59:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:59:26] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:59:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:59:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:59:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:59:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:59:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:59:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:59:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3078475) [2026-01-27 20:59:27] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3078475) [2026-01-27 20:59:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3078475) [2026-01-27 20:59:27] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3078475) [2026-01-27 20:59:27] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3078475) [2026-01-27 20:59:27] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3078475) [2026-01-27 20:59:27] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3078475) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3078475) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.61s/it]
(EngineCore_DP0 pid=3078475) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:41<00:45, 22.63s/it]
(EngineCore_DP0 pid=3078475) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:17<00:28, 28.97s/it]
(EngineCore_DP0 pid=3078475) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 33.98s/it]
(EngineCore_DP0 pid=3078475) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 29.80s/it]
(EngineCore_DP0 pid=3078475) 
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=3078475) 2026-01-27 21:01:35,140 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3078475) 2026-01-27 21:01:35,194 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 11721.05it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [01:04<9:10:51, 64.68s/it, est. speed input: 0.25 toks/s, output: 3.96 toks/s]
Processed prompts:   1%|          | 3/512 [01:05<2:24:35, 17.04s/it, est. speed input: 0.73 toks/s, output: 11.74 toks/s]
Processed prompts:   7%|         | 34/512 [01:06<07:58,  1.00s/it, est. speed input: 8.23 toks/s, output: 131.62 toks/s]
Processed prompts:  12%|        | 63/512 [01:06<03:24,  2.19it/s, est. speed input: 15.09 toks/s, output: 241.45 toks/s]
Processed prompts:  18%|        | 91/512 [01:07<01:52,  3.74it/s, est. speed input: 21.59 toks/s, output: 345.45 toks/s]
Processed prompts:  23%|       | 117/512 [01:08<01:10,  5.60it/s, est. speed input: 27.50 toks/s, output: 440.07 toks/s]
Processed prompts:  28%|       | 141/512 [01:08<00:45,  8.07it/s, est. speed input: 33.02 toks/s, output: 528.36 toks/s]
Processed prompts:  32%|      | 164/512 [01:08<00:30, 11.25it/s, est. speed input: 38.28 toks/s, output: 612.40 toks/s]
Processed prompts:  36%|      | 185/512 [01:08<00:21, 15.02it/s, est. speed input: 43.03 toks/s, output: 688.51 toks/s]
Processed prompts:  40%|      | 205/512 [01:09<00:15, 19.61it/s, est. speed input: 47.53 toks/s, output: 760.49 toks/s]
Processed prompts:  44%|     | 224/512 [01:09<00:11, 24.92it/s, est. speed input: 51.77 toks/s, output: 828.33 toks/s]
Processed prompts:  47%|     | 242/512 [01:09<00:08, 30.75it/s, est. speed input: 55.76 toks/s, output: 892.09 toks/s]
Processed prompts:  50%|     | 258/512 [01:09<00:07, 36.08it/s, est. speed input: 59.25 toks/s, output: 948.05 toks/s]
Processed prompts:  53%|    | 273/512 [01:09<00:05, 41.46it/s, est. speed input: 62.51 toks/s, output: 1000.23 toks/s]
Processed prompts:  56%|    | 288/512 [01:10<00:04, 48.69it/s, est. speed input: 65.80 toks/s, output: 1052.76 toks/s]
Processed prompts:  59%|    | 302/512 [01:10<00:03, 54.10it/s, est. speed input: 68.82 toks/s, output: 1101.13 toks/s]
Processed prompts:  62%|   | 315/512 [01:10<00:03, 57.78it/s, est. speed input: 71.60 toks/s, output: 1145.58 toks/s]
Processed prompts:  64%|   | 327/512 [01:10<00:03, 60.36it/s, est. speed input: 74.14 toks/s, output: 1186.31 toks/s]
Processed prompts:  66%|   | 338/512 [01:10<00:02, 61.64it/s, est. speed input: 76.46 toks/s, output: 1223.32 toks/s]
Processed prompts:  68%|   | 348/512 [01:10<00:02, 64.06it/s, est. speed input: 78.57 toks/s, output: 1257.07 toks/s]
Processed prompts:  70%|   | 358/512 [01:11<00:02, 63.87it/s, est. speed input: 80.65 toks/s, output: 1290.32 toks/s]
Processed prompts:  72%|  | 367/512 [01:11<00:02, 64.74it/s, est. speed input: 82.52 toks/s, output: 1320.28 toks/s]
Processed prompts:  73%|  | 376/512 [01:11<00:02, 62.99it/s, est. speed input: 84.36 toks/s, output: 1349.74 toks/s]
Processed prompts:  75%|  | 384/512 [01:11<00:02, 62.45it/s, est. speed input: 86.00 toks/s, output: 1375.92 toks/s]
Processed prompts:  77%|  | 392/512 [01:11<00:02, 59.09it/s, est. speed input: 87.60 toks/s, output: 1401.54 toks/s]
Processed prompts:  78%|  | 399/512 [01:11<00:01, 59.24it/s, est. speed input: 89.01 toks/s, output: 1424.23 toks/s]
Processed prompts:  79%|  | 406/512 [01:11<00:01, 56.64it/s, est. speed input: 90.40 toks/s, output: 1446.41 toks/s]
Processed prompts:  80%|  | 412/512 [01:11<00:01, 55.76it/s, est. speed input: 91.59 toks/s, output: 1465.49 toks/s]
Processed prompts:  82%| | 418/512 [01:12<00:01, 52.21it/s, est. speed input: 92.75 toks/s, output: 1484.03 toks/s]
Processed prompts:  83%| | 424/512 [01:12<00:01, 52.90it/s, est. speed input: 93.94 toks/s, output: 1503.05 toks/s]
Processed prompts:  84%| | 430/512 [01:12<00:01, 53.70it/s, est. speed input: 95.13 toks/s, output: 1522.06 toks/s]
Processed prompts:  85%| | 436/512 [01:12<00:01, 50.98it/s, est. speed input: 96.28 toks/s, output: 1540.47 toks/s]
Processed prompts:  86%| | 442/512 [01:12<00:01, 41.22it/s, est. speed input: 97.32 toks/s, output: 1557.06 toks/s]
Processed prompts:  87%| | 447/512 [01:12<00:01, 42.94it/s, est. speed input: 98.28 toks/s, output: 1572.46 toks/s]
Processed prompts:  88%| | 452/512 [01:12<00:01, 40.66it/s, est. speed input: 99.19 toks/s, output: 1586.98 toks/s]
Processed prompts:  89%| | 457/512 [01:13<00:01, 35.04it/s, est. speed input: 100.02 toks/s, output: 1600.27 toks/s]
Processed prompts:  90%| | 462/512 [01:13<00:01, 32.09it/s, est. speed input: 100.85 toks/s, output: 1613.61 toks/s]
Processed prompts:  91%| | 466/512 [01:13<00:01, 32.71it/s, est. speed input: 101.56 toks/s, output: 1625.03 toks/s]
Processed prompts:  92%|| 470/512 [01:13<00:01, 28.68it/s, est. speed input: 102.17 toks/s, output: 1634.77 toks/s]
Processed prompts:  93%|| 474/512 [01:13<00:01, 26.66it/s, est. speed input: 102.79 toks/s, output: 1644.69 toks/s]
Processed prompts:  93%|| 478/512 [01:13<00:01, 25.40it/s, est. speed input: 103.41 toks/s, output: 1654.61 toks/s]
Processed prompts:  94%|| 482/512 [01:14<00:01, 24.68it/s, est. speed input: 104.03 toks/s, output: 1664.55 toks/s]
Processed prompts:  95%|| 485/512 [01:14<00:01, 19.59it/s, est. speed input: 104.32 toks/s, output: 1669.12 toks/s]
Processed prompts:  95%|| 488/512 [01:14<00:01, 16.68it/s, est. speed input: 104.60 toks/s, output: 1673.60 toks/s]
Processed prompts:  96%|| 490/512 [01:14<00:01, 15.50it/s, est. speed input: 104.79 toks/s, output: 1676.71 toks/s]
Processed prompts:  96%|| 492/512 [01:14<00:01, 14.63it/s, est. speed input: 104.99 toks/s, output: 1679.85 toks/s]
Processed prompts:  96%|| 494/512 [01:15<00:01, 13.98it/s, est. speed input: 105.19 toks/s, output: 1683.00 toks/s]
Processed prompts:  97%|| 496/512 [01:15<00:01, 13.52it/s, est. speed input: 105.39 toks/s, output: 1686.17 toks/s]
Processed prompts:  97%|| 497/512 [01:29<00:01, 13.52it/s, est. speed input: 105.45 toks/s, output: 1687.26 toks/s]
Processed prompts:  97%|| 498/512 [01:30<00:29,  2.12s/it, est. speed input: 87.64 toks/s, output: 1402.31 toks/s] 
Processed prompts:  98%|| 501/512 [01:31<00:15,  1.37s/it, est. speed input: 88.01 toks/s, output: 1408.22 toks/s]
Processed prompts: 100%|| 511/512 [01:31<00:00,  1.95it/s, est. speed input: 89.62 toks/s, output: 1433.89 toks/s]
Processed prompts: 100%|| 512/512 [01:31<00:00,  1.95it/s, est. speed input: 89.79 toks/s, output: 1436.70 toks/s]
Processed prompts: 100%|| 512/512 [01:31<00:00,  5.61it/s, est. speed input: 89.79 toks/s, output: 1436.70 toks/s]
[rank0]:[W127 21:03:08.313375002 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

