
========== M=16 ==========
Time: 2026-01-25 18:48:56
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-FP8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:49:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:49:00 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=302011) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) ================================================================
(EngineCore_DP0 pid=302011) Internal Triton PTX codegen error
(EngineCore_DP0 pid=302011) `ptxas` stderr:
(EngineCore_DP0 pid=302011) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpm3tvxh9t.ptx -o /tmp/tmpm3tvxh9t.ptx.o
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) //
(EngineCore_DP0 pid=302011) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=302011) //
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) .version 8.7
(EngineCore_DP0 pid=302011) .target sm_121a
(EngineCore_DP0 pid=302011) .address_size 64
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) 	// .globl	_quant_slide_fp8_kernel // -- Begin function _quant_slide_fp8_kernel
(EngineCore_DP0 pid=302011) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=302011)                                         // @_quant_slide_fp8_kernel
(EngineCore_DP0 pid=302011) .visible .entry _quant_slide_fp8_kernel(
(EngineCore_DP0 pid=302011) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_0,
(EngineCore_DP0 pid=302011) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_1,
(EngineCore_DP0 pid=302011) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_2,
(EngineCore_DP0 pid=302011) 	.param .u32 _quant_slide_fp8_kernel_param_3,
(EngineCore_DP0 pid=302011) 	.param .u32 _quant_slide_fp8_kernel_param_4,
(EngineCore_DP0 pid=302011) 	.param .u32 _quant_slide_fp8_kernel_param_5,
(EngineCore_DP0 pid=302011) 	.param .u32 _quant_slide_fp8_kernel_param_6,
(EngineCore_DP0 pid=302011) 	.param .u32 _quant_slide_fp8_kernel_param_7,
(EngineCore_DP0 pid=302011) 	.param .u32 _quant_slide_fp8_kernel_param_8,
(EngineCore_DP0 pid=302011) 	.param .u32 _quant_slide_fp8_kernel_param_9,
(EngineCore_DP0 pid=302011) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_10,
(EngineCore_DP0 pid=302011) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_11
(EngineCore_DP0 pid=302011) )
(EngineCore_DP0 pid=302011) .reqntid 1024
(EngineCore_DP0 pid=302011) {
(EngineCore_DP0 pid=302011) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=302011) 	.reg .b16 	%rs<25>;
(EngineCore_DP0 pid=302011) 	.reg .b32 	%r<114>;
(EngineCore_DP0 pid=302011) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=302011) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=302011) $L__func_begin0:
(EngineCore_DP0 pid=302011) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) // %bb.0:
(EngineCore_DP0 pid=302011) 	ld.param.b32 	%r19, [_quant_slide_fp8_kernel_param_7];
(EngineCore_DP0 pid=302011) 	ld.param.b32 	%r18, [_quant_slide_fp8_kernel_param_5];
(EngineCore_DP0 pid=302011) 	ld.param.b32 	%r17, [_quant_slide_fp8_kernel_param_4];
(EngineCore_DP0 pid=302011) 	ld.param.b64 	%rd3, [_quant_slide_fp8_kernel_param_2];
(EngineCore_DP0 pid=302011) 	ld.param.b64 	%rd4, [_quant_slide_fp8_kernel_param_0];
(EngineCore_DP0 pid=302011) $L__tmp0:
(EngineCore_DP0 pid=302011) 	.loc	1 177 24                        // quant_slide_tuned_Llama3.2-1B.py:177:24
(EngineCore_DP0 pid=302011) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=302011) 	ld.param.b32 	%r21, [_quant_slide_fp8_kernel_param_8];
(EngineCore_DP0 pid=302011) 	.loc	1 182 26                        // quant_slide_tuned_Llama3.2-1B.py:182:26
(EngineCore_DP0 pid=302011) 	mul.lo.s32 	%r22, %r21, %r1;
(EngineCore_DP0 pid=302011) 	.loc	1 182 20                        // quant_slide_tuned_Llama3.2-1B.py:182:20
(EngineCore_DP0 pid=302011) 	mad.wide.s32 	%rd1, %r22, 2, %rd4;
(EngineCore_DP0 pid=302011) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=302011) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=302011) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=302011) 	setp.lt.s32 	%p1, %r18, 1;
(EngineCore_DP0 pid=302011) 	mov.b32 	%r112, 0f2B8CBCCC;
(EngineCore_DP0 pid=302011) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=302011) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=302011) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=302011) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=302011) 	shl.b32 	%r3, %r2, 2;
(EngineCore_DP0 pid=302011) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=302011) 	shr.u32 	%r31, %r2, 3;
(EngineCore_DP0 pid=302011) 	and.b32 	%r32, %r31, 124;
(EngineCore_DP0 pid=302011) 	mov.b32 	%r33, global_smem;
(EngineCore_DP0 pid=302011) 	add.s32 	%r39, %r33, %r32;
(EngineCore_DP0 pid=302011) 	add.s32 	%r42, %r33, %r3;
(EngineCore_DP0 pid=302011) 	mov.b32 	%r37, 0;
(EngineCore_DP0 pid=302011) 	mov.b32 	%r110, 0f00000000;
(EngineCore_DP0 pid=302011) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=302011) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=302011) 	mov.b32 	%r111, %r37;
(EngineCore_DP0 pid=302011) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=302011) 	.loc	1 189 22                        // quant_slide_tuned_Llama3.2-1B.py:189:22
(EngineCore_DP0 pid=302011) 	add.s32 	%r45, %r3, %r111;
(EngineCore_DP0 pid=302011) 	setp.lt.s32 	%p2, %r45, %r17;
(EngineCore_DP0 pid=302011) 	.loc	1 190 29                        // quant_slide_tuned_Llama3.2-1B.py:190:29
(EngineCore_DP0 pid=302011) 	mad.wide.s32 	%rd6, %r45, 2, %rd1;
(EngineCore_DP0 pid=302011) 	.loc	1 190 21                        // quant_slide_tuned_Llama3.2-1B.py:190:21
(EngineCore_DP0 pid=302011) 	// begin inline asm
(EngineCore_DP0 pid=302011) 	mov.u32 %r35, %r37;
(EngineCore_DP0 pid=302011) 	mov.u32 %r36, %r37;
(EngineCore_DP0 pid=302011) 	@%p2 ld.global.v2.b32 { %r35, %r36 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=302011) 	// end inline asm
(EngineCore_DP0 pid=302011) 	mov.b32 	{%rs1, %rs2}, %r35;
(EngineCore_DP0 pid=302011) 	mov.b32 	{%rs3, %rs4}, %r36;
(EngineCore_DP0 pid=302011) 	.loc	1 191 50                        // quant_slide_tuned_Llama3.2-1B.py:191:50
(EngineCore_DP0 pid=302011) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=302011) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=302011) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=302011) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=302011) $L__tmp1:
(EngineCore_DP0 pid=302011) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	bar.sync 	0;
(EngineCore_DP0 pid=302011) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=302011) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=302011) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=302011) 	cvt.f32.bf16 	%r46, %rs11;
(EngineCore_DP0 pid=302011) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	shfl.sync.bfly.b32 	%r47, %r46, 16, 31, -1;
(EngineCore_DP0 pid=302011) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	max.f32 	%r48, %r46, %r47;
(EngineCore_DP0 pid=302011) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	shfl.sync.bfly.b32 	%r49, %r48, 8, 31, -1;
(EngineCore_DP0 pid=302011) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	max.f32 	%r50, %r48, %r49;
(EngineCore_DP0 pid=302011) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	shfl.sync.bfly.b32 	%r51, %r50, 4, 31, -1;
(EngineCore_DP0 pid=302011) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	max.f32 	%r52, %r50, %r51;
(EngineCore_DP0 pid=302011) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	shfl.sync.bfly.b32 	%r53, %r52, 2, 31, -1;
(EngineCore_DP0 pid=302011) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	max.f32 	%r54, %r52, %r53;
(EngineCore_DP0 pid=302011) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	shfl.sync.bfly.b32 	%r55, %r54, 1, 31, -1;
(EngineCore_DP0 pid=302011) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	max.f32 	%r40, %r54, %r55;
(EngineCore_DP0 pid=302011) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	// begin inline asm
(EngineCore_DP0 pid=302011) 	@%p3 st.shared.b32 [ %r39 + 0 ], %r40;
(EngineCore_DP0 pid=302011) 	// end inline asm
(EngineCore_DP0 pid=302011) 	bar.sync 	0;
(EngineCore_DP0 pid=302011) 	// begin inline asm
(EngineCore_DP0 pid=302011) 	@%p4 ld.shared.b32 %r41, [ %r42 + 0 ];
(EngineCore_DP0 pid=302011) 	// end inline asm
(EngineCore_DP0 pid=302011) 	shfl.sync.bfly.b32 	%r56, %r41, 16, 31, -1;
(EngineCore_DP0 pid=302011) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	max.f32 	%r57, %r41, %r56;
(EngineCore_DP0 pid=302011) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	shfl.sync.bfly.b32 	%r58, %r57, 8, 31, -1;
(EngineCore_DP0 pid=302011) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	max.f32 	%r59, %r57, %r58;
(EngineCore_DP0 pid=302011) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	shfl.sync.bfly.b32 	%r60, %r59, 4, 31, -1;
(EngineCore_DP0 pid=302011) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	max.f32 	%r61, %r59, %r60;
(EngineCore_DP0 pid=302011) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	shfl.sync.bfly.b32 	%r62, %r61, 2, 31, -1;
(EngineCore_DP0 pid=302011) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	max.f32 	%r63, %r61, %r62;
(EngineCore_DP0 pid=302011) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	shfl.sync.bfly.b32 	%r64, %r63, 1, 31, -1;
(EngineCore_DP0 pid=302011) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	max.f32 	%r44, %r63, %r64;
(EngineCore_DP0 pid=302011) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302011) 	// begin inline asm
(EngineCore_DP0 pid=302011) 	@%p19 st.shared.b32 [ %r42 + 0 ], %r44;
(EngineCore_DP0 pid=302011) 	// end inline asm
(EngineCore_DP0 pid=302011) 	bar.sync 	0;
(EngineCore_DP0 pid=302011) 	ld.shared.b32 	%r65, [global_smem];
(EngineCore_DP0 pid=302011) $L__tmp2:
(EngineCore_DP0 pid=302011) 	.loc	1 191 36                        // quant_slide_tuned_Llama3.2-1B.py:191:36
(EngineCore_DP0 pid=302011) 	max.f32 	%r110, %r110, %r65;
(EngineCore_DP0 pid=302011) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=302011) 	add.s32 	%r111, %r111, 4096;
(EngineCore_DP0 pid=302011) 	setp.lt.s32 	%p6, %r111, %r18;
(EngineCore_DP0 pid=302011) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=302011) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=302011) 	.loc	1 193 32                        // quant_slide_tuned_Llama3.2-1B.py:193:32
(EngineCore_DP0 pid=302011) 	max.f32 	%r112, %r110, 0f2B8CBCCC;
(EngineCore_DP0 pid=302011) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=302011) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=302011) 	mov.b32 	%r67, 0f43E00000;
(EngineCore_DP0 pid=302011) 	.loc	1 194 32                        // quant_slide_tuned_Llama3.2-1B.py:194:32
(EngineCore_DP0 pid=302011) 	div.full.f32 	%r68, %r112, %r67;
(EngineCore_DP0 pid=302011) 	.loc	1 194 41                        // quant_slide_tuned_Llama3.2-1B.py:194:41
(EngineCore_DP0 pid=302011) 	max.f32 	%r66, %r68, 0f36924925;
(EngineCore_DP0 pid=302011) 	.loc	1 196 25                        // quant_slide_tuned_Llama3.2-1B.py:196:25
(EngineCore_DP0 pid=302011) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=302011) 	.loc	1 196 30                        // quant_slide_tuned_Llama3.2-1B.py:196:30
(EngineCore_DP0 pid=302011) 	// begin inline asm
(EngineCore_DP0 pid=302011) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r66 };
(EngineCore_DP0 pid=302011) 	// end inline asm
(EngineCore_DP0 pid=302011) 	.loc	1 199 29                        // quant_slide_tuned_Llama3.2-1B.py:199:29
(EngineCore_DP0 pid=302011) 	shl.b32 	%r14, %r19, 1;
(EngineCore_DP0 pid=302011) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=302011) 	setp.lt.s32 	%p8, %r14, 1;
(EngineCore_DP0 pid=302011) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=302011) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=302011) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=302011) 	ld.param.b32 	%r23, [_quant_slide_fp8_kernel_param_9];
(EngineCore_DP0 pid=302011) 	shr.s32 	%r24, %r23, 31;
(EngineCore_DP0 pid=302011) 	shr.u32 	%r25, %r24, 30;
(EngineCore_DP0 pid=302011) 	add.s32 	%r26, %r23, %r25;
(EngineCore_DP0 pid=302011) 	shr.s32 	%r27, %r26, 2;
(EngineCore_DP0 pid=302011) 	ld.param.b64 	%rd5, [_quant_slide_fp8_kernel_param_1];
(EngineCore_DP0 pid=302011) 	mul.lo.s32 	%r28, %r27, %r1;
(EngineCore_DP0 pid=302011) 	mad.wide.s32 	%rd2, %r28, 4, %rd5;
(EngineCore_DP0 pid=302011) 	div.full.f32 	%r13, %r67, %r112;
(EngineCore_DP0 pid=302011) 	mov.b32 	%r113, 0;
(EngineCore_DP0 pid=302011) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=302011)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=302011) 	.loc	1 203 30                        // quant_slide_tuned_Llama3.2-1B.py:203:30
(EngineCore_DP0 pid=302011) 	add.s32 	%r80, %r2, %r113;
(EngineCore_DP0 pid=302011) 	setp.lt.s32 	%p13, %r80, %r14;
(EngineCore_DP0 pid=302011) 	.loc	1 206 24                        // quant_slide_tuned_Llama3.2-1B.py:206:24
(EngineCore_DP0 pid=302011) 	shr.u32 	%r81, %r80, 31;
(EngineCore_DP0 pid=302011) 	add.s32 	%r82, %r80, %r81;
(EngineCore_DP0 pid=302011) 	shr.u32 	%r83, %r82, 1;
(EngineCore_DP0 pid=302011) 	.loc	1 207 23                        // quant_slide_tuned_Llama3.2-1B.py:207:23
(EngineCore_DP0 pid=302011) 	and.b32 	%r84, %r82, 2147483646;
(EngineCore_DP0 pid=302011) 	sub.s32 	%r85, %r80, %r84;
(EngineCore_DP0 pid=302011) 	.loc	1 208 30                        // quant_slide_tuned_Llama3.2-1B.py:208:30
(EngineCore_DP0 pid=302011) 	shl.b32 	%r86, %r85, 1;
(EngineCore_DP0 pid=302011) 	.loc	1 208 26                        // quant_slide_tuned_Llama3.2-1B.py:208:26
(EngineCore_DP0 pid=302011) 	mad.lo.s32 	%r87, %r83, 6, %r86;
(EngineCore_DP0 pid=302011) 	.loc	1 211 53                        // quant_slide_tuned_Llama3.2-1B.py:211:53
(EngineCore_DP0 pid=302011) 	setp.lt.s32 	%p14, %r87, %r17;
(EngineCore_DP0 pid=302011) 	.loc	1 211 37                        // quant_slide_tuned_Llama3.2-1B.py:211:37
(EngineCore_DP0 pid=302011) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=302011) 	.loc	1 210 29                        // quant_slide_tuned_Llama3.2-1B.py:210:29
(EngineCore_DP0 pid=302011) 	mad.wide.s32 	%rd8, %r87, 2, %rd1;
(EngineCore_DP0 pid=302011) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=302011) 	.loc	1 210 21                        // quant_slide_tuned_Llama3.2-1B.py:210:21
(EngineCore_DP0 pid=302011) 	// begin inline asm
(EngineCore_DP0 pid=302011) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=302011) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=302011) 	// end inline asm
(EngineCore_DP0 pid=302011) 	.loc	1 211 79                        // quant_slide_tuned_Llama3.2-1B.py:211:79
(EngineCore_DP0 pid=302011) 	cvt.f32.bf16 	%r88, %rs12;
(EngineCore_DP0 pid=302011) 	.loc	1 213 48                        // quant_slide_tuned_Llama3.2-1B.py:213:48
(EngineCore_DP0 pid=302011) 	or.b32 	%r89, %r87, 1;
(EngineCore_DP0 pid=302011) 	.loc	1 213 53                        // quant_slide_tuned_Llama3.2-1B.py:213:53
(EngineCore_DP0 pid=302011) 	setp.lt.s32 	%p15, %r89, %r17;
(EngineCore_DP0 pid=302011) 	.loc	1 213 37                        // quant_slide_tuned_Llama3.2-1B.py:213:37
(EngineCore_DP0 pid=302011) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=302011) 	.loc	1 212 39                        // quant_slide_tuned_Llama3.2-1B.py:212:39
(EngineCore_DP0 pid=302011) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=302011) 	.loc	1 212 21                        // quant_slide_tuned_Llama3.2-1B.py:212:21
(EngineCore_DP0 pid=302011) 	// begin inline asm
(EngineCore_DP0 pid=302011) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=302011) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=302011) 	// end inline asm
(EngineCore_DP0 pid=302011) 	.loc	1 213 79                        // quant_slide_tuned_Llama3.2-1B.py:213:79
(EngineCore_DP0 pid=302011) 	cvt.f32.bf16 	%r90, %rs14;
(EngineCore_DP0 pid=302011) 	.loc	1 215 48                        // quant_slide_tuned_Llama3.2-1B.py:215:48
(EngineCore_DP0 pid=302011) 	add.s32 	%r91, %r87, 2;
(EngineCore_DP0 pid=302011) 	.loc	1 215 53                        // quant_slide_tuned_Llama3.2-1B.py:215:53
(EngineCore_DP0 pid=302011) 	setp.lt.s32 	%p16, %r91, %r17;
(EngineCore_DP0 pid=302011) 	.loc	1 215 37                        // quant_slide_tuned_Llama3.2-1B.py:215:37
(EngineCore_DP0 pid=302011) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=302011) 	.loc	1 214 39                        // quant_slide_tuned_Llama3.2-1B.py:214:39
(EngineCore_DP0 pid=302011) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=302011) 	.loc	1 214 21                        // quant_slide_tuned_Llama3.2-1B.py:214:21
(EngineCore_DP0 pid=302011) 	// begin inline asm
(EngineCore_DP0 pid=302011) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=302011) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=302011) 	// end inline asm
(EngineCore_DP0 pid=302011) 	.loc	1 215 79                        // quant_slide_tuned_Llama3.2-1B.py:215:79
(EngineCore_DP0 pid=302011) 	cvt.f32.bf16 	%r92, %rs16;
(EngineCore_DP0 pid=302011) 	.loc	1 217 48                        // quant_slide_tuned_Llama3.2-1B.py:217:48
(EngineCore_DP0 pid=302011) 	add.s32 	%r93, %r87, 3;
(EngineCore_DP0 pid=302011) 	.loc	1 217 53                        // quant_slide_tuned_Llama3.2-1B.py:217:53
(EngineCore_DP0 pid=302011) 	setp.lt.s32 	%p17, %r93, %r17;
(EngineCore_DP0 pid=302011) 	.loc	1 217 37                        // quant_slide_tuned_Llama3.2-1B.py:217:37
(EngineCore_DP0 pid=302011) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=302011) 	.loc	1 216 39                        // quant_slide_tuned_Llama3.2-1B.py:216:39
(EngineCore_DP0 pid=302011) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=302011) 	.loc	1 216 21                        // quant_slide_tuned_Llama3.2-1B.py:216:21
(EngineCore_DP0 pid=302011) 	// begin inline asm
(EngineCore_DP0 pid=302011) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=302011) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=302011) 	// end inline asm
(EngineCore_DP0 pid=302011) 	.loc	1 217 79                        // quant_slide_tuned_Llama3.2-1B.py:217:79
(EngineCore_DP0 pid=302011) 	cvt.f32.bf16 	%r94, %rs18;
(EngineCore_DP0 pid=302011) 	.loc	1 219 27                        // quant_slide_tuned_Llama3.2-1B.py:219:27
(EngineCore_DP0 pid=302011) 	mul.f32 	%r95, %r13, %r88;
(EngineCore_DP0 pid=302011) 	mov.b32 	%r96, 0f43E00000;
(EngineCore_DP0 pid=302011) 	.loc	1 219 48                        // quant_slide_tuned_Llama3.2-1B.py:219:48
(EngineCore_DP0 pid=302011) 	min.xorsign.abs.f32 	%r70, %r95, %r96;
(EngineCore_DP0 pid=302011) 	.loc	1 219 60                        // quant_slide_tuned_Llama3.2-1B.py:219:60
(EngineCore_DP0 pid=302011) 	// begin inline asm
(EngineCore_DP0 pid=302011) 	cvt.rn.satfinite.e4m3x2.f32  %rs20, %r71, %r70; 
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) 	// end inline asm
(EngineCore_DP0 pid=302011) 	.loc	1 220 27                        // quant_slide_tuned_Llama3.2-1B.py:220:27
(EngineCore_DP0 pid=302011) 	mul.f32 	%r97, %r13, %r90;
(EngineCore_DP0 pid=302011) 	.loc	1 220 48                        // quant_slide_tuned_Llama3.2-1B.py:220:48
(EngineCore_DP0 pid=302011) 	min.xorsign.abs.f32 	%r72, %r97, %r96;
(EngineCore_DP0 pid=302011) 	.loc	1 220 60                        // quant_slide_tuned_Llama3.2-1B.py:220:60
(EngineCore_DP0 pid=302011) 	// begin inline asm
(EngineCore_DP0 pid=302011) 	cvt.rn.satfinite.e4m3x2.f32  %rs21, %r73, %r72; 
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) 	// end inline asm
(EngineCore_DP0 pid=302011) 	.loc	1 221 27                        // quant_slide_tuned_Llama3.2-1B.py:221:27
(EngineCore_DP0 pid=302011) 	mul.f32 	%r98, %r13, %r92;
(EngineCore_DP0 pid=302011) 	.loc	1 221 48                        // quant_slide_tuned_Llama3.2-1B.py:221:48
(EngineCore_DP0 pid=302011) 	min.xorsign.abs.f32 	%r74, %r98, %r96;
(EngineCore_DP0 pid=302011) 	.loc	1 221 60                        // quant_slide_tuned_Llama3.2-1B.py:221:60
(EngineCore_DP0 pid=302011) 	// begin inline asm
(EngineCore_DP0 pid=302011) 	cvt.rn.satfinite.e4m3x2.f32  %rs22, %r75, %r74; 
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) 	// end inline asm
(EngineCore_DP0 pid=302011) 	.loc	1 222 27                        // quant_slide_tuned_Llama3.2-1B.py:222:27
(EngineCore_DP0 pid=302011) 	mul.f32 	%r99, %r13, %r94;
(EngineCore_DP0 pid=302011) 	.loc	1 222 48                        // quant_slide_tuned_Llama3.2-1B.py:222:48
(EngineCore_DP0 pid=302011) 	min.xorsign.abs.f32 	%r76, %r99, %r96;
(EngineCore_DP0 pid=302011) 	.loc	1 222 60                        // quant_slide_tuned_Llama3.2-1B.py:222:60
(EngineCore_DP0 pid=302011) 	// begin inline asm
(EngineCore_DP0 pid=302011) 	cvt.rn.satfinite.e4m3x2.f32  %rs23, %r77, %r76; 
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) 	// end inline asm
(EngineCore_DP0 pid=302011) 	.loc	1 224 45                        // quant_slide_tuned_Llama3.2-1B.py:224:45
(EngineCore_DP0 pid=302011) 	cvt.u32.u16 	%r100, %rs20;
(EngineCore_DP0 pid=302011) 	and.b32 	%r101, %r100, 255;
(EngineCore_DP0 pid=302011) 	.loc	1 226 45                        // quant_slide_tuned_Llama3.2-1B.py:226:45
(EngineCore_DP0 pid=302011) 	cvt.u32.u16 	%r102, %rs22;
(EngineCore_DP0 pid=302011) 	and.b32 	%r103, %r102, 255;
(EngineCore_DP0 pid=302011) 	.loc	1 227 45                        // quant_slide_tuned_Llama3.2-1B.py:227:45
(EngineCore_DP0 pid=302011) 	cvt.u32.u16 	%r104, %rs23;
(EngineCore_DP0 pid=302011) 	.loc	1 229 30                        // quant_slide_tuned_Llama3.2-1B.py:229:30
(EngineCore_DP0 pid=302011) 	and.b16 	%rs24, %rs21, 255;
(EngineCore_DP0 pid=302011) 	mul.wide.u16 	%r105, %rs24, 256;
(EngineCore_DP0 pid=302011) 	.loc	1 229 24                        // quant_slide_tuned_Llama3.2-1B.py:229:24
(EngineCore_DP0 pid=302011) 	or.b32 	%r106, %r105, %r101;
(EngineCore_DP0 pid=302011) 	.loc	1 229 42                        // quant_slide_tuned_Llama3.2-1B.py:229:42
(EngineCore_DP0 pid=302011) 	shl.b32 	%r107, %r103, 16;
(EngineCore_DP0 pid=302011) 	.loc	1 229 36                        // quant_slide_tuned_Llama3.2-1B.py:229:36
(EngineCore_DP0 pid=302011) 	or.b32 	%r108, %r106, %r107;
(EngineCore_DP0 pid=302011) 	.loc	1 229 55                        // quant_slide_tuned_Llama3.2-1B.py:229:55
(EngineCore_DP0 pid=302011) 	shl.b32 	%r109, %r104, 24;
(EngineCore_DP0 pid=302011) 	.loc	1 229 49                        // quant_slide_tuned_Llama3.2-1B.py:229:49
(EngineCore_DP0 pid=302011) 	or.b32 	%r78, %r108, %r109;
(EngineCore_DP0 pid=302011) 	.loc	1 230 29                        // quant_slide_tuned_Llama3.2-1B.py:230:29
(EngineCore_DP0 pid=302011) 	mad.wide.s32 	%rd12, %r80, 4, %rd2;
(EngineCore_DP0 pid=302011) 	.loc	1 230 39                        // quant_slide_tuned_Llama3.2-1B.py:230:39
(EngineCore_DP0 pid=302011) 	// begin inline asm
(EngineCore_DP0 pid=302011) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r78 };
(EngineCore_DP0 pid=302011) 	// end inline asm
(EngineCore_DP0 pid=302011) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=302011) 	add.s32 	%r113, %r113, 1024;
(EngineCore_DP0 pid=302011) 	setp.lt.s32 	%p18, %r113, %r14;
(EngineCore_DP0 pid=302011) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=302011) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=302011) 	.loc	1 201 4                         // quant_slide_tuned_Llama3.2-1B.py:201:4
(EngineCore_DP0 pid=302011) 	ret;
(EngineCore_DP0 pid=302011) $L__tmp3:
(EngineCore_DP0 pid=302011) $L__func_end0:
(EngineCore_DP0 pid=302011)                                         // -- End function
(EngineCore_DP0 pid=302011) }
(EngineCore_DP0 pid=302011) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=302011) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=302011) 	.section	.debug_abbrev
(EngineCore_DP0 pid=302011) 	{
(EngineCore_DP0 pid=302011) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=302011) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=302011) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=302011) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=302011) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=302011) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=302011) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=302011) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=302011) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=302011) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=302011) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=302011) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=302011) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=302011) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=302011) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=302011) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=302011) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=302011) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=302011) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=302011) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=302011) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=302011) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=302011) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=302011) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=302011) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=302011) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=302011) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=302011) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=302011) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=302011) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=302011) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=302011) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=302011) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=302011) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=302011) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=302011) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=302011) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=302011) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=302011) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=302011) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=302011) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=302011) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=302011) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=302011) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=302011) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=302011) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=302011) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=302011) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=302011) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=302011) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=302011) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=302011) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=302011) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=302011) 	}
(EngineCore_DP0 pid=302011) 	.section	.debug_info
(EngineCore_DP0 pid=302011) 	{
(EngineCore_DP0 pid=302011) .b32 222                                // Length of Unit
(EngineCore_DP0 pid=302011) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=302011) .b8 0
(EngineCore_DP0 pid=302011) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=302011) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=302011) .b8 1                                   // Abbrev [1] 0xb:0xd7 DW_TAG_compile_unit
(EngineCore_DP0 pid=302011) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=302011) .b8 114
(EngineCore_DP0 pid=302011) .b8 105
(EngineCore_DP0 pid=302011) .b8 116
(EngineCore_DP0 pid=302011) .b8 111
(EngineCore_DP0 pid=302011) .b8 110
(EngineCore_DP0 pid=302011) .b8 0
(EngineCore_DP0 pid=302011) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=302011) .b8 0
(EngineCore_DP0 pid=302011) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=302011) .b8 117
(EngineCore_DP0 pid=302011) .b8 97
(EngineCore_DP0 pid=302011) .b8 110
(EngineCore_DP0 pid=302011) .b8 116
(EngineCore_DP0 pid=302011) .b8 95
(EngineCore_DP0 pid=302011) .b8 115
(EngineCore_DP0 pid=302011) .b8 108
(EngineCore_DP0 pid=302011) .b8 105
(EngineCore_DP0 pid=302011) .b8 100
(EngineCore_DP0 pid=302011) .b8 101
(EngineCore_DP0 pid=302011) .b8 95
(EngineCore_DP0 pid=302011) .b8 116
(EngineCore_DP0 pid=302011) .b8 117
(EngineCore_DP0 pid=302011) .b8 110
(EngineCore_DP0 pid=302011) .b8 101
(EngineCore_DP0 pid=302011) .b8 100
(EngineCore_DP0 pid=302011) .b8 95
(EngineCore_DP0 pid=302011) .b8 76
(EngineCore_DP0 pid=302011) .b8 108
(EngineCore_DP0 pid=302011) .b8 97
(EngineCore_DP0 pid=302011) .b8 109
(EngineCore_DP0 pid=302011) .b8 97
(EngineCore_DP0 pid=302011) .b8 51
(EngineCore_DP0 pid=302011) .b8 46
(EngineCore_DP0 pid=302011) .b8 50
(EngineCore_DP0 pid=302011) .b8 45
(EngineCore_DP0 pid=302011) .b8 49
(EngineCore_DP0 pid=302011) .b8 66
(EngineCore_DP0 pid=302011) .b8 46
(EngineCore_DP0 pid=302011) .b8 112
(EngineCore_DP0 pid=302011) .b8 121
(EngineCore_DP0 pid=302011) .b8 0
(EngineCore_DP0 pid=302011) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=302011) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=302011) .b8 114
(EngineCore_DP0 pid=302011) .b8 111
(EngineCore_DP0 pid=302011) .b8 111
(EngineCore_DP0 pid=302011) .b8 116
(EngineCore_DP0 pid=302011) .b8 47
(EngineCore_DP0 pid=302011) .b8 118
(EngineCore_DP0 pid=302011) .b8 108
(EngineCore_DP0 pid=302011) .b8 108
(EngineCore_DP0 pid=302011) .b8 109
(EngineCore_DP0 pid=302011) .b8 98
(EngineCore_DP0 pid=302011) .b8 101
(EngineCore_DP0 pid=302011) .b8 110
(EngineCore_DP0 pid=302011) .b8 99
(EngineCore_DP0 pid=302011) .b8 104
(EngineCore_DP0 pid=302011) .b8 47
(EngineCore_DP0 pid=302011) .b8 115
(EngineCore_DP0 pid=302011) .b8 108
(EngineCore_DP0 pid=302011) .b8 105
(EngineCore_DP0 pid=302011) .b8 100
(EngineCore_DP0 pid=302011) .b8 101
(EngineCore_DP0 pid=302011) .b8 115
(EngineCore_DP0 pid=302011) .b8 112
(EngineCore_DP0 pid=302011) .b8 97
(EngineCore_DP0 pid=302011) .b8 114
(EngineCore_DP0 pid=302011) .b8 115
(EngineCore_DP0 pid=302011) .b8 101
(EngineCore_DP0 pid=302011) .b8 47
(EngineCore_DP0 pid=302011) .b8 99
(EngineCore_DP0 pid=302011) .b8 115
(EngineCore_DP0 pid=302011) .b8 114
(EngineCore_DP0 pid=302011) .b8 99
(EngineCore_DP0 pid=302011) .b8 47
(EngineCore_DP0 pid=302011) .b8 102
(EngineCore_DP0 pid=302011) .b8 117
(EngineCore_DP0 pid=302011) .b8 115
(EngineCore_DP0 pid=302011) .b8 101
(EngineCore_DP0 pid=302011) .b8 100
(EngineCore_DP0 pid=302011) .b8 95
(EngineCore_DP0 pid=302011) .b8 113
(EngineCore_DP0 pid=302011) .b8 117
(EngineCore_DP0 pid=302011) .b8 97
(EngineCore_DP0 pid=302011) .b8 110
(EngineCore_DP0 pid=302011) .b8 116
(EngineCore_DP0 pid=302011) .b8 95
(EngineCore_DP0 pid=302011) .b8 115
(EngineCore_DP0 pid=302011) .b8 108
(EngineCore_DP0 pid=302011) .b8 105
(EngineCore_DP0 pid=302011) .b8 100
(EngineCore_DP0 pid=302011) .b8 101
(EngineCore_DP0 pid=302011) .b8 95
(EngineCore_DP0 pid=302011) .b8 116
(EngineCore_DP0 pid=302011) .b8 114
(EngineCore_DP0 pid=302011) .b8 105
(EngineCore_DP0 pid=302011) .b8 116
(EngineCore_DP0 pid=302011) .b8 111
(EngineCore_DP0 pid=302011) .b8 110
(EngineCore_DP0 pid=302011) .b8 47
(EngineCore_DP0 pid=302011) .b8 98
(EngineCore_DP0 pid=302011) .b8 117
(EngineCore_DP0 pid=302011) .b8 105
(EngineCore_DP0 pid=302011) .b8 108
(EngineCore_DP0 pid=302011) .b8 100
(EngineCore_DP0 pid=302011) .b8 47
(EngineCore_DP0 pid=302011) .b8 71
(EngineCore_DP0 pid=302011) .b8 66
(EngineCore_DP0 pid=302011) .b8 49
(EngineCore_DP0 pid=302011) .b8 48
(EngineCore_DP0 pid=302011) .b8 95
(EngineCore_DP0 pid=302011) .b8 99
(EngineCore_DP0 pid=302011) .b8 99
(EngineCore_DP0 pid=302011) .b8 49
(EngineCore_DP0 pid=302011) .b8 50
(EngineCore_DP0 pid=302011) .b8 49
(EngineCore_DP0 pid=302011) .b8 95
(EngineCore_DP0 pid=302011) .b8 112
(EngineCore_DP0 pid=302011) .b8 121
(EngineCore_DP0 pid=302011) .b8 51
(EngineCore_DP0 pid=302011) .b8 49
(EngineCore_DP0 pid=302011) .b8 50
(EngineCore_DP0 pid=302011) .b8 95
(EngineCore_DP0 pid=302011) .b8 99
(EngineCore_DP0 pid=302011) .b8 117
(EngineCore_DP0 pid=302011) .b8 49
(EngineCore_DP0 pid=302011) .b8 50
(EngineCore_DP0 pid=302011) .b8 57
(EngineCore_DP0 pid=302011) .b8 95
(EngineCore_DP0 pid=302011) .b8 97
(EngineCore_DP0 pid=302011) .b8 97
(EngineCore_DP0 pid=302011) .b8 114
(EngineCore_DP0 pid=302011) .b8 99
(EngineCore_DP0 pid=302011) .b8 104
(EngineCore_DP0 pid=302011) .b8 54
(EngineCore_DP0 pid=302011) .b8 52
(EngineCore_DP0 pid=302011) .b8 0
(EngineCore_DP0 pid=302011) .b8 2                                   // Abbrev [2] 0x99:0x1a DW_TAG_subprogram
(EngineCore_DP0 pid=302011) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=302011) .b8 113
(EngineCore_DP0 pid=302011) .b8 117
(EngineCore_DP0 pid=302011) .b8 97
(EngineCore_DP0 pid=302011) .b8 110
(EngineCore_DP0 pid=302011) .b8 116
(EngineCore_DP0 pid=302011) .b8 95
(EngineCore_DP0 pid=302011) .b8 115
(EngineCore_DP0 pid=302011) .b8 108
(EngineCore_DP0 pid=302011) .b8 105
(EngineCore_DP0 pid=302011) .b8 100
(EngineCore_DP0 pid=302011) .b8 101
(EngineCore_DP0 pid=302011) .b8 95
(EngineCore_DP0 pid=302011) .b8 102
(EngineCore_DP0 pid=302011) .b8 112
(EngineCore_DP0 pid=302011) .b8 56
(EngineCore_DP0 pid=302011) .b8 95
(EngineCore_DP0 pid=302011) .b8 107
(EngineCore_DP0 pid=302011) .b8 101
(EngineCore_DP0 pid=302011) .b8 114
(EngineCore_DP0 pid=302011) .b8 110
(EngineCore_DP0 pid=302011) .b8 101
(EngineCore_DP0 pid=302011) .b8 108
(EngineCore_DP0 pid=302011) .b8 0
(EngineCore_DP0 pid=302011) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=302011) .b8 3                                   // Abbrev [3] 0xb3:0x2e DW_TAG_subprogram
(EngineCore_DP0 pid=302011) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=302011) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=302011) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=302011) .b8 4                                   // Abbrev [4] 0xc8:0x18 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=302011) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=302011) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=302011) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=302011) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=302011) .b8 191                                 // DW_AT_call_line
(EngineCore_DP0 pid=302011) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=302011) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=302011) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=302011) 	}
(EngineCore_DP0 pid=302011) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) ================================================================
(EngineCore_DP0 pid=302011) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpm3tvxh9t.ptx', '-o', '/tmp/tmpm3tvxh9t.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866] 
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866] 
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866] 
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpm3tvxh9t.ptx -o /tmp/tmpm3tvxh9t.ptx.o
(EngineCore_DP0 pid=302011) ERROR 01-25 18:49:15 [core.py:866] 

STDERR:
[2026-01-25 18:49:00] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:49:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:49:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:49:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:49:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:49:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:49:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:49:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:49:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:49:03] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:49:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:49:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:49:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:49:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:49:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:49:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:49:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:49:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=302011) [2026-01-25 18:49:04] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=302011) [2026-01-25 18:49:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=302011) [2026-01-25 18:49:04] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=302011) [2026-01-25 18:49:04] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=302011) [2026-01-25 18:49:04] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=302011) [2026-01-25 18:49:04] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=302011) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=302011) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.27s/it]
(EngineCore_DP0 pid=302011) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.27s/it]
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) [2026-01-25 18:49:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=302011) [2026-01-25 18:49:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=302011) [2026-01-25 18:49:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=302011) [2026-01-25 18:49:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=302011) [2026-01-25 18:49:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=302011) [2026-01-25 18:49:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=302011) [2026-01-25 18:49:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=302011) [2026-01-25 18:49:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=302011) Process EngineCore_DP0:
(EngineCore_DP0 pid=302011) Traceback (most recent call last):
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=302011)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=302011)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=302011)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=302011) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpm3tvxh9t.ptx', '-o', '/tmp/tmpm3tvxh9t.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) Traceback (most recent call last):
(EngineCore_DP0 pid=302011)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=302011)     self.run()
(EngineCore_DP0 pid=302011)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=302011)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=302011)     raise e
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=302011)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=302011)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=302011)     super().__init__(
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=302011)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=302011)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=302011)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=302011)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=302011)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=302011)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=302011)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=302011)     return func(*args, **kwargs)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=302011)     return func(*args, **kwargs)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=302011)     self.model_runner.profile_run()
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=302011)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=302011)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=302011)     return func(*args, **kwargs)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=302011)     outputs = self.model(
(EngineCore_DP0 pid=302011)               ^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302011)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302011)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=302011)     model_output = self.model(
(EngineCore_DP0 pid=302011)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=302011)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=302011)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=302011)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302011)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302011)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=302011)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=302011)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302011)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302011)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=302011)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=302011)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302011)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302011)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=302011)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=302011)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=302011)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=302011)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=302011)     return self._linear_fn(
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=302011)     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=302011)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=302011)     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=302011)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=302011)     return fn(input, L)
(EngineCore_DP0 pid=302011)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=302011)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=302011)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=302011)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=302011)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=302011)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=302011)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=302011)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=302011)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=302011)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=302011)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=302011)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302011)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=302011)     raise PTXASError(error)
(EngineCore_DP0 pid=302011) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=302011) `ptxas` stderr:
(EngineCore_DP0 pid=302011) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=302011) 
(EngineCore_DP0 pid=302011) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpm3tvxh9t.ptx -o /tmp/tmpm3tvxh9t.ptx.o
(EngineCore_DP0 pid=302011) 
[rank0]:[W125 18:49:15.806732709 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=128 ==========
Time: 2026-01-25 18:49:17
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:49:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:49:20 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=302504) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) ================================================================
(EngineCore_DP0 pid=302504) Internal Triton PTX codegen error
(EngineCore_DP0 pid=302504) `ptxas` stderr:
(EngineCore_DP0 pid=302504) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmppexlqa4z.ptx -o /tmp/tmppexlqa4z.ptx.o
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) //
(EngineCore_DP0 pid=302504) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=302504) //
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) .version 8.7
(EngineCore_DP0 pid=302504) .target sm_121a
(EngineCore_DP0 pid=302504) .address_size 64
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) 	// .globl	_quant_slide_fp8_kernel // -- Begin function _quant_slide_fp8_kernel
(EngineCore_DP0 pid=302504) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=302504)                                         // @_quant_slide_fp8_kernel
(EngineCore_DP0 pid=302504) .visible .entry _quant_slide_fp8_kernel(
(EngineCore_DP0 pid=302504) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_0,
(EngineCore_DP0 pid=302504) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_1,
(EngineCore_DP0 pid=302504) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_2,
(EngineCore_DP0 pid=302504) 	.param .u32 _quant_slide_fp8_kernel_param_3,
(EngineCore_DP0 pid=302504) 	.param .u32 _quant_slide_fp8_kernel_param_4,
(EngineCore_DP0 pid=302504) 	.param .u32 _quant_slide_fp8_kernel_param_5,
(EngineCore_DP0 pid=302504) 	.param .u32 _quant_slide_fp8_kernel_param_6,
(EngineCore_DP0 pid=302504) 	.param .u32 _quant_slide_fp8_kernel_param_7,
(EngineCore_DP0 pid=302504) 	.param .u32 _quant_slide_fp8_kernel_param_8,
(EngineCore_DP0 pid=302504) 	.param .u32 _quant_slide_fp8_kernel_param_9,
(EngineCore_DP0 pid=302504) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_10,
(EngineCore_DP0 pid=302504) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_11
(EngineCore_DP0 pid=302504) )
(EngineCore_DP0 pid=302504) .reqntid 1024
(EngineCore_DP0 pid=302504) {
(EngineCore_DP0 pid=302504) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=302504) 	.reg .b16 	%rs<25>;
(EngineCore_DP0 pid=302504) 	.reg .b32 	%r<114>;
(EngineCore_DP0 pid=302504) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=302504) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=302504) $L__func_begin0:
(EngineCore_DP0 pid=302504) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) // %bb.0:
(EngineCore_DP0 pid=302504) 	ld.param.b32 	%r19, [_quant_slide_fp8_kernel_param_7];
(EngineCore_DP0 pid=302504) 	ld.param.b32 	%r18, [_quant_slide_fp8_kernel_param_5];
(EngineCore_DP0 pid=302504) 	ld.param.b32 	%r17, [_quant_slide_fp8_kernel_param_4];
(EngineCore_DP0 pid=302504) 	ld.param.b64 	%rd3, [_quant_slide_fp8_kernel_param_2];
(EngineCore_DP0 pid=302504) 	ld.param.b64 	%rd4, [_quant_slide_fp8_kernel_param_0];
(EngineCore_DP0 pid=302504) $L__tmp0:
(EngineCore_DP0 pid=302504) 	.loc	1 177 24                        // quant_slide_tuned_Llama3.2-1B.py:177:24
(EngineCore_DP0 pid=302504) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=302504) 	ld.param.b32 	%r21, [_quant_slide_fp8_kernel_param_8];
(EngineCore_DP0 pid=302504) 	.loc	1 182 26                        // quant_slide_tuned_Llama3.2-1B.py:182:26
(EngineCore_DP0 pid=302504) 	mul.lo.s32 	%r22, %r21, %r1;
(EngineCore_DP0 pid=302504) 	.loc	1 182 20                        // quant_slide_tuned_Llama3.2-1B.py:182:20
(EngineCore_DP0 pid=302504) 	mad.wide.s32 	%rd1, %r22, 2, %rd4;
(EngineCore_DP0 pid=302504) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=302504) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=302504) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=302504) 	setp.lt.s32 	%p1, %r18, 1;
(EngineCore_DP0 pid=302504) 	mov.b32 	%r112, 0f2B8CBCCC;
(EngineCore_DP0 pid=302504) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=302504) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=302504) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=302504) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=302504) 	shl.b32 	%r3, %r2, 2;
(EngineCore_DP0 pid=302504) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=302504) 	shr.u32 	%r31, %r2, 3;
(EngineCore_DP0 pid=302504) 	and.b32 	%r32, %r31, 124;
(EngineCore_DP0 pid=302504) 	mov.b32 	%r33, global_smem;
(EngineCore_DP0 pid=302504) 	add.s32 	%r39, %r33, %r32;
(EngineCore_DP0 pid=302504) 	add.s32 	%r42, %r33, %r3;
(EngineCore_DP0 pid=302504) 	mov.b32 	%r37, 0;
(EngineCore_DP0 pid=302504) 	mov.b32 	%r110, 0f00000000;
(EngineCore_DP0 pid=302504) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=302504) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=302504) 	mov.b32 	%r111, %r37;
(EngineCore_DP0 pid=302504) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=302504) 	.loc	1 189 22                        // quant_slide_tuned_Llama3.2-1B.py:189:22
(EngineCore_DP0 pid=302504) 	add.s32 	%r45, %r3, %r111;
(EngineCore_DP0 pid=302504) 	setp.lt.s32 	%p2, %r45, %r17;
(EngineCore_DP0 pid=302504) 	.loc	1 190 29                        // quant_slide_tuned_Llama3.2-1B.py:190:29
(EngineCore_DP0 pid=302504) 	mad.wide.s32 	%rd6, %r45, 2, %rd1;
(EngineCore_DP0 pid=302504) 	.loc	1 190 21                        // quant_slide_tuned_Llama3.2-1B.py:190:21
(EngineCore_DP0 pid=302504) 	// begin inline asm
(EngineCore_DP0 pid=302504) 	mov.u32 %r35, %r37;
(EngineCore_DP0 pid=302504) 	mov.u32 %r36, %r37;
(EngineCore_DP0 pid=302504) 	@%p2 ld.global.v2.b32 { %r35, %r36 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=302504) 	// end inline asm
(EngineCore_DP0 pid=302504) 	mov.b32 	{%rs1, %rs2}, %r35;
(EngineCore_DP0 pid=302504) 	mov.b32 	{%rs3, %rs4}, %r36;
(EngineCore_DP0 pid=302504) 	.loc	1 191 50                        // quant_slide_tuned_Llama3.2-1B.py:191:50
(EngineCore_DP0 pid=302504) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=302504) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=302504) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=302504) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=302504) $L__tmp1:
(EngineCore_DP0 pid=302504) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	bar.sync 	0;
(EngineCore_DP0 pid=302504) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=302504) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=302504) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=302504) 	cvt.f32.bf16 	%r46, %rs11;
(EngineCore_DP0 pid=302504) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	shfl.sync.bfly.b32 	%r47, %r46, 16, 31, -1;
(EngineCore_DP0 pid=302504) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	max.f32 	%r48, %r46, %r47;
(EngineCore_DP0 pid=302504) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	shfl.sync.bfly.b32 	%r49, %r48, 8, 31, -1;
(EngineCore_DP0 pid=302504) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	max.f32 	%r50, %r48, %r49;
(EngineCore_DP0 pid=302504) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	shfl.sync.bfly.b32 	%r51, %r50, 4, 31, -1;
(EngineCore_DP0 pid=302504) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	max.f32 	%r52, %r50, %r51;
(EngineCore_DP0 pid=302504) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	shfl.sync.bfly.b32 	%r53, %r52, 2, 31, -1;
(EngineCore_DP0 pid=302504) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	max.f32 	%r54, %r52, %r53;
(EngineCore_DP0 pid=302504) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	shfl.sync.bfly.b32 	%r55, %r54, 1, 31, -1;
(EngineCore_DP0 pid=302504) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	max.f32 	%r40, %r54, %r55;
(EngineCore_DP0 pid=302504) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	// begin inline asm
(EngineCore_DP0 pid=302504) 	@%p3 st.shared.b32 [ %r39 + 0 ], %r40;
(EngineCore_DP0 pid=302504) 	// end inline asm
(EngineCore_DP0 pid=302504) 	bar.sync 	0;
(EngineCore_DP0 pid=302504) 	// begin inline asm
(EngineCore_DP0 pid=302504) 	@%p4 ld.shared.b32 %r41, [ %r42 + 0 ];
(EngineCore_DP0 pid=302504) 	// end inline asm
(EngineCore_DP0 pid=302504) 	shfl.sync.bfly.b32 	%r56, %r41, 16, 31, -1;
(EngineCore_DP0 pid=302504) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	max.f32 	%r57, %r41, %r56;
(EngineCore_DP0 pid=302504) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	shfl.sync.bfly.b32 	%r58, %r57, 8, 31, -1;
(EngineCore_DP0 pid=302504) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	max.f32 	%r59, %r57, %r58;
(EngineCore_DP0 pid=302504) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	shfl.sync.bfly.b32 	%r60, %r59, 4, 31, -1;
(EngineCore_DP0 pid=302504) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	max.f32 	%r61, %r59, %r60;
(EngineCore_DP0 pid=302504) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	shfl.sync.bfly.b32 	%r62, %r61, 2, 31, -1;
(EngineCore_DP0 pid=302504) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	max.f32 	%r63, %r61, %r62;
(EngineCore_DP0 pid=302504) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	shfl.sync.bfly.b32 	%r64, %r63, 1, 31, -1;
(EngineCore_DP0 pid=302504) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	max.f32 	%r44, %r63, %r64;
(EngineCore_DP0 pid=302504) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302504) 	// begin inline asm
(EngineCore_DP0 pid=302504) 	@%p19 st.shared.b32 [ %r42 + 0 ], %r44;
(EngineCore_DP0 pid=302504) 	// end inline asm
(EngineCore_DP0 pid=302504) 	bar.sync 	0;
(EngineCore_DP0 pid=302504) 	ld.shared.b32 	%r65, [global_smem];
(EngineCore_DP0 pid=302504) $L__tmp2:
(EngineCore_DP0 pid=302504) 	.loc	1 191 36                        // quant_slide_tuned_Llama3.2-1B.py:191:36
(EngineCore_DP0 pid=302504) 	max.f32 	%r110, %r110, %r65;
(EngineCore_DP0 pid=302504) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=302504) 	add.s32 	%r111, %r111, 4096;
(EngineCore_DP0 pid=302504) 	setp.lt.s32 	%p6, %r111, %r18;
(EngineCore_DP0 pid=302504) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=302504) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=302504) 	.loc	1 193 32                        // quant_slide_tuned_Llama3.2-1B.py:193:32
(EngineCore_DP0 pid=302504) 	max.f32 	%r112, %r110, 0f2B8CBCCC;
(EngineCore_DP0 pid=302504) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=302504) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=302504) 	mov.b32 	%r67, 0f43E00000;
(EngineCore_DP0 pid=302504) 	.loc	1 194 32                        // quant_slide_tuned_Llama3.2-1B.py:194:32
(EngineCore_DP0 pid=302504) 	div.full.f32 	%r68, %r112, %r67;
(EngineCore_DP0 pid=302504) 	.loc	1 194 41                        // quant_slide_tuned_Llama3.2-1B.py:194:41
(EngineCore_DP0 pid=302504) 	max.f32 	%r66, %r68, 0f36924925;
(EngineCore_DP0 pid=302504) 	.loc	1 196 25                        // quant_slide_tuned_Llama3.2-1B.py:196:25
(EngineCore_DP0 pid=302504) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=302504) 	.loc	1 196 30                        // quant_slide_tuned_Llama3.2-1B.py:196:30
(EngineCore_DP0 pid=302504) 	// begin inline asm
(EngineCore_DP0 pid=302504) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r66 };
(EngineCore_DP0 pid=302504) 	// end inline asm
(EngineCore_DP0 pid=302504) 	.loc	1 199 29                        // quant_slide_tuned_Llama3.2-1B.py:199:29
(EngineCore_DP0 pid=302504) 	shl.b32 	%r14, %r19, 1;
(EngineCore_DP0 pid=302504) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=302504) 	setp.lt.s32 	%p8, %r14, 1;
(EngineCore_DP0 pid=302504) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=302504) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=302504) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=302504) 	ld.param.b32 	%r23, [_quant_slide_fp8_kernel_param_9];
(EngineCore_DP0 pid=302504) 	shr.s32 	%r24, %r23, 31;
(EngineCore_DP0 pid=302504) 	shr.u32 	%r25, %r24, 30;
(EngineCore_DP0 pid=302504) 	add.s32 	%r26, %r23, %r25;
(EngineCore_DP0 pid=302504) 	shr.s32 	%r27, %r26, 2;
(EngineCore_DP0 pid=302504) 	ld.param.b64 	%rd5, [_quant_slide_fp8_kernel_param_1];
(EngineCore_DP0 pid=302504) 	mul.lo.s32 	%r28, %r27, %r1;
(EngineCore_DP0 pid=302504) 	mad.wide.s32 	%rd2, %r28, 4, %rd5;
(EngineCore_DP0 pid=302504) 	div.full.f32 	%r13, %r67, %r112;
(EngineCore_DP0 pid=302504) 	mov.b32 	%r113, 0;
(EngineCore_DP0 pid=302504) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=302504)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=302504) 	.loc	1 203 30                        // quant_slide_tuned_Llama3.2-1B.py:203:30
(EngineCore_DP0 pid=302504) 	add.s32 	%r80, %r2, %r113;
(EngineCore_DP0 pid=302504) 	setp.lt.s32 	%p13, %r80, %r14;
(EngineCore_DP0 pid=302504) 	.loc	1 206 24                        // quant_slide_tuned_Llama3.2-1B.py:206:24
(EngineCore_DP0 pid=302504) 	shr.u32 	%r81, %r80, 31;
(EngineCore_DP0 pid=302504) 	add.s32 	%r82, %r80, %r81;
(EngineCore_DP0 pid=302504) 	shr.u32 	%r83, %r82, 1;
(EngineCore_DP0 pid=302504) 	.loc	1 207 23                        // quant_slide_tuned_Llama3.2-1B.py:207:23
(EngineCore_DP0 pid=302504) 	and.b32 	%r84, %r82, 2147483646;
(EngineCore_DP0 pid=302504) 	sub.s32 	%r85, %r80, %r84;
(EngineCore_DP0 pid=302504) 	.loc	1 208 30                        // quant_slide_tuned_Llama3.2-1B.py:208:30
(EngineCore_DP0 pid=302504) 	shl.b32 	%r86, %r85, 1;
(EngineCore_DP0 pid=302504) 	.loc	1 208 26                        // quant_slide_tuned_Llama3.2-1B.py:208:26
(EngineCore_DP0 pid=302504) 	mad.lo.s32 	%r87, %r83, 6, %r86;
(EngineCore_DP0 pid=302504) 	.loc	1 211 53                        // quant_slide_tuned_Llama3.2-1B.py:211:53
(EngineCore_DP0 pid=302504) 	setp.lt.s32 	%p14, %r87, %r17;
(EngineCore_DP0 pid=302504) 	.loc	1 211 37                        // quant_slide_tuned_Llama3.2-1B.py:211:37
(EngineCore_DP0 pid=302504) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=302504) 	.loc	1 210 29                        // quant_slide_tuned_Llama3.2-1B.py:210:29
(EngineCore_DP0 pid=302504) 	mad.wide.s32 	%rd8, %r87, 2, %rd1;
(EngineCore_DP0 pid=302504) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=302504) 	.loc	1 210 21                        // quant_slide_tuned_Llama3.2-1B.py:210:21
(EngineCore_DP0 pid=302504) 	// begin inline asm
(EngineCore_DP0 pid=302504) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=302504) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=302504) 	// end inline asm
(EngineCore_DP0 pid=302504) 	.loc	1 211 79                        // quant_slide_tuned_Llama3.2-1B.py:211:79
(EngineCore_DP0 pid=302504) 	cvt.f32.bf16 	%r88, %rs12;
(EngineCore_DP0 pid=302504) 	.loc	1 213 48                        // quant_slide_tuned_Llama3.2-1B.py:213:48
(EngineCore_DP0 pid=302504) 	or.b32 	%r89, %r87, 1;
(EngineCore_DP0 pid=302504) 	.loc	1 213 53                        // quant_slide_tuned_Llama3.2-1B.py:213:53
(EngineCore_DP0 pid=302504) 	setp.lt.s32 	%p15, %r89, %r17;
(EngineCore_DP0 pid=302504) 	.loc	1 213 37                        // quant_slide_tuned_Llama3.2-1B.py:213:37
(EngineCore_DP0 pid=302504) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=302504) 	.loc	1 212 39                        // quant_slide_tuned_Llama3.2-1B.py:212:39
(EngineCore_DP0 pid=302504) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=302504) 	.loc	1 212 21                        // quant_slide_tuned_Llama3.2-1B.py:212:21
(EngineCore_DP0 pid=302504) 	// begin inline asm
(EngineCore_DP0 pid=302504) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=302504) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=302504) 	// end inline asm
(EngineCore_DP0 pid=302504) 	.loc	1 213 79                        // quant_slide_tuned_Llama3.2-1B.py:213:79
(EngineCore_DP0 pid=302504) 	cvt.f32.bf16 	%r90, %rs14;
(EngineCore_DP0 pid=302504) 	.loc	1 215 48                        // quant_slide_tuned_Llama3.2-1B.py:215:48
(EngineCore_DP0 pid=302504) 	add.s32 	%r91, %r87, 2;
(EngineCore_DP0 pid=302504) 	.loc	1 215 53                        // quant_slide_tuned_Llama3.2-1B.py:215:53
(EngineCore_DP0 pid=302504) 	setp.lt.s32 	%p16, %r91, %r17;
(EngineCore_DP0 pid=302504) 	.loc	1 215 37                        // quant_slide_tuned_Llama3.2-1B.py:215:37
(EngineCore_DP0 pid=302504) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=302504) 	.loc	1 214 39                        // quant_slide_tuned_Llama3.2-1B.py:214:39
(EngineCore_DP0 pid=302504) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=302504) 	.loc	1 214 21                        // quant_slide_tuned_Llama3.2-1B.py:214:21
(EngineCore_DP0 pid=302504) 	// begin inline asm
(EngineCore_DP0 pid=302504) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=302504) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=302504) 	// end inline asm
(EngineCore_DP0 pid=302504) 	.loc	1 215 79                        // quant_slide_tuned_Llama3.2-1B.py:215:79
(EngineCore_DP0 pid=302504) 	cvt.f32.bf16 	%r92, %rs16;
(EngineCore_DP0 pid=302504) 	.loc	1 217 48                        // quant_slide_tuned_Llama3.2-1B.py:217:48
(EngineCore_DP0 pid=302504) 	add.s32 	%r93, %r87, 3;
(EngineCore_DP0 pid=302504) 	.loc	1 217 53                        // quant_slide_tuned_Llama3.2-1B.py:217:53
(EngineCore_DP0 pid=302504) 	setp.lt.s32 	%p17, %r93, %r17;
(EngineCore_DP0 pid=302504) 	.loc	1 217 37                        // quant_slide_tuned_Llama3.2-1B.py:217:37
(EngineCore_DP0 pid=302504) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=302504) 	.loc	1 216 39                        // quant_slide_tuned_Llama3.2-1B.py:216:39
(EngineCore_DP0 pid=302504) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=302504) 	.loc	1 216 21                        // quant_slide_tuned_Llama3.2-1B.py:216:21
(EngineCore_DP0 pid=302504) 	// begin inline asm
(EngineCore_DP0 pid=302504) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=302504) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=302504) 	// end inline asm
(EngineCore_DP0 pid=302504) 	.loc	1 217 79                        // quant_slide_tuned_Llama3.2-1B.py:217:79
(EngineCore_DP0 pid=302504) 	cvt.f32.bf16 	%r94, %rs18;
(EngineCore_DP0 pid=302504) 	.loc	1 219 27                        // quant_slide_tuned_Llama3.2-1B.py:219:27
(EngineCore_DP0 pid=302504) 	mul.f32 	%r95, %r13, %r88;
(EngineCore_DP0 pid=302504) 	mov.b32 	%r96, 0f43E00000;
(EngineCore_DP0 pid=302504) 	.loc	1 219 48                        // quant_slide_tuned_Llama3.2-1B.py:219:48
(EngineCore_DP0 pid=302504) 	min.xorsign.abs.f32 	%r70, %r95, %r96;
(EngineCore_DP0 pid=302504) 	.loc	1 219 60                        // quant_slide_tuned_Llama3.2-1B.py:219:60
(EngineCore_DP0 pid=302504) 	// begin inline asm
(EngineCore_DP0 pid=302504) 	cvt.rn.satfinite.e4m3x2.f32  %rs20, %r71, %r70; 
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) 	// end inline asm
(EngineCore_DP0 pid=302504) 	.loc	1 220 27                        // quant_slide_tuned_Llama3.2-1B.py:220:27
(EngineCore_DP0 pid=302504) 	mul.f32 	%r97, %r13, %r90;
(EngineCore_DP0 pid=302504) 	.loc	1 220 48                        // quant_slide_tuned_Llama3.2-1B.py:220:48
(EngineCore_DP0 pid=302504) 	min.xorsign.abs.f32 	%r72, %r97, %r96;
(EngineCore_DP0 pid=302504) 	.loc	1 220 60                        // quant_slide_tuned_Llama3.2-1B.py:220:60
(EngineCore_DP0 pid=302504) 	// begin inline asm
(EngineCore_DP0 pid=302504) 	cvt.rn.satfinite.e4m3x2.f32  %rs21, %r73, %r72; 
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) 	// end inline asm
(EngineCore_DP0 pid=302504) 	.loc	1 221 27                        // quant_slide_tuned_Llama3.2-1B.py:221:27
(EngineCore_DP0 pid=302504) 	mul.f32 	%r98, %r13, %r92;
(EngineCore_DP0 pid=302504) 	.loc	1 221 48                        // quant_slide_tuned_Llama3.2-1B.py:221:48
(EngineCore_DP0 pid=302504) 	min.xorsign.abs.f32 	%r74, %r98, %r96;
(EngineCore_DP0 pid=302504) 	.loc	1 221 60                        // quant_slide_tuned_Llama3.2-1B.py:221:60
(EngineCore_DP0 pid=302504) 	// begin inline asm
(EngineCore_DP0 pid=302504) 	cvt.rn.satfinite.e4m3x2.f32  %rs22, %r75, %r74; 
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) 	// end inline asm
(EngineCore_DP0 pid=302504) 	.loc	1 222 27                        // quant_slide_tuned_Llama3.2-1B.py:222:27
(EngineCore_DP0 pid=302504) 	mul.f32 	%r99, %r13, %r94;
(EngineCore_DP0 pid=302504) 	.loc	1 222 48                        // quant_slide_tuned_Llama3.2-1B.py:222:48
(EngineCore_DP0 pid=302504) 	min.xorsign.abs.f32 	%r76, %r99, %r96;
(EngineCore_DP0 pid=302504) 	.loc	1 222 60                        // quant_slide_tuned_Llama3.2-1B.py:222:60
(EngineCore_DP0 pid=302504) 	// begin inline asm
(EngineCore_DP0 pid=302504) 	cvt.rn.satfinite.e4m3x2.f32  %rs23, %r77, %r76; 
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) 	// end inline asm
(EngineCore_DP0 pid=302504) 	.loc	1 224 45                        // quant_slide_tuned_Llama3.2-1B.py:224:45
(EngineCore_DP0 pid=302504) 	cvt.u32.u16 	%r100, %rs20;
(EngineCore_DP0 pid=302504) 	and.b32 	%r101, %r100, 255;
(EngineCore_DP0 pid=302504) 	.loc	1 226 45                        // quant_slide_tuned_Llama3.2-1B.py:226:45
(EngineCore_DP0 pid=302504) 	cvt.u32.u16 	%r102, %rs22;
(EngineCore_DP0 pid=302504) 	and.b32 	%r103, %r102, 255;
(EngineCore_DP0 pid=302504) 	.loc	1 227 45                        // quant_slide_tuned_Llama3.2-1B.py:227:45
(EngineCore_DP0 pid=302504) 	cvt.u32.u16 	%r104, %rs23;
(EngineCore_DP0 pid=302504) 	.loc	1 229 30                        // quant_slide_tuned_Llama3.2-1B.py:229:30
(EngineCore_DP0 pid=302504) 	and.b16 	%rs24, %rs21, 255;
(EngineCore_DP0 pid=302504) 	mul.wide.u16 	%r105, %rs24, 256;
(EngineCore_DP0 pid=302504) 	.loc	1 229 24                        // quant_slide_tuned_Llama3.2-1B.py:229:24
(EngineCore_DP0 pid=302504) 	or.b32 	%r106, %r105, %r101;
(EngineCore_DP0 pid=302504) 	.loc	1 229 42                        // quant_slide_tuned_Llama3.2-1B.py:229:42
(EngineCore_DP0 pid=302504) 	shl.b32 	%r107, %r103, 16;
(EngineCore_DP0 pid=302504) 	.loc	1 229 36                        // quant_slide_tuned_Llama3.2-1B.py:229:36
(EngineCore_DP0 pid=302504) 	or.b32 	%r108, %r106, %r107;
(EngineCore_DP0 pid=302504) 	.loc	1 229 55                        // quant_slide_tuned_Llama3.2-1B.py:229:55
(EngineCore_DP0 pid=302504) 	shl.b32 	%r109, %r104, 24;
(EngineCore_DP0 pid=302504) 	.loc	1 229 49                        // quant_slide_tuned_Llama3.2-1B.py:229:49
(EngineCore_DP0 pid=302504) 	or.b32 	%r78, %r108, %r109;
(EngineCore_DP0 pid=302504) 	.loc	1 230 29                        // quant_slide_tuned_Llama3.2-1B.py:230:29
(EngineCore_DP0 pid=302504) 	mad.wide.s32 	%rd12, %r80, 4, %rd2;
(EngineCore_DP0 pid=302504) 	.loc	1 230 39                        // quant_slide_tuned_Llama3.2-1B.py:230:39
(EngineCore_DP0 pid=302504) 	// begin inline asm
(EngineCore_DP0 pid=302504) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r78 };
(EngineCore_DP0 pid=302504) 	// end inline asm
(EngineCore_DP0 pid=302504) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=302504) 	add.s32 	%r113, %r113, 1024;
(EngineCore_DP0 pid=302504) 	setp.lt.s32 	%p18, %r113, %r14;
(EngineCore_DP0 pid=302504) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=302504) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=302504) 	.loc	1 201 4                         // quant_slide_tuned_Llama3.2-1B.py:201:4
(EngineCore_DP0 pid=302504) 	ret;
(EngineCore_DP0 pid=302504) $L__tmp3:
(EngineCore_DP0 pid=302504) $L__func_end0:
(EngineCore_DP0 pid=302504)                                         // -- End function
(EngineCore_DP0 pid=302504) }
(EngineCore_DP0 pid=302504) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=302504) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=302504) 	.section	.debug_abbrev
(EngineCore_DP0 pid=302504) 	{
(EngineCore_DP0 pid=302504) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=302504) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=302504) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=302504) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=302504) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=302504) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=302504) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=302504) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=302504) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=302504) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=302504) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=302504) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=302504) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=302504) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=302504) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=302504) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=302504) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=302504) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=302504) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=302504) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=302504) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=302504) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=302504) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=302504) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=302504) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=302504) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=302504) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=302504) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=302504) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=302504) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=302504) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=302504) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=302504) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=302504) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=302504) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=302504) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=302504) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=302504) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=302504) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=302504) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=302504) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=302504) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=302504) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=302504) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=302504) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=302504) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=302504) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=302504) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=302504) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=302504) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=302504) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=302504) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=302504) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=302504) 	}
(EngineCore_DP0 pid=302504) 	.section	.debug_info
(EngineCore_DP0 pid=302504) 	{
(EngineCore_DP0 pid=302504) .b32 222                                // Length of Unit
(EngineCore_DP0 pid=302504) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=302504) .b8 0
(EngineCore_DP0 pid=302504) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=302504) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=302504) .b8 1                                   // Abbrev [1] 0xb:0xd7 DW_TAG_compile_unit
(EngineCore_DP0 pid=302504) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=302504) .b8 114
(EngineCore_DP0 pid=302504) .b8 105
(EngineCore_DP0 pid=302504) .b8 116
(EngineCore_DP0 pid=302504) .b8 111
(EngineCore_DP0 pid=302504) .b8 110
(EngineCore_DP0 pid=302504) .b8 0
(EngineCore_DP0 pid=302504) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=302504) .b8 0
(EngineCore_DP0 pid=302504) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=302504) .b8 117
(EngineCore_DP0 pid=302504) .b8 97
(EngineCore_DP0 pid=302504) .b8 110
(EngineCore_DP0 pid=302504) .b8 116
(EngineCore_DP0 pid=302504) .b8 95
(EngineCore_DP0 pid=302504) .b8 115
(EngineCore_DP0 pid=302504) .b8 108
(EngineCore_DP0 pid=302504) .b8 105
(EngineCore_DP0 pid=302504) .b8 100
(EngineCore_DP0 pid=302504) .b8 101
(EngineCore_DP0 pid=302504) .b8 95
(EngineCore_DP0 pid=302504) .b8 116
(EngineCore_DP0 pid=302504) .b8 117
(EngineCore_DP0 pid=302504) .b8 110
(EngineCore_DP0 pid=302504) .b8 101
(EngineCore_DP0 pid=302504) .b8 100
(EngineCore_DP0 pid=302504) .b8 95
(EngineCore_DP0 pid=302504) .b8 76
(EngineCore_DP0 pid=302504) .b8 108
(EngineCore_DP0 pid=302504) .b8 97
(EngineCore_DP0 pid=302504) .b8 109
(EngineCore_DP0 pid=302504) .b8 97
(EngineCore_DP0 pid=302504) .b8 51
(EngineCore_DP0 pid=302504) .b8 46
(EngineCore_DP0 pid=302504) .b8 50
(EngineCore_DP0 pid=302504) .b8 45
(EngineCore_DP0 pid=302504) .b8 49
(EngineCore_DP0 pid=302504) .b8 66
(EngineCore_DP0 pid=302504) .b8 46
(EngineCore_DP0 pid=302504) .b8 112
(EngineCore_DP0 pid=302504) .b8 121
(EngineCore_DP0 pid=302504) .b8 0
(EngineCore_DP0 pid=302504) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=302504) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=302504) .b8 114
(EngineCore_DP0 pid=302504) .b8 111
(EngineCore_DP0 pid=302504) .b8 111
(EngineCore_DP0 pid=302504) .b8 116
(EngineCore_DP0 pid=302504) .b8 47
(EngineCore_DP0 pid=302504) .b8 118
(EngineCore_DP0 pid=302504) .b8 108
(EngineCore_DP0 pid=302504) .b8 108
(EngineCore_DP0 pid=302504) .b8 109
(EngineCore_DP0 pid=302504) .b8 98
(EngineCore_DP0 pid=302504) .b8 101
(EngineCore_DP0 pid=302504) .b8 110
(EngineCore_DP0 pid=302504) .b8 99
(EngineCore_DP0 pid=302504) .b8 104
(EngineCore_DP0 pid=302504) .b8 47
(EngineCore_DP0 pid=302504) .b8 115
(EngineCore_DP0 pid=302504) .b8 108
(EngineCore_DP0 pid=302504) .b8 105
(EngineCore_DP0 pid=302504) .b8 100
(EngineCore_DP0 pid=302504) .b8 101
(EngineCore_DP0 pid=302504) .b8 115
(EngineCore_DP0 pid=302504) .b8 112
(EngineCore_DP0 pid=302504) .b8 97
(EngineCore_DP0 pid=302504) .b8 114
(EngineCore_DP0 pid=302504) .b8 115
(EngineCore_DP0 pid=302504) .b8 101
(EngineCore_DP0 pid=302504) .b8 47
(EngineCore_DP0 pid=302504) .b8 99
(EngineCore_DP0 pid=302504) .b8 115
(EngineCore_DP0 pid=302504) .b8 114
(EngineCore_DP0 pid=302504) .b8 99
(EngineCore_DP0 pid=302504) .b8 47
(EngineCore_DP0 pid=302504) .b8 102
(EngineCore_DP0 pid=302504) .b8 117
(EngineCore_DP0 pid=302504) .b8 115
(EngineCore_DP0 pid=302504) .b8 101
(EngineCore_DP0 pid=302504) .b8 100
(EngineCore_DP0 pid=302504) .b8 95
(EngineCore_DP0 pid=302504) .b8 113
(EngineCore_DP0 pid=302504) .b8 117
(EngineCore_DP0 pid=302504) .b8 97
(EngineCore_DP0 pid=302504) .b8 110
(EngineCore_DP0 pid=302504) .b8 116
(EngineCore_DP0 pid=302504) .b8 95
(EngineCore_DP0 pid=302504) .b8 115
(EngineCore_DP0 pid=302504) .b8 108
(EngineCore_DP0 pid=302504) .b8 105
(EngineCore_DP0 pid=302504) .b8 100
(EngineCore_DP0 pid=302504) .b8 101
(EngineCore_DP0 pid=302504) .b8 95
(EngineCore_DP0 pid=302504) .b8 116
(EngineCore_DP0 pid=302504) .b8 114
(EngineCore_DP0 pid=302504) .b8 105
(EngineCore_DP0 pid=302504) .b8 116
(EngineCore_DP0 pid=302504) .b8 111
(EngineCore_DP0 pid=302504) .b8 110
(EngineCore_DP0 pid=302504) .b8 47
(EngineCore_DP0 pid=302504) .b8 98
(EngineCore_DP0 pid=302504) .b8 117
(EngineCore_DP0 pid=302504) .b8 105
(EngineCore_DP0 pid=302504) .b8 108
(EngineCore_DP0 pid=302504) .b8 100
(EngineCore_DP0 pid=302504) .b8 47
(EngineCore_DP0 pid=302504) .b8 71
(EngineCore_DP0 pid=302504) .b8 66
(EngineCore_DP0 pid=302504) .b8 49
(EngineCore_DP0 pid=302504) .b8 48
(EngineCore_DP0 pid=302504) .b8 95
(EngineCore_DP0 pid=302504) .b8 99
(EngineCore_DP0 pid=302504) .b8 99
(EngineCore_DP0 pid=302504) .b8 49
(EngineCore_DP0 pid=302504) .b8 50
(EngineCore_DP0 pid=302504) .b8 49
(EngineCore_DP0 pid=302504) .b8 95
(EngineCore_DP0 pid=302504) .b8 112
(EngineCore_DP0 pid=302504) .b8 121
(EngineCore_DP0 pid=302504) .b8 51
(EngineCore_DP0 pid=302504) .b8 49
(EngineCore_DP0 pid=302504) .b8 50
(EngineCore_DP0 pid=302504) .b8 95
(EngineCore_DP0 pid=302504) .b8 99
(EngineCore_DP0 pid=302504) .b8 117
(EngineCore_DP0 pid=302504) .b8 49
(EngineCore_DP0 pid=302504) .b8 50
(EngineCore_DP0 pid=302504) .b8 57
(EngineCore_DP0 pid=302504) .b8 95
(EngineCore_DP0 pid=302504) .b8 97
(EngineCore_DP0 pid=302504) .b8 97
(EngineCore_DP0 pid=302504) .b8 114
(EngineCore_DP0 pid=302504) .b8 99
(EngineCore_DP0 pid=302504) .b8 104
(EngineCore_DP0 pid=302504) .b8 54
(EngineCore_DP0 pid=302504) .b8 52
(EngineCore_DP0 pid=302504) .b8 0
(EngineCore_DP0 pid=302504) .b8 2                                   // Abbrev [2] 0x99:0x1a DW_TAG_subprogram
(EngineCore_DP0 pid=302504) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=302504) .b8 113
(EngineCore_DP0 pid=302504) .b8 117
(EngineCore_DP0 pid=302504) .b8 97
(EngineCore_DP0 pid=302504) .b8 110
(EngineCore_DP0 pid=302504) .b8 116
(EngineCore_DP0 pid=302504) .b8 95
(EngineCore_DP0 pid=302504) .b8 115
(EngineCore_DP0 pid=302504) .b8 108
(EngineCore_DP0 pid=302504) .b8 105
(EngineCore_DP0 pid=302504) .b8 100
(EngineCore_DP0 pid=302504) .b8 101
(EngineCore_DP0 pid=302504) .b8 95
(EngineCore_DP0 pid=302504) .b8 102
(EngineCore_DP0 pid=302504) .b8 112
(EngineCore_DP0 pid=302504) .b8 56
(EngineCore_DP0 pid=302504) .b8 95
(EngineCore_DP0 pid=302504) .b8 107
(EngineCore_DP0 pid=302504) .b8 101
(EngineCore_DP0 pid=302504) .b8 114
(EngineCore_DP0 pid=302504) .b8 110
(EngineCore_DP0 pid=302504) .b8 101
(EngineCore_DP0 pid=302504) .b8 108
(EngineCore_DP0 pid=302504) .b8 0
(EngineCore_DP0 pid=302504) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=302504) .b8 3                                   // Abbrev [3] 0xb3:0x2e DW_TAG_subprogram
(EngineCore_DP0 pid=302504) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=302504) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=302504) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=302504) .b8 4                                   // Abbrev [4] 0xc8:0x18 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=302504) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=302504) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=302504) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=302504) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=302504) .b8 191                                 // DW_AT_call_line
(EngineCore_DP0 pid=302504) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=302504) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=302504) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=302504) 	}
(EngineCore_DP0 pid=302504) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) ================================================================
(EngineCore_DP0 pid=302504) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmppexlqa4z.ptx', '-o', '/tmp/tmppexlqa4z.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866] 
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866] 
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866] 
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmppexlqa4z.ptx -o /tmp/tmppexlqa4z.ptx.o
(EngineCore_DP0 pid=302504) ERROR 01-25 18:49:35 [core.py:866] 

STDERR:
[2026-01-25 18:49:20] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:49:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:49:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:49:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:49:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:49:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:49:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:49:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:49:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:49:24] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:49:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:49:24] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:49:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:49:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:49:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:49:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:49:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:49:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=302504) [2026-01-25 18:49:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=302504) [2026-01-25 18:49:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=302504) [2026-01-25 18:49:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=302504) [2026-01-25 18:49:25] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=302504) [2026-01-25 18:49:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=302504) [2026-01-25 18:49:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=302504) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=302504) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.29s/it]
(EngineCore_DP0 pid=302504) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.29s/it]
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) [2026-01-25 18:49:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=302504) [2026-01-25 18:49:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=302504) [2026-01-25 18:49:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=302504) [2026-01-25 18:49:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=302504) [2026-01-25 18:49:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=302504) [2026-01-25 18:49:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=302504) [2026-01-25 18:49:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=302504) [2026-01-25 18:49:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=302504) Process EngineCore_DP0:
(EngineCore_DP0 pid=302504) Traceback (most recent call last):
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=302504)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=302504)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=302504)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=302504) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmppexlqa4z.ptx', '-o', '/tmp/tmppexlqa4z.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) Traceback (most recent call last):
(EngineCore_DP0 pid=302504)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=302504)     self.run()
(EngineCore_DP0 pid=302504)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=302504)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=302504)     raise e
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=302504)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=302504)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=302504)     super().__init__(
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=302504)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=302504)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=302504)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=302504)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=302504)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=302504)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=302504)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=302504)     return func(*args, **kwargs)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=302504)     return func(*args, **kwargs)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=302504)     self.model_runner.profile_run()
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=302504)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=302504)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=302504)     return func(*args, **kwargs)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=302504)     outputs = self.model(
(EngineCore_DP0 pid=302504)               ^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302504)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302504)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=302504)     model_output = self.model(
(EngineCore_DP0 pid=302504)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=302504)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=302504)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=302504)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302504)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302504)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=302504)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=302504)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302504)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302504)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=302504)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=302504)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302504)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302504)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=302504)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=302504)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=302504)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=302504)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=302504)     return self._linear_fn(
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=302504)     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=302504)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=302504)     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=302504)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=302504)     return fn(input, L)
(EngineCore_DP0 pid=302504)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=302504)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=302504)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=302504)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=302504)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=302504)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=302504)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=302504)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=302504)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=302504)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=302504)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=302504)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302504)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=302504)     raise PTXASError(error)
(EngineCore_DP0 pid=302504) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=302504) `ptxas` stderr:
(EngineCore_DP0 pid=302504) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=302504) 
(EngineCore_DP0 pid=302504) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmppexlqa4z.ptx -o /tmp/tmppexlqa4z.ptx.o
(EngineCore_DP0 pid=302504) 
[rank0]:[W125 18:49:36.306649889 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=128

========== M=256 ==========
Time: 2026-01-25 18:49:37
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:49:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:49:41 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=302967) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) ================================================================
(EngineCore_DP0 pid=302967) Internal Triton PTX codegen error
(EngineCore_DP0 pid=302967) `ptxas` stderr:
(EngineCore_DP0 pid=302967) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpfmv_fl6z.ptx -o /tmp/tmpfmv_fl6z.ptx.o
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) //
(EngineCore_DP0 pid=302967) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=302967) //
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) .version 8.7
(EngineCore_DP0 pid=302967) .target sm_121a
(EngineCore_DP0 pid=302967) .address_size 64
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) 	// .globl	_quant_slide_fp8_kernel // -- Begin function _quant_slide_fp8_kernel
(EngineCore_DP0 pid=302967) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=302967)                                         // @_quant_slide_fp8_kernel
(EngineCore_DP0 pid=302967) .visible .entry _quant_slide_fp8_kernel(
(EngineCore_DP0 pid=302967) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_0,
(EngineCore_DP0 pid=302967) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_1,
(EngineCore_DP0 pid=302967) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_2,
(EngineCore_DP0 pid=302967) 	.param .u32 _quant_slide_fp8_kernel_param_3,
(EngineCore_DP0 pid=302967) 	.param .u32 _quant_slide_fp8_kernel_param_4,
(EngineCore_DP0 pid=302967) 	.param .u32 _quant_slide_fp8_kernel_param_5,
(EngineCore_DP0 pid=302967) 	.param .u32 _quant_slide_fp8_kernel_param_6,
(EngineCore_DP0 pid=302967) 	.param .u32 _quant_slide_fp8_kernel_param_7,
(EngineCore_DP0 pid=302967) 	.param .u32 _quant_slide_fp8_kernel_param_8,
(EngineCore_DP0 pid=302967) 	.param .u32 _quant_slide_fp8_kernel_param_9,
(EngineCore_DP0 pid=302967) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_10,
(EngineCore_DP0 pid=302967) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_11
(EngineCore_DP0 pid=302967) )
(EngineCore_DP0 pid=302967) .reqntid 1024
(EngineCore_DP0 pid=302967) {
(EngineCore_DP0 pid=302967) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=302967) 	.reg .b16 	%rs<25>;
(EngineCore_DP0 pid=302967) 	.reg .b32 	%r<114>;
(EngineCore_DP0 pid=302967) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=302967) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=302967) $L__func_begin0:
(EngineCore_DP0 pid=302967) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) // %bb.0:
(EngineCore_DP0 pid=302967) 	ld.param.b32 	%r19, [_quant_slide_fp8_kernel_param_7];
(EngineCore_DP0 pid=302967) 	ld.param.b32 	%r18, [_quant_slide_fp8_kernel_param_5];
(EngineCore_DP0 pid=302967) 	ld.param.b32 	%r17, [_quant_slide_fp8_kernel_param_4];
(EngineCore_DP0 pid=302967) 	ld.param.b64 	%rd3, [_quant_slide_fp8_kernel_param_2];
(EngineCore_DP0 pid=302967) 	ld.param.b64 	%rd4, [_quant_slide_fp8_kernel_param_0];
(EngineCore_DP0 pid=302967) $L__tmp0:
(EngineCore_DP0 pid=302967) 	.loc	1 177 24                        // quant_slide_tuned_Llama3.2-1B.py:177:24
(EngineCore_DP0 pid=302967) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=302967) 	ld.param.b32 	%r21, [_quant_slide_fp8_kernel_param_8];
(EngineCore_DP0 pid=302967) 	.loc	1 182 26                        // quant_slide_tuned_Llama3.2-1B.py:182:26
(EngineCore_DP0 pid=302967) 	mul.lo.s32 	%r22, %r21, %r1;
(EngineCore_DP0 pid=302967) 	.loc	1 182 20                        // quant_slide_tuned_Llama3.2-1B.py:182:20
(EngineCore_DP0 pid=302967) 	mad.wide.s32 	%rd1, %r22, 2, %rd4;
(EngineCore_DP0 pid=302967) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=302967) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=302967) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=302967) 	setp.lt.s32 	%p1, %r18, 1;
(EngineCore_DP0 pid=302967) 	mov.b32 	%r112, 0f2B8CBCCC;
(EngineCore_DP0 pid=302967) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=302967) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=302967) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=302967) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=302967) 	shl.b32 	%r3, %r2, 2;
(EngineCore_DP0 pid=302967) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=302967) 	shr.u32 	%r31, %r2, 3;
(EngineCore_DP0 pid=302967) 	and.b32 	%r32, %r31, 124;
(EngineCore_DP0 pid=302967) 	mov.b32 	%r33, global_smem;
(EngineCore_DP0 pid=302967) 	add.s32 	%r39, %r33, %r32;
(EngineCore_DP0 pid=302967) 	add.s32 	%r42, %r33, %r3;
(EngineCore_DP0 pid=302967) 	mov.b32 	%r37, 0;
(EngineCore_DP0 pid=302967) 	mov.b32 	%r110, 0f00000000;
(EngineCore_DP0 pid=302967) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=302967) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=302967) 	mov.b32 	%r111, %r37;
(EngineCore_DP0 pid=302967) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=302967) 	.loc	1 189 22                        // quant_slide_tuned_Llama3.2-1B.py:189:22
(EngineCore_DP0 pid=302967) 	add.s32 	%r45, %r3, %r111;
(EngineCore_DP0 pid=302967) 	setp.lt.s32 	%p2, %r45, %r17;
(EngineCore_DP0 pid=302967) 	.loc	1 190 29                        // quant_slide_tuned_Llama3.2-1B.py:190:29
(EngineCore_DP0 pid=302967) 	mad.wide.s32 	%rd6, %r45, 2, %rd1;
(EngineCore_DP0 pid=302967) 	.loc	1 190 21                        // quant_slide_tuned_Llama3.2-1B.py:190:21
(EngineCore_DP0 pid=302967) 	// begin inline asm
(EngineCore_DP0 pid=302967) 	mov.u32 %r35, %r37;
(EngineCore_DP0 pid=302967) 	mov.u32 %r36, %r37;
(EngineCore_DP0 pid=302967) 	@%p2 ld.global.v2.b32 { %r35, %r36 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=302967) 	// end inline asm
(EngineCore_DP0 pid=302967) 	mov.b32 	{%rs1, %rs2}, %r35;
(EngineCore_DP0 pid=302967) 	mov.b32 	{%rs3, %rs4}, %r36;
(EngineCore_DP0 pid=302967) 	.loc	1 191 50                        // quant_slide_tuned_Llama3.2-1B.py:191:50
(EngineCore_DP0 pid=302967) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=302967) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=302967) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=302967) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=302967) $L__tmp1:
(EngineCore_DP0 pid=302967) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	bar.sync 	0;
(EngineCore_DP0 pid=302967) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=302967) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=302967) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=302967) 	cvt.f32.bf16 	%r46, %rs11;
(EngineCore_DP0 pid=302967) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	shfl.sync.bfly.b32 	%r47, %r46, 16, 31, -1;
(EngineCore_DP0 pid=302967) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	max.f32 	%r48, %r46, %r47;
(EngineCore_DP0 pid=302967) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	shfl.sync.bfly.b32 	%r49, %r48, 8, 31, -1;
(EngineCore_DP0 pid=302967) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	max.f32 	%r50, %r48, %r49;
(EngineCore_DP0 pid=302967) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	shfl.sync.bfly.b32 	%r51, %r50, 4, 31, -1;
(EngineCore_DP0 pid=302967) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	max.f32 	%r52, %r50, %r51;
(EngineCore_DP0 pid=302967) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	shfl.sync.bfly.b32 	%r53, %r52, 2, 31, -1;
(EngineCore_DP0 pid=302967) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	max.f32 	%r54, %r52, %r53;
(EngineCore_DP0 pid=302967) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	shfl.sync.bfly.b32 	%r55, %r54, 1, 31, -1;
(EngineCore_DP0 pid=302967) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	max.f32 	%r40, %r54, %r55;
(EngineCore_DP0 pid=302967) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	// begin inline asm
(EngineCore_DP0 pid=302967) 	@%p3 st.shared.b32 [ %r39 + 0 ], %r40;
(EngineCore_DP0 pid=302967) 	// end inline asm
(EngineCore_DP0 pid=302967) 	bar.sync 	0;
(EngineCore_DP0 pid=302967) 	// begin inline asm
(EngineCore_DP0 pid=302967) 	@%p4 ld.shared.b32 %r41, [ %r42 + 0 ];
(EngineCore_DP0 pid=302967) 	// end inline asm
(EngineCore_DP0 pid=302967) 	shfl.sync.bfly.b32 	%r56, %r41, 16, 31, -1;
(EngineCore_DP0 pid=302967) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	max.f32 	%r57, %r41, %r56;
(EngineCore_DP0 pid=302967) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	shfl.sync.bfly.b32 	%r58, %r57, 8, 31, -1;
(EngineCore_DP0 pid=302967) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	max.f32 	%r59, %r57, %r58;
(EngineCore_DP0 pid=302967) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	shfl.sync.bfly.b32 	%r60, %r59, 4, 31, -1;
(EngineCore_DP0 pid=302967) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	max.f32 	%r61, %r59, %r60;
(EngineCore_DP0 pid=302967) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	shfl.sync.bfly.b32 	%r62, %r61, 2, 31, -1;
(EngineCore_DP0 pid=302967) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	max.f32 	%r63, %r61, %r62;
(EngineCore_DP0 pid=302967) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	shfl.sync.bfly.b32 	%r64, %r63, 1, 31, -1;
(EngineCore_DP0 pid=302967) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	max.f32 	%r44, %r63, %r64;
(EngineCore_DP0 pid=302967) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=302967) 	// begin inline asm
(EngineCore_DP0 pid=302967) 	@%p19 st.shared.b32 [ %r42 + 0 ], %r44;
(EngineCore_DP0 pid=302967) 	// end inline asm
(EngineCore_DP0 pid=302967) 	bar.sync 	0;
(EngineCore_DP0 pid=302967) 	ld.shared.b32 	%r65, [global_smem];
(EngineCore_DP0 pid=302967) $L__tmp2:
(EngineCore_DP0 pid=302967) 	.loc	1 191 36                        // quant_slide_tuned_Llama3.2-1B.py:191:36
(EngineCore_DP0 pid=302967) 	max.f32 	%r110, %r110, %r65;
(EngineCore_DP0 pid=302967) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=302967) 	add.s32 	%r111, %r111, 4096;
(EngineCore_DP0 pid=302967) 	setp.lt.s32 	%p6, %r111, %r18;
(EngineCore_DP0 pid=302967) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=302967) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=302967) 	.loc	1 193 32                        // quant_slide_tuned_Llama3.2-1B.py:193:32
(EngineCore_DP0 pid=302967) 	max.f32 	%r112, %r110, 0f2B8CBCCC;
(EngineCore_DP0 pid=302967) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=302967) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=302967) 	mov.b32 	%r67, 0f43E00000;
(EngineCore_DP0 pid=302967) 	.loc	1 194 32                        // quant_slide_tuned_Llama3.2-1B.py:194:32
(EngineCore_DP0 pid=302967) 	div.full.f32 	%r68, %r112, %r67;
(EngineCore_DP0 pid=302967) 	.loc	1 194 41                        // quant_slide_tuned_Llama3.2-1B.py:194:41
(EngineCore_DP0 pid=302967) 	max.f32 	%r66, %r68, 0f36924925;
(EngineCore_DP0 pid=302967) 	.loc	1 196 25                        // quant_slide_tuned_Llama3.2-1B.py:196:25
(EngineCore_DP0 pid=302967) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=302967) 	.loc	1 196 30                        // quant_slide_tuned_Llama3.2-1B.py:196:30
(EngineCore_DP0 pid=302967) 	// begin inline asm
(EngineCore_DP0 pid=302967) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r66 };
(EngineCore_DP0 pid=302967) 	// end inline asm
(EngineCore_DP0 pid=302967) 	.loc	1 199 29                        // quant_slide_tuned_Llama3.2-1B.py:199:29
(EngineCore_DP0 pid=302967) 	shl.b32 	%r14, %r19, 1;
(EngineCore_DP0 pid=302967) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=302967) 	setp.lt.s32 	%p8, %r14, 1;
(EngineCore_DP0 pid=302967) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=302967) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=302967) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=302967) 	ld.param.b32 	%r23, [_quant_slide_fp8_kernel_param_9];
(EngineCore_DP0 pid=302967) 	shr.s32 	%r24, %r23, 31;
(EngineCore_DP0 pid=302967) 	shr.u32 	%r25, %r24, 30;
(EngineCore_DP0 pid=302967) 	add.s32 	%r26, %r23, %r25;
(EngineCore_DP0 pid=302967) 	shr.s32 	%r27, %r26, 2;
(EngineCore_DP0 pid=302967) 	ld.param.b64 	%rd5, [_quant_slide_fp8_kernel_param_1];
(EngineCore_DP0 pid=302967) 	mul.lo.s32 	%r28, %r27, %r1;
(EngineCore_DP0 pid=302967) 	mad.wide.s32 	%rd2, %r28, 4, %rd5;
(EngineCore_DP0 pid=302967) 	div.full.f32 	%r13, %r67, %r112;
(EngineCore_DP0 pid=302967) 	mov.b32 	%r113, 0;
(EngineCore_DP0 pid=302967) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=302967)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=302967) 	.loc	1 203 30                        // quant_slide_tuned_Llama3.2-1B.py:203:30
(EngineCore_DP0 pid=302967) 	add.s32 	%r80, %r2, %r113;
(EngineCore_DP0 pid=302967) 	setp.lt.s32 	%p13, %r80, %r14;
(EngineCore_DP0 pid=302967) 	.loc	1 206 24                        // quant_slide_tuned_Llama3.2-1B.py:206:24
(EngineCore_DP0 pid=302967) 	shr.u32 	%r81, %r80, 31;
(EngineCore_DP0 pid=302967) 	add.s32 	%r82, %r80, %r81;
(EngineCore_DP0 pid=302967) 	shr.u32 	%r83, %r82, 1;
(EngineCore_DP0 pid=302967) 	.loc	1 207 23                        // quant_slide_tuned_Llama3.2-1B.py:207:23
(EngineCore_DP0 pid=302967) 	and.b32 	%r84, %r82, 2147483646;
(EngineCore_DP0 pid=302967) 	sub.s32 	%r85, %r80, %r84;
(EngineCore_DP0 pid=302967) 	.loc	1 208 30                        // quant_slide_tuned_Llama3.2-1B.py:208:30
(EngineCore_DP0 pid=302967) 	shl.b32 	%r86, %r85, 1;
(EngineCore_DP0 pid=302967) 	.loc	1 208 26                        // quant_slide_tuned_Llama3.2-1B.py:208:26
(EngineCore_DP0 pid=302967) 	mad.lo.s32 	%r87, %r83, 6, %r86;
(EngineCore_DP0 pid=302967) 	.loc	1 211 53                        // quant_slide_tuned_Llama3.2-1B.py:211:53
(EngineCore_DP0 pid=302967) 	setp.lt.s32 	%p14, %r87, %r17;
(EngineCore_DP0 pid=302967) 	.loc	1 211 37                        // quant_slide_tuned_Llama3.2-1B.py:211:37
(EngineCore_DP0 pid=302967) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=302967) 	.loc	1 210 29                        // quant_slide_tuned_Llama3.2-1B.py:210:29
(EngineCore_DP0 pid=302967) 	mad.wide.s32 	%rd8, %r87, 2, %rd1;
(EngineCore_DP0 pid=302967) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=302967) 	.loc	1 210 21                        // quant_slide_tuned_Llama3.2-1B.py:210:21
(EngineCore_DP0 pid=302967) 	// begin inline asm
(EngineCore_DP0 pid=302967) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=302967) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=302967) 	// end inline asm
(EngineCore_DP0 pid=302967) 	.loc	1 211 79                        // quant_slide_tuned_Llama3.2-1B.py:211:79
(EngineCore_DP0 pid=302967) 	cvt.f32.bf16 	%r88, %rs12;
(EngineCore_DP0 pid=302967) 	.loc	1 213 48                        // quant_slide_tuned_Llama3.2-1B.py:213:48
(EngineCore_DP0 pid=302967) 	or.b32 	%r89, %r87, 1;
(EngineCore_DP0 pid=302967) 	.loc	1 213 53                        // quant_slide_tuned_Llama3.2-1B.py:213:53
(EngineCore_DP0 pid=302967) 	setp.lt.s32 	%p15, %r89, %r17;
(EngineCore_DP0 pid=302967) 	.loc	1 213 37                        // quant_slide_tuned_Llama3.2-1B.py:213:37
(EngineCore_DP0 pid=302967) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=302967) 	.loc	1 212 39                        // quant_slide_tuned_Llama3.2-1B.py:212:39
(EngineCore_DP0 pid=302967) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=302967) 	.loc	1 212 21                        // quant_slide_tuned_Llama3.2-1B.py:212:21
(EngineCore_DP0 pid=302967) 	// begin inline asm
(EngineCore_DP0 pid=302967) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=302967) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=302967) 	// end inline asm
(EngineCore_DP0 pid=302967) 	.loc	1 213 79                        // quant_slide_tuned_Llama3.2-1B.py:213:79
(EngineCore_DP0 pid=302967) 	cvt.f32.bf16 	%r90, %rs14;
(EngineCore_DP0 pid=302967) 	.loc	1 215 48                        // quant_slide_tuned_Llama3.2-1B.py:215:48
(EngineCore_DP0 pid=302967) 	add.s32 	%r91, %r87, 2;
(EngineCore_DP0 pid=302967) 	.loc	1 215 53                        // quant_slide_tuned_Llama3.2-1B.py:215:53
(EngineCore_DP0 pid=302967) 	setp.lt.s32 	%p16, %r91, %r17;
(EngineCore_DP0 pid=302967) 	.loc	1 215 37                        // quant_slide_tuned_Llama3.2-1B.py:215:37
(EngineCore_DP0 pid=302967) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=302967) 	.loc	1 214 39                        // quant_slide_tuned_Llama3.2-1B.py:214:39
(EngineCore_DP0 pid=302967) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=302967) 	.loc	1 214 21                        // quant_slide_tuned_Llama3.2-1B.py:214:21
(EngineCore_DP0 pid=302967) 	// begin inline asm
(EngineCore_DP0 pid=302967) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=302967) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=302967) 	// end inline asm
(EngineCore_DP0 pid=302967) 	.loc	1 215 79                        // quant_slide_tuned_Llama3.2-1B.py:215:79
(EngineCore_DP0 pid=302967) 	cvt.f32.bf16 	%r92, %rs16;
(EngineCore_DP0 pid=302967) 	.loc	1 217 48                        // quant_slide_tuned_Llama3.2-1B.py:217:48
(EngineCore_DP0 pid=302967) 	add.s32 	%r93, %r87, 3;
(EngineCore_DP0 pid=302967) 	.loc	1 217 53                        // quant_slide_tuned_Llama3.2-1B.py:217:53
(EngineCore_DP0 pid=302967) 	setp.lt.s32 	%p17, %r93, %r17;
(EngineCore_DP0 pid=302967) 	.loc	1 217 37                        // quant_slide_tuned_Llama3.2-1B.py:217:37
(EngineCore_DP0 pid=302967) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=302967) 	.loc	1 216 39                        // quant_slide_tuned_Llama3.2-1B.py:216:39
(EngineCore_DP0 pid=302967) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=302967) 	.loc	1 216 21                        // quant_slide_tuned_Llama3.2-1B.py:216:21
(EngineCore_DP0 pid=302967) 	// begin inline asm
(EngineCore_DP0 pid=302967) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=302967) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=302967) 	// end inline asm
(EngineCore_DP0 pid=302967) 	.loc	1 217 79                        // quant_slide_tuned_Llama3.2-1B.py:217:79
(EngineCore_DP0 pid=302967) 	cvt.f32.bf16 	%r94, %rs18;
(EngineCore_DP0 pid=302967) 	.loc	1 219 27                        // quant_slide_tuned_Llama3.2-1B.py:219:27
(EngineCore_DP0 pid=302967) 	mul.f32 	%r95, %r13, %r88;
(EngineCore_DP0 pid=302967) 	mov.b32 	%r96, 0f43E00000;
(EngineCore_DP0 pid=302967) 	.loc	1 219 48                        // quant_slide_tuned_Llama3.2-1B.py:219:48
(EngineCore_DP0 pid=302967) 	min.xorsign.abs.f32 	%r70, %r95, %r96;
(EngineCore_DP0 pid=302967) 	.loc	1 219 60                        // quant_slide_tuned_Llama3.2-1B.py:219:60
(EngineCore_DP0 pid=302967) 	// begin inline asm
(EngineCore_DP0 pid=302967) 	cvt.rn.satfinite.e4m3x2.f32  %rs20, %r71, %r70; 
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) 	// end inline asm
(EngineCore_DP0 pid=302967) 	.loc	1 220 27                        // quant_slide_tuned_Llama3.2-1B.py:220:27
(EngineCore_DP0 pid=302967) 	mul.f32 	%r97, %r13, %r90;
(EngineCore_DP0 pid=302967) 	.loc	1 220 48                        // quant_slide_tuned_Llama3.2-1B.py:220:48
(EngineCore_DP0 pid=302967) 	min.xorsign.abs.f32 	%r72, %r97, %r96;
(EngineCore_DP0 pid=302967) 	.loc	1 220 60                        // quant_slide_tuned_Llama3.2-1B.py:220:60
(EngineCore_DP0 pid=302967) 	// begin inline asm
(EngineCore_DP0 pid=302967) 	cvt.rn.satfinite.e4m3x2.f32  %rs21, %r73, %r72; 
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) 	// end inline asm
(EngineCore_DP0 pid=302967) 	.loc	1 221 27                        // quant_slide_tuned_Llama3.2-1B.py:221:27
(EngineCore_DP0 pid=302967) 	mul.f32 	%r98, %r13, %r92;
(EngineCore_DP0 pid=302967) 	.loc	1 221 48                        // quant_slide_tuned_Llama3.2-1B.py:221:48
(EngineCore_DP0 pid=302967) 	min.xorsign.abs.f32 	%r74, %r98, %r96;
(EngineCore_DP0 pid=302967) 	.loc	1 221 60                        // quant_slide_tuned_Llama3.2-1B.py:221:60
(EngineCore_DP0 pid=302967) 	// begin inline asm
(EngineCore_DP0 pid=302967) 	cvt.rn.satfinite.e4m3x2.f32  %rs22, %r75, %r74; 
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) 	// end inline asm
(EngineCore_DP0 pid=302967) 	.loc	1 222 27                        // quant_slide_tuned_Llama3.2-1B.py:222:27
(EngineCore_DP0 pid=302967) 	mul.f32 	%r99, %r13, %r94;
(EngineCore_DP0 pid=302967) 	.loc	1 222 48                        // quant_slide_tuned_Llama3.2-1B.py:222:48
(EngineCore_DP0 pid=302967) 	min.xorsign.abs.f32 	%r76, %r99, %r96;
(EngineCore_DP0 pid=302967) 	.loc	1 222 60                        // quant_slide_tuned_Llama3.2-1B.py:222:60
(EngineCore_DP0 pid=302967) 	// begin inline asm
(EngineCore_DP0 pid=302967) 	cvt.rn.satfinite.e4m3x2.f32  %rs23, %r77, %r76; 
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) 	// end inline asm
(EngineCore_DP0 pid=302967) 	.loc	1 224 45                        // quant_slide_tuned_Llama3.2-1B.py:224:45
(EngineCore_DP0 pid=302967) 	cvt.u32.u16 	%r100, %rs20;
(EngineCore_DP0 pid=302967) 	and.b32 	%r101, %r100, 255;
(EngineCore_DP0 pid=302967) 	.loc	1 226 45                        // quant_slide_tuned_Llama3.2-1B.py:226:45
(EngineCore_DP0 pid=302967) 	cvt.u32.u16 	%r102, %rs22;
(EngineCore_DP0 pid=302967) 	and.b32 	%r103, %r102, 255;
(EngineCore_DP0 pid=302967) 	.loc	1 227 45                        // quant_slide_tuned_Llama3.2-1B.py:227:45
(EngineCore_DP0 pid=302967) 	cvt.u32.u16 	%r104, %rs23;
(EngineCore_DP0 pid=302967) 	.loc	1 229 30                        // quant_slide_tuned_Llama3.2-1B.py:229:30
(EngineCore_DP0 pid=302967) 	and.b16 	%rs24, %rs21, 255;
(EngineCore_DP0 pid=302967) 	mul.wide.u16 	%r105, %rs24, 256;
(EngineCore_DP0 pid=302967) 	.loc	1 229 24                        // quant_slide_tuned_Llama3.2-1B.py:229:24
(EngineCore_DP0 pid=302967) 	or.b32 	%r106, %r105, %r101;
(EngineCore_DP0 pid=302967) 	.loc	1 229 42                        // quant_slide_tuned_Llama3.2-1B.py:229:42
(EngineCore_DP0 pid=302967) 	shl.b32 	%r107, %r103, 16;
(EngineCore_DP0 pid=302967) 	.loc	1 229 36                        // quant_slide_tuned_Llama3.2-1B.py:229:36
(EngineCore_DP0 pid=302967) 	or.b32 	%r108, %r106, %r107;
(EngineCore_DP0 pid=302967) 	.loc	1 229 55                        // quant_slide_tuned_Llama3.2-1B.py:229:55
(EngineCore_DP0 pid=302967) 	shl.b32 	%r109, %r104, 24;
(EngineCore_DP0 pid=302967) 	.loc	1 229 49                        // quant_slide_tuned_Llama3.2-1B.py:229:49
(EngineCore_DP0 pid=302967) 	or.b32 	%r78, %r108, %r109;
(EngineCore_DP0 pid=302967) 	.loc	1 230 29                        // quant_slide_tuned_Llama3.2-1B.py:230:29
(EngineCore_DP0 pid=302967) 	mad.wide.s32 	%rd12, %r80, 4, %rd2;
(EngineCore_DP0 pid=302967) 	.loc	1 230 39                        // quant_slide_tuned_Llama3.2-1B.py:230:39
(EngineCore_DP0 pid=302967) 	// begin inline asm
(EngineCore_DP0 pid=302967) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r78 };
(EngineCore_DP0 pid=302967) 	// end inline asm
(EngineCore_DP0 pid=302967) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=302967) 	add.s32 	%r113, %r113, 1024;
(EngineCore_DP0 pid=302967) 	setp.lt.s32 	%p18, %r113, %r14;
(EngineCore_DP0 pid=302967) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=302967) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=302967) 	.loc	1 201 4                         // quant_slide_tuned_Llama3.2-1B.py:201:4
(EngineCore_DP0 pid=302967) 	ret;
(EngineCore_DP0 pid=302967) $L__tmp3:
(EngineCore_DP0 pid=302967) $L__func_end0:
(EngineCore_DP0 pid=302967)                                         // -- End function
(EngineCore_DP0 pid=302967) }
(EngineCore_DP0 pid=302967) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=302967) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=302967) 	.section	.debug_abbrev
(EngineCore_DP0 pid=302967) 	{
(EngineCore_DP0 pid=302967) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=302967) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=302967) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=302967) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=302967) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=302967) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=302967) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=302967) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=302967) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=302967) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=302967) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=302967) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=302967) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=302967) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=302967) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=302967) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=302967) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=302967) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=302967) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=302967) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=302967) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=302967) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=302967) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=302967) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=302967) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=302967) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=302967) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=302967) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=302967) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=302967) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=302967) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=302967) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=302967) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=302967) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=302967) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=302967) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=302967) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=302967) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=302967) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=302967) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=302967) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=302967) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=302967) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=302967) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=302967) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=302967) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=302967) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=302967) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=302967) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=302967) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=302967) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=302967) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=302967) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=302967) 	}
(EngineCore_DP0 pid=302967) 	.section	.debug_info
(EngineCore_DP0 pid=302967) 	{
(EngineCore_DP0 pid=302967) .b32 222                                // Length of Unit
(EngineCore_DP0 pid=302967) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=302967) .b8 0
(EngineCore_DP0 pid=302967) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=302967) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=302967) .b8 1                                   // Abbrev [1] 0xb:0xd7 DW_TAG_compile_unit
(EngineCore_DP0 pid=302967) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=302967) .b8 114
(EngineCore_DP0 pid=302967) .b8 105
(EngineCore_DP0 pid=302967) .b8 116
(EngineCore_DP0 pid=302967) .b8 111
(EngineCore_DP0 pid=302967) .b8 110
(EngineCore_DP0 pid=302967) .b8 0
(EngineCore_DP0 pid=302967) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=302967) .b8 0
(EngineCore_DP0 pid=302967) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=302967) .b8 117
(EngineCore_DP0 pid=302967) .b8 97
(EngineCore_DP0 pid=302967) .b8 110
(EngineCore_DP0 pid=302967) .b8 116
(EngineCore_DP0 pid=302967) .b8 95
(EngineCore_DP0 pid=302967) .b8 115
(EngineCore_DP0 pid=302967) .b8 108
(EngineCore_DP0 pid=302967) .b8 105
(EngineCore_DP0 pid=302967) .b8 100
(EngineCore_DP0 pid=302967) .b8 101
(EngineCore_DP0 pid=302967) .b8 95
(EngineCore_DP0 pid=302967) .b8 116
(EngineCore_DP0 pid=302967) .b8 117
(EngineCore_DP0 pid=302967) .b8 110
(EngineCore_DP0 pid=302967) .b8 101
(EngineCore_DP0 pid=302967) .b8 100
(EngineCore_DP0 pid=302967) .b8 95
(EngineCore_DP0 pid=302967) .b8 76
(EngineCore_DP0 pid=302967) .b8 108
(EngineCore_DP0 pid=302967) .b8 97
(EngineCore_DP0 pid=302967) .b8 109
(EngineCore_DP0 pid=302967) .b8 97
(EngineCore_DP0 pid=302967) .b8 51
(EngineCore_DP0 pid=302967) .b8 46
(EngineCore_DP0 pid=302967) .b8 50
(EngineCore_DP0 pid=302967) .b8 45
(EngineCore_DP0 pid=302967) .b8 49
(EngineCore_DP0 pid=302967) .b8 66
(EngineCore_DP0 pid=302967) .b8 46
(EngineCore_DP0 pid=302967) .b8 112
(EngineCore_DP0 pid=302967) .b8 121
(EngineCore_DP0 pid=302967) .b8 0
(EngineCore_DP0 pid=302967) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=302967) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=302967) .b8 114
(EngineCore_DP0 pid=302967) .b8 111
(EngineCore_DP0 pid=302967) .b8 111
(EngineCore_DP0 pid=302967) .b8 116
(EngineCore_DP0 pid=302967) .b8 47
(EngineCore_DP0 pid=302967) .b8 118
(EngineCore_DP0 pid=302967) .b8 108
(EngineCore_DP0 pid=302967) .b8 108
(EngineCore_DP0 pid=302967) .b8 109
(EngineCore_DP0 pid=302967) .b8 98
(EngineCore_DP0 pid=302967) .b8 101
(EngineCore_DP0 pid=302967) .b8 110
(EngineCore_DP0 pid=302967) .b8 99
(EngineCore_DP0 pid=302967) .b8 104
(EngineCore_DP0 pid=302967) .b8 47
(EngineCore_DP0 pid=302967) .b8 115
(EngineCore_DP0 pid=302967) .b8 108
(EngineCore_DP0 pid=302967) .b8 105
(EngineCore_DP0 pid=302967) .b8 100
(EngineCore_DP0 pid=302967) .b8 101
(EngineCore_DP0 pid=302967) .b8 115
(EngineCore_DP0 pid=302967) .b8 112
(EngineCore_DP0 pid=302967) .b8 97
(EngineCore_DP0 pid=302967) .b8 114
(EngineCore_DP0 pid=302967) .b8 115
(EngineCore_DP0 pid=302967) .b8 101
(EngineCore_DP0 pid=302967) .b8 47
(EngineCore_DP0 pid=302967) .b8 99
(EngineCore_DP0 pid=302967) .b8 115
(EngineCore_DP0 pid=302967) .b8 114
(EngineCore_DP0 pid=302967) .b8 99
(EngineCore_DP0 pid=302967) .b8 47
(EngineCore_DP0 pid=302967) .b8 102
(EngineCore_DP0 pid=302967) .b8 117
(EngineCore_DP0 pid=302967) .b8 115
(EngineCore_DP0 pid=302967) .b8 101
(EngineCore_DP0 pid=302967) .b8 100
(EngineCore_DP0 pid=302967) .b8 95
(EngineCore_DP0 pid=302967) .b8 113
(EngineCore_DP0 pid=302967) .b8 117
(EngineCore_DP0 pid=302967) .b8 97
(EngineCore_DP0 pid=302967) .b8 110
(EngineCore_DP0 pid=302967) .b8 116
(EngineCore_DP0 pid=302967) .b8 95
(EngineCore_DP0 pid=302967) .b8 115
(EngineCore_DP0 pid=302967) .b8 108
(EngineCore_DP0 pid=302967) .b8 105
(EngineCore_DP0 pid=302967) .b8 100
(EngineCore_DP0 pid=302967) .b8 101
(EngineCore_DP0 pid=302967) .b8 95
(EngineCore_DP0 pid=302967) .b8 116
(EngineCore_DP0 pid=302967) .b8 114
(EngineCore_DP0 pid=302967) .b8 105
(EngineCore_DP0 pid=302967) .b8 116
(EngineCore_DP0 pid=302967) .b8 111
(EngineCore_DP0 pid=302967) .b8 110
(EngineCore_DP0 pid=302967) .b8 47
(EngineCore_DP0 pid=302967) .b8 98
(EngineCore_DP0 pid=302967) .b8 117
(EngineCore_DP0 pid=302967) .b8 105
(EngineCore_DP0 pid=302967) .b8 108
(EngineCore_DP0 pid=302967) .b8 100
(EngineCore_DP0 pid=302967) .b8 47
(EngineCore_DP0 pid=302967) .b8 71
(EngineCore_DP0 pid=302967) .b8 66
(EngineCore_DP0 pid=302967) .b8 49
(EngineCore_DP0 pid=302967) .b8 48
(EngineCore_DP0 pid=302967) .b8 95
(EngineCore_DP0 pid=302967) .b8 99
(EngineCore_DP0 pid=302967) .b8 99
(EngineCore_DP0 pid=302967) .b8 49
(EngineCore_DP0 pid=302967) .b8 50
(EngineCore_DP0 pid=302967) .b8 49
(EngineCore_DP0 pid=302967) .b8 95
(EngineCore_DP0 pid=302967) .b8 112
(EngineCore_DP0 pid=302967) .b8 121
(EngineCore_DP0 pid=302967) .b8 51
(EngineCore_DP0 pid=302967) .b8 49
(EngineCore_DP0 pid=302967) .b8 50
(EngineCore_DP0 pid=302967) .b8 95
(EngineCore_DP0 pid=302967) .b8 99
(EngineCore_DP0 pid=302967) .b8 117
(EngineCore_DP0 pid=302967) .b8 49
(EngineCore_DP0 pid=302967) .b8 50
(EngineCore_DP0 pid=302967) .b8 57
(EngineCore_DP0 pid=302967) .b8 95
(EngineCore_DP0 pid=302967) .b8 97
(EngineCore_DP0 pid=302967) .b8 97
(EngineCore_DP0 pid=302967) .b8 114
(EngineCore_DP0 pid=302967) .b8 99
(EngineCore_DP0 pid=302967) .b8 104
(EngineCore_DP0 pid=302967) .b8 54
(EngineCore_DP0 pid=302967) .b8 52
(EngineCore_DP0 pid=302967) .b8 0
(EngineCore_DP0 pid=302967) .b8 2                                   // Abbrev [2] 0x99:0x1a DW_TAG_subprogram
(EngineCore_DP0 pid=302967) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=302967) .b8 113
(EngineCore_DP0 pid=302967) .b8 117
(EngineCore_DP0 pid=302967) .b8 97
(EngineCore_DP0 pid=302967) .b8 110
(EngineCore_DP0 pid=302967) .b8 116
(EngineCore_DP0 pid=302967) .b8 95
(EngineCore_DP0 pid=302967) .b8 115
(EngineCore_DP0 pid=302967) .b8 108
(EngineCore_DP0 pid=302967) .b8 105
(EngineCore_DP0 pid=302967) .b8 100
(EngineCore_DP0 pid=302967) .b8 101
(EngineCore_DP0 pid=302967) .b8 95
(EngineCore_DP0 pid=302967) .b8 102
(EngineCore_DP0 pid=302967) .b8 112
(EngineCore_DP0 pid=302967) .b8 56
(EngineCore_DP0 pid=302967) .b8 95
(EngineCore_DP0 pid=302967) .b8 107
(EngineCore_DP0 pid=302967) .b8 101
(EngineCore_DP0 pid=302967) .b8 114
(EngineCore_DP0 pid=302967) .b8 110
(EngineCore_DP0 pid=302967) .b8 101
(EngineCore_DP0 pid=302967) .b8 108
(EngineCore_DP0 pid=302967) .b8 0
(EngineCore_DP0 pid=302967) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=302967) .b8 3                                   // Abbrev [3] 0xb3:0x2e DW_TAG_subprogram
(EngineCore_DP0 pid=302967) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=302967) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=302967) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=302967) .b8 4                                   // Abbrev [4] 0xc8:0x18 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=302967) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=302967) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=302967) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=302967) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=302967) .b8 191                                 // DW_AT_call_line
(EngineCore_DP0 pid=302967) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=302967) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=302967) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=302967) 	}
(EngineCore_DP0 pid=302967) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) ================================================================
(EngineCore_DP0 pid=302967) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpfmv_fl6z.ptx', '-o', '/tmp/tmpfmv_fl6z.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866] 
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866] 
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866] 
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpfmv_fl6z.ptx -o /tmp/tmpfmv_fl6z.ptx.o
(EngineCore_DP0 pid=302967) ERROR 01-25 18:49:56 [core.py:866] 

STDERR:
[2026-01-25 18:49:41] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:49:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:49:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:49:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:49:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:49:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:49:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:49:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:49:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:49:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:49:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:49:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:49:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:49:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:49:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:49:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:49:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:49:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:49:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=302967) [2026-01-25 18:49:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=302967) [2026-01-25 18:49:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=302967) [2026-01-25 18:49:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=302967) [2026-01-25 18:49:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=302967) [2026-01-25 18:49:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=302967) [2026-01-25 18:49:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=302967) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=302967) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.49s/it]
(EngineCore_DP0 pid=302967) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.49s/it]
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) [2026-01-25 18:49:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=302967) [2026-01-25 18:49:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=302967) [2026-01-25 18:49:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=302967) [2026-01-25 18:49:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=302967) [2026-01-25 18:49:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=302967) [2026-01-25 18:49:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=302967) [2026-01-25 18:49:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=302967) [2026-01-25 18:49:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=302967) Process EngineCore_DP0:
(EngineCore_DP0 pid=302967) Traceback (most recent call last):
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=302967)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=302967)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=302967)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=302967) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpfmv_fl6z.ptx', '-o', '/tmp/tmpfmv_fl6z.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) Traceback (most recent call last):
(EngineCore_DP0 pid=302967)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=302967)     self.run()
(EngineCore_DP0 pid=302967)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=302967)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=302967)     raise e
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=302967)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=302967)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=302967)     super().__init__(
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=302967)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=302967)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=302967)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=302967)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=302967)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=302967)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=302967)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=302967)     return func(*args, **kwargs)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=302967)     return func(*args, **kwargs)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=302967)     self.model_runner.profile_run()
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=302967)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=302967)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=302967)     return func(*args, **kwargs)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=302967)     outputs = self.model(
(EngineCore_DP0 pid=302967)               ^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302967)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302967)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=302967)     model_output = self.model(
(EngineCore_DP0 pid=302967)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=302967)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=302967)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=302967)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302967)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302967)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=302967)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=302967)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302967)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302967)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=302967)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=302967)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=302967)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=302967)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=302967)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=302967)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=302967)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=302967)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=302967)     return self._linear_fn(
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=302967)     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=302967)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=302967)     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=302967)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=302967)     return fn(input, L)
(EngineCore_DP0 pid=302967)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=302967)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=302967)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=302967)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=302967)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=302967)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=302967)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=302967)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=302967)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=302967)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=302967)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=302967)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=302967)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=302967)     raise PTXASError(error)
(EngineCore_DP0 pid=302967) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=302967) `ptxas` stderr:
(EngineCore_DP0 pid=302967) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=302967) 
(EngineCore_DP0 pid=302967) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpfmv_fl6z.ptx -o /tmp/tmpfmv_fl6z.ptx.o
(EngineCore_DP0 pid=302967) 
[rank0]:[W125 18:49:56.960466228 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=256

========== M=16 ==========
Time: 2026-01-26 02:40:02
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-FP8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:40:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:40:05 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=757310) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=757310) WARNING 01-26 02:40:25 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.42 requests/s, 1475.53 total tokens/s, 1388.73 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096

STDERR:
[2026-01-26 02:40:05] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:40:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:40:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:40:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:40:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:40:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:40:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:40:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:40:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:40:09] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:40:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:40:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:40:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:40:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:40:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:40:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:40:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:40:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=757310) [2026-01-26 02:40:10] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=757310) [2026-01-26 02:40:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=757310) [2026-01-26 02:40:10] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=757310) [2026-01-26 02:40:10] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=757310) [2026-01-26 02:40:10] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=757310) [2026-01-26 02:40:10] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=757310) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=757310) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.94s/it]
(EngineCore_DP0 pid=757310) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.94s/it]
(EngineCore_DP0 pid=757310) 
(EngineCore_DP0 pid=757310) [2026-01-26 02:40:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=757310) [2026-01-26 02:40:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=757310) [2026-01-26 02:40:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=757310) [2026-01-26 02:40:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=757310) [2026-01-26 02:40:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=757310) [2026-01-26 02:40:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=757310) [2026-01-26 02:40:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=757310) [2026-01-26 02:40:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=757310) 2026-01-26 02:40:24,539 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=757310) 2026-01-26 02:40:24,547 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|| 16/16 [00:00<00:00, 8018.74it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|         | 1/16 [00:02<00:43,  2.93s/it, est. speed input: 5.46 toks/s, output: 87.28 toks/s]
Processed prompts: 100%|| 16/16 [00:02<00:00,  2.93s/it, est. speed input: 86.88 toks/s, output: 1390.06 toks/s]
Processed prompts: 100%|| 16/16 [00:02<00:00,  5.43it/s, est. speed input: 86.88 toks/s, output: 1390.06 toks/s]
[rank0]:[W126 02:40:28.524664748 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 02:40:30
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:40:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:40:34 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=757903) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=757903) WARNING 01-26 02:40:53 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 22.68 requests/s, 6169.50 total tokens/s, 5806.59 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 02:40:34] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:40:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:40:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:40:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:40:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:40:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:40:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:40:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:40:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:40:37] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:40:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:40:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:40:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:40:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:40:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:40:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:40:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:40:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:40:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=757903) [2026-01-26 02:40:38] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=757903) [2026-01-26 02:40:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=757903) [2026-01-26 02:40:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=757903) [2026-01-26 02:40:38] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=757903) [2026-01-26 02:40:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=757903) [2026-01-26 02:40:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=757903) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=757903) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.23s/it]
(EngineCore_DP0 pid=757903) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.23s/it]
(EngineCore_DP0 pid=757903) 
(EngineCore_DP0 pid=757903) [2026-01-26 02:40:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=757903) [2026-01-26 02:40:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=757903) [2026-01-26 02:40:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=757903) [2026-01-26 02:40:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=757903) [2026-01-26 02:40:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=757903) [2026-01-26 02:40:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=757903) [2026-01-26 02:40:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=757903) [2026-01-26 02:40:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=757903) 2026-01-26 02:40:53,304 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=757903) 2026-01-26 02:40:53,311 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 12744.41it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:05<11:05,  5.24s/it, est. speed input: 3.06 toks/s, output: 48.88 toks/s]
Processed prompts:   2%|         | 3/128 [00:05<02:54,  1.40s/it, est. speed input: 8.98 toks/s, output: 143.75 toks/s]
Processed prompts:  38%|      | 48/128 [00:05<00:04, 17.08it/s, est. speed input: 140.97 toks/s, output: 2255.45 toks/s]
Processed prompts:  68%|   | 87/128 [00:05<00:01, 35.23it/s, est. speed input: 250.14 toks/s, output: 4002.26 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 35.23it/s, est. speed input: 363.62 toks/s, output: 5817.87 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 22.73it/s, est. speed input: 363.62 toks/s, output: 5817.87 toks/s]
[rank0]:[W126 02:40:59.007415382 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 02:41:02
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:41:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:41:05 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=758526) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=758526) WARNING 01-26 02:41:25 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 28.93 requests/s, 7870.08 total tokens/s, 7407.14 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 02:41:05] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:41:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:41:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:41:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:41:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:41:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:41:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:41:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:41:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:41:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:41:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:41:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:41:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:41:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:41:09] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:41:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:41:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:41:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:41:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:41:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:41:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:41:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:41:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:41:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:41:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:41:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:41:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:41:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=758526) [2026-01-26 02:41:10] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=758526) [2026-01-26 02:41:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=758526) [2026-01-26 02:41:10] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=758526) [2026-01-26 02:41:10] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=758526) [2026-01-26 02:41:10] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=758526) [2026-01-26 02:41:10] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=758526) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=758526) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.30s/it]
(EngineCore_DP0 pid=758526) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.30s/it]
(EngineCore_DP0 pid=758526) 
(EngineCore_DP0 pid=758526) [2026-01-26 02:41:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=758526) [2026-01-26 02:41:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=758526) [2026-01-26 02:41:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=758526) [2026-01-26 02:41:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=758526) [2026-01-26 02:41:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=758526) [2026-01-26 02:41:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=758526) [2026-01-26 02:41:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=758526) [2026-01-26 02:41:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=758526) 2026-01-26 02:41:24,901 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=758526) 2026-01-26 02:41:24,908 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 12664.59it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<32:20,  7.61s/it, est. speed input: 2.10 toks/s, output: 33.64 toks/s]
Processed prompts:  14%|        | 35/256 [00:07<00:34,  6.35it/s, est. speed input: 72.26 toks/s, output: 1156.10 toks/s]
Processed prompts:  30%|       | 76/256 [00:07<00:10, 16.51it/s, est. speed input: 154.37 toks/s, output: 2469.95 toks/s]
Processed prompts:  43%|     | 109/256 [00:07<00:05, 27.26it/s, est. speed input: 218.10 toks/s, output: 3489.55 toks/s]
Processed prompts:  54%|    | 137/256 [00:08<00:03, 38.82it/s, est. speed input: 270.08 toks/s, output: 4321.31 toks/s]
Processed prompts:  65%|   | 167/256 [00:08<00:01, 54.47it/s, est. speed input: 324.42 toks/s, output: 5190.71 toks/s]
Processed prompts:  76%|  | 194/256 [00:08<00:00, 70.18it/s, est. speed input: 370.97 toks/s, output: 5935.53 toks/s]
Processed prompts:  87%| | 222/256 [00:08<00:00, 90.55it/s, est. speed input: 419.00 toks/s, output: 6703.92 toks/s]
Processed prompts:  96%|| 246/256 [00:08<00:00, 98.24it/s, est. speed input: 454.13 toks/s, output: 7266.04 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 98.24it/s, est. speed input: 464.06 toks/s, output: 7424.91 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 29.00it/s, est. speed input: 464.06 toks/s, output: 7424.91 toks/s]
[rank0]:[W126 02:41:34.836837302 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:05:38
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:05:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:05:42 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2846802) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2846802) WARNING 01-27 17:06:02 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.85 requests/s, 4310.67 total tokens/s, 4057.11 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:05:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:05:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:05:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:05:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:05:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:05:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:05:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:05:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:05:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:05:46] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:05:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:05:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:05:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:05:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:05:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:05:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:05:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:05:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2846802) [2026-01-27 17:05:47] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2846802) [2026-01-27 17:05:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2846802) [2026-01-27 17:05:47] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2846802) [2026-01-27 17:05:47] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2846802) [2026-01-27 17:05:47] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2846802) [2026-01-27 17:05:47] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2846802) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2846802) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.21s/it]
(EngineCore_DP0 pid=2846802) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.21s/it]
(EngineCore_DP0 pid=2846802) 
(EngineCore_DP0 pid=2846802) [2026-01-27 17:05:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=2846802) [2026-01-27 17:05:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=2846802) [2026-01-27 17:05:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=2846802) [2026-01-27 17:05:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=2846802) [2026-01-27 17:05:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=2846802) [2026-01-27 17:05:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=2846802) [2026-01-27 17:05:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=2846802) [2026-01-27 17:05:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=2846802) 2026-01-27 17:06:02,170 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2846802) 2026-01-27 17:06:02,177 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 4650.08it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:03<04:05,  3.89s/it, est. speed input: 4.11 toks/s, output: 65.78 toks/s]
Processed prompts:  52%|    | 33/64 [00:04<00:02, 11.54it/s, est. speed input: 131.96 toks/s, output: 2111.41 toks/s]
Processed prompts: 100%|| 64/64 [00:04<00:00, 11.54it/s, est. speed input: 254.51 toks/s, output: 4072.18 toks/s]
Processed prompts: 100%|| 64/64 [00:04<00:00, 15.91it/s, est. speed input: 254.51 toks/s, output: 4072.18 toks/s]
[rank0]:[W127 17:06:07.080211061 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:06:09
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:06:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:06:12 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2847410) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2847410) WARNING 01-27 17:06:32 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 24.23 requests/s, 6590.09 total tokens/s, 6202.44 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:06:12] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:06:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:06:12] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:06:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:06:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:06:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:06:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:06:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:06:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:06:16] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:06:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:06:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:06:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:06:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:06:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:06:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:06:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:06:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2847410) [2026-01-27 17:06:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2847410) [2026-01-27 17:06:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2847410) [2026-01-27 17:06:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2847410) [2026-01-27 17:06:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2847410) [2026-01-27 17:06:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2847410) [2026-01-27 17:06:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2847410) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2847410) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.05s/it]
(EngineCore_DP0 pid=2847410) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.05s/it]
(EngineCore_DP0 pid=2847410) 
(EngineCore_DP0 pid=2847410) [2026-01-27 17:06:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=2847410) [2026-01-27 17:06:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=2847410) [2026-01-27 17:06:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=2847410) [2026-01-27 17:06:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=2847410) [2026-01-27 17:06:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=2847410) [2026-01-27 17:06:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=2847410) [2026-01-27 17:06:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=2847410) [2026-01-27 17:06:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=2847410) 2026-01-27 17:06:31,856 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2847410) 2026-01-27 17:06:31,863 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 13144.75it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:04<10:31,  4.97s/it, est. speed input: 3.22 toks/s, output: 51.50 toks/s]
Processed prompts:  27%|       | 34/128 [00:05<00:10,  9.38it/s, est. speed input: 107.01 toks/s, output: 1712.12 toks/s]
Processed prompts:  68%|   | 87/128 [00:05<00:01, 29.03it/s, est. speed input: 267.30 toks/s, output: 4276.73 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 29.03it/s, est. speed input: 388.45 toks/s, output: 6215.21 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 24.28it/s, est. speed input: 388.45 toks/s, output: 6215.21 toks/s]
[rank0]:[W127 17:06:38.037657208 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:06:40
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:06:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:06:43 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2848024) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2848024) WARNING 01-27 17:07:03 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 29.25 requests/s, 7954.90 total tokens/s, 7486.97 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:06:43] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:06:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:06:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:06:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:06:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:06:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:06:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:06:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:06:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:06:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:06:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:06:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:06:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:06:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:06:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:06:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:06:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:06:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:06:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2848024) [2026-01-27 17:06:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2848024) [2026-01-27 17:06:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2848024) [2026-01-27 17:06:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2848024) [2026-01-27 17:06:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2848024) [2026-01-27 17:06:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2848024) [2026-01-27 17:06:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2848024) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2848024) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.12s/it]
(EngineCore_DP0 pid=2848024) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.12s/it]
(EngineCore_DP0 pid=2848024) 
(EngineCore_DP0 pid=2848024) [2026-01-27 17:06:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=2848024) [2026-01-27 17:06:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=2848024) [2026-01-27 17:06:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=2848024) [2026-01-27 17:06:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=2848024) [2026-01-27 17:06:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=2848024) [2026-01-27 17:06:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=2848024) [2026-01-27 17:06:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=2848024) [2026-01-27 17:06:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=2848024) 2026-01-27 17:07:03,080 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2848024) 2026-01-27 17:07:03,097 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 4317.35it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<31:30,  7.41s/it, est. speed input: 2.16 toks/s, output: 34.53 toks/s]
Processed prompts:   7%|         | 19/256 [00:07<01:07,  3.53it/s, est. speed input: 40.43 toks/s, output: 646.88 toks/s]
Processed prompts:  24%|       | 62/256 [00:07<00:13, 14.61it/s, est. speed input: 129.63 toks/s, output: 2074.13 toks/s]
Processed prompts:  38%|      | 98/256 [00:07<00:05, 26.50it/s, est. speed input: 200.91 toks/s, output: 3214.50 toks/s]
Processed prompts:  53%|    | 136/256 [00:07<00:02, 42.62it/s, est. speed input: 273.92 toks/s, output: 4382.66 toks/s]
Processed prompts:  65%|   | 166/256 [00:08<00:01, 58.00it/s, est. speed input: 329.27 toks/s, output: 5268.23 toks/s]
Processed prompts:  75%|  | 193/256 [00:08<00:00, 73.44it/s, est. speed input: 376.73 toks/s, output: 6027.64 toks/s]
Processed prompts:  84%| | 216/256 [00:08<00:00, 88.65it/s, est. speed input: 416.13 toks/s, output: 6658.02 toks/s]
Processed prompts:  93%|| 239/256 [00:08<00:00, 105.54it/s, est. speed input: 454.57 toks/s, output: 7273.16 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 105.54it/s, est. speed input: 471.19 toks/s, output: 7539.06 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 29.45it/s, est. speed input: 471.19 toks/s, output: 7539.06 toks/s] 
[rank0]:[W127 17:07:12.761177198 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:07:15
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:07:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:07:18 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2848699) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2848699) WARNING 01-27 17:07:38 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 30.21 requests/s, 8217.91 total tokens/s, 7734.51 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:07:18] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:07:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:07:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:07:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:07:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:07:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:07:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:07:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:07:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:07:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:07:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:07:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:07:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:07:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:07:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:07:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:07:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:07:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:07:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:07:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:07:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:07:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:07:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:07:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:07:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:07:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:07:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:07:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2848699) [2026-01-27 17:07:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2848699) [2026-01-27 17:07:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2848699) [2026-01-27 17:07:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2848699) [2026-01-27 17:07:23] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2848699) [2026-01-27 17:07:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2848699) [2026-01-27 17:07:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2848699) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2848699) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.75s/it]
(EngineCore_DP0 pid=2848699) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.75s/it]
(EngineCore_DP0 pid=2848699) 
(EngineCore_DP0 pid=2848699) [2026-01-27 17:07:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=2848699) [2026-01-27 17:07:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=2848699) [2026-01-27 17:07:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=2848699) [2026-01-27 17:07:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=2848699) [2026-01-27 17:07:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=2848699) [2026-01-27 17:07:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=2848699) [2026-01-27 17:07:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=2848699) [2026-01-27 17:07:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=2848699) 2026-01-27 17:07:37,840 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2848699) 2026-01-27 17:07:37,847 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 13369.13it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:12<1:48:38, 12.76s/it, est. speed input: 1.25 toks/s, output: 20.07 toks/s]
Processed prompts:   1%|          | 3/512 [00:12<28:22,  3.35s/it, est. speed input: 3.73 toks/s, output: 59.72 toks/s]  
Processed prompts:  12%|        | 63/512 [00:13<00:47,  9.50it/s, est. speed input: 77.25 toks/s, output: 1236.04 toks/s]
Processed prompts:  23%|       | 117/512 [00:13<00:19, 20.56it/s, est. speed input: 141.56 toks/s, output: 2265.01 toks/s]
Processed prompts:  32%|      | 164/512 [00:13<00:10, 33.31it/s, est. speed input: 196.31 toks/s, output: 3141.00 toks/s]
Processed prompts:  40%|      | 205/512 [00:13<00:06, 47.88it/s, est. speed input: 243.21 toks/s, output: 3891.28 toks/s]
Processed prompts:  47%|     | 242/512 [00:13<00:04, 64.50it/s, est. speed input: 284.66 toks/s, output: 4554.58 toks/s]
Processed prompts:  53%|    | 273/512 [00:13<00:02, 79.77it/s, est. speed input: 317.97 toks/s, output: 5087.56 toks/s]
Processed prompts:  62%|   | 315/512 [00:13<00:01, 107.25it/s, est. speed input: 363.46 toks/s, output: 5815.33 toks/s]
Processed prompts:  68%|   | 348/512 [00:13<00:01, 128.63it/s, est. speed input: 398.02 toks/s, output: 6368.28 toks/s]
Processed prompts:  74%|  | 377/512 [00:14<00:00, 147.89it/s, est. speed input: 427.78 toks/s, output: 6844.46 toks/s]
Processed prompts:  79%|  | 406/512 [00:14<00:00, 162.11it/s, est. speed input: 456.41 toks/s, output: 7302.52 toks/s]
Processed prompts:  85%| | 433/512 [00:14<00:00, 164.30it/s, est. speed input: 481.41 toks/s, output: 7702.60 toks/s]
Processed prompts:  89%| | 457/512 [00:14<00:00, 167.54it/s, est. speed input: 503.38 toks/s, output: 8054.08 toks/s]
Processed prompts:  94%|| 479/512 [00:14<00:00, 165.35it/s, est. speed input: 522.63 toks/s, output: 8362.16 toks/s]
Processed prompts:  97%|| 499/512 [00:16<00:00, 31.80it/s, est. speed input: 473.10 toks/s, output: 7569.53 toks/s] 
Processed prompts: 100%|| 512/512 [00:16<00:00, 31.80it/s, est. speed input: 484.53 toks/s, output: 7752.46 toks/s]
Processed prompts: 100%|| 512/512 [00:16<00:00, 30.28it/s, est. speed input: 484.53 toks/s, output: 7752.46 toks/s]
[rank0]:[W127 17:07:55.754441961 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:40:18
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-3B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:40:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:40:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2883319) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2883319) WARNING 01-27 17:40:55 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.89 requests/s, 2145.47 total tokens/s, 2019.26 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:40:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:40:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:40:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:40:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:40:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:40:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:40:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:40:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:40:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:40:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:40:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:40:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:40:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:40:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:40:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:40:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:40:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:40:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2883319) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2883319) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.97s/it]
(EngineCore_DP0 pid=2883319) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.97s/it]
(EngineCore_DP0 pid=2883319) 
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=2883319) 2026-01-27 17:40:54,623 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2883319) 2026-01-27 17:40:54,636 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 11824.31it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:07<08:12,  7.82s/it, est. speed input: 2.05 toks/s, output: 32.73 toks/s]
Processed prompts:  58%|    | 37/64 [00:07<00:04,  6.54it/s, est. speed input: 74.38 toks/s, output: 1190.10 toks/s]
Processed prompts: 100%|| 64/64 [00:08<00:00, 12.92it/s, est. speed input: 126.31 toks/s, output: 2020.89 toks/s]
Processed prompts: 100%|| 64/64 [00:08<00:00, 12.92it/s, est. speed input: 126.31 toks/s, output: 2020.89 toks/s]
Processed prompts: 100%|| 64/64 [00:08<00:00,  7.89it/s, est. speed input: 126.31 toks/s, output: 2020.89 toks/s]
[rank0]:[W127 17:41:03.588642224 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:41:05
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-3B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:41:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:41:09 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2884161) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2884161) WARNING 01-27 17:41:41 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 12.26 requests/s, 3334.67 total tokens/s, 3138.51 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:41:09] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:41:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:41:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:41:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:41:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:41:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:41:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:41:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:41:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:41:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:41:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:41:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:41:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:41:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:41:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:41:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:41:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:41:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2884161) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2884161) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.56s/it]
(EngineCore_DP0 pid=2884161) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.56s/it]
(EngineCore_DP0 pid=2884161) 
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=2884161) 2026-01-27 17:41:40,625 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2884161) 2026-01-27 17:41:40,636 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 4229.76it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:09<20:57,  9.90s/it, est. speed input: 1.62 toks/s, output: 25.86 toks/s]
Processed prompts:  14%|        | 18/128 [00:10<00:43,  2.51it/s, est. speed input: 28.73 toks/s, output: 459.62 toks/s]
Processed prompts:  48%|     | 62/128 [00:10<00:05, 11.07it/s, est. speed input: 97.44 toks/s, output: 1559.06 toks/s]
Processed prompts:  68%|   | 87/128 [00:10<00:02, 17.55it/s, est. speed input: 135.30 toks/s, output: 2164.85 toks/s]
Processed prompts:  92%|| 118/128 [00:10<00:00, 28.49it/s, est. speed input: 181.74 toks/s, output: 2907.78 toks/s]
Processed prompts: 100%|| 128/128 [00:10<00:00, 28.49it/s, est. speed input: 196.75 toks/s, output: 3148.04 toks/s]
Processed prompts: 100%|| 128/128 [00:10<00:00, 12.30it/s, est. speed input: 196.75 toks/s, output: 3148.04 toks/s]
[rank0]:[W127 17:41:51.950011894 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:41:54
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-3B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:41:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:41:57 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2885028) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2885028) WARNING 01-27 17:42:29 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.44 requests/s, 4200.83 total tokens/s, 3953.72 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:41:57] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:41:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:41:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:41:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:41:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:41:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:41:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:41:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:41:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:42:01] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:42:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:42:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:42:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:42:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:42:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:42:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:42:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:42:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:02] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:02] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:02] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:02] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:02] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2885028) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2885028) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.43s/it]
(EngineCore_DP0 pid=2885028) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.43s/it]
(EngineCore_DP0 pid=2885028) 
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=2885028) 2026-01-27 17:42:28,617 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2885028) 2026-01-27 17:42:28,627 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 12457.99it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:14<1:01:24, 14.45s/it, est. speed input: 1.11 toks/s, output: 17.72 toks/s]
Processed prompts:   7%|         | 18/256 [00:14<02:18,  1.72it/s, est. speed input: 19.71 toks/s, output: 315.38 toks/s]
Processed prompts:  19%|        | 48/256 [00:14<00:36,  5.77it/s, est. speed input: 52.01 toks/s, output: 832.18 toks/s]
Processed prompts:  29%|       | 75/256 [00:14<00:16, 10.68it/s, est. speed input: 80.39 toks/s, output: 1286.28 toks/s]
Processed prompts:  38%|      | 98/256 [00:15<00:09, 16.20it/s, est. speed input: 103.97 toks/s, output: 1663.45 toks/s]
Processed prompts:  46%|     | 118/256 [00:15<00:06, 22.59it/s, est. speed input: 124.20 toks/s, output: 1987.19 toks/s]
Processed prompts:  53%|    | 136/256 [00:15<00:04, 29.94it/s, est. speed input: 142.07 toks/s, output: 2273.08 toks/s]
Processed prompts:  59%|    | 152/256 [00:15<00:02, 38.15it/s, est. speed input: 157.71 toks/s, output: 2523.41 toks/s]
Processed prompts:  67%|   | 172/256 [00:15<00:01, 50.16it/s, est. speed input: 176.91 toks/s, output: 2830.54 toks/s]
Processed prompts:  73%|  | 188/256 [00:15<00:01, 60.03it/s, est. speed input: 191.83 toks/s, output: 3069.30 toks/s]
Processed prompts:  79%|  | 203/256 [00:15<00:00, 69.24it/s, est. speed input: 205.50 toks/s, output: 3288.04 toks/s]
Processed prompts:  85%| | 217/256 [00:15<00:00, 75.09it/s, est. speed input: 217.72 toks/s, output: 3483.53 toks/s]
Processed prompts:  90%| | 230/256 [00:16<00:00, 77.04it/s, est. speed input: 228.53 toks/s, output: 3656.43 toks/s]
Processed prompts:  95%|| 242/256 [00:16<00:00, 78.07it/s, est. speed input: 238.27 toks/s, output: 3812.24 toks/s]
Processed prompts:  99%|| 253/256 [00:16<00:00, 66.69it/s, est. speed input: 245.51 toks/s, output: 3928.15 toks/s]
Processed prompts: 100%|| 256/256 [00:16<00:00, 66.69it/s, est. speed input: 247.43 toks/s, output: 3958.90 toks/s]
Processed prompts: 100%|| 256/256 [00:16<00:00, 15.46it/s, est. speed input: 247.43 toks/s, output: 3958.90 toks/s]
[rank0]:[W127 17:42:46.071983846 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:42:48
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-3B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:42:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:42:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2885969) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2885969) WARNING 01-27 17:43:23 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.78 requests/s, 4292.53 total tokens/s, 4040.02 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:42:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:42:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:42:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:42:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:42:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:42:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:42:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:42:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:42:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:42:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:42:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:42:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:42:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:42:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:42:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:42:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:42:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:42:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2885969) [2026-01-27 17:42:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2885969) [2026-01-27 17:42:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2885969) [2026-01-27 17:42:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2885969) [2026-01-27 17:42:56] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2885969) [2026-01-27 17:42:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2885969) [2026-01-27 17:42:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2885969) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2885969) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.58s/it]
(EngineCore_DP0 pid=2885969) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.58s/it]
(EngineCore_DP0 pid=2885969) 
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=2885969) 2026-01-27 17:43:22,849 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2885969) 2026-01-27 17:43:22,859 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 6815.35it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:24<3:25:46, 24.16s/it, est. speed input: 0.66 toks/s, output: 10.60 toks/s]
Processed prompts:   0%|          | 2/512 [00:24<1:25:48, 10.10s/it, est. speed input: 1.31 toks/s, output: 20.97 toks/s]
Processed prompts:   6%|         | 33/512 [00:24<03:02,  2.63it/s, est. speed input: 21.42 toks/s, output: 342.67 toks/s]
Processed prompts:  12%|        | 63/512 [00:24<01:15,  5.97it/s, est. speed input: 40.52 toks/s, output: 648.32 toks/s]
Processed prompts:  18%|        | 91/512 [00:25<00:41, 10.16it/s, est. speed input: 58.04 toks/s, output: 928.60 toks/s]
Processed prompts:  23%|       | 117/512 [00:25<00:25, 15.24it/s, est. speed input: 73.97 toks/s, output: 1183.55 toks/s]
Processed prompts:  28%|       | 141/512 [00:25<00:17, 21.81it/s, est. speed input: 88.75 toks/s, output: 1419.92 toks/s]
Processed prompts:  32%|      | 164/512 [00:25<00:11, 30.06it/s, est. speed input: 102.76 toks/s, output: 1644.20 toks/s]
Processed prompts:  36%|      | 185/512 [00:25<00:08, 39.42it/s, est. speed input: 115.38 toks/s, output: 1846.10 toks/s]
Processed prompts:  40%|      | 205/512 [00:25<00:06, 50.77it/s, est. speed input: 127.34 toks/s, output: 2037.51 toks/s]
Processed prompts:  44%|     | 224/512 [00:25<00:04, 63.62it/s, est. speed input: 138.61 toks/s, output: 2217.74 toks/s]
Processed prompts:  50%|     | 258/512 [00:26<00:02, 86.67it/s, est. speed input: 158.48 toks/s, output: 2535.67 toks/s]
Processed prompts:  56%|    | 288/512 [00:26<00:02, 106.73it/s, est. speed input: 175.85 toks/s, output: 2813.65 toks/s]
Processed prompts:  62%|   | 315/512 [00:26<00:01, 119.42it/s, est. speed input: 191.14 toks/s, output: 3058.16 toks/s]
Processed prompts:  66%|   | 338/512 [00:26<00:01, 126.86it/s, est. speed input: 203.92 toks/s, output: 3262.78 toks/s]
Processed prompts:  70%|   | 358/512 [00:26<00:01, 132.38it/s, est. speed input: 214.92 toks/s, output: 3438.78 toks/s]
Processed prompts:  73%|  | 376/512 [00:26<00:00, 137.44it/s, est. speed input: 224.76 toks/s, output: 3596.09 toks/s]
Processed prompts:  77%|  | 393/512 [00:26<00:00, 140.71it/s, est. speed input: 233.94 toks/s, output: 3743.00 toks/s]
Processed prompts:  80%|  | 409/512 [00:26<00:00, 143.94it/s, est. speed input: 242.53 toks/s, output: 3880.45 toks/s]
Processed prompts:  83%| | 425/512 [00:27<00:00, 136.31it/s, est. speed input: 250.76 toks/s, output: 4012.20 toks/s]
Processed prompts:  86%| | 440/512 [00:27<00:00, 130.34it/s, est. speed input: 258.38 toks/s, output: 4134.09 toks/s]
Processed prompts:  89%| | 454/512 [00:27<00:00, 117.84it/s, est. speed input: 265.13 toks/s, output: 4242.03 toks/s]
Processed prompts:  91%| | 467/512 [00:27<00:00, 110.66it/s, est. speed input: 271.35 toks/s, output: 4341.57 toks/s]
Processed prompts:  94%|| 479/512 [00:27<00:00, 97.18it/s, est. speed input: 276.63 toks/s, output: 4426.09 toks/s] 
Processed prompts:  96%|| 490/512 [00:27<00:00, 73.10it/s, est. speed input: 280.32 toks/s, output: 4485.05 toks/s]
Processed prompts:  97%|| 499/512 [00:32<00:01,  8.42it/s, est. speed input: 247.15 toks/s, output: 3954.41 toks/s]
Processed prompts: 100%|| 512/512 [00:32<00:00,  8.42it/s, est. speed input: 253.10 toks/s, output: 4049.56 toks/s]
Processed prompts: 100%|| 512/512 [00:32<00:00, 15.82it/s, est. speed input: 253.10 toks/s, output: 4049.56 toks/s]
[rank0]:[W127 17:43:56.239466575 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 18:36:47
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:36:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:36:50 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2939319) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2939319) WARNING 01-27 18:38:01 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.94 requests/s, 1343.24 total tokens/s, 1264.23 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 18:36:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:36:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:36:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:36:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:36:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:36:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:36:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:36:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:36:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:36:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:36:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:36:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:36:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:36:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:36:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:36:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2939319) [2026-01-27 18:36:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2939319) [2026-01-27 18:36:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2939319) [2026-01-27 18:36:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2939319) [2026-01-27 18:36:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2939319) [2026-01-27 18:36:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2939319) [2026-01-27 18:36:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2939319) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2939319) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:23<00:23, 23.75s/it]
(EngineCore_DP0 pid=2939319) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:57<00:00, 29.39s/it]
(EngineCore_DP0 pid=2939319) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:57<00:00, 28.55s/it]
(EngineCore_DP0 pid=2939319) 
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=2939319) 2026-01-27 18:38:00,429 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2939319) 2026-01-27 18:38:00,447 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 9981.61it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:12<13:18, 12.67s/it, est. speed input: 1.26 toks/s, output: 20.21 toks/s]
Processed prompts:  31%|      | 20/64 [00:12<00:20,  2.20it/s, est. speed input: 25.06 toks/s, output: 400.93 toks/s]
Processed prompts:  77%|  | 49/64 [00:12<00:02,  6.66it/s, est. speed input: 60.71 toks/s, output: 971.44 toks/s]
Processed prompts: 100%|| 64/64 [00:12<00:00,  6.66it/s, est. speed input: 79.06 toks/s, output: 1264.97 toks/s]
Processed prompts: 100%|| 64/64 [00:12<00:00,  4.94it/s, est. speed input: 79.06 toks/s, output: 1264.97 toks/s]
[rank0]:[W127 18:38:14.295555224 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 18:38:16
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:38:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:38:20 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2941003) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2941003) WARNING 01-27 18:39:29 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 8.15 requests/s, 2216.89 total tokens/s, 2086.48 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 18:38:20] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:38:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:38:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:38:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:38:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:38:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:38:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:38:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:38:23] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:38:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:38:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:38:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:38:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:38:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:38:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:38:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2941003) [2026-01-27 18:38:24] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2941003) [2026-01-27 18:38:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2941003) [2026-01-27 18:38:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2941003) [2026-01-27 18:38:25] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2941003) [2026-01-27 18:38:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2941003) [2026-01-27 18:38:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2941003) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2941003) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:23<00:23, 23.59s/it]
(EngineCore_DP0 pid=2941003) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:56<00:00, 29.19s/it]
(EngineCore_DP0 pid=2941003) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:56<00:00, 28.35s/it]
(EngineCore_DP0 pid=2941003) 
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=2941003) 2026-01-27 18:39:28,626 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2941003) 2026-01-27 18:39:28,641 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 10960.35it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:15<31:46, 15.01s/it, est. speed input: 1.07 toks/s, output: 17.06 toks/s]
Processed prompts:  14%|        | 18/128 [00:15<01:06,  1.66it/s, est. speed input: 19.01 toks/s, output: 304.16 toks/s]
Processed prompts:  38%|      | 48/128 [00:15<00:14,  5.59it/s, est. speed input: 50.24 toks/s, output: 803.85 toks/s]
Processed prompts:  59%|    | 75/128 [00:15<00:05, 10.38it/s, est. speed input: 77.76 toks/s, output: 1244.19 toks/s]
Processed prompts:  77%|  | 98/128 [00:15<00:01, 15.84it/s, est. speed input: 100.72 toks/s, output: 1611.47 toks/s]
Processed prompts:  99%|| 127/128 [00:15<00:00, 25.18it/s, est. speed input: 129.49 toks/s, output: 2071.86 toks/s]
Processed prompts: 100%|| 128/128 [00:15<00:00, 25.18it/s, est. speed input: 130.51 toks/s, output: 2088.16 toks/s]
Processed prompts: 100%|| 128/128 [00:15<00:00,  8.16it/s, est. speed input: 130.51 toks/s, output: 2088.16 toks/s]
[rank0]:[W127 18:39:45.281962066 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 18:39:47
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:39:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:39:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2942767) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2942767) WARNING 01-27 18:40:59 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.85 requests/s, 2950.26 total tokens/s, 2776.72 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 18:39:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:39:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:39:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:39:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:39:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:39:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:39:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:39:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:39:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:39:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:39:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:39:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:39:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:39:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:39:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:39:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2942767) [2026-01-27 18:39:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2942767) [2026-01-27 18:39:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2942767) [2026-01-27 18:39:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2942767) [2026-01-27 18:39:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2942767) [2026-01-27 18:39:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2942767) [2026-01-27 18:39:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2942767) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2942767) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:23<00:23, 23.42s/it]
(EngineCore_DP0 pid=2942767) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:56<00:00, 29.19s/it]
(EngineCore_DP0 pid=2942767) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:56<00:00, 28.33s/it]
(EngineCore_DP0 pid=2942767) 
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=2942767) 2026-01-27 18:40:58,966 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2942767) 2026-01-27 18:40:58,981 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 11114.76it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:20<1:26:37, 20.38s/it, est. speed input: 0.78 toks/s, output: 12.56 toks/s]
Processed prompts:   2%|         | 4/256 [00:20<16:18,  3.88s/it, est. speed input: 3.12 toks/s, output: 49.98 toks/s]  
Processed prompts:   8%|         | 20/256 [00:20<02:09,  1.83it/s, est. speed input: 15.54 toks/s, output: 248.62 toks/s]
Processed prompts:  14%|        | 35/256 [00:20<00:57,  3.87it/s, est. speed input: 27.04 toks/s, output: 432.67 toks/s]
Processed prompts:  19%|        | 49/256 [00:20<00:31,  6.49it/s, est. speed input: 37.65 toks/s, output: 602.36 toks/s]
Processed prompts:  30%|       | 76/256 [00:20<00:13, 13.32it/s, est. speed input: 57.92 toks/s, output: 926.76 toks/s]
Processed prompts:  39%|      | 99/256 [00:21<00:07, 20.54it/s, est. speed input: 74.79 toks/s, output: 1196.62 toks/s]
Processed prompts:  46%|     | 119/256 [00:21<00:04, 28.35it/s, est. speed input: 89.23 toks/s, output: 1427.73 toks/s]
Processed prompts:  54%|    | 137/256 [00:21<00:03, 36.47it/s, est. speed input: 101.97 toks/s, output: 1631.54 toks/s]
Processed prompts:  60%|    | 153/256 [00:21<00:02, 44.69it/s, est. speed input: 113.12 toks/s, output: 1809.97 toks/s]
Processed prompts:  65%|   | 167/256 [00:21<00:01, 51.59it/s, est. speed input: 122.63 toks/s, output: 1962.07 toks/s]
Processed prompts:  70%|   | 179/256 [00:21<00:01, 57.90it/s, est. speed input: 130.68 toks/s, output: 2090.96 toks/s]
Processed prompts:  74%|  | 190/256 [00:22<00:01, 65.09it/s, est. speed input: 138.08 toks/s, output: 2209.20 toks/s]
Processed prompts:  79%|  | 201/256 [00:22<00:00, 68.47it/s, est. speed input: 145.17 toks/s, output: 2322.75 toks/s]
Processed prompts:  82%| | 211/256 [00:22<00:00, 68.62it/s, est. speed input: 151.41 toks/s, output: 2422.48 toks/s]
Processed prompts:  86%| | 221/256 [00:22<00:00, 69.62it/s, est. speed input: 157.61 toks/s, output: 2521.72 toks/s]
Processed prompts:  90%| | 230/256 [00:22<00:00, 61.27it/s, est. speed input: 162.58 toks/s, output: 2601.30 toks/s]
Processed prompts:  93%|| 238/256 [00:22<00:00, 56.26it/s, est. speed input: 166.92 toks/s, output: 2670.70 toks/s]
Processed prompts:  96%|| 245/256 [00:23<00:00, 46.97it/s, est. speed input: 170.11 toks/s, output: 2721.77 toks/s]
Processed prompts:  98%|| 251/256 [00:23<00:00, 39.26it/s, est. speed input: 172.47 toks/s, output: 2759.55 toks/s]
Processed prompts: 100%|| 256/256 [00:23<00:00, 30.88it/s, est. speed input: 173.72 toks/s, output: 2779.55 toks/s]
Processed prompts: 100%|| 256/256 [00:23<00:00, 30.88it/s, est. speed input: 173.72 toks/s, output: 2779.55 toks/s]
Processed prompts: 100%|| 256/256 [00:23<00:00, 10.86it/s, est. speed input: 173.72 toks/s, output: 2779.55 toks/s]
[rank0]:[W127 18:41:23.812289090 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 18:41:26
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:41:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:41:29 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2944729) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2944729) WARNING 01-27 18:42:39 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 11.02 requests/s, 2997.23 total tokens/s, 2820.92 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 18:41:29] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:41:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:41:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:41:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:41:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:41:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:41:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:41:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:41:33] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:41:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:41:33] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:41:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:41:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:41:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:41:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:41:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2944729) [2026-01-27 18:41:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2944729) [2026-01-27 18:41:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2944729) [2026-01-27 18:41:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2944729) [2026-01-27 18:41:34] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2944729) [2026-01-27 18:41:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2944729) [2026-01-27 18:41:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2944729) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2944729) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:24<00:24, 24.13s/it]
(EngineCore_DP0 pid=2944729) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:57<00:00, 29.83s/it]
(EngineCore_DP0 pid=2944729) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:57<00:00, 28.98s/it]
(EngineCore_DP0 pid=2944729) 
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=2944729) 2026-01-27 18:42:39,014 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2944729) 2026-01-27 18:42:39,039 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 12618.90it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:32<4:39:52, 32.86s/it, est. speed input: 0.49 toks/s, output: 7.79 toks/s]
Processed prompts:   1%|          | 6/512 [00:33<34:31,  4.09s/it, est. speed input: 2.89 toks/s, output: 46.28 toks/s] 
Processed prompts:   7%|         | 37/512 [00:33<03:45,  2.10it/s, est. speed input: 17.67 toks/s, output: 282.72 toks/s]
Processed prompts:  13%|        | 66/512 [00:33<01:39,  4.48it/s, est. speed input: 31.25 toks/s, output: 499.93 toks/s]
Processed prompts:  18%|        | 93/512 [00:34<00:56,  7.45it/s, est. speed input: 43.65 toks/s, output: 698.43 toks/s]
Processed prompts:  23%|       | 119/512 [00:34<00:35, 11.21it/s, est. speed input: 55.39 toks/s, output: 886.29 toks/s]
Processed prompts:  28%|       | 143/512 [00:34<00:22, 16.14it/s, est. speed input: 66.31 toks/s, output: 1060.93 toks/s]
Processed prompts:  32%|      | 166/512 [00:34<00:15, 22.44it/s, est. speed input: 76.69 toks/s, output: 1227.11 toks/s]
Processed prompts:  37%|      | 187/512 [00:34<00:10, 29.88it/s, est. speed input: 86.09 toks/s, output: 1377.43 toks/s]
Processed prompts:  40%|      | 207/512 [00:34<00:07, 38.85it/s, est. speed input: 94.97 toks/s, output: 1519.56 toks/s]
Processed prompts:  44%|     | 226/512 [00:34<00:05, 49.17it/s, est. speed input: 103.34 toks/s, output: 1653.51 toks/s]
Processed prompts:  47%|     | 243/512 [00:35<00:04, 59.37it/s, est. speed input: 110.74 toks/s, output: 1771.91 toks/s]
Processed prompts:  51%|     | 259/512 [00:35<00:03, 68.73it/s, est. speed input: 117.61 toks/s, output: 1881.72 toks/s]
Processed prompts:  54%|    | 274/512 [00:35<00:03, 79.22it/s, est. speed input: 124.04 toks/s, output: 1984.72 toks/s]
Processed prompts:  59%|    | 302/512 [00:35<00:02, 99.15it/s, est. speed input: 136.03 toks/s, output: 2176.50 toks/s]
Processed prompts:  64%|   | 327/512 [00:35<00:01, 107.52it/s, est. speed input: 146.49 toks/s, output: 2343.83 toks/s]
Processed prompts:  68%|   | 348/512 [00:35<00:01, 113.26it/s, est. speed input: 155.20 toks/s, output: 2483.14 toks/s]
Processed prompts:  72%|  | 367/512 [00:36<00:01, 116.30it/s, est. speed input: 162.98 toks/s, output: 2607.68 toks/s]
Processed prompts:  75%|  | 384/512 [00:36<00:01, 114.86it/s, est. speed input: 169.81 toks/s, output: 2716.92 toks/s]
Processed prompts:  78%|  | 397/512 [00:36<00:00, 117.27it/s, est. speed input: 175.06 toks/s, output: 2800.95 toks/s]
Processed prompts:  80%|  | 410/512 [00:36<00:00, 111.30it/s, est. speed input: 180.12 toks/s, output: 2881.89 toks/s]
Processed prompts:  82%| | 422/512 [00:36<00:00, 105.37it/s, est. speed input: 184.72 toks/s, output: 2955.44 toks/s]
Processed prompts:  85%| | 433/512 [00:36<00:00, 94.42it/s, est. speed input: 188.73 toks/s, output: 3019.67 toks/s] 
Processed prompts:  87%| | 443/512 [00:36<00:00, 95.67it/s, est. speed input: 192.56 toks/s, output: 3081.01 toks/s]
Processed prompts:  88%| | 453/512 [00:36<00:00, 79.47it/s, est. speed input: 195.91 toks/s, output: 3134.48 toks/s]
Processed prompts:  90%| | 462/512 [00:37<00:00, 74.63it/s, est. speed input: 199.03 toks/s, output: 3184.43 toks/s]
Processed prompts:  92%|| 470/512 [00:37<00:00, 64.35it/s, est. speed input: 201.50 toks/s, output: 3224.01 toks/s]
Processed prompts:  93%|| 477/512 [00:37<00:00, 60.94it/s, est. speed input: 203.76 toks/s, output: 3260.21 toks/s]
Processed prompts:  95%|| 484/512 [00:37<00:00, 47.88it/s, est. speed input: 205.43 toks/s, output: 3286.82 toks/s]
Processed prompts:  96%|| 490/512 [00:37<00:00, 38.56it/s, est. speed input: 206.57 toks/s, output: 3305.13 toks/s]
Processed prompts:  97%|| 495/512 [00:38<00:00, 33.97it/s, est. speed input: 207.53 toks/s, output: 3320.43 toks/s]
Processed prompts:  97%|| 499/512 [00:46<00:05,  2.37it/s, est. speed input: 172.75 toks/s, output: 2763.99 toks/s]
Processed prompts: 100%|| 512/512 [00:46<00:00,  4.39it/s, est. speed input: 176.47 toks/s, output: 2823.50 toks/s]
Processed prompts: 100%|| 512/512 [00:46<00:00,  4.39it/s, est. speed input: 176.47 toks/s, output: 2823.50 toks/s]
Processed prompts: 100%|| 512/512 [00:46<00:00, 11.03it/s, est. speed input: 176.47 toks/s, output: 2823.50 toks/s]
[rank0]:[W127 18:43:26.639234502 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 20:22:45
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:22:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:22:52 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3045784) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3045784) WARNING 01-27 20:25:03 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.59 requests/s, 703.65 total tokens/s, 662.25 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 20:22:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:22:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:22:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:22:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:22:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:22:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:22:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:22:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:22:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:22:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:22:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:22:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:22:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:22:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:22:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:22:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3045784) [2026-01-27 20:22:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3045784) [2026-01-27 20:22:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3045784) [2026-01-27 20:22:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3045784) [2026-01-27 20:22:57] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3045784) [2026-01-27 20:22:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3045784) [2026-01-27 20:22:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3045784) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3045784) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.63s/it]
(EngineCore_DP0 pid=3045784) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:42<00:47, 23.54s/it]
(EngineCore_DP0 pid=3045784) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:14<00:27, 27.36s/it]
(EngineCore_DP0 pid=3045784) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:47<00:00, 29.77s/it]
(EngineCore_DP0 pid=3045784) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:47<00:00, 27.00s/it]
(EngineCore_DP0 pid=3045784) 
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=3045784) 2026-01-27 20:24:56,672 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3045784) 2026-01-27 20:24:56,966 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|         | 1/64 [00:00<00:12,  4.98it/s]
Adding requests:   5%|         | 3/64 [00:00<00:06,  9.77it/s]
Adding requests:   9%|         | 6/64 [00:00<00:03, 15.71it/s]
Adding requests:  16%|        | 10/64 [00:00<00:02, 22.64it/s]
Adding requests:  22%|       | 14/64 [00:00<00:01, 27.14it/s]
Adding requests:  33%|      | 21/64 [00:00<00:01, 38.40it/s]
Adding requests:  50%|     | 32/64 [00:00<00:00, 57.92it/s]
Adding requests:  67%|   | 43/64 [00:00<00:00, 72.60it/s]
Adding requests:  88%| | 56/64 [00:01<00:00, 88.86it/s]
Adding requests: 100%|| 64/64 [00:01<00:00, 55.64it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:22<24:01, 22.88s/it, est. speed input: 0.70 toks/s, output: 11.19 toks/s]
Processed prompts:   9%|         | 6/64 [00:23<02:44,  2.84s/it, est. speed input: 4.16 toks/s, output: 66.58 toks/s]
Processed prompts:  34%|      | 22/64 [00:23<00:24,  1.75it/s, est. speed input: 15.18 toks/s, output: 242.95 toks/s]
Processed prompts:  58%|    | 37/64 [00:23<00:07,  3.57it/s, est. speed input: 25.42 toks/s, output: 406.76 toks/s]
Processed prompts: 100%|| 64/64 [00:23<00:00,  8.05it/s, est. speed input: 43.62 toks/s, output: 697.99 toks/s]
Processed prompts: 100%|| 64/64 [00:23<00:00,  8.05it/s, est. speed input: 43.62 toks/s, output: 697.99 toks/s]
Processed prompts: 100%|| 64/64 [00:23<00:00,  2.73it/s, est. speed input: 43.62 toks/s, output: 697.99 toks/s]
[rank0]:[W127 20:25:29.569773967 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 20:25:46
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:25:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:25:53 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3048472) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3048472) WARNING 01-27 20:27:56 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.35 requests/s, 1183.05 total tokens/s, 1113.46 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 20:25:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:25:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:25:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:25:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:25:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:25:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:25:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:25:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:25:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:25:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:25:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:25:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:25:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:25:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:25:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:25:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3048472) [2026-01-27 20:25:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3048472) [2026-01-27 20:25:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3048472) [2026-01-27 20:25:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3048472) [2026-01-27 20:25:57] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3048472) [2026-01-27 20:25:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3048472) [2026-01-27 20:25:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3048472) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3048472) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:24,  8.24s/it]
(EngineCore_DP0 pid=3048472) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:43<00:48, 24.04s/it]
(EngineCore_DP0 pid=3048472) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:15<00:27, 27.68s/it]
(EngineCore_DP0 pid=3048472) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:48<00:00, 30.03s/it]
(EngineCore_DP0 pid=3048472) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:48<00:00, 27.25s/it]
(EngineCore_DP0 pid=3048472) 
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=3048472) 2026-01-27 20:27:55,385 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3048472) 2026-01-27 20:27:55,424 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  11%|         | 14/128 [00:00<00:00, 136.49it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 1100.84it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:28<59:20, 28.04s/it, est. speed input: 0.57 toks/s, output: 9.13 toks/s]
Processed prompts:   2%|         | 2/128 [00:28<24:24, 11.62s/it, est. speed input: 1.14 toks/s, output: 18.18 toks/s]
Processed prompts:  14%|        | 18/128 [00:28<01:29,  1.23it/s, est. speed input: 10.18 toks/s, output: 162.81 toks/s]
Processed prompts:  26%|       | 33/128 [00:28<00:34,  2.72it/s, est. speed input: 18.56 toks/s, output: 296.93 toks/s]
Processed prompts:  38%|      | 48/128 [00:28<00:16,  4.81it/s, est. speed input: 26.89 toks/s, output: 430.32 toks/s]
Processed prompts:  48%|     | 62/128 [00:28<00:08,  7.47it/s, est. speed input: 34.59 toks/s, output: 553.50 toks/s]
Processed prompts:  59%|    | 75/128 [00:28<00:04, 10.76it/s, est. speed input: 41.66 toks/s, output: 666.63 toks/s]
Processed prompts:  68%|   | 87/128 [00:28<00:02, 14.75it/s, est. speed input: 48.13 toks/s, output: 770.11 toks/s]
Processed prompts:  77%|  | 98/128 [00:29<00:01, 19.49it/s, est. speed input: 54.02 toks/s, output: 864.27 toks/s]
Processed prompts:  92%|| 118/128 [00:29<00:00, 30.42it/s, est. speed input: 64.69 toks/s, output: 1035.03 toks/s]
Processed prompts: 100%|| 128/128 [00:29<00:00, 30.42it/s, est. speed input: 69.99 toks/s, output: 1119.85 toks/s]
Processed prompts: 100%|| 128/128 [00:29<00:00,  4.37it/s, est. speed input: 69.99 toks/s, output: 1119.85 toks/s]
[rank0]:[W127 20:28:26.208987319 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 20:28:35
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:28:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:28:40 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3050982) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3050982) WARNING 01-27 20:30:44 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.84 requests/s, 1587.50 total tokens/s, 1494.12 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 20:28:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:28:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:28:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:28:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:28:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:28:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:28:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:28:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:28:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:28:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:28:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:28:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:28:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:28:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:28:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:28:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3050982) [2026-01-27 20:28:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3050982) [2026-01-27 20:28:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3050982) [2026-01-27 20:28:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3050982) [2026-01-27 20:28:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3050982) [2026-01-27 20:28:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3050982) [2026-01-27 20:28:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3050982) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3050982) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.55s/it]
(EngineCore_DP0 pid=3050982) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:43<00:47, 23.99s/it]
(EngineCore_DP0 pid=3050982) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:15<00:27, 27.73s/it]
(EngineCore_DP0 pid=3050982) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:48<00:00, 29.88s/it]
(EngineCore_DP0 pid=3050982) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:48<00:00, 27.18s/it]
(EngineCore_DP0 pid=3050982) 
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=3050982) 2026-01-27 20:30:42,905 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3050982) 2026-01-27 20:30:42,978 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:38,  6.66it/s]
Adding requests:   1%|          | 3/256 [00:00<00:22, 11.06it/s]
Adding requests:   2%|         | 6/256 [00:00<00:14, 17.34it/s]
Adding requests:   4%|         | 9/256 [00:00<00:11, 20.88it/s]
Adding requests:   5%|         | 12/256 [00:00<00:10, 23.69it/s]
Adding requests:   7%|         | 18/256 [00:00<00:07, 33.54it/s]
Adding requests:  11%|         | 27/256 [00:00<00:04, 49.86it/s]
Adding requests:  14%|        | 35/256 [00:00<00:03, 58.76it/s]
Adding requests:  19%|        | 48/256 [00:01<00:02, 79.12it/s]
Adding requests:  27%|       | 68/256 [00:01<00:01, 113.21it/s]
Adding requests:  36%|      | 93/256 [00:01<00:01, 152.81it/s]
Adding requests:  49%|     | 125/256 [00:01<00:00, 201.10it/s]
Adding requests:  62%|   | 158/256 [00:01<00:00, 237.79it/s]
Adding requests:  71%|   | 182/256 [00:01<00:00, 167.53it/s]
Adding requests:  90%| | 231/256 [00:01<00:00, 240.87it/s]
Adding requests: 100%|| 256/256 [00:01<00:00, 139.30it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:35<2:32:15, 35.83s/it, est. speed input: 0.45 toks/s, output: 7.15 toks/s]
Processed prompts:   1%|          | 2/256 [00:35<1:02:50, 14.84s/it, est. speed input: 0.89 toks/s, output: 14.23 toks/s]
Processed prompts:   3%|         | 7/256 [00:36<11:57,  2.88s/it, est. speed input: 3.10 toks/s, output: 49.57 toks/s]  
Processed prompts:   4%|         | 10/256 [00:36<07:04,  1.73s/it, est. speed input: 4.41 toks/s, output: 70.49 toks/s]
Processed prompts:   5%|         | 13/256 [00:36<04:29,  1.11s/it, est. speed input: 5.70 toks/s, output: 91.23 toks/s]
Processed prompts:   7%|         | 17/256 [00:36<02:40,  1.49it/s, est. speed input: 7.42 toks/s, output: 118.67 toks/s]
Processed prompts:  10%|         | 26/256 [00:36<01:08,  3.35it/s, est. speed input: 11.29 toks/s, output: 180.68 toks/s]
Processed prompts:  14%|        | 36/256 [00:37<00:36,  6.04it/s, est. speed input: 15.56 toks/s, output: 248.96 toks/s]
Processed prompts:  20%|        | 50/256 [00:37<00:18, 11.00it/s, est. speed input: 21.51 toks/s, output: 344.17 toks/s]
Processed prompts:  25%|       | 63/256 [00:37<00:11, 16.77it/s, est. speed input: 27.00 toks/s, output: 431.96 toks/s]
Processed prompts:  30%|       | 76/256 [00:37<00:07, 23.75it/s, est. speed input: 32.44 toks/s, output: 519.12 toks/s]
Processed prompts:  34%|      | 88/256 [00:37<00:05, 30.40it/s, est. speed input: 37.41 toks/s, output: 598.51 toks/s]
Processed prompts:  39%|      | 99/256 [00:37<00:04, 37.32it/s, est. speed input: 41.93 toks/s, output: 670.91 toks/s]
Processed prompts:  43%|     | 109/256 [00:37<00:03, 43.59it/s, est. speed input: 46.01 toks/s, output: 736.16 toks/s]
Processed prompts:  46%|     | 119/256 [00:38<00:02, 48.07it/s, est. speed input: 50.03 toks/s, output: 800.44 toks/s]
Processed prompts:  50%|     | 128/256 [00:38<00:02, 52.96it/s, est. speed input: 53.64 toks/s, output: 858.21 toks/s]
Processed prompts:  54%|    | 137/256 [00:38<00:02, 55.77it/s, est. speed input: 57.20 toks/s, output: 915.22 toks/s]
Processed prompts:  57%|    | 145/256 [00:38<00:01, 55.78it/s, est. speed input: 60.32 toks/s, output: 965.06 toks/s]
Processed prompts:  60%|    | 153/256 [00:38<00:01, 59.19it/s, est. speed input: 63.46 toks/s, output: 1015.32 toks/s]
Processed prompts:  62%|   | 160/256 [00:38<00:01, 59.99it/s, est. speed input: 66.17 toks/s, output: 1058.70 toks/s]
Processed prompts:  65%|   | 167/256 [00:38<00:01, 61.26it/s, est. speed input: 68.87 toks/s, output: 1101.95 toks/s]
Processed prompts:  68%|   | 174/256 [00:38<00:01, 62.09it/s, est. speed input: 71.56 toks/s, output: 1144.94 toks/s]
Processed prompts:  71%|   | 181/256 [00:39<00:01, 63.81it/s, est. speed input: 74.24 toks/s, output: 1187.88 toks/s]
Processed prompts:  73%|  | 188/256 [00:39<00:01, 65.06it/s, est. speed input: 76.91 toks/s, output: 1230.59 toks/s]
Processed prompts:  76%|  | 195/256 [00:39<00:01, 52.52it/s, est. speed input: 79.38 toks/s, output: 1270.05 toks/s]
Processed prompts:  79%|  | 202/256 [00:39<00:01, 46.71it/s, est. speed input: 81.83 toks/s, output: 1309.33 toks/s]
Processed prompts:  82%| | 210/256 [00:39<00:01, 43.93it/s, est. speed input: 84.64 toks/s, output: 1354.16 toks/s]
Processed prompts:  84%| | 216/256 [00:39<00:00, 40.55it/s, est. speed input: 86.66 toks/s, output: 1386.53 toks/s]
Processed prompts:  87%| | 222/256 [00:40<00:00, 38.44it/s, est. speed input: 88.67 toks/s, output: 1418.72 toks/s]
Processed prompts:  89%| | 227/256 [00:40<00:00, 40.10it/s, est. speed input: 90.42 toks/s, output: 1446.77 toks/s]
Processed prompts:  91%| | 232/256 [00:40<00:00, 31.85it/s, est. speed input: 91.84 toks/s, output: 1469.49 toks/s]
Processed prompts:  92%|| 236/256 [00:40<00:00, 29.87it/s, est. speed input: 93.05 toks/s, output: 1488.84 toks/s]
Processed prompts:  94%|| 240/256 [00:40<00:00, 28.29it/s, est. speed input: 94.25 toks/s, output: 1507.97 toks/s]
Processed prompts:  95%|| 243/256 [00:40<00:00, 25.78it/s, est. speed input: 95.06 toks/s, output: 1521.01 toks/s]
Processed prompts:  96%|| 246/256 [00:41<00:00, 20.88it/s, est. speed input: 95.69 toks/s, output: 1530.96 toks/s]
Processed prompts:  97%|| 249/256 [00:41<00:00, 18.22it/s, est. speed input: 96.32 toks/s, output: 1541.07 toks/s]
Processed prompts:  98%|| 251/256 [00:41<00:00, 16.98it/s, est. speed input: 96.73 toks/s, output: 1547.76 toks/s]
Processed prompts:  99%|| 253/256 [00:41<00:00, 16.13it/s, est. speed input: 97.16 toks/s, output: 1554.59 toks/s]
Processed prompts: 100%|| 255/256 [00:41<00:00, 15.43it/s, est. speed input: 97.58 toks/s, output: 1561.33 toks/s]
Processed prompts: 100%|| 256/256 [00:41<00:00, 15.43it/s, est. speed input: 97.69 toks/s, output: 1563.00 toks/s]
Processed prompts: 100%|| 256/256 [00:41<00:00,  6.11it/s, est. speed input: 97.69 toks/s, output: 1563.00 toks/s]
[rank0]:[W127 20:31:28.535085591 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 20:31:41
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:31:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:31:48 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3053779) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3053779) WARNING 01-27 20:33:53 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.96 requests/s, 1620.85 total tokens/s, 1525.51 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 20:31:48] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:31:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:31:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:31:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:31:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:31:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:31:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:31:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:31:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:31:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:31:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:31:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:31:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:31:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:31:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:31:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3053779) [2026-01-27 20:31:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3053779) [2026-01-27 20:31:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3053779) [2026-01-27 20:31:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3053779) [2026-01-27 20:31:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3053779) [2026-01-27 20:31:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3053779) [2026-01-27 20:31:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3053779) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3053779) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.50s/it]
(EngineCore_DP0 pid=3053779) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:43<00:48, 24.19s/it]
(EngineCore_DP0 pid=3053779) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:16<00:27, 27.94s/it]
(EngineCore_DP0 pid=3053779) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:50<00:00, 30.37s/it]
(EngineCore_DP0 pid=3053779) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:50<00:00, 27.55s/it]
(EngineCore_DP0 pid=3053779) 
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=3053779) 2026-01-27 20:33:52,058 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3053779) 2026-01-27 20:33:52,107 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/512 [00:00<00:54,  9.35it/s]
Adding requests:   2%|         | 9/512 [00:00<00:10, 48.40it/s]
Adding requests:  14%|        | 71/512 [00:00<00:01, 300.62it/s]
Adding requests:  30%|       | 156/512 [00:00<00:00, 512.65it/s]
Adding requests:  45%|     | 231/512 [00:00<00:00, 595.63it/s]
Adding requests:  63%|   | 323/512 [00:00<00:00, 702.91it/s]
Adding requests:  86%| | 441/512 [00:00<00:00, 857.32it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 674.10it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:59<8:28:52, 59.75s/it, est. speed input: 0.27 toks/s, output: 4.28 toks/s]
Processed prompts:   0%|          | 2/512 [01:00<3:32:37, 25.02s/it, est. speed input: 0.53 toks/s, output: 8.47 toks/s]
Processed prompts:   6%|         | 33/512 [01:01<07:33,  1.06it/s, est. speed input: 8.64 toks/s, output: 138.17 toks/s]
Processed prompts:  12%|        | 63/512 [01:01<03:07,  2.39it/s, est. speed input: 16.31 toks/s, output: 261.00 toks/s]
Processed prompts:  18%|        | 91/512 [01:02<01:43,  4.05it/s, est. speed input: 23.33 toks/s, output: 373.27 toks/s]
Processed prompts:  23%|       | 117/512 [01:03<01:05,  6.05it/s, est. speed input: 29.71 toks/s, output: 475.38 toks/s]
Processed prompts:  28%|       | 141/512 [01:03<00:42,  8.69it/s, est. speed input: 35.67 toks/s, output: 570.64 toks/s]
Processed prompts:  32%|      | 164/512 [01:03<00:28, 12.09it/s, est. speed input: 41.33 toks/s, output: 661.31 toks/s]
Processed prompts:  36%|      | 185/512 [01:03<00:20, 16.10it/s, est. speed input: 46.46 toks/s, output: 743.34 toks/s]
Processed prompts:  40%|      | 205/512 [01:03<00:14, 20.97it/s, est. speed input: 51.31 toks/s, output: 820.96 toks/s]
Processed prompts:  44%|     | 224/512 [01:04<00:10, 26.67it/s, est. speed input: 55.89 toks/s, output: 894.21 toks/s]
Processed prompts:  47%|     | 242/512 [01:04<00:08, 32.86it/s, est. speed input: 60.18 toks/s, output: 962.95 toks/s]
Processed prompts:  50%|     | 258/512 [01:04<00:06, 38.63it/s, est. speed input: 63.96 toks/s, output: 1023.38 toks/s]
Processed prompts:  53%|    | 273/512 [01:04<00:05, 44.29it/s, est. speed input: 67.48 toks/s, output: 1079.64 toks/s]
Processed prompts:  56%|    | 288/512 [01:04<00:04, 51.82it/s, est. speed input: 71.01 toks/s, output: 1136.24 toks/s]
Processed prompts:  59%|    | 302/512 [01:05<00:03, 57.05it/s, est. speed input: 74.27 toks/s, output: 1188.25 toks/s]
Processed prompts:  62%|   | 315/512 [01:05<00:03, 61.29it/s, est. speed input: 77.26 toks/s, output: 1236.23 toks/s]
Processed prompts:  64%|   | 327/512 [01:05<00:02, 62.73it/s, est. speed input: 79.99 toks/s, output: 1279.83 toks/s]
Processed prompts:  66%|   | 338/512 [01:05<00:02, 63.93it/s, est. speed input: 82.48 toks/s, output: 1319.61 toks/s]
Processed prompts:  68%|   | 348/512 [01:05<00:02, 66.21it/s, est. speed input: 84.74 toks/s, output: 1355.87 toks/s]
Processed prompts:  70%|   | 358/512 [01:05<00:02, 66.08it/s, est. speed input: 86.98 toks/s, output: 1391.61 toks/s]
Processed prompts:  72%|  | 367/512 [01:05<00:02, 67.57it/s, est. speed input: 88.99 toks/s, output: 1423.91 toks/s]
Processed prompts:  73%|  | 376/512 [01:06<00:02, 65.55it/s, est. speed input: 90.97 toks/s, output: 1455.55 toks/s]
Processed prompts:  75%|  | 384/512 [01:06<00:01, 66.01it/s, est. speed input: 92.74 toks/s, output: 1483.85 toks/s]
Processed prompts:  77%|  | 392/512 [01:06<00:01, 62.04it/s, est. speed input: 94.46 toks/s, output: 1511.33 toks/s]
Processed prompts:  78%|  | 399/512 [01:06<00:01, 61.91it/s, est. speed input: 95.98 toks/s, output: 1535.69 toks/s]
Processed prompts:  79%|  | 406/512 [01:06<00:01, 59.24it/s, est. speed input: 97.47 toks/s, output: 1559.52 toks/s]
Processed prompts:  80%|  | 412/512 [01:06<00:01, 58.18it/s, est. speed input: 98.75 toks/s, output: 1579.99 toks/s]
Processed prompts:  82%| | 418/512 [01:06<00:01, 54.78it/s, est. speed input: 100.00 toks/s, output: 1599.93 toks/s]
Processed prompts:  83%| | 424/512 [01:06<00:01, 55.65it/s, est. speed input: 101.27 toks/s, output: 1620.39 toks/s]
Processed prompts:  84%| | 430/512 [01:07<00:01, 56.54it/s, est. speed input: 102.55 toks/s, output: 1640.83 toks/s]
Processed prompts:  85%| | 436/512 [01:07<00:01, 51.71it/s, est. speed input: 103.76 toks/s, output: 1660.24 toks/s]
Processed prompts:  86%| | 442/512 [01:07<00:01, 42.00it/s, est. speed input: 104.87 toks/s, output: 1677.87 toks/s]
Processed prompts:  88%| | 450/512 [01:07<00:01, 39.17it/s, est. speed input: 106.40 toks/s, output: 1702.45 toks/s]
Processed prompts:  89%| | 456/512 [01:07<00:01, 36.87it/s, est. speed input: 107.52 toks/s, output: 1720.37 toks/s]
Processed prompts:  90%| | 462/512 [01:08<00:01, 35.55it/s, est. speed input: 108.64 toks/s, output: 1738.29 toks/s]
Processed prompts:  91%| | 466/512 [01:08<00:01, 35.66it/s, est. speed input: 109.41 toks/s, output: 1750.49 toks/s]
Processed prompts:  92%|| 470/512 [01:08<00:01, 31.52it/s, est. speed input: 110.06 toks/s, output: 1760.98 toks/s]
Processed prompts:  93%|| 474/512 [01:08<00:01, 28.71it/s, est. speed input: 110.71 toks/s, output: 1771.38 toks/s]
Processed prompts:  93%|| 478/512 [01:08<00:01, 27.22it/s, est. speed input: 111.37 toks/s, output: 1781.95 toks/s]
Processed prompts:  94%|| 482/512 [01:08<00:01, 25.35it/s, est. speed input: 112.00 toks/s, output: 1791.99 toks/s]
Processed prompts:  95%|| 485/512 [01:09<00:01, 20.43it/s, est. speed input: 112.30 toks/s, output: 1796.77 toks/s]
Processed prompts:  95%|| 488/512 [01:09<00:01, 17.68it/s, est. speed input: 112.60 toks/s, output: 1801.62 toks/s]
Processed prompts:  96%|| 490/512 [01:09<00:01, 16.38it/s, est. speed input: 112.80 toks/s, output: 1804.82 toks/s]
Processed prompts:  96%|| 492/512 [01:09<00:01, 15.28it/s, est. speed input: 113.00 toks/s, output: 1807.93 toks/s]
Processed prompts:  96%|| 494/512 [01:09<00:01, 14.68it/s, est. speed input: 113.20 toks/s, output: 1811.27 toks/s]
Processed prompts:  97%|| 496/512 [01:09<00:01, 14.17it/s, est. speed input: 113.41 toks/s, output: 1814.56 toks/s]
Processed prompts:  97%|| 498/512 [01:24<00:27,  2.00s/it, est. speed input: 93.98 toks/s, output: 1503.75 toks/s] 
Processed prompts:  98%|| 501/512 [01:24<00:14,  1.30s/it, est. speed input: 94.38 toks/s, output: 1510.07 toks/s]
Processed prompts: 100%|| 511/512 [01:25<00:00,  2.05it/s, est. speed input: 96.09 toks/s, output: 1537.49 toks/s]
Processed prompts: 100%|| 512/512 [01:25<00:00,  2.05it/s, est. speed input: 96.28 toks/s, output: 1540.49 toks/s]
Processed prompts: 100%|| 512/512 [01:25<00:00,  6.02it/s, est. speed input: 96.28 toks/s, output: 1540.49 toks/s]
[rank0]:[W127 20:35:19.702036887 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

