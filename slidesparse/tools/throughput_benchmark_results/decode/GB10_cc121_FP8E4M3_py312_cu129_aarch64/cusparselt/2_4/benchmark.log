
========== M=16 ==========
Time: 2026-01-25 18:46:56
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-FP8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:47:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:47:00 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=299207) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) ================================================================
(EngineCore_DP0 pid=299207) Internal Triton PTX codegen error
(EngineCore_DP0 pid=299207) `ptxas` stderr:
(EngineCore_DP0 pid=299207) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpvbklx7pt.ptx -o /tmp/tmpvbklx7pt.ptx.o
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) //
(EngineCore_DP0 pid=299207) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=299207) //
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) .version 8.7
(EngineCore_DP0 pid=299207) .target sm_121a
(EngineCore_DP0 pid=299207) .address_size 64
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) 	// .globl	_quant_slide_fp8_kernel // -- Begin function _quant_slide_fp8_kernel
(EngineCore_DP0 pid=299207) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=299207)                                         // @_quant_slide_fp8_kernel
(EngineCore_DP0 pid=299207) .visible .entry _quant_slide_fp8_kernel(
(EngineCore_DP0 pid=299207) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_0,
(EngineCore_DP0 pid=299207) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_1,
(EngineCore_DP0 pid=299207) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_2,
(EngineCore_DP0 pid=299207) 	.param .u32 _quant_slide_fp8_kernel_param_3,
(EngineCore_DP0 pid=299207) 	.param .u32 _quant_slide_fp8_kernel_param_4,
(EngineCore_DP0 pid=299207) 	.param .u32 _quant_slide_fp8_kernel_param_5,
(EngineCore_DP0 pid=299207) 	.param .u32 _quant_slide_fp8_kernel_param_6,
(EngineCore_DP0 pid=299207) 	.param .u32 _quant_slide_fp8_kernel_param_7,
(EngineCore_DP0 pid=299207) 	.param .u32 _quant_slide_fp8_kernel_param_8,
(EngineCore_DP0 pid=299207) 	.param .u32 _quant_slide_fp8_kernel_param_9,
(EngineCore_DP0 pid=299207) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_10,
(EngineCore_DP0 pid=299207) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_11
(EngineCore_DP0 pid=299207) )
(EngineCore_DP0 pid=299207) .reqntid 1024
(EngineCore_DP0 pid=299207) {
(EngineCore_DP0 pid=299207) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=299207) 	.reg .b16 	%rs<25>;
(EngineCore_DP0 pid=299207) 	.reg .b32 	%r<108>;
(EngineCore_DP0 pid=299207) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=299207) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=299207) $L__func_begin0:
(EngineCore_DP0 pid=299207) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) // %bb.0:
(EngineCore_DP0 pid=299207) 	ld.param.b32 	%r20, [_quant_slide_fp8_kernel_param_7];
(EngineCore_DP0 pid=299207) 	ld.param.b32 	%r19, [_quant_slide_fp8_kernel_param_5];
(EngineCore_DP0 pid=299207) 	ld.param.b32 	%r18, [_quant_slide_fp8_kernel_param_4];
(EngineCore_DP0 pid=299207) 	ld.param.b64 	%rd3, [_quant_slide_fp8_kernel_param_2];
(EngineCore_DP0 pid=299207) 	ld.param.b64 	%rd4, [_quant_slide_fp8_kernel_param_0];
(EngineCore_DP0 pid=299207) $L__tmp0:
(EngineCore_DP0 pid=299207) 	.loc	1 177 24                        // quant_slide_tuned_Llama3.2-1B.py:177:24
(EngineCore_DP0 pid=299207) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=299207) 	ld.param.b32 	%r22, [_quant_slide_fp8_kernel_param_8];
(EngineCore_DP0 pid=299207) 	.loc	1 182 26                        // quant_slide_tuned_Llama3.2-1B.py:182:26
(EngineCore_DP0 pid=299207) 	mul.lo.s32 	%r23, %r22, %r1;
(EngineCore_DP0 pid=299207) 	.loc	1 182 20                        // quant_slide_tuned_Llama3.2-1B.py:182:20
(EngineCore_DP0 pid=299207) 	mad.wide.s32 	%rd1, %r23, 2, %rd4;
(EngineCore_DP0 pid=299207) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=299207) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=299207) 	shl.b32 	%r106, %r2, 2;
(EngineCore_DP0 pid=299207) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=299207) 	setp.lt.s32 	%p1, %r19, 1;
(EngineCore_DP0 pid=299207) 	mov.b32 	%r105, 0f2B8CBCCC;
(EngineCore_DP0 pid=299207) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=299207) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=299207) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=299207) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=299207) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=299207) 	shr.u32 	%r32, %r2, 3;
(EngineCore_DP0 pid=299207) 	and.b32 	%r33, %r32, 124;
(EngineCore_DP0 pid=299207) 	mov.b32 	%r34, global_smem;
(EngineCore_DP0 pid=299207) 	add.s32 	%r40, %r34, %r33;
(EngineCore_DP0 pid=299207) 	shl.b32 	%r35, %r2, 2;
(EngineCore_DP0 pid=299207) 	add.s32 	%r43, %r34, %r35;
(EngineCore_DP0 pid=299207) 	mov.b32 	%r38, 0;
(EngineCore_DP0 pid=299207) 	mov.b32 	%r103, 0f00000000;
(EngineCore_DP0 pid=299207) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=299207) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=299207) 	mov.b32 	%r104, %r38;
(EngineCore_DP0 pid=299207) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=299207) 	.loc	1 189 22                        // quant_slide_tuned_Llama3.2-1B.py:189:22
(EngineCore_DP0 pid=299207) 	add.s32 	%r46, %r106, %r104;
(EngineCore_DP0 pid=299207) 	setp.lt.s32 	%p2, %r46, %r18;
(EngineCore_DP0 pid=299207) 	.loc	1 190 29                        // quant_slide_tuned_Llama3.2-1B.py:190:29
(EngineCore_DP0 pid=299207) 	mad.wide.s32 	%rd6, %r46, 2, %rd1;
(EngineCore_DP0 pid=299207) 	.loc	1 190 21                        // quant_slide_tuned_Llama3.2-1B.py:190:21
(EngineCore_DP0 pid=299207) 	// begin inline asm
(EngineCore_DP0 pid=299207) 	mov.u32 %r36, %r38;
(EngineCore_DP0 pid=299207) 	mov.u32 %r37, %r38;
(EngineCore_DP0 pid=299207) 	@%p2 ld.global.v2.b32 { %r36, %r37 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=299207) 	// end inline asm
(EngineCore_DP0 pid=299207) 	mov.b32 	{%rs1, %rs2}, %r36;
(EngineCore_DP0 pid=299207) 	mov.b32 	{%rs3, %rs4}, %r37;
(EngineCore_DP0 pid=299207) 	.loc	1 191 50                        // quant_slide_tuned_Llama3.2-1B.py:191:50
(EngineCore_DP0 pid=299207) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=299207) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=299207) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=299207) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=299207) $L__tmp1:
(EngineCore_DP0 pid=299207) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	bar.sync 	0;
(EngineCore_DP0 pid=299207) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=299207) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=299207) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=299207) 	cvt.f32.bf16 	%r47, %rs11;
(EngineCore_DP0 pid=299207) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	shfl.sync.bfly.b32 	%r48, %r47, 16, 31, -1;
(EngineCore_DP0 pid=299207) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	max.f32 	%r49, %r47, %r48;
(EngineCore_DP0 pid=299207) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	shfl.sync.bfly.b32 	%r50, %r49, 8, 31, -1;
(EngineCore_DP0 pid=299207) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	max.f32 	%r51, %r49, %r50;
(EngineCore_DP0 pid=299207) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	shfl.sync.bfly.b32 	%r52, %r51, 4, 31, -1;
(EngineCore_DP0 pid=299207) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	max.f32 	%r53, %r51, %r52;
(EngineCore_DP0 pid=299207) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	shfl.sync.bfly.b32 	%r54, %r53, 2, 31, -1;
(EngineCore_DP0 pid=299207) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	max.f32 	%r55, %r53, %r54;
(EngineCore_DP0 pid=299207) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	shfl.sync.bfly.b32 	%r56, %r55, 1, 31, -1;
(EngineCore_DP0 pid=299207) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	max.f32 	%r41, %r55, %r56;
(EngineCore_DP0 pid=299207) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	// begin inline asm
(EngineCore_DP0 pid=299207) 	@%p3 st.shared.b32 [ %r40 + 0 ], %r41;
(EngineCore_DP0 pid=299207) 	// end inline asm
(EngineCore_DP0 pid=299207) 	bar.sync 	0;
(EngineCore_DP0 pid=299207) 	// begin inline asm
(EngineCore_DP0 pid=299207) 	@%p4 ld.shared.b32 %r42, [ %r43 + 0 ];
(EngineCore_DP0 pid=299207) 	// end inline asm
(EngineCore_DP0 pid=299207) 	shfl.sync.bfly.b32 	%r57, %r42, 16, 31, -1;
(EngineCore_DP0 pid=299207) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	max.f32 	%r58, %r42, %r57;
(EngineCore_DP0 pid=299207) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	shfl.sync.bfly.b32 	%r59, %r58, 8, 31, -1;
(EngineCore_DP0 pid=299207) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	max.f32 	%r60, %r58, %r59;
(EngineCore_DP0 pid=299207) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	shfl.sync.bfly.b32 	%r61, %r60, 4, 31, -1;
(EngineCore_DP0 pid=299207) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	max.f32 	%r62, %r60, %r61;
(EngineCore_DP0 pid=299207) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	shfl.sync.bfly.b32 	%r63, %r62, 2, 31, -1;
(EngineCore_DP0 pid=299207) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	max.f32 	%r64, %r62, %r63;
(EngineCore_DP0 pid=299207) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	shfl.sync.bfly.b32 	%r65, %r64, 1, 31, -1;
(EngineCore_DP0 pid=299207) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	max.f32 	%r45, %r64, %r65;
(EngineCore_DP0 pid=299207) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299207) 	// begin inline asm
(EngineCore_DP0 pid=299207) 	@%p19 st.shared.b32 [ %r43 + 0 ], %r45;
(EngineCore_DP0 pid=299207) 	// end inline asm
(EngineCore_DP0 pid=299207) 	bar.sync 	0;
(EngineCore_DP0 pid=299207) 	ld.shared.b32 	%r66, [global_smem];
(EngineCore_DP0 pid=299207) $L__tmp2:
(EngineCore_DP0 pid=299207) 	.loc	1 191 36                        // quant_slide_tuned_Llama3.2-1B.py:191:36
(EngineCore_DP0 pid=299207) 	max.f32 	%r103, %r103, %r66;
(EngineCore_DP0 pid=299207) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=299207) 	add.s32 	%r104, %r104, 4096;
(EngineCore_DP0 pid=299207) 	setp.lt.s32 	%p6, %r104, %r19;
(EngineCore_DP0 pid=299207) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=299207) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=299207) 	.loc	1 193 32                        // quant_slide_tuned_Llama3.2-1B.py:193:32
(EngineCore_DP0 pid=299207) 	max.f32 	%r105, %r103, 0f2B8CBCCC;
(EngineCore_DP0 pid=299207) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=299207) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=299207) 	mov.b32 	%r68, 0f43E00000;
(EngineCore_DP0 pid=299207) 	.loc	1 194 32                        // quant_slide_tuned_Llama3.2-1B.py:194:32
(EngineCore_DP0 pid=299207) 	div.full.f32 	%r69, %r105, %r68;
(EngineCore_DP0 pid=299207) 	.loc	1 194 41                        // quant_slide_tuned_Llama3.2-1B.py:194:41
(EngineCore_DP0 pid=299207) 	max.f32 	%r67, %r69, 0f36924925;
(EngineCore_DP0 pid=299207) 	.loc	1 196 25                        // quant_slide_tuned_Llama3.2-1B.py:196:25
(EngineCore_DP0 pid=299207) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=299207) 	.loc	1 196 30                        // quant_slide_tuned_Llama3.2-1B.py:196:30
(EngineCore_DP0 pid=299207) 	// begin inline asm
(EngineCore_DP0 pid=299207) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r67 };
(EngineCore_DP0 pid=299207) 	// end inline asm
(EngineCore_DP0 pid=299207) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=299207) 	setp.lt.s32 	%p8, %r20, 1;
(EngineCore_DP0 pid=299207) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=299207) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=299207) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=299207) 	ld.param.b32 	%r24, [_quant_slide_fp8_kernel_param_9];
(EngineCore_DP0 pid=299207) 	shr.s32 	%r25, %r24, 31;
(EngineCore_DP0 pid=299207) 	shr.u32 	%r26, %r25, 30;
(EngineCore_DP0 pid=299207) 	add.s32 	%r27, %r24, %r26;
(EngineCore_DP0 pid=299207) 	shr.s32 	%r28, %r27, 2;
(EngineCore_DP0 pid=299207) 	ld.param.b64 	%rd5, [_quant_slide_fp8_kernel_param_1];
(EngineCore_DP0 pid=299207) 	mul.lo.s32 	%r29, %r28, %r1;
(EngineCore_DP0 pid=299207) 	mad.wide.s32 	%rd2, %r29, 4, %rd5;
(EngineCore_DP0 pid=299207) 	div.full.f32 	%r13, %r68, %r105;
(EngineCore_DP0 pid=299207) 	mov.b32 	%r107, 0;
(EngineCore_DP0 pid=299207) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=299207)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=299207) 	.loc	1 203 30                        // quant_slide_tuned_Llama3.2-1B.py:203:30
(EngineCore_DP0 pid=299207) 	add.s32 	%r80, %r2, %r107;
(EngineCore_DP0 pid=299207) 	setp.lt.s32 	%p13, %r80, %r20;
(EngineCore_DP0 pid=299207) 	.loc	1 211 53                        // quant_slide_tuned_Llama3.2-1B.py:211:53
(EngineCore_DP0 pid=299207) 	setp.lt.s32 	%p14, %r106, %r18;
(EngineCore_DP0 pid=299207) 	.loc	1 211 37                        // quant_slide_tuned_Llama3.2-1B.py:211:37
(EngineCore_DP0 pid=299207) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=299207) 	.loc	1 210 29                        // quant_slide_tuned_Llama3.2-1B.py:210:29
(EngineCore_DP0 pid=299207) 	mad.wide.s32 	%rd8, %r106, 2, %rd1;
(EngineCore_DP0 pid=299207) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=299207) 	.loc	1 210 21                        // quant_slide_tuned_Llama3.2-1B.py:210:21
(EngineCore_DP0 pid=299207) 	// begin inline asm
(EngineCore_DP0 pid=299207) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=299207) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=299207) 	// end inline asm
(EngineCore_DP0 pid=299207) 	.loc	1 211 79                        // quant_slide_tuned_Llama3.2-1B.py:211:79
(EngineCore_DP0 pid=299207) 	cvt.f32.bf16 	%r81, %rs12;
(EngineCore_DP0 pid=299207) 	.loc	1 213 53                        // quant_slide_tuned_Llama3.2-1B.py:213:53
(EngineCore_DP0 pid=299207) 	add.s32 	%r82, %r106, 1;
(EngineCore_DP0 pid=299207) 	setp.lt.s32 	%p15, %r82, %r18;
(EngineCore_DP0 pid=299207) 	.loc	1 213 37                        // quant_slide_tuned_Llama3.2-1B.py:213:37
(EngineCore_DP0 pid=299207) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=299207) 	.loc	1 212 39                        // quant_slide_tuned_Llama3.2-1B.py:212:39
(EngineCore_DP0 pid=299207) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=299207) 	.loc	1 212 21                        // quant_slide_tuned_Llama3.2-1B.py:212:21
(EngineCore_DP0 pid=299207) 	// begin inline asm
(EngineCore_DP0 pid=299207) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=299207) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=299207) 	// end inline asm
(EngineCore_DP0 pid=299207) 	.loc	1 213 79                        // quant_slide_tuned_Llama3.2-1B.py:213:79
(EngineCore_DP0 pid=299207) 	cvt.f32.bf16 	%r83, %rs14;
(EngineCore_DP0 pid=299207) 	.loc	1 215 53                        // quant_slide_tuned_Llama3.2-1B.py:215:53
(EngineCore_DP0 pid=299207) 	add.s32 	%r84, %r106, 2;
(EngineCore_DP0 pid=299207) 	setp.lt.s32 	%p16, %r84, %r18;
(EngineCore_DP0 pid=299207) 	.loc	1 215 37                        // quant_slide_tuned_Llama3.2-1B.py:215:37
(EngineCore_DP0 pid=299207) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=299207) 	.loc	1 214 39                        // quant_slide_tuned_Llama3.2-1B.py:214:39
(EngineCore_DP0 pid=299207) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=299207) 	.loc	1 214 21                        // quant_slide_tuned_Llama3.2-1B.py:214:21
(EngineCore_DP0 pid=299207) 	// begin inline asm
(EngineCore_DP0 pid=299207) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=299207) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=299207) 	// end inline asm
(EngineCore_DP0 pid=299207) 	.loc	1 215 79                        // quant_slide_tuned_Llama3.2-1B.py:215:79
(EngineCore_DP0 pid=299207) 	cvt.f32.bf16 	%r85, %rs16;
(EngineCore_DP0 pid=299207) 	.loc	1 217 53                        // quant_slide_tuned_Llama3.2-1B.py:217:53
(EngineCore_DP0 pid=299207) 	add.s32 	%r86, %r106, 3;
(EngineCore_DP0 pid=299207) 	setp.lt.s32 	%p17, %r86, %r18;
(EngineCore_DP0 pid=299207) 	.loc	1 217 37                        // quant_slide_tuned_Llama3.2-1B.py:217:37
(EngineCore_DP0 pid=299207) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=299207) 	.loc	1 216 39                        // quant_slide_tuned_Llama3.2-1B.py:216:39
(EngineCore_DP0 pid=299207) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=299207) 	.loc	1 216 21                        // quant_slide_tuned_Llama3.2-1B.py:216:21
(EngineCore_DP0 pid=299207) 	// begin inline asm
(EngineCore_DP0 pid=299207) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=299207) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=299207) 	// end inline asm
(EngineCore_DP0 pid=299207) 	.loc	1 217 79                        // quant_slide_tuned_Llama3.2-1B.py:217:79
(EngineCore_DP0 pid=299207) 	cvt.f32.bf16 	%r87, %rs18;
(EngineCore_DP0 pid=299207) 	.loc	1 219 27                        // quant_slide_tuned_Llama3.2-1B.py:219:27
(EngineCore_DP0 pid=299207) 	mul.f32 	%r88, %r13, %r81;
(EngineCore_DP0 pid=299207) 	mov.b32 	%r89, 0f43E00000;
(EngineCore_DP0 pid=299207) 	.loc	1 219 48                        // quant_slide_tuned_Llama3.2-1B.py:219:48
(EngineCore_DP0 pid=299207) 	min.xorsign.abs.f32 	%r71, %r88, %r89;
(EngineCore_DP0 pid=299207) 	.loc	1 219 60                        // quant_slide_tuned_Llama3.2-1B.py:219:60
(EngineCore_DP0 pid=299207) 	// begin inline asm
(EngineCore_DP0 pid=299207) 	cvt.rn.satfinite.e4m3x2.f32  %rs20, %r72, %r71; 
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) 	// end inline asm
(EngineCore_DP0 pid=299207) 	.loc	1 220 27                        // quant_slide_tuned_Llama3.2-1B.py:220:27
(EngineCore_DP0 pid=299207) 	mul.f32 	%r90, %r13, %r83;
(EngineCore_DP0 pid=299207) 	.loc	1 220 48                        // quant_slide_tuned_Llama3.2-1B.py:220:48
(EngineCore_DP0 pid=299207) 	min.xorsign.abs.f32 	%r73, %r90, %r89;
(EngineCore_DP0 pid=299207) 	.loc	1 220 60                        // quant_slide_tuned_Llama3.2-1B.py:220:60
(EngineCore_DP0 pid=299207) 	// begin inline asm
(EngineCore_DP0 pid=299207) 	cvt.rn.satfinite.e4m3x2.f32  %rs21, %r74, %r73; 
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) 	// end inline asm
(EngineCore_DP0 pid=299207) 	.loc	1 221 27                        // quant_slide_tuned_Llama3.2-1B.py:221:27
(EngineCore_DP0 pid=299207) 	mul.f32 	%r91, %r13, %r85;
(EngineCore_DP0 pid=299207) 	.loc	1 221 48                        // quant_slide_tuned_Llama3.2-1B.py:221:48
(EngineCore_DP0 pid=299207) 	min.xorsign.abs.f32 	%r75, %r91, %r89;
(EngineCore_DP0 pid=299207) 	.loc	1 221 60                        // quant_slide_tuned_Llama3.2-1B.py:221:60
(EngineCore_DP0 pid=299207) 	// begin inline asm
(EngineCore_DP0 pid=299207) 	cvt.rn.satfinite.e4m3x2.f32  %rs22, %r76, %r75; 
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) 	// end inline asm
(EngineCore_DP0 pid=299207) 	.loc	1 222 27                        // quant_slide_tuned_Llama3.2-1B.py:222:27
(EngineCore_DP0 pid=299207) 	mul.f32 	%r92, %r13, %r87;
(EngineCore_DP0 pid=299207) 	.loc	1 222 48                        // quant_slide_tuned_Llama3.2-1B.py:222:48
(EngineCore_DP0 pid=299207) 	min.xorsign.abs.f32 	%r77, %r92, %r89;
(EngineCore_DP0 pid=299207) 	.loc	1 222 60                        // quant_slide_tuned_Llama3.2-1B.py:222:60
(EngineCore_DP0 pid=299207) 	// begin inline asm
(EngineCore_DP0 pid=299207) 	cvt.rn.satfinite.e4m3x2.f32  %rs23, %r78, %r77; 
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) 	// end inline asm
(EngineCore_DP0 pid=299207) 	.loc	1 224 45                        // quant_slide_tuned_Llama3.2-1B.py:224:45
(EngineCore_DP0 pid=299207) 	cvt.u32.u16 	%r93, %rs20;
(EngineCore_DP0 pid=299207) 	and.b32 	%r94, %r93, 255;
(EngineCore_DP0 pid=299207) 	.loc	1 226 45                        // quant_slide_tuned_Llama3.2-1B.py:226:45
(EngineCore_DP0 pid=299207) 	cvt.u32.u16 	%r95, %rs22;
(EngineCore_DP0 pid=299207) 	and.b32 	%r96, %r95, 255;
(EngineCore_DP0 pid=299207) 	.loc	1 227 45                        // quant_slide_tuned_Llama3.2-1B.py:227:45
(EngineCore_DP0 pid=299207) 	cvt.u32.u16 	%r97, %rs23;
(EngineCore_DP0 pid=299207) 	.loc	1 229 30                        // quant_slide_tuned_Llama3.2-1B.py:229:30
(EngineCore_DP0 pid=299207) 	and.b16 	%rs24, %rs21, 255;
(EngineCore_DP0 pid=299207) 	mul.wide.u16 	%r98, %rs24, 256;
(EngineCore_DP0 pid=299207) 	.loc	1 229 24                        // quant_slide_tuned_Llama3.2-1B.py:229:24
(EngineCore_DP0 pid=299207) 	or.b32 	%r99, %r98, %r94;
(EngineCore_DP0 pid=299207) 	.loc	1 229 42                        // quant_slide_tuned_Llama3.2-1B.py:229:42
(EngineCore_DP0 pid=299207) 	shl.b32 	%r100, %r96, 16;
(EngineCore_DP0 pid=299207) 	.loc	1 229 36                        // quant_slide_tuned_Llama3.2-1B.py:229:36
(EngineCore_DP0 pid=299207) 	or.b32 	%r101, %r99, %r100;
(EngineCore_DP0 pid=299207) 	.loc	1 229 55                        // quant_slide_tuned_Llama3.2-1B.py:229:55
(EngineCore_DP0 pid=299207) 	shl.b32 	%r102, %r97, 24;
(EngineCore_DP0 pid=299207) 	.loc	1 229 49                        // quant_slide_tuned_Llama3.2-1B.py:229:49
(EngineCore_DP0 pid=299207) 	or.b32 	%r79, %r101, %r102;
(EngineCore_DP0 pid=299207) 	.loc	1 230 29                        // quant_slide_tuned_Llama3.2-1B.py:230:29
(EngineCore_DP0 pid=299207) 	mad.wide.s32 	%rd12, %r80, 4, %rd2;
(EngineCore_DP0 pid=299207) 	.loc	1 230 39                        // quant_slide_tuned_Llama3.2-1B.py:230:39
(EngineCore_DP0 pid=299207) 	// begin inline asm
(EngineCore_DP0 pid=299207) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r79 };
(EngineCore_DP0 pid=299207) 	// end inline asm
(EngineCore_DP0 pid=299207) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=299207) 	add.s32 	%r107, %r107, 1024;
(EngineCore_DP0 pid=299207) 	add.s32 	%r106, %r106, 4096;
(EngineCore_DP0 pid=299207) 	setp.lt.s32 	%p18, %r107, %r20;
(EngineCore_DP0 pid=299207) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=299207) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=299207) 	.loc	1 201 4                         // quant_slide_tuned_Llama3.2-1B.py:201:4
(EngineCore_DP0 pid=299207) 	ret;
(EngineCore_DP0 pid=299207) $L__tmp3:
(EngineCore_DP0 pid=299207) $L__func_end0:
(EngineCore_DP0 pid=299207)                                         // -- End function
(EngineCore_DP0 pid=299207) }
(EngineCore_DP0 pid=299207) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=299207) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=299207) 	.section	.debug_abbrev
(EngineCore_DP0 pid=299207) 	{
(EngineCore_DP0 pid=299207) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=299207) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=299207) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=299207) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=299207) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=299207) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=299207) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=299207) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=299207) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=299207) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=299207) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=299207) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=299207) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=299207) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=299207) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=299207) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=299207) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=299207) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=299207) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=299207) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=299207) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=299207) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=299207) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=299207) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=299207) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=299207) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=299207) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=299207) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=299207) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=299207) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=299207) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=299207) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=299207) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=299207) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=299207) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=299207) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=299207) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=299207) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=299207) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=299207) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=299207) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=299207) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=299207) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=299207) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=299207) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=299207) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=299207) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=299207) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=299207) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=299207) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=299207) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=299207) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=299207) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=299207) 	}
(EngineCore_DP0 pid=299207) 	.section	.debug_info
(EngineCore_DP0 pid=299207) 	{
(EngineCore_DP0 pid=299207) .b32 222                                // Length of Unit
(EngineCore_DP0 pid=299207) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=299207) .b8 0
(EngineCore_DP0 pid=299207) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=299207) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=299207) .b8 1                                   // Abbrev [1] 0xb:0xd7 DW_TAG_compile_unit
(EngineCore_DP0 pid=299207) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=299207) .b8 114
(EngineCore_DP0 pid=299207) .b8 105
(EngineCore_DP0 pid=299207) .b8 116
(EngineCore_DP0 pid=299207) .b8 111
(EngineCore_DP0 pid=299207) .b8 110
(EngineCore_DP0 pid=299207) .b8 0
(EngineCore_DP0 pid=299207) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=299207) .b8 0
(EngineCore_DP0 pid=299207) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=299207) .b8 117
(EngineCore_DP0 pid=299207) .b8 97
(EngineCore_DP0 pid=299207) .b8 110
(EngineCore_DP0 pid=299207) .b8 116
(EngineCore_DP0 pid=299207) .b8 95
(EngineCore_DP0 pid=299207) .b8 115
(EngineCore_DP0 pid=299207) .b8 108
(EngineCore_DP0 pid=299207) .b8 105
(EngineCore_DP0 pid=299207) .b8 100
(EngineCore_DP0 pid=299207) .b8 101
(EngineCore_DP0 pid=299207) .b8 95
(EngineCore_DP0 pid=299207) .b8 116
(EngineCore_DP0 pid=299207) .b8 117
(EngineCore_DP0 pid=299207) .b8 110
(EngineCore_DP0 pid=299207) .b8 101
(EngineCore_DP0 pid=299207) .b8 100
(EngineCore_DP0 pid=299207) .b8 95
(EngineCore_DP0 pid=299207) .b8 76
(EngineCore_DP0 pid=299207) .b8 108
(EngineCore_DP0 pid=299207) .b8 97
(EngineCore_DP0 pid=299207) .b8 109
(EngineCore_DP0 pid=299207) .b8 97
(EngineCore_DP0 pid=299207) .b8 51
(EngineCore_DP0 pid=299207) .b8 46
(EngineCore_DP0 pid=299207) .b8 50
(EngineCore_DP0 pid=299207) .b8 45
(EngineCore_DP0 pid=299207) .b8 49
(EngineCore_DP0 pid=299207) .b8 66
(EngineCore_DP0 pid=299207) .b8 46
(EngineCore_DP0 pid=299207) .b8 112
(EngineCore_DP0 pid=299207) .b8 121
(EngineCore_DP0 pid=299207) .b8 0
(EngineCore_DP0 pid=299207) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=299207) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=299207) .b8 114
(EngineCore_DP0 pid=299207) .b8 111
(EngineCore_DP0 pid=299207) .b8 111
(EngineCore_DP0 pid=299207) .b8 116
(EngineCore_DP0 pid=299207) .b8 47
(EngineCore_DP0 pid=299207) .b8 118
(EngineCore_DP0 pid=299207) .b8 108
(EngineCore_DP0 pid=299207) .b8 108
(EngineCore_DP0 pid=299207) .b8 109
(EngineCore_DP0 pid=299207) .b8 98
(EngineCore_DP0 pid=299207) .b8 101
(EngineCore_DP0 pid=299207) .b8 110
(EngineCore_DP0 pid=299207) .b8 99
(EngineCore_DP0 pid=299207) .b8 104
(EngineCore_DP0 pid=299207) .b8 47
(EngineCore_DP0 pid=299207) .b8 115
(EngineCore_DP0 pid=299207) .b8 108
(EngineCore_DP0 pid=299207) .b8 105
(EngineCore_DP0 pid=299207) .b8 100
(EngineCore_DP0 pid=299207) .b8 101
(EngineCore_DP0 pid=299207) .b8 115
(EngineCore_DP0 pid=299207) .b8 112
(EngineCore_DP0 pid=299207) .b8 97
(EngineCore_DP0 pid=299207) .b8 114
(EngineCore_DP0 pid=299207) .b8 115
(EngineCore_DP0 pid=299207) .b8 101
(EngineCore_DP0 pid=299207) .b8 47
(EngineCore_DP0 pid=299207) .b8 99
(EngineCore_DP0 pid=299207) .b8 115
(EngineCore_DP0 pid=299207) .b8 114
(EngineCore_DP0 pid=299207) .b8 99
(EngineCore_DP0 pid=299207) .b8 47
(EngineCore_DP0 pid=299207) .b8 102
(EngineCore_DP0 pid=299207) .b8 117
(EngineCore_DP0 pid=299207) .b8 115
(EngineCore_DP0 pid=299207) .b8 101
(EngineCore_DP0 pid=299207) .b8 100
(EngineCore_DP0 pid=299207) .b8 95
(EngineCore_DP0 pid=299207) .b8 113
(EngineCore_DP0 pid=299207) .b8 117
(EngineCore_DP0 pid=299207) .b8 97
(EngineCore_DP0 pid=299207) .b8 110
(EngineCore_DP0 pid=299207) .b8 116
(EngineCore_DP0 pid=299207) .b8 95
(EngineCore_DP0 pid=299207) .b8 115
(EngineCore_DP0 pid=299207) .b8 108
(EngineCore_DP0 pid=299207) .b8 105
(EngineCore_DP0 pid=299207) .b8 100
(EngineCore_DP0 pid=299207) .b8 101
(EngineCore_DP0 pid=299207) .b8 95
(EngineCore_DP0 pid=299207) .b8 116
(EngineCore_DP0 pid=299207) .b8 114
(EngineCore_DP0 pid=299207) .b8 105
(EngineCore_DP0 pid=299207) .b8 116
(EngineCore_DP0 pid=299207) .b8 111
(EngineCore_DP0 pid=299207) .b8 110
(EngineCore_DP0 pid=299207) .b8 47
(EngineCore_DP0 pid=299207) .b8 98
(EngineCore_DP0 pid=299207) .b8 117
(EngineCore_DP0 pid=299207) .b8 105
(EngineCore_DP0 pid=299207) .b8 108
(EngineCore_DP0 pid=299207) .b8 100
(EngineCore_DP0 pid=299207) .b8 47
(EngineCore_DP0 pid=299207) .b8 71
(EngineCore_DP0 pid=299207) .b8 66
(EngineCore_DP0 pid=299207) .b8 49
(EngineCore_DP0 pid=299207) .b8 48
(EngineCore_DP0 pid=299207) .b8 95
(EngineCore_DP0 pid=299207) .b8 99
(EngineCore_DP0 pid=299207) .b8 99
(EngineCore_DP0 pid=299207) .b8 49
(EngineCore_DP0 pid=299207) .b8 50
(EngineCore_DP0 pid=299207) .b8 49
(EngineCore_DP0 pid=299207) .b8 95
(EngineCore_DP0 pid=299207) .b8 112
(EngineCore_DP0 pid=299207) .b8 121
(EngineCore_DP0 pid=299207) .b8 51
(EngineCore_DP0 pid=299207) .b8 49
(EngineCore_DP0 pid=299207) .b8 50
(EngineCore_DP0 pid=299207) .b8 95
(EngineCore_DP0 pid=299207) .b8 99
(EngineCore_DP0 pid=299207) .b8 117
(EngineCore_DP0 pid=299207) .b8 49
(EngineCore_DP0 pid=299207) .b8 50
(EngineCore_DP0 pid=299207) .b8 57
(EngineCore_DP0 pid=299207) .b8 95
(EngineCore_DP0 pid=299207) .b8 97
(EngineCore_DP0 pid=299207) .b8 97
(EngineCore_DP0 pid=299207) .b8 114
(EngineCore_DP0 pid=299207) .b8 99
(EngineCore_DP0 pid=299207) .b8 104
(EngineCore_DP0 pid=299207) .b8 54
(EngineCore_DP0 pid=299207) .b8 52
(EngineCore_DP0 pid=299207) .b8 0
(EngineCore_DP0 pid=299207) .b8 2                                   // Abbrev [2] 0x99:0x1a DW_TAG_subprogram
(EngineCore_DP0 pid=299207) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=299207) .b8 113
(EngineCore_DP0 pid=299207) .b8 117
(EngineCore_DP0 pid=299207) .b8 97
(EngineCore_DP0 pid=299207) .b8 110
(EngineCore_DP0 pid=299207) .b8 116
(EngineCore_DP0 pid=299207) .b8 95
(EngineCore_DP0 pid=299207) .b8 115
(EngineCore_DP0 pid=299207) .b8 108
(EngineCore_DP0 pid=299207) .b8 105
(EngineCore_DP0 pid=299207) .b8 100
(EngineCore_DP0 pid=299207) .b8 101
(EngineCore_DP0 pid=299207) .b8 95
(EngineCore_DP0 pid=299207) .b8 102
(EngineCore_DP0 pid=299207) .b8 112
(EngineCore_DP0 pid=299207) .b8 56
(EngineCore_DP0 pid=299207) .b8 95
(EngineCore_DP0 pid=299207) .b8 107
(EngineCore_DP0 pid=299207) .b8 101
(EngineCore_DP0 pid=299207) .b8 114
(EngineCore_DP0 pid=299207) .b8 110
(EngineCore_DP0 pid=299207) .b8 101
(EngineCore_DP0 pid=299207) .b8 108
(EngineCore_DP0 pid=299207) .b8 0
(EngineCore_DP0 pid=299207) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=299207) .b8 3                                   // Abbrev [3] 0xb3:0x2e DW_TAG_subprogram
(EngineCore_DP0 pid=299207) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=299207) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=299207) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=299207) .b8 4                                   // Abbrev [4] 0xc8:0x18 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=299207) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=299207) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=299207) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=299207) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=299207) .b8 191                                 // DW_AT_call_line
(EngineCore_DP0 pid=299207) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=299207) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=299207) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=299207) 	}
(EngineCore_DP0 pid=299207) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) ================================================================
(EngineCore_DP0 pid=299207) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpvbklx7pt.ptx', '-o', '/tmp/tmpvbklx7pt.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866] 
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866] 
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866] 
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpvbklx7pt.ptx -o /tmp/tmpvbklx7pt.ptx.o
(EngineCore_DP0 pid=299207) ERROR 01-25 18:47:14 [core.py:866] 

STDERR:
[2026-01-25 18:47:00] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:47:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:47:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:47:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:47:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:47:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:47:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:47:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:47:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:47:03] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:47:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:47:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:47:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:47:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:47:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:47:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:47:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:47:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=299207) [2026-01-25 18:47:04] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=299207) [2026-01-25 18:47:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=299207) [2026-01-25 18:47:04] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=299207) [2026-01-25 18:47:04] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=299207) [2026-01-25 18:47:04] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=299207) [2026-01-25 18:47:04] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=299207) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=299207) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.46s/it]
(EngineCore_DP0 pid=299207) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.46s/it]
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) [2026-01-25 18:47:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=299207) [2026-01-25 18:47:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=299207) [2026-01-25 18:47:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=299207) [2026-01-25 18:47:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=299207) [2026-01-25 18:47:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=299207) [2026-01-25 18:47:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=299207) [2026-01-25 18:47:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=299207) [2026-01-25 18:47:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=299207) Process EngineCore_DP0:
(EngineCore_DP0 pid=299207) Traceback (most recent call last):
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=299207)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=299207)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=299207)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=299207) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpvbklx7pt.ptx', '-o', '/tmp/tmpvbklx7pt.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) Traceback (most recent call last):
(EngineCore_DP0 pid=299207)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=299207)     self.run()
(EngineCore_DP0 pid=299207)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=299207)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=299207)     raise e
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=299207)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=299207)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=299207)     super().__init__(
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=299207)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=299207)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=299207)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=299207)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=299207)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=299207)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=299207)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=299207)     return func(*args, **kwargs)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=299207)     return func(*args, **kwargs)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=299207)     self.model_runner.profile_run()
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=299207)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=299207)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=299207)     return func(*args, **kwargs)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=299207)     outputs = self.model(
(EngineCore_DP0 pid=299207)               ^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299207)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299207)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=299207)     model_output = self.model(
(EngineCore_DP0 pid=299207)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=299207)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=299207)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=299207)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299207)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299207)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=299207)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=299207)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299207)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299207)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=299207)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=299207)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299207)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299207)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=299207)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=299207)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=299207)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=299207)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=299207)     return self._linear_fn(
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=299207)     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=299207)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=299207)     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=299207)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=299207)     return fn(input, L)
(EngineCore_DP0 pid=299207)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=299207)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=299207)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=299207)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=299207)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=299207)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=299207)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=299207)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=299207)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=299207)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=299207)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=299207)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299207)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=299207)     raise PTXASError(error)
(EngineCore_DP0 pid=299207) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=299207) `ptxas` stderr:
(EngineCore_DP0 pid=299207) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=299207) 
(EngineCore_DP0 pid=299207) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpvbklx7pt.ptx -o /tmp/tmpvbklx7pt.ptx.o
(EngineCore_DP0 pid=299207) 
[rank0]:[W125 18:47:14.929118592 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=128 ==========
Time: 2026-01-25 18:47:16
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:47:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:47:19 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=299669) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) ================================================================
(EngineCore_DP0 pid=299669) Internal Triton PTX codegen error
(EngineCore_DP0 pid=299669) `ptxas` stderr:
(EngineCore_DP0 pid=299669) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp_53i18_l.ptx -o /tmp/tmp_53i18_l.ptx.o
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) //
(EngineCore_DP0 pid=299669) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=299669) //
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) .version 8.7
(EngineCore_DP0 pid=299669) .target sm_121a
(EngineCore_DP0 pid=299669) .address_size 64
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) 	// .globl	_quant_slide_fp8_kernel // -- Begin function _quant_slide_fp8_kernel
(EngineCore_DP0 pid=299669) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=299669)                                         // @_quant_slide_fp8_kernel
(EngineCore_DP0 pid=299669) .visible .entry _quant_slide_fp8_kernel(
(EngineCore_DP0 pid=299669) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_0,
(EngineCore_DP0 pid=299669) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_1,
(EngineCore_DP0 pid=299669) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_2,
(EngineCore_DP0 pid=299669) 	.param .u32 _quant_slide_fp8_kernel_param_3,
(EngineCore_DP0 pid=299669) 	.param .u32 _quant_slide_fp8_kernel_param_4,
(EngineCore_DP0 pid=299669) 	.param .u32 _quant_slide_fp8_kernel_param_5,
(EngineCore_DP0 pid=299669) 	.param .u32 _quant_slide_fp8_kernel_param_6,
(EngineCore_DP0 pid=299669) 	.param .u32 _quant_slide_fp8_kernel_param_7,
(EngineCore_DP0 pid=299669) 	.param .u32 _quant_slide_fp8_kernel_param_8,
(EngineCore_DP0 pid=299669) 	.param .u32 _quant_slide_fp8_kernel_param_9,
(EngineCore_DP0 pid=299669) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_10,
(EngineCore_DP0 pid=299669) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_11
(EngineCore_DP0 pid=299669) )
(EngineCore_DP0 pid=299669) .reqntid 1024
(EngineCore_DP0 pid=299669) {
(EngineCore_DP0 pid=299669) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=299669) 	.reg .b16 	%rs<25>;
(EngineCore_DP0 pid=299669) 	.reg .b32 	%r<108>;
(EngineCore_DP0 pid=299669) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=299669) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=299669) $L__func_begin0:
(EngineCore_DP0 pid=299669) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) // %bb.0:
(EngineCore_DP0 pid=299669) 	ld.param.b32 	%r20, [_quant_slide_fp8_kernel_param_7];
(EngineCore_DP0 pid=299669) 	ld.param.b32 	%r19, [_quant_slide_fp8_kernel_param_5];
(EngineCore_DP0 pid=299669) 	ld.param.b32 	%r18, [_quant_slide_fp8_kernel_param_4];
(EngineCore_DP0 pid=299669) 	ld.param.b64 	%rd3, [_quant_slide_fp8_kernel_param_2];
(EngineCore_DP0 pid=299669) 	ld.param.b64 	%rd4, [_quant_slide_fp8_kernel_param_0];
(EngineCore_DP0 pid=299669) $L__tmp0:
(EngineCore_DP0 pid=299669) 	.loc	1 177 24                        // quant_slide_tuned_Llama3.2-1B.py:177:24
(EngineCore_DP0 pid=299669) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=299669) 	ld.param.b32 	%r22, [_quant_slide_fp8_kernel_param_8];
(EngineCore_DP0 pid=299669) 	.loc	1 182 26                        // quant_slide_tuned_Llama3.2-1B.py:182:26
(EngineCore_DP0 pid=299669) 	mul.lo.s32 	%r23, %r22, %r1;
(EngineCore_DP0 pid=299669) 	.loc	1 182 20                        // quant_slide_tuned_Llama3.2-1B.py:182:20
(EngineCore_DP0 pid=299669) 	mad.wide.s32 	%rd1, %r23, 2, %rd4;
(EngineCore_DP0 pid=299669) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=299669) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=299669) 	shl.b32 	%r106, %r2, 2;
(EngineCore_DP0 pid=299669) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=299669) 	setp.lt.s32 	%p1, %r19, 1;
(EngineCore_DP0 pid=299669) 	mov.b32 	%r105, 0f2B8CBCCC;
(EngineCore_DP0 pid=299669) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=299669) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=299669) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=299669) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=299669) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=299669) 	shr.u32 	%r32, %r2, 3;
(EngineCore_DP0 pid=299669) 	and.b32 	%r33, %r32, 124;
(EngineCore_DP0 pid=299669) 	mov.b32 	%r34, global_smem;
(EngineCore_DP0 pid=299669) 	add.s32 	%r40, %r34, %r33;
(EngineCore_DP0 pid=299669) 	shl.b32 	%r35, %r2, 2;
(EngineCore_DP0 pid=299669) 	add.s32 	%r43, %r34, %r35;
(EngineCore_DP0 pid=299669) 	mov.b32 	%r38, 0;
(EngineCore_DP0 pid=299669) 	mov.b32 	%r103, 0f00000000;
(EngineCore_DP0 pid=299669) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=299669) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=299669) 	mov.b32 	%r104, %r38;
(EngineCore_DP0 pid=299669) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=299669) 	.loc	1 189 22                        // quant_slide_tuned_Llama3.2-1B.py:189:22
(EngineCore_DP0 pid=299669) 	add.s32 	%r46, %r106, %r104;
(EngineCore_DP0 pid=299669) 	setp.lt.s32 	%p2, %r46, %r18;
(EngineCore_DP0 pid=299669) 	.loc	1 190 29                        // quant_slide_tuned_Llama3.2-1B.py:190:29
(EngineCore_DP0 pid=299669) 	mad.wide.s32 	%rd6, %r46, 2, %rd1;
(EngineCore_DP0 pid=299669) 	.loc	1 190 21                        // quant_slide_tuned_Llama3.2-1B.py:190:21
(EngineCore_DP0 pid=299669) 	// begin inline asm
(EngineCore_DP0 pid=299669) 	mov.u32 %r36, %r38;
(EngineCore_DP0 pid=299669) 	mov.u32 %r37, %r38;
(EngineCore_DP0 pid=299669) 	@%p2 ld.global.v2.b32 { %r36, %r37 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=299669) 	// end inline asm
(EngineCore_DP0 pid=299669) 	mov.b32 	{%rs1, %rs2}, %r36;
(EngineCore_DP0 pid=299669) 	mov.b32 	{%rs3, %rs4}, %r37;
(EngineCore_DP0 pid=299669) 	.loc	1 191 50                        // quant_slide_tuned_Llama3.2-1B.py:191:50
(EngineCore_DP0 pid=299669) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=299669) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=299669) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=299669) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=299669) $L__tmp1:
(EngineCore_DP0 pid=299669) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	bar.sync 	0;
(EngineCore_DP0 pid=299669) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=299669) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=299669) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=299669) 	cvt.f32.bf16 	%r47, %rs11;
(EngineCore_DP0 pid=299669) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	shfl.sync.bfly.b32 	%r48, %r47, 16, 31, -1;
(EngineCore_DP0 pid=299669) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	max.f32 	%r49, %r47, %r48;
(EngineCore_DP0 pid=299669) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	shfl.sync.bfly.b32 	%r50, %r49, 8, 31, -1;
(EngineCore_DP0 pid=299669) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	max.f32 	%r51, %r49, %r50;
(EngineCore_DP0 pid=299669) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	shfl.sync.bfly.b32 	%r52, %r51, 4, 31, -1;
(EngineCore_DP0 pid=299669) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	max.f32 	%r53, %r51, %r52;
(EngineCore_DP0 pid=299669) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	shfl.sync.bfly.b32 	%r54, %r53, 2, 31, -1;
(EngineCore_DP0 pid=299669) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	max.f32 	%r55, %r53, %r54;
(EngineCore_DP0 pid=299669) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	shfl.sync.bfly.b32 	%r56, %r55, 1, 31, -1;
(EngineCore_DP0 pid=299669) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	max.f32 	%r41, %r55, %r56;
(EngineCore_DP0 pid=299669) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	// begin inline asm
(EngineCore_DP0 pid=299669) 	@%p3 st.shared.b32 [ %r40 + 0 ], %r41;
(EngineCore_DP0 pid=299669) 	// end inline asm
(EngineCore_DP0 pid=299669) 	bar.sync 	0;
(EngineCore_DP0 pid=299669) 	// begin inline asm
(EngineCore_DP0 pid=299669) 	@%p4 ld.shared.b32 %r42, [ %r43 + 0 ];
(EngineCore_DP0 pid=299669) 	// end inline asm
(EngineCore_DP0 pid=299669) 	shfl.sync.bfly.b32 	%r57, %r42, 16, 31, -1;
(EngineCore_DP0 pid=299669) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	max.f32 	%r58, %r42, %r57;
(EngineCore_DP0 pid=299669) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	shfl.sync.bfly.b32 	%r59, %r58, 8, 31, -1;
(EngineCore_DP0 pid=299669) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	max.f32 	%r60, %r58, %r59;
(EngineCore_DP0 pid=299669) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	shfl.sync.bfly.b32 	%r61, %r60, 4, 31, -1;
(EngineCore_DP0 pid=299669) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	max.f32 	%r62, %r60, %r61;
(EngineCore_DP0 pid=299669) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	shfl.sync.bfly.b32 	%r63, %r62, 2, 31, -1;
(EngineCore_DP0 pid=299669) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	max.f32 	%r64, %r62, %r63;
(EngineCore_DP0 pid=299669) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	shfl.sync.bfly.b32 	%r65, %r64, 1, 31, -1;
(EngineCore_DP0 pid=299669) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	max.f32 	%r45, %r64, %r65;
(EngineCore_DP0 pid=299669) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=299669) 	// begin inline asm
(EngineCore_DP0 pid=299669) 	@%p19 st.shared.b32 [ %r43 + 0 ], %r45;
(EngineCore_DP0 pid=299669) 	// end inline asm
(EngineCore_DP0 pid=299669) 	bar.sync 	0;
(EngineCore_DP0 pid=299669) 	ld.shared.b32 	%r66, [global_smem];
(EngineCore_DP0 pid=299669) $L__tmp2:
(EngineCore_DP0 pid=299669) 	.loc	1 191 36                        // quant_slide_tuned_Llama3.2-1B.py:191:36
(EngineCore_DP0 pid=299669) 	max.f32 	%r103, %r103, %r66;
(EngineCore_DP0 pid=299669) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=299669) 	add.s32 	%r104, %r104, 4096;
(EngineCore_DP0 pid=299669) 	setp.lt.s32 	%p6, %r104, %r19;
(EngineCore_DP0 pid=299669) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=299669) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=299669) 	.loc	1 193 32                        // quant_slide_tuned_Llama3.2-1B.py:193:32
(EngineCore_DP0 pid=299669) 	max.f32 	%r105, %r103, 0f2B8CBCCC;
(EngineCore_DP0 pid=299669) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=299669) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=299669) 	mov.b32 	%r68, 0f43E00000;
(EngineCore_DP0 pid=299669) 	.loc	1 194 32                        // quant_slide_tuned_Llama3.2-1B.py:194:32
(EngineCore_DP0 pid=299669) 	div.full.f32 	%r69, %r105, %r68;
(EngineCore_DP0 pid=299669) 	.loc	1 194 41                        // quant_slide_tuned_Llama3.2-1B.py:194:41
(EngineCore_DP0 pid=299669) 	max.f32 	%r67, %r69, 0f36924925;
(EngineCore_DP0 pid=299669) 	.loc	1 196 25                        // quant_slide_tuned_Llama3.2-1B.py:196:25
(EngineCore_DP0 pid=299669) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=299669) 	.loc	1 196 30                        // quant_slide_tuned_Llama3.2-1B.py:196:30
(EngineCore_DP0 pid=299669) 	// begin inline asm
(EngineCore_DP0 pid=299669) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r67 };
(EngineCore_DP0 pid=299669) 	// end inline asm
(EngineCore_DP0 pid=299669) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=299669) 	setp.lt.s32 	%p8, %r20, 1;
(EngineCore_DP0 pid=299669) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=299669) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=299669) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=299669) 	ld.param.b32 	%r24, [_quant_slide_fp8_kernel_param_9];
(EngineCore_DP0 pid=299669) 	shr.s32 	%r25, %r24, 31;
(EngineCore_DP0 pid=299669) 	shr.u32 	%r26, %r25, 30;
(EngineCore_DP0 pid=299669) 	add.s32 	%r27, %r24, %r26;
(EngineCore_DP0 pid=299669) 	shr.s32 	%r28, %r27, 2;
(EngineCore_DP0 pid=299669) 	ld.param.b64 	%rd5, [_quant_slide_fp8_kernel_param_1];
(EngineCore_DP0 pid=299669) 	mul.lo.s32 	%r29, %r28, %r1;
(EngineCore_DP0 pid=299669) 	mad.wide.s32 	%rd2, %r29, 4, %rd5;
(EngineCore_DP0 pid=299669) 	div.full.f32 	%r13, %r68, %r105;
(EngineCore_DP0 pid=299669) 	mov.b32 	%r107, 0;
(EngineCore_DP0 pid=299669) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=299669)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=299669) 	.loc	1 203 30                        // quant_slide_tuned_Llama3.2-1B.py:203:30
(EngineCore_DP0 pid=299669) 	add.s32 	%r80, %r2, %r107;
(EngineCore_DP0 pid=299669) 	setp.lt.s32 	%p13, %r80, %r20;
(EngineCore_DP0 pid=299669) 	.loc	1 211 53                        // quant_slide_tuned_Llama3.2-1B.py:211:53
(EngineCore_DP0 pid=299669) 	setp.lt.s32 	%p14, %r106, %r18;
(EngineCore_DP0 pid=299669) 	.loc	1 211 37                        // quant_slide_tuned_Llama3.2-1B.py:211:37
(EngineCore_DP0 pid=299669) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=299669) 	.loc	1 210 29                        // quant_slide_tuned_Llama3.2-1B.py:210:29
(EngineCore_DP0 pid=299669) 	mad.wide.s32 	%rd8, %r106, 2, %rd1;
(EngineCore_DP0 pid=299669) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=299669) 	.loc	1 210 21                        // quant_slide_tuned_Llama3.2-1B.py:210:21
(EngineCore_DP0 pid=299669) 	// begin inline asm
(EngineCore_DP0 pid=299669) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=299669) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=299669) 	// end inline asm
(EngineCore_DP0 pid=299669) 	.loc	1 211 79                        // quant_slide_tuned_Llama3.2-1B.py:211:79
(EngineCore_DP0 pid=299669) 	cvt.f32.bf16 	%r81, %rs12;
(EngineCore_DP0 pid=299669) 	.loc	1 213 53                        // quant_slide_tuned_Llama3.2-1B.py:213:53
(EngineCore_DP0 pid=299669) 	add.s32 	%r82, %r106, 1;
(EngineCore_DP0 pid=299669) 	setp.lt.s32 	%p15, %r82, %r18;
(EngineCore_DP0 pid=299669) 	.loc	1 213 37                        // quant_slide_tuned_Llama3.2-1B.py:213:37
(EngineCore_DP0 pid=299669) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=299669) 	.loc	1 212 39                        // quant_slide_tuned_Llama3.2-1B.py:212:39
(EngineCore_DP0 pid=299669) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=299669) 	.loc	1 212 21                        // quant_slide_tuned_Llama3.2-1B.py:212:21
(EngineCore_DP0 pid=299669) 	// begin inline asm
(EngineCore_DP0 pid=299669) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=299669) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=299669) 	// end inline asm
(EngineCore_DP0 pid=299669) 	.loc	1 213 79                        // quant_slide_tuned_Llama3.2-1B.py:213:79
(EngineCore_DP0 pid=299669) 	cvt.f32.bf16 	%r83, %rs14;
(EngineCore_DP0 pid=299669) 	.loc	1 215 53                        // quant_slide_tuned_Llama3.2-1B.py:215:53
(EngineCore_DP0 pid=299669) 	add.s32 	%r84, %r106, 2;
(EngineCore_DP0 pid=299669) 	setp.lt.s32 	%p16, %r84, %r18;
(EngineCore_DP0 pid=299669) 	.loc	1 215 37                        // quant_slide_tuned_Llama3.2-1B.py:215:37
(EngineCore_DP0 pid=299669) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=299669) 	.loc	1 214 39                        // quant_slide_tuned_Llama3.2-1B.py:214:39
(EngineCore_DP0 pid=299669) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=299669) 	.loc	1 214 21                        // quant_slide_tuned_Llama3.2-1B.py:214:21
(EngineCore_DP0 pid=299669) 	// begin inline asm
(EngineCore_DP0 pid=299669) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=299669) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=299669) 	// end inline asm
(EngineCore_DP0 pid=299669) 	.loc	1 215 79                        // quant_slide_tuned_Llama3.2-1B.py:215:79
(EngineCore_DP0 pid=299669) 	cvt.f32.bf16 	%r85, %rs16;
(EngineCore_DP0 pid=299669) 	.loc	1 217 53                        // quant_slide_tuned_Llama3.2-1B.py:217:53
(EngineCore_DP0 pid=299669) 	add.s32 	%r86, %r106, 3;
(EngineCore_DP0 pid=299669) 	setp.lt.s32 	%p17, %r86, %r18;
(EngineCore_DP0 pid=299669) 	.loc	1 217 37                        // quant_slide_tuned_Llama3.2-1B.py:217:37
(EngineCore_DP0 pid=299669) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=299669) 	.loc	1 216 39                        // quant_slide_tuned_Llama3.2-1B.py:216:39
(EngineCore_DP0 pid=299669) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=299669) 	.loc	1 216 21                        // quant_slide_tuned_Llama3.2-1B.py:216:21
(EngineCore_DP0 pid=299669) 	// begin inline asm
(EngineCore_DP0 pid=299669) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=299669) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=299669) 	// end inline asm
(EngineCore_DP0 pid=299669) 	.loc	1 217 79                        // quant_slide_tuned_Llama3.2-1B.py:217:79
(EngineCore_DP0 pid=299669) 	cvt.f32.bf16 	%r87, %rs18;
(EngineCore_DP0 pid=299669) 	.loc	1 219 27                        // quant_slide_tuned_Llama3.2-1B.py:219:27
(EngineCore_DP0 pid=299669) 	mul.f32 	%r88, %r13, %r81;
(EngineCore_DP0 pid=299669) 	mov.b32 	%r89, 0f43E00000;
(EngineCore_DP0 pid=299669) 	.loc	1 219 48                        // quant_slide_tuned_Llama3.2-1B.py:219:48
(EngineCore_DP0 pid=299669) 	min.xorsign.abs.f32 	%r71, %r88, %r89;
(EngineCore_DP0 pid=299669) 	.loc	1 219 60                        // quant_slide_tuned_Llama3.2-1B.py:219:60
(EngineCore_DP0 pid=299669) 	// begin inline asm
(EngineCore_DP0 pid=299669) 	cvt.rn.satfinite.e4m3x2.f32  %rs20, %r72, %r71; 
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) 	// end inline asm
(EngineCore_DP0 pid=299669) 	.loc	1 220 27                        // quant_slide_tuned_Llama3.2-1B.py:220:27
(EngineCore_DP0 pid=299669) 	mul.f32 	%r90, %r13, %r83;
(EngineCore_DP0 pid=299669) 	.loc	1 220 48                        // quant_slide_tuned_Llama3.2-1B.py:220:48
(EngineCore_DP0 pid=299669) 	min.xorsign.abs.f32 	%r73, %r90, %r89;
(EngineCore_DP0 pid=299669) 	.loc	1 220 60                        // quant_slide_tuned_Llama3.2-1B.py:220:60
(EngineCore_DP0 pid=299669) 	// begin inline asm
(EngineCore_DP0 pid=299669) 	cvt.rn.satfinite.e4m3x2.f32  %rs21, %r74, %r73; 
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) 	// end inline asm
(EngineCore_DP0 pid=299669) 	.loc	1 221 27                        // quant_slide_tuned_Llama3.2-1B.py:221:27
(EngineCore_DP0 pid=299669) 	mul.f32 	%r91, %r13, %r85;
(EngineCore_DP0 pid=299669) 	.loc	1 221 48                        // quant_slide_tuned_Llama3.2-1B.py:221:48
(EngineCore_DP0 pid=299669) 	min.xorsign.abs.f32 	%r75, %r91, %r89;
(EngineCore_DP0 pid=299669) 	.loc	1 221 60                        // quant_slide_tuned_Llama3.2-1B.py:221:60
(EngineCore_DP0 pid=299669) 	// begin inline asm
(EngineCore_DP0 pid=299669) 	cvt.rn.satfinite.e4m3x2.f32  %rs22, %r76, %r75; 
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) 	// end inline asm
(EngineCore_DP0 pid=299669) 	.loc	1 222 27                        // quant_slide_tuned_Llama3.2-1B.py:222:27
(EngineCore_DP0 pid=299669) 	mul.f32 	%r92, %r13, %r87;
(EngineCore_DP0 pid=299669) 	.loc	1 222 48                        // quant_slide_tuned_Llama3.2-1B.py:222:48
(EngineCore_DP0 pid=299669) 	min.xorsign.abs.f32 	%r77, %r92, %r89;
(EngineCore_DP0 pid=299669) 	.loc	1 222 60                        // quant_slide_tuned_Llama3.2-1B.py:222:60
(EngineCore_DP0 pid=299669) 	// begin inline asm
(EngineCore_DP0 pid=299669) 	cvt.rn.satfinite.e4m3x2.f32  %rs23, %r78, %r77; 
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) 	// end inline asm
(EngineCore_DP0 pid=299669) 	.loc	1 224 45                        // quant_slide_tuned_Llama3.2-1B.py:224:45
(EngineCore_DP0 pid=299669) 	cvt.u32.u16 	%r93, %rs20;
(EngineCore_DP0 pid=299669) 	and.b32 	%r94, %r93, 255;
(EngineCore_DP0 pid=299669) 	.loc	1 226 45                        // quant_slide_tuned_Llama3.2-1B.py:226:45
(EngineCore_DP0 pid=299669) 	cvt.u32.u16 	%r95, %rs22;
(EngineCore_DP0 pid=299669) 	and.b32 	%r96, %r95, 255;
(EngineCore_DP0 pid=299669) 	.loc	1 227 45                        // quant_slide_tuned_Llama3.2-1B.py:227:45
(EngineCore_DP0 pid=299669) 	cvt.u32.u16 	%r97, %rs23;
(EngineCore_DP0 pid=299669) 	.loc	1 229 30                        // quant_slide_tuned_Llama3.2-1B.py:229:30
(EngineCore_DP0 pid=299669) 	and.b16 	%rs24, %rs21, 255;
(EngineCore_DP0 pid=299669) 	mul.wide.u16 	%r98, %rs24, 256;
(EngineCore_DP0 pid=299669) 	.loc	1 229 24                        // quant_slide_tuned_Llama3.2-1B.py:229:24
(EngineCore_DP0 pid=299669) 	or.b32 	%r99, %r98, %r94;
(EngineCore_DP0 pid=299669) 	.loc	1 229 42                        // quant_slide_tuned_Llama3.2-1B.py:229:42
(EngineCore_DP0 pid=299669) 	shl.b32 	%r100, %r96, 16;
(EngineCore_DP0 pid=299669) 	.loc	1 229 36                        // quant_slide_tuned_Llama3.2-1B.py:229:36
(EngineCore_DP0 pid=299669) 	or.b32 	%r101, %r99, %r100;
(EngineCore_DP0 pid=299669) 	.loc	1 229 55                        // quant_slide_tuned_Llama3.2-1B.py:229:55
(EngineCore_DP0 pid=299669) 	shl.b32 	%r102, %r97, 24;
(EngineCore_DP0 pid=299669) 	.loc	1 229 49                        // quant_slide_tuned_Llama3.2-1B.py:229:49
(EngineCore_DP0 pid=299669) 	or.b32 	%r79, %r101, %r102;
(EngineCore_DP0 pid=299669) 	.loc	1 230 29                        // quant_slide_tuned_Llama3.2-1B.py:230:29
(EngineCore_DP0 pid=299669) 	mad.wide.s32 	%rd12, %r80, 4, %rd2;
(EngineCore_DP0 pid=299669) 	.loc	1 230 39                        // quant_slide_tuned_Llama3.2-1B.py:230:39
(EngineCore_DP0 pid=299669) 	// begin inline asm
(EngineCore_DP0 pid=299669) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r79 };
(EngineCore_DP0 pid=299669) 	// end inline asm
(EngineCore_DP0 pid=299669) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=299669) 	add.s32 	%r107, %r107, 1024;
(EngineCore_DP0 pid=299669) 	add.s32 	%r106, %r106, 4096;
(EngineCore_DP0 pid=299669) 	setp.lt.s32 	%p18, %r107, %r20;
(EngineCore_DP0 pid=299669) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=299669) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=299669) 	.loc	1 201 4                         // quant_slide_tuned_Llama3.2-1B.py:201:4
(EngineCore_DP0 pid=299669) 	ret;
(EngineCore_DP0 pid=299669) $L__tmp3:
(EngineCore_DP0 pid=299669) $L__func_end0:
(EngineCore_DP0 pid=299669)                                         // -- End function
(EngineCore_DP0 pid=299669) }
(EngineCore_DP0 pid=299669) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=299669) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=299669) 	.section	.debug_abbrev
(EngineCore_DP0 pid=299669) 	{
(EngineCore_DP0 pid=299669) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=299669) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=299669) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=299669) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=299669) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=299669) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=299669) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=299669) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=299669) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=299669) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=299669) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=299669) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=299669) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=299669) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=299669) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=299669) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=299669) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=299669) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=299669) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=299669) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=299669) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=299669) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=299669) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=299669) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=299669) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=299669) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=299669) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=299669) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=299669) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=299669) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=299669) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=299669) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=299669) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=299669) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=299669) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=299669) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=299669) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=299669) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=299669) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=299669) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=299669) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=299669) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=299669) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=299669) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=299669) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=299669) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=299669) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=299669) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=299669) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=299669) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=299669) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=299669) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=299669) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=299669) 	}
(EngineCore_DP0 pid=299669) 	.section	.debug_info
(EngineCore_DP0 pid=299669) 	{
(EngineCore_DP0 pid=299669) .b32 222                                // Length of Unit
(EngineCore_DP0 pid=299669) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=299669) .b8 0
(EngineCore_DP0 pid=299669) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=299669) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=299669) .b8 1                                   // Abbrev [1] 0xb:0xd7 DW_TAG_compile_unit
(EngineCore_DP0 pid=299669) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=299669) .b8 114
(EngineCore_DP0 pid=299669) .b8 105
(EngineCore_DP0 pid=299669) .b8 116
(EngineCore_DP0 pid=299669) .b8 111
(EngineCore_DP0 pid=299669) .b8 110
(EngineCore_DP0 pid=299669) .b8 0
(EngineCore_DP0 pid=299669) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=299669) .b8 0
(EngineCore_DP0 pid=299669) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=299669) .b8 117
(EngineCore_DP0 pid=299669) .b8 97
(EngineCore_DP0 pid=299669) .b8 110
(EngineCore_DP0 pid=299669) .b8 116
(EngineCore_DP0 pid=299669) .b8 95
(EngineCore_DP0 pid=299669) .b8 115
(EngineCore_DP0 pid=299669) .b8 108
(EngineCore_DP0 pid=299669) .b8 105
(EngineCore_DP0 pid=299669) .b8 100
(EngineCore_DP0 pid=299669) .b8 101
(EngineCore_DP0 pid=299669) .b8 95
(EngineCore_DP0 pid=299669) .b8 116
(EngineCore_DP0 pid=299669) .b8 117
(EngineCore_DP0 pid=299669) .b8 110
(EngineCore_DP0 pid=299669) .b8 101
(EngineCore_DP0 pid=299669) .b8 100
(EngineCore_DP0 pid=299669) .b8 95
(EngineCore_DP0 pid=299669) .b8 76
(EngineCore_DP0 pid=299669) .b8 108
(EngineCore_DP0 pid=299669) .b8 97
(EngineCore_DP0 pid=299669) .b8 109
(EngineCore_DP0 pid=299669) .b8 97
(EngineCore_DP0 pid=299669) .b8 51
(EngineCore_DP0 pid=299669) .b8 46
(EngineCore_DP0 pid=299669) .b8 50
(EngineCore_DP0 pid=299669) .b8 45
(EngineCore_DP0 pid=299669) .b8 49
(EngineCore_DP0 pid=299669) .b8 66
(EngineCore_DP0 pid=299669) .b8 46
(EngineCore_DP0 pid=299669) .b8 112
(EngineCore_DP0 pid=299669) .b8 121
(EngineCore_DP0 pid=299669) .b8 0
(EngineCore_DP0 pid=299669) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=299669) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=299669) .b8 114
(EngineCore_DP0 pid=299669) .b8 111
(EngineCore_DP0 pid=299669) .b8 111
(EngineCore_DP0 pid=299669) .b8 116
(EngineCore_DP0 pid=299669) .b8 47
(EngineCore_DP0 pid=299669) .b8 118
(EngineCore_DP0 pid=299669) .b8 108
(EngineCore_DP0 pid=299669) .b8 108
(EngineCore_DP0 pid=299669) .b8 109
(EngineCore_DP0 pid=299669) .b8 98
(EngineCore_DP0 pid=299669) .b8 101
(EngineCore_DP0 pid=299669) .b8 110
(EngineCore_DP0 pid=299669) .b8 99
(EngineCore_DP0 pid=299669) .b8 104
(EngineCore_DP0 pid=299669) .b8 47
(EngineCore_DP0 pid=299669) .b8 115
(EngineCore_DP0 pid=299669) .b8 108
(EngineCore_DP0 pid=299669) .b8 105
(EngineCore_DP0 pid=299669) .b8 100
(EngineCore_DP0 pid=299669) .b8 101
(EngineCore_DP0 pid=299669) .b8 115
(EngineCore_DP0 pid=299669) .b8 112
(EngineCore_DP0 pid=299669) .b8 97
(EngineCore_DP0 pid=299669) .b8 114
(EngineCore_DP0 pid=299669) .b8 115
(EngineCore_DP0 pid=299669) .b8 101
(EngineCore_DP0 pid=299669) .b8 47
(EngineCore_DP0 pid=299669) .b8 99
(EngineCore_DP0 pid=299669) .b8 115
(EngineCore_DP0 pid=299669) .b8 114
(EngineCore_DP0 pid=299669) .b8 99
(EngineCore_DP0 pid=299669) .b8 47
(EngineCore_DP0 pid=299669) .b8 102
(EngineCore_DP0 pid=299669) .b8 117
(EngineCore_DP0 pid=299669) .b8 115
(EngineCore_DP0 pid=299669) .b8 101
(EngineCore_DP0 pid=299669) .b8 100
(EngineCore_DP0 pid=299669) .b8 95
(EngineCore_DP0 pid=299669) .b8 113
(EngineCore_DP0 pid=299669) .b8 117
(EngineCore_DP0 pid=299669) .b8 97
(EngineCore_DP0 pid=299669) .b8 110
(EngineCore_DP0 pid=299669) .b8 116
(EngineCore_DP0 pid=299669) .b8 95
(EngineCore_DP0 pid=299669) .b8 115
(EngineCore_DP0 pid=299669) .b8 108
(EngineCore_DP0 pid=299669) .b8 105
(EngineCore_DP0 pid=299669) .b8 100
(EngineCore_DP0 pid=299669) .b8 101
(EngineCore_DP0 pid=299669) .b8 95
(EngineCore_DP0 pid=299669) .b8 116
(EngineCore_DP0 pid=299669) .b8 114
(EngineCore_DP0 pid=299669) .b8 105
(EngineCore_DP0 pid=299669) .b8 116
(EngineCore_DP0 pid=299669) .b8 111
(EngineCore_DP0 pid=299669) .b8 110
(EngineCore_DP0 pid=299669) .b8 47
(EngineCore_DP0 pid=299669) .b8 98
(EngineCore_DP0 pid=299669) .b8 117
(EngineCore_DP0 pid=299669) .b8 105
(EngineCore_DP0 pid=299669) .b8 108
(EngineCore_DP0 pid=299669) .b8 100
(EngineCore_DP0 pid=299669) .b8 47
(EngineCore_DP0 pid=299669) .b8 71
(EngineCore_DP0 pid=299669) .b8 66
(EngineCore_DP0 pid=299669) .b8 49
(EngineCore_DP0 pid=299669) .b8 48
(EngineCore_DP0 pid=299669) .b8 95
(EngineCore_DP0 pid=299669) .b8 99
(EngineCore_DP0 pid=299669) .b8 99
(EngineCore_DP0 pid=299669) .b8 49
(EngineCore_DP0 pid=299669) .b8 50
(EngineCore_DP0 pid=299669) .b8 49
(EngineCore_DP0 pid=299669) .b8 95
(EngineCore_DP0 pid=299669) .b8 112
(EngineCore_DP0 pid=299669) .b8 121
(EngineCore_DP0 pid=299669) .b8 51
(EngineCore_DP0 pid=299669) .b8 49
(EngineCore_DP0 pid=299669) .b8 50
(EngineCore_DP0 pid=299669) .b8 95
(EngineCore_DP0 pid=299669) .b8 99
(EngineCore_DP0 pid=299669) .b8 117
(EngineCore_DP0 pid=299669) .b8 49
(EngineCore_DP0 pid=299669) .b8 50
(EngineCore_DP0 pid=299669) .b8 57
(EngineCore_DP0 pid=299669) .b8 95
(EngineCore_DP0 pid=299669) .b8 97
(EngineCore_DP0 pid=299669) .b8 97
(EngineCore_DP0 pid=299669) .b8 114
(EngineCore_DP0 pid=299669) .b8 99
(EngineCore_DP0 pid=299669) .b8 104
(EngineCore_DP0 pid=299669) .b8 54
(EngineCore_DP0 pid=299669) .b8 52
(EngineCore_DP0 pid=299669) .b8 0
(EngineCore_DP0 pid=299669) .b8 2                                   // Abbrev [2] 0x99:0x1a DW_TAG_subprogram
(EngineCore_DP0 pid=299669) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=299669) .b8 113
(EngineCore_DP0 pid=299669) .b8 117
(EngineCore_DP0 pid=299669) .b8 97
(EngineCore_DP0 pid=299669) .b8 110
(EngineCore_DP0 pid=299669) .b8 116
(EngineCore_DP0 pid=299669) .b8 95
(EngineCore_DP0 pid=299669) .b8 115
(EngineCore_DP0 pid=299669) .b8 108
(EngineCore_DP0 pid=299669) .b8 105
(EngineCore_DP0 pid=299669) .b8 100
(EngineCore_DP0 pid=299669) .b8 101
(EngineCore_DP0 pid=299669) .b8 95
(EngineCore_DP0 pid=299669) .b8 102
(EngineCore_DP0 pid=299669) .b8 112
(EngineCore_DP0 pid=299669) .b8 56
(EngineCore_DP0 pid=299669) .b8 95
(EngineCore_DP0 pid=299669) .b8 107
(EngineCore_DP0 pid=299669) .b8 101
(EngineCore_DP0 pid=299669) .b8 114
(EngineCore_DP0 pid=299669) .b8 110
(EngineCore_DP0 pid=299669) .b8 101
(EngineCore_DP0 pid=299669) .b8 108
(EngineCore_DP0 pid=299669) .b8 0
(EngineCore_DP0 pid=299669) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=299669) .b8 3                                   // Abbrev [3] 0xb3:0x2e DW_TAG_subprogram
(EngineCore_DP0 pid=299669) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=299669) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=299669) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=299669) .b8 4                                   // Abbrev [4] 0xc8:0x18 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=299669) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=299669) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=299669) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=299669) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=299669) .b8 191                                 // DW_AT_call_line
(EngineCore_DP0 pid=299669) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=299669) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=299669) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=299669) 	}
(EngineCore_DP0 pid=299669) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) ================================================================
(EngineCore_DP0 pid=299669) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmp_53i18_l.ptx', '-o', '/tmp/tmp_53i18_l.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866] 
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866] 
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866] 
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp_53i18_l.ptx -o /tmp/tmp_53i18_l.ptx.o
(EngineCore_DP0 pid=299669) ERROR 01-25 18:47:33 [core.py:866] 

STDERR:
[2026-01-25 18:47:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:47:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:47:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:47:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:47:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:47:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:47:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:47:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:47:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:47:23] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:47:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:47:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:47:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:47:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:47:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:47:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:47:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:47:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=299669) [2026-01-25 18:47:24] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=299669) [2026-01-25 18:47:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=299669) [2026-01-25 18:47:24] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=299669) [2026-01-25 18:47:24] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=299669) [2026-01-25 18:47:24] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=299669) [2026-01-25 18:47:24] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=299669) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=299669) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.28s/it]
(EngineCore_DP0 pid=299669) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.28s/it]
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) [2026-01-25 18:47:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=299669) [2026-01-25 18:47:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=299669) [2026-01-25 18:47:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=299669) [2026-01-25 18:47:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=299669) [2026-01-25 18:47:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=299669) [2026-01-25 18:47:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=299669) [2026-01-25 18:47:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=299669) [2026-01-25 18:47:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=299669) Process EngineCore_DP0:
(EngineCore_DP0 pid=299669) Traceback (most recent call last):
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=299669)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=299669)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=299669)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=299669) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmp_53i18_l.ptx', '-o', '/tmp/tmp_53i18_l.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) Traceback (most recent call last):
(EngineCore_DP0 pid=299669)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=299669)     self.run()
(EngineCore_DP0 pid=299669)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=299669)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=299669)     raise e
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=299669)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=299669)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=299669)     super().__init__(
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=299669)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=299669)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=299669)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=299669)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=299669)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=299669)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=299669)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=299669)     return func(*args, **kwargs)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=299669)     return func(*args, **kwargs)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=299669)     self.model_runner.profile_run()
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=299669)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=299669)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=299669)     return func(*args, **kwargs)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=299669)     outputs = self.model(
(EngineCore_DP0 pid=299669)               ^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299669)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299669)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=299669)     model_output = self.model(
(EngineCore_DP0 pid=299669)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=299669)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=299669)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=299669)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299669)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299669)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=299669)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=299669)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299669)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299669)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=299669)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=299669)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=299669)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=299669)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=299669)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=299669)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=299669)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=299669)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=299669)     return self._linear_fn(
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=299669)     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=299669)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=299669)     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=299669)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=299669)     return fn(input, L)
(EngineCore_DP0 pid=299669)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=299669)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=299669)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=299669)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=299669)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=299669)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=299669)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=299669)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=299669)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=299669)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=299669)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=299669)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=299669)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=299669)     raise PTXASError(error)
(EngineCore_DP0 pid=299669) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=299669) `ptxas` stderr:
(EngineCore_DP0 pid=299669) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=299669) 
(EngineCore_DP0 pid=299669) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp_53i18_l.ptx -o /tmp/tmp_53i18_l.ptx.o
(EngineCore_DP0 pid=299669) 
[rank0]:[W125 18:47:34.303431557 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=128

========== M=256 ==========
Time: 2026-01-25 18:47:35
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:47:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:47:39 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=300132) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) ================================================================
(EngineCore_DP0 pid=300132) Internal Triton PTX codegen error
(EngineCore_DP0 pid=300132) `ptxas` stderr:
(EngineCore_DP0 pid=300132) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp75xlnwzf.ptx -o /tmp/tmp75xlnwzf.ptx.o
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) //
(EngineCore_DP0 pid=300132) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=300132) //
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) .version 8.7
(EngineCore_DP0 pid=300132) .target sm_121a
(EngineCore_DP0 pid=300132) .address_size 64
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) 	// .globl	_quant_slide_fp8_kernel // -- Begin function _quant_slide_fp8_kernel
(EngineCore_DP0 pid=300132) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=300132)                                         // @_quant_slide_fp8_kernel
(EngineCore_DP0 pid=300132) .visible .entry _quant_slide_fp8_kernel(
(EngineCore_DP0 pid=300132) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_0,
(EngineCore_DP0 pid=300132) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_1,
(EngineCore_DP0 pid=300132) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_2,
(EngineCore_DP0 pid=300132) 	.param .u32 _quant_slide_fp8_kernel_param_3,
(EngineCore_DP0 pid=300132) 	.param .u32 _quant_slide_fp8_kernel_param_4,
(EngineCore_DP0 pid=300132) 	.param .u32 _quant_slide_fp8_kernel_param_5,
(EngineCore_DP0 pid=300132) 	.param .u32 _quant_slide_fp8_kernel_param_6,
(EngineCore_DP0 pid=300132) 	.param .u32 _quant_slide_fp8_kernel_param_7,
(EngineCore_DP0 pid=300132) 	.param .u32 _quant_slide_fp8_kernel_param_8,
(EngineCore_DP0 pid=300132) 	.param .u32 _quant_slide_fp8_kernel_param_9,
(EngineCore_DP0 pid=300132) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_10,
(EngineCore_DP0 pid=300132) 	.param .u64 .ptr .global .align 1 _quant_slide_fp8_kernel_param_11
(EngineCore_DP0 pid=300132) )
(EngineCore_DP0 pid=300132) .reqntid 1024
(EngineCore_DP0 pid=300132) {
(EngineCore_DP0 pid=300132) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=300132) 	.reg .b16 	%rs<25>;
(EngineCore_DP0 pid=300132) 	.reg .b32 	%r<108>;
(EngineCore_DP0 pid=300132) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=300132) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=300132) $L__func_begin0:
(EngineCore_DP0 pid=300132) 	.loc	1 167 0                         // quant_slide_tuned_Llama3.2-1B.py:167:0
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) // %bb.0:
(EngineCore_DP0 pid=300132) 	ld.param.b32 	%r20, [_quant_slide_fp8_kernel_param_7];
(EngineCore_DP0 pid=300132) 	ld.param.b32 	%r19, [_quant_slide_fp8_kernel_param_5];
(EngineCore_DP0 pid=300132) 	ld.param.b32 	%r18, [_quant_slide_fp8_kernel_param_4];
(EngineCore_DP0 pid=300132) 	ld.param.b64 	%rd3, [_quant_slide_fp8_kernel_param_2];
(EngineCore_DP0 pid=300132) 	ld.param.b64 	%rd4, [_quant_slide_fp8_kernel_param_0];
(EngineCore_DP0 pid=300132) $L__tmp0:
(EngineCore_DP0 pid=300132) 	.loc	1 177 24                        // quant_slide_tuned_Llama3.2-1B.py:177:24
(EngineCore_DP0 pid=300132) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=300132) 	ld.param.b32 	%r22, [_quant_slide_fp8_kernel_param_8];
(EngineCore_DP0 pid=300132) 	.loc	1 182 26                        // quant_slide_tuned_Llama3.2-1B.py:182:26
(EngineCore_DP0 pid=300132) 	mul.lo.s32 	%r23, %r22, %r1;
(EngineCore_DP0 pid=300132) 	.loc	1 182 20                        // quant_slide_tuned_Llama3.2-1B.py:182:20
(EngineCore_DP0 pid=300132) 	mad.wide.s32 	%rd1, %r23, 2, %rd4;
(EngineCore_DP0 pid=300132) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=300132) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=300132) 	shl.b32 	%r106, %r2, 2;
(EngineCore_DP0 pid=300132) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=300132) 	setp.lt.s32 	%p1, %r19, 1;
(EngineCore_DP0 pid=300132) 	mov.b32 	%r105, 0f2B8CBCCC;
(EngineCore_DP0 pid=300132) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=300132) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=300132) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=300132) 	.loc	1 188 32                        // quant_slide_tuned_Llama3.2-1B.py:188:32
(EngineCore_DP0 pid=300132) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=300132) 	shr.u32 	%r32, %r2, 3;
(EngineCore_DP0 pid=300132) 	and.b32 	%r33, %r32, 124;
(EngineCore_DP0 pid=300132) 	mov.b32 	%r34, global_smem;
(EngineCore_DP0 pid=300132) 	add.s32 	%r40, %r34, %r33;
(EngineCore_DP0 pid=300132) 	shl.b32 	%r35, %r2, 2;
(EngineCore_DP0 pid=300132) 	add.s32 	%r43, %r34, %r35;
(EngineCore_DP0 pid=300132) 	mov.b32 	%r38, 0;
(EngineCore_DP0 pid=300132) 	mov.b32 	%r103, 0f00000000;
(EngineCore_DP0 pid=300132) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=300132) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=300132) 	mov.b32 	%r104, %r38;
(EngineCore_DP0 pid=300132) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=300132) 	.loc	1 189 22                        // quant_slide_tuned_Llama3.2-1B.py:189:22
(EngineCore_DP0 pid=300132) 	add.s32 	%r46, %r106, %r104;
(EngineCore_DP0 pid=300132) 	setp.lt.s32 	%p2, %r46, %r18;
(EngineCore_DP0 pid=300132) 	.loc	1 190 29                        // quant_slide_tuned_Llama3.2-1B.py:190:29
(EngineCore_DP0 pid=300132) 	mad.wide.s32 	%rd6, %r46, 2, %rd1;
(EngineCore_DP0 pid=300132) 	.loc	1 190 21                        // quant_slide_tuned_Llama3.2-1B.py:190:21
(EngineCore_DP0 pid=300132) 	// begin inline asm
(EngineCore_DP0 pid=300132) 	mov.u32 %r36, %r38;
(EngineCore_DP0 pid=300132) 	mov.u32 %r37, %r38;
(EngineCore_DP0 pid=300132) 	@%p2 ld.global.v2.b32 { %r36, %r37 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=300132) 	// end inline asm
(EngineCore_DP0 pid=300132) 	mov.b32 	{%rs1, %rs2}, %r36;
(EngineCore_DP0 pid=300132) 	mov.b32 	{%rs3, %rs4}, %r37;
(EngineCore_DP0 pid=300132) 	.loc	1 191 50                        // quant_slide_tuned_Llama3.2-1B.py:191:50
(EngineCore_DP0 pid=300132) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=300132) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=300132) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=300132) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=300132) $L__tmp1:
(EngineCore_DP0 pid=300132) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	bar.sync 	0;
(EngineCore_DP0 pid=300132) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=300132) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=300132) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=300132) 	cvt.f32.bf16 	%r47, %rs11;
(EngineCore_DP0 pid=300132) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	shfl.sync.bfly.b32 	%r48, %r47, 16, 31, -1;
(EngineCore_DP0 pid=300132) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	max.f32 	%r49, %r47, %r48;
(EngineCore_DP0 pid=300132) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	shfl.sync.bfly.b32 	%r50, %r49, 8, 31, -1;
(EngineCore_DP0 pid=300132) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	max.f32 	%r51, %r49, %r50;
(EngineCore_DP0 pid=300132) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	shfl.sync.bfly.b32 	%r52, %r51, 4, 31, -1;
(EngineCore_DP0 pid=300132) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	max.f32 	%r53, %r51, %r52;
(EngineCore_DP0 pid=300132) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	shfl.sync.bfly.b32 	%r54, %r53, 2, 31, -1;
(EngineCore_DP0 pid=300132) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	max.f32 	%r55, %r53, %r54;
(EngineCore_DP0 pid=300132) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	shfl.sync.bfly.b32 	%r56, %r55, 1, 31, -1;
(EngineCore_DP0 pid=300132) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	max.f32 	%r41, %r55, %r56;
(EngineCore_DP0 pid=300132) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	// begin inline asm
(EngineCore_DP0 pid=300132) 	@%p3 st.shared.b32 [ %r40 + 0 ], %r41;
(EngineCore_DP0 pid=300132) 	// end inline asm
(EngineCore_DP0 pid=300132) 	bar.sync 	0;
(EngineCore_DP0 pid=300132) 	// begin inline asm
(EngineCore_DP0 pid=300132) 	@%p4 ld.shared.b32 %r42, [ %r43 + 0 ];
(EngineCore_DP0 pid=300132) 	// end inline asm
(EngineCore_DP0 pid=300132) 	shfl.sync.bfly.b32 	%r57, %r42, 16, 31, -1;
(EngineCore_DP0 pid=300132) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	max.f32 	%r58, %r42, %r57;
(EngineCore_DP0 pid=300132) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	shfl.sync.bfly.b32 	%r59, %r58, 8, 31, -1;
(EngineCore_DP0 pid=300132) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	max.f32 	%r60, %r58, %r59;
(EngineCore_DP0 pid=300132) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	shfl.sync.bfly.b32 	%r61, %r60, 4, 31, -1;
(EngineCore_DP0 pid=300132) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	max.f32 	%r62, %r60, %r61;
(EngineCore_DP0 pid=300132) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	shfl.sync.bfly.b32 	%r63, %r62, 2, 31, -1;
(EngineCore_DP0 pid=300132) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	max.f32 	%r64, %r62, %r63;
(EngineCore_DP0 pid=300132) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	shfl.sync.bfly.b32 	%r65, %r64, 1, 31, -1;
(EngineCore_DP0 pid=300132) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	max.f32 	%r45, %r64, %r65;
(EngineCore_DP0 pid=300132) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:191:43 ]
(EngineCore_DP0 pid=300132) 	// begin inline asm
(EngineCore_DP0 pid=300132) 	@%p19 st.shared.b32 [ %r43 + 0 ], %r45;
(EngineCore_DP0 pid=300132) 	// end inline asm
(EngineCore_DP0 pid=300132) 	bar.sync 	0;
(EngineCore_DP0 pid=300132) 	ld.shared.b32 	%r66, [global_smem];
(EngineCore_DP0 pid=300132) $L__tmp2:
(EngineCore_DP0 pid=300132) 	.loc	1 191 36                        // quant_slide_tuned_Llama3.2-1B.py:191:36
(EngineCore_DP0 pid=300132) 	max.f32 	%r103, %r103, %r66;
(EngineCore_DP0 pid=300132) 	.loc	1 187 35                        // quant_slide_tuned_Llama3.2-1B.py:187:35
(EngineCore_DP0 pid=300132) 	add.s32 	%r104, %r104, 4096;
(EngineCore_DP0 pid=300132) 	setp.lt.s32 	%p6, %r104, %r19;
(EngineCore_DP0 pid=300132) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=300132) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=300132) 	.loc	1 193 32                        // quant_slide_tuned_Llama3.2-1B.py:193:32
(EngineCore_DP0 pid=300132) 	max.f32 	%r105, %r103, 0f2B8CBCCC;
(EngineCore_DP0 pid=300132) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=300132) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=300132) 	mov.b32 	%r68, 0f43E00000;
(EngineCore_DP0 pid=300132) 	.loc	1 194 32                        // quant_slide_tuned_Llama3.2-1B.py:194:32
(EngineCore_DP0 pid=300132) 	div.full.f32 	%r69, %r105, %r68;
(EngineCore_DP0 pid=300132) 	.loc	1 194 41                        // quant_slide_tuned_Llama3.2-1B.py:194:41
(EngineCore_DP0 pid=300132) 	max.f32 	%r67, %r69, 0f36924925;
(EngineCore_DP0 pid=300132) 	.loc	1 196 25                        // quant_slide_tuned_Llama3.2-1B.py:196:25
(EngineCore_DP0 pid=300132) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=300132) 	.loc	1 196 30                        // quant_slide_tuned_Llama3.2-1B.py:196:30
(EngineCore_DP0 pid=300132) 	// begin inline asm
(EngineCore_DP0 pid=300132) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r67 };
(EngineCore_DP0 pid=300132) 	// end inline asm
(EngineCore_DP0 pid=300132) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=300132) 	setp.lt.s32 	%p8, %r20, 1;
(EngineCore_DP0 pid=300132) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=300132) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=300132) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=300132) 	ld.param.b32 	%r24, [_quant_slide_fp8_kernel_param_9];
(EngineCore_DP0 pid=300132) 	shr.s32 	%r25, %r24, 31;
(EngineCore_DP0 pid=300132) 	shr.u32 	%r26, %r25, 30;
(EngineCore_DP0 pid=300132) 	add.s32 	%r27, %r24, %r26;
(EngineCore_DP0 pid=300132) 	shr.s32 	%r28, %r27, 2;
(EngineCore_DP0 pid=300132) 	ld.param.b64 	%rd5, [_quant_slide_fp8_kernel_param_1];
(EngineCore_DP0 pid=300132) 	mul.lo.s32 	%r29, %r28, %r1;
(EngineCore_DP0 pid=300132) 	mad.wide.s32 	%rd2, %r29, 4, %rd5;
(EngineCore_DP0 pid=300132) 	div.full.f32 	%r13, %r68, %r105;
(EngineCore_DP0 pid=300132) 	mov.b32 	%r107, 0;
(EngineCore_DP0 pid=300132) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=300132)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=300132) 	.loc	1 203 30                        // quant_slide_tuned_Llama3.2-1B.py:203:30
(EngineCore_DP0 pid=300132) 	add.s32 	%r80, %r2, %r107;
(EngineCore_DP0 pid=300132) 	setp.lt.s32 	%p13, %r80, %r20;
(EngineCore_DP0 pid=300132) 	.loc	1 211 53                        // quant_slide_tuned_Llama3.2-1B.py:211:53
(EngineCore_DP0 pid=300132) 	setp.lt.s32 	%p14, %r106, %r18;
(EngineCore_DP0 pid=300132) 	.loc	1 211 37                        // quant_slide_tuned_Llama3.2-1B.py:211:37
(EngineCore_DP0 pid=300132) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=300132) 	.loc	1 210 29                        // quant_slide_tuned_Llama3.2-1B.py:210:29
(EngineCore_DP0 pid=300132) 	mad.wide.s32 	%rd8, %r106, 2, %rd1;
(EngineCore_DP0 pid=300132) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=300132) 	.loc	1 210 21                        // quant_slide_tuned_Llama3.2-1B.py:210:21
(EngineCore_DP0 pid=300132) 	// begin inline asm
(EngineCore_DP0 pid=300132) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=300132) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=300132) 	// end inline asm
(EngineCore_DP0 pid=300132) 	.loc	1 211 79                        // quant_slide_tuned_Llama3.2-1B.py:211:79
(EngineCore_DP0 pid=300132) 	cvt.f32.bf16 	%r81, %rs12;
(EngineCore_DP0 pid=300132) 	.loc	1 213 53                        // quant_slide_tuned_Llama3.2-1B.py:213:53
(EngineCore_DP0 pid=300132) 	add.s32 	%r82, %r106, 1;
(EngineCore_DP0 pid=300132) 	setp.lt.s32 	%p15, %r82, %r18;
(EngineCore_DP0 pid=300132) 	.loc	1 213 37                        // quant_slide_tuned_Llama3.2-1B.py:213:37
(EngineCore_DP0 pid=300132) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=300132) 	.loc	1 212 39                        // quant_slide_tuned_Llama3.2-1B.py:212:39
(EngineCore_DP0 pid=300132) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=300132) 	.loc	1 212 21                        // quant_slide_tuned_Llama3.2-1B.py:212:21
(EngineCore_DP0 pid=300132) 	// begin inline asm
(EngineCore_DP0 pid=300132) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=300132) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=300132) 	// end inline asm
(EngineCore_DP0 pid=300132) 	.loc	1 213 79                        // quant_slide_tuned_Llama3.2-1B.py:213:79
(EngineCore_DP0 pid=300132) 	cvt.f32.bf16 	%r83, %rs14;
(EngineCore_DP0 pid=300132) 	.loc	1 215 53                        // quant_slide_tuned_Llama3.2-1B.py:215:53
(EngineCore_DP0 pid=300132) 	add.s32 	%r84, %r106, 2;
(EngineCore_DP0 pid=300132) 	setp.lt.s32 	%p16, %r84, %r18;
(EngineCore_DP0 pid=300132) 	.loc	1 215 37                        // quant_slide_tuned_Llama3.2-1B.py:215:37
(EngineCore_DP0 pid=300132) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=300132) 	.loc	1 214 39                        // quant_slide_tuned_Llama3.2-1B.py:214:39
(EngineCore_DP0 pid=300132) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=300132) 	.loc	1 214 21                        // quant_slide_tuned_Llama3.2-1B.py:214:21
(EngineCore_DP0 pid=300132) 	// begin inline asm
(EngineCore_DP0 pid=300132) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=300132) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=300132) 	// end inline asm
(EngineCore_DP0 pid=300132) 	.loc	1 215 79                        // quant_slide_tuned_Llama3.2-1B.py:215:79
(EngineCore_DP0 pid=300132) 	cvt.f32.bf16 	%r85, %rs16;
(EngineCore_DP0 pid=300132) 	.loc	1 217 53                        // quant_slide_tuned_Llama3.2-1B.py:217:53
(EngineCore_DP0 pid=300132) 	add.s32 	%r86, %r106, 3;
(EngineCore_DP0 pid=300132) 	setp.lt.s32 	%p17, %r86, %r18;
(EngineCore_DP0 pid=300132) 	.loc	1 217 37                        // quant_slide_tuned_Llama3.2-1B.py:217:37
(EngineCore_DP0 pid=300132) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=300132) 	.loc	1 216 39                        // quant_slide_tuned_Llama3.2-1B.py:216:39
(EngineCore_DP0 pid=300132) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=300132) 	.loc	1 216 21                        // quant_slide_tuned_Llama3.2-1B.py:216:21
(EngineCore_DP0 pid=300132) 	// begin inline asm
(EngineCore_DP0 pid=300132) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=300132) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=300132) 	// end inline asm
(EngineCore_DP0 pid=300132) 	.loc	1 217 79                        // quant_slide_tuned_Llama3.2-1B.py:217:79
(EngineCore_DP0 pid=300132) 	cvt.f32.bf16 	%r87, %rs18;
(EngineCore_DP0 pid=300132) 	.loc	1 219 27                        // quant_slide_tuned_Llama3.2-1B.py:219:27
(EngineCore_DP0 pid=300132) 	mul.f32 	%r88, %r13, %r81;
(EngineCore_DP0 pid=300132) 	mov.b32 	%r89, 0f43E00000;
(EngineCore_DP0 pid=300132) 	.loc	1 219 48                        // quant_slide_tuned_Llama3.2-1B.py:219:48
(EngineCore_DP0 pid=300132) 	min.xorsign.abs.f32 	%r71, %r88, %r89;
(EngineCore_DP0 pid=300132) 	.loc	1 219 60                        // quant_slide_tuned_Llama3.2-1B.py:219:60
(EngineCore_DP0 pid=300132) 	// begin inline asm
(EngineCore_DP0 pid=300132) 	cvt.rn.satfinite.e4m3x2.f32  %rs20, %r72, %r71; 
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) 	// end inline asm
(EngineCore_DP0 pid=300132) 	.loc	1 220 27                        // quant_slide_tuned_Llama3.2-1B.py:220:27
(EngineCore_DP0 pid=300132) 	mul.f32 	%r90, %r13, %r83;
(EngineCore_DP0 pid=300132) 	.loc	1 220 48                        // quant_slide_tuned_Llama3.2-1B.py:220:48
(EngineCore_DP0 pid=300132) 	min.xorsign.abs.f32 	%r73, %r90, %r89;
(EngineCore_DP0 pid=300132) 	.loc	1 220 60                        // quant_slide_tuned_Llama3.2-1B.py:220:60
(EngineCore_DP0 pid=300132) 	// begin inline asm
(EngineCore_DP0 pid=300132) 	cvt.rn.satfinite.e4m3x2.f32  %rs21, %r74, %r73; 
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) 	// end inline asm
(EngineCore_DP0 pid=300132) 	.loc	1 221 27                        // quant_slide_tuned_Llama3.2-1B.py:221:27
(EngineCore_DP0 pid=300132) 	mul.f32 	%r91, %r13, %r85;
(EngineCore_DP0 pid=300132) 	.loc	1 221 48                        // quant_slide_tuned_Llama3.2-1B.py:221:48
(EngineCore_DP0 pid=300132) 	min.xorsign.abs.f32 	%r75, %r91, %r89;
(EngineCore_DP0 pid=300132) 	.loc	1 221 60                        // quant_slide_tuned_Llama3.2-1B.py:221:60
(EngineCore_DP0 pid=300132) 	// begin inline asm
(EngineCore_DP0 pid=300132) 	cvt.rn.satfinite.e4m3x2.f32  %rs22, %r76, %r75; 
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) 	// end inline asm
(EngineCore_DP0 pid=300132) 	.loc	1 222 27                        // quant_slide_tuned_Llama3.2-1B.py:222:27
(EngineCore_DP0 pid=300132) 	mul.f32 	%r92, %r13, %r87;
(EngineCore_DP0 pid=300132) 	.loc	1 222 48                        // quant_slide_tuned_Llama3.2-1B.py:222:48
(EngineCore_DP0 pid=300132) 	min.xorsign.abs.f32 	%r77, %r92, %r89;
(EngineCore_DP0 pid=300132) 	.loc	1 222 60                        // quant_slide_tuned_Llama3.2-1B.py:222:60
(EngineCore_DP0 pid=300132) 	// begin inline asm
(EngineCore_DP0 pid=300132) 	cvt.rn.satfinite.e4m3x2.f32  %rs23, %r78, %r77; 
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) 	// end inline asm
(EngineCore_DP0 pid=300132) 	.loc	1 224 45                        // quant_slide_tuned_Llama3.2-1B.py:224:45
(EngineCore_DP0 pid=300132) 	cvt.u32.u16 	%r93, %rs20;
(EngineCore_DP0 pid=300132) 	and.b32 	%r94, %r93, 255;
(EngineCore_DP0 pid=300132) 	.loc	1 226 45                        // quant_slide_tuned_Llama3.2-1B.py:226:45
(EngineCore_DP0 pid=300132) 	cvt.u32.u16 	%r95, %rs22;
(EngineCore_DP0 pid=300132) 	and.b32 	%r96, %r95, 255;
(EngineCore_DP0 pid=300132) 	.loc	1 227 45                        // quant_slide_tuned_Llama3.2-1B.py:227:45
(EngineCore_DP0 pid=300132) 	cvt.u32.u16 	%r97, %rs23;
(EngineCore_DP0 pid=300132) 	.loc	1 229 30                        // quant_slide_tuned_Llama3.2-1B.py:229:30
(EngineCore_DP0 pid=300132) 	and.b16 	%rs24, %rs21, 255;
(EngineCore_DP0 pid=300132) 	mul.wide.u16 	%r98, %rs24, 256;
(EngineCore_DP0 pid=300132) 	.loc	1 229 24                        // quant_slide_tuned_Llama3.2-1B.py:229:24
(EngineCore_DP0 pid=300132) 	or.b32 	%r99, %r98, %r94;
(EngineCore_DP0 pid=300132) 	.loc	1 229 42                        // quant_slide_tuned_Llama3.2-1B.py:229:42
(EngineCore_DP0 pid=300132) 	shl.b32 	%r100, %r96, 16;
(EngineCore_DP0 pid=300132) 	.loc	1 229 36                        // quant_slide_tuned_Llama3.2-1B.py:229:36
(EngineCore_DP0 pid=300132) 	or.b32 	%r101, %r99, %r100;
(EngineCore_DP0 pid=300132) 	.loc	1 229 55                        // quant_slide_tuned_Llama3.2-1B.py:229:55
(EngineCore_DP0 pid=300132) 	shl.b32 	%r102, %r97, 24;
(EngineCore_DP0 pid=300132) 	.loc	1 229 49                        // quant_slide_tuned_Llama3.2-1B.py:229:49
(EngineCore_DP0 pid=300132) 	or.b32 	%r79, %r101, %r102;
(EngineCore_DP0 pid=300132) 	.loc	1 230 29                        // quant_slide_tuned_Llama3.2-1B.py:230:29
(EngineCore_DP0 pid=300132) 	mad.wide.s32 	%rd12, %r80, 4, %rd2;
(EngineCore_DP0 pid=300132) 	.loc	1 230 39                        // quant_slide_tuned_Llama3.2-1B.py:230:39
(EngineCore_DP0 pid=300132) 	// begin inline asm
(EngineCore_DP0 pid=300132) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r79 };
(EngineCore_DP0 pid=300132) 	// end inline asm
(EngineCore_DP0 pid=300132) 	.loc	1 201 41                        // quant_slide_tuned_Llama3.2-1B.py:201:41
(EngineCore_DP0 pid=300132) 	add.s32 	%r107, %r107, 1024;
(EngineCore_DP0 pid=300132) 	add.s32 	%r106, %r106, 4096;
(EngineCore_DP0 pid=300132) 	setp.lt.s32 	%p18, %r107, %r20;
(EngineCore_DP0 pid=300132) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=300132) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=300132) 	.loc	1 201 4                         // quant_slide_tuned_Llama3.2-1B.py:201:4
(EngineCore_DP0 pid=300132) 	ret;
(EngineCore_DP0 pid=300132) $L__tmp3:
(EngineCore_DP0 pid=300132) $L__func_end0:
(EngineCore_DP0 pid=300132)                                         // -- End function
(EngineCore_DP0 pid=300132) }
(EngineCore_DP0 pid=300132) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=300132) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=300132) 	.section	.debug_abbrev
(EngineCore_DP0 pid=300132) 	{
(EngineCore_DP0 pid=300132) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=300132) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=300132) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=300132) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=300132) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=300132) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=300132) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=300132) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=300132) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=300132) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=300132) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=300132) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=300132) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=300132) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=300132) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=300132) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=300132) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=300132) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=300132) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=300132) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=300132) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=300132) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=300132) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=300132) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=300132) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=300132) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=300132) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=300132) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=300132) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=300132) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=300132) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=300132) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=300132) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=300132) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=300132) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=300132) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=300132) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=300132) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=300132) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=300132) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=300132) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=300132) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=300132) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=300132) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=300132) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=300132) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=300132) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=300132) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=300132) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=300132) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=300132) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=300132) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=300132) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=300132) 	}
(EngineCore_DP0 pid=300132) 	.section	.debug_info
(EngineCore_DP0 pid=300132) 	{
(EngineCore_DP0 pid=300132) .b32 222                                // Length of Unit
(EngineCore_DP0 pid=300132) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=300132) .b8 0
(EngineCore_DP0 pid=300132) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=300132) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=300132) .b8 1                                   // Abbrev [1] 0xb:0xd7 DW_TAG_compile_unit
(EngineCore_DP0 pid=300132) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=300132) .b8 114
(EngineCore_DP0 pid=300132) .b8 105
(EngineCore_DP0 pid=300132) .b8 116
(EngineCore_DP0 pid=300132) .b8 111
(EngineCore_DP0 pid=300132) .b8 110
(EngineCore_DP0 pid=300132) .b8 0
(EngineCore_DP0 pid=300132) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=300132) .b8 0
(EngineCore_DP0 pid=300132) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=300132) .b8 117
(EngineCore_DP0 pid=300132) .b8 97
(EngineCore_DP0 pid=300132) .b8 110
(EngineCore_DP0 pid=300132) .b8 116
(EngineCore_DP0 pid=300132) .b8 95
(EngineCore_DP0 pid=300132) .b8 115
(EngineCore_DP0 pid=300132) .b8 108
(EngineCore_DP0 pid=300132) .b8 105
(EngineCore_DP0 pid=300132) .b8 100
(EngineCore_DP0 pid=300132) .b8 101
(EngineCore_DP0 pid=300132) .b8 95
(EngineCore_DP0 pid=300132) .b8 116
(EngineCore_DP0 pid=300132) .b8 117
(EngineCore_DP0 pid=300132) .b8 110
(EngineCore_DP0 pid=300132) .b8 101
(EngineCore_DP0 pid=300132) .b8 100
(EngineCore_DP0 pid=300132) .b8 95
(EngineCore_DP0 pid=300132) .b8 76
(EngineCore_DP0 pid=300132) .b8 108
(EngineCore_DP0 pid=300132) .b8 97
(EngineCore_DP0 pid=300132) .b8 109
(EngineCore_DP0 pid=300132) .b8 97
(EngineCore_DP0 pid=300132) .b8 51
(EngineCore_DP0 pid=300132) .b8 46
(EngineCore_DP0 pid=300132) .b8 50
(EngineCore_DP0 pid=300132) .b8 45
(EngineCore_DP0 pid=300132) .b8 49
(EngineCore_DP0 pid=300132) .b8 66
(EngineCore_DP0 pid=300132) .b8 46
(EngineCore_DP0 pid=300132) .b8 112
(EngineCore_DP0 pid=300132) .b8 121
(EngineCore_DP0 pid=300132) .b8 0
(EngineCore_DP0 pid=300132) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=300132) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=300132) .b8 114
(EngineCore_DP0 pid=300132) .b8 111
(EngineCore_DP0 pid=300132) .b8 111
(EngineCore_DP0 pid=300132) .b8 116
(EngineCore_DP0 pid=300132) .b8 47
(EngineCore_DP0 pid=300132) .b8 118
(EngineCore_DP0 pid=300132) .b8 108
(EngineCore_DP0 pid=300132) .b8 108
(EngineCore_DP0 pid=300132) .b8 109
(EngineCore_DP0 pid=300132) .b8 98
(EngineCore_DP0 pid=300132) .b8 101
(EngineCore_DP0 pid=300132) .b8 110
(EngineCore_DP0 pid=300132) .b8 99
(EngineCore_DP0 pid=300132) .b8 104
(EngineCore_DP0 pid=300132) .b8 47
(EngineCore_DP0 pid=300132) .b8 115
(EngineCore_DP0 pid=300132) .b8 108
(EngineCore_DP0 pid=300132) .b8 105
(EngineCore_DP0 pid=300132) .b8 100
(EngineCore_DP0 pid=300132) .b8 101
(EngineCore_DP0 pid=300132) .b8 115
(EngineCore_DP0 pid=300132) .b8 112
(EngineCore_DP0 pid=300132) .b8 97
(EngineCore_DP0 pid=300132) .b8 114
(EngineCore_DP0 pid=300132) .b8 115
(EngineCore_DP0 pid=300132) .b8 101
(EngineCore_DP0 pid=300132) .b8 47
(EngineCore_DP0 pid=300132) .b8 99
(EngineCore_DP0 pid=300132) .b8 115
(EngineCore_DP0 pid=300132) .b8 114
(EngineCore_DP0 pid=300132) .b8 99
(EngineCore_DP0 pid=300132) .b8 47
(EngineCore_DP0 pid=300132) .b8 102
(EngineCore_DP0 pid=300132) .b8 117
(EngineCore_DP0 pid=300132) .b8 115
(EngineCore_DP0 pid=300132) .b8 101
(EngineCore_DP0 pid=300132) .b8 100
(EngineCore_DP0 pid=300132) .b8 95
(EngineCore_DP0 pid=300132) .b8 113
(EngineCore_DP0 pid=300132) .b8 117
(EngineCore_DP0 pid=300132) .b8 97
(EngineCore_DP0 pid=300132) .b8 110
(EngineCore_DP0 pid=300132) .b8 116
(EngineCore_DP0 pid=300132) .b8 95
(EngineCore_DP0 pid=300132) .b8 115
(EngineCore_DP0 pid=300132) .b8 108
(EngineCore_DP0 pid=300132) .b8 105
(EngineCore_DP0 pid=300132) .b8 100
(EngineCore_DP0 pid=300132) .b8 101
(EngineCore_DP0 pid=300132) .b8 95
(EngineCore_DP0 pid=300132) .b8 116
(EngineCore_DP0 pid=300132) .b8 114
(EngineCore_DP0 pid=300132) .b8 105
(EngineCore_DP0 pid=300132) .b8 116
(EngineCore_DP0 pid=300132) .b8 111
(EngineCore_DP0 pid=300132) .b8 110
(EngineCore_DP0 pid=300132) .b8 47
(EngineCore_DP0 pid=300132) .b8 98
(EngineCore_DP0 pid=300132) .b8 117
(EngineCore_DP0 pid=300132) .b8 105
(EngineCore_DP0 pid=300132) .b8 108
(EngineCore_DP0 pid=300132) .b8 100
(EngineCore_DP0 pid=300132) .b8 47
(EngineCore_DP0 pid=300132) .b8 71
(EngineCore_DP0 pid=300132) .b8 66
(EngineCore_DP0 pid=300132) .b8 49
(EngineCore_DP0 pid=300132) .b8 48
(EngineCore_DP0 pid=300132) .b8 95
(EngineCore_DP0 pid=300132) .b8 99
(EngineCore_DP0 pid=300132) .b8 99
(EngineCore_DP0 pid=300132) .b8 49
(EngineCore_DP0 pid=300132) .b8 50
(EngineCore_DP0 pid=300132) .b8 49
(EngineCore_DP0 pid=300132) .b8 95
(EngineCore_DP0 pid=300132) .b8 112
(EngineCore_DP0 pid=300132) .b8 121
(EngineCore_DP0 pid=300132) .b8 51
(EngineCore_DP0 pid=300132) .b8 49
(EngineCore_DP0 pid=300132) .b8 50
(EngineCore_DP0 pid=300132) .b8 95
(EngineCore_DP0 pid=300132) .b8 99
(EngineCore_DP0 pid=300132) .b8 117
(EngineCore_DP0 pid=300132) .b8 49
(EngineCore_DP0 pid=300132) .b8 50
(EngineCore_DP0 pid=300132) .b8 57
(EngineCore_DP0 pid=300132) .b8 95
(EngineCore_DP0 pid=300132) .b8 97
(EngineCore_DP0 pid=300132) .b8 97
(EngineCore_DP0 pid=300132) .b8 114
(EngineCore_DP0 pid=300132) .b8 99
(EngineCore_DP0 pid=300132) .b8 104
(EngineCore_DP0 pid=300132) .b8 54
(EngineCore_DP0 pid=300132) .b8 52
(EngineCore_DP0 pid=300132) .b8 0
(EngineCore_DP0 pid=300132) .b8 2                                   // Abbrev [2] 0x99:0x1a DW_TAG_subprogram
(EngineCore_DP0 pid=300132) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=300132) .b8 113
(EngineCore_DP0 pid=300132) .b8 117
(EngineCore_DP0 pid=300132) .b8 97
(EngineCore_DP0 pid=300132) .b8 110
(EngineCore_DP0 pid=300132) .b8 116
(EngineCore_DP0 pid=300132) .b8 95
(EngineCore_DP0 pid=300132) .b8 115
(EngineCore_DP0 pid=300132) .b8 108
(EngineCore_DP0 pid=300132) .b8 105
(EngineCore_DP0 pid=300132) .b8 100
(EngineCore_DP0 pid=300132) .b8 101
(EngineCore_DP0 pid=300132) .b8 95
(EngineCore_DP0 pid=300132) .b8 102
(EngineCore_DP0 pid=300132) .b8 112
(EngineCore_DP0 pid=300132) .b8 56
(EngineCore_DP0 pid=300132) .b8 95
(EngineCore_DP0 pid=300132) .b8 107
(EngineCore_DP0 pid=300132) .b8 101
(EngineCore_DP0 pid=300132) .b8 114
(EngineCore_DP0 pid=300132) .b8 110
(EngineCore_DP0 pid=300132) .b8 101
(EngineCore_DP0 pid=300132) .b8 108
(EngineCore_DP0 pid=300132) .b8 0
(EngineCore_DP0 pid=300132) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=300132) .b8 3                                   // Abbrev [3] 0xb3:0x2e DW_TAG_subprogram
(EngineCore_DP0 pid=300132) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=300132) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=300132) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=300132) .b8 4                                   // Abbrev [4] 0xc8:0x18 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=300132) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=300132) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=300132) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=300132) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=300132) .b8 191                                 // DW_AT_call_line
(EngineCore_DP0 pid=300132) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=300132) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=300132) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=300132) 	}
(EngineCore_DP0 pid=300132) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) ================================================================
(EngineCore_DP0 pid=300132) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmp75xlnwzf.ptx', '-o', '/tmp/tmp75xlnwzf.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866] 
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866] 
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866] 
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp75xlnwzf.ptx -o /tmp/tmp75xlnwzf.ptx.o
(EngineCore_DP0 pid=300132) ERROR 01-25 18:47:53 [core.py:866] 

STDERR:
[2026-01-25 18:47:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:47:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:47:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:47:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:47:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:47:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:47:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:47:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:47:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:47:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:47:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:47:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:47:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:47:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:47:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:47:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:47:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:47:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:47:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=300132) [2026-01-25 18:47:43] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=300132) [2026-01-25 18:47:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=300132) [2026-01-25 18:47:43] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=300132) [2026-01-25 18:47:43] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=300132) [2026-01-25 18:47:43] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=300132) [2026-01-25 18:47:43] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=300132) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=300132) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.24s/it]
(EngineCore_DP0 pid=300132) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.24s/it]
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) [2026-01-25 18:47:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=300132) [2026-01-25 18:47:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=300132) [2026-01-25 18:47:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=300132) [2026-01-25 18:47:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=300132) [2026-01-25 18:47:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=300132) [2026-01-25 18:47:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=300132) [2026-01-25 18:47:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=300132) [2026-01-25 18:47:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=300132) Process EngineCore_DP0:
(EngineCore_DP0 pid=300132) Traceback (most recent call last):
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=300132)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=300132)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=300132)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=300132) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmp75xlnwzf.ptx', '-o', '/tmp/tmp75xlnwzf.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) Traceback (most recent call last):
(EngineCore_DP0 pid=300132)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=300132)     self.run()
(EngineCore_DP0 pid=300132)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=300132)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=300132)     raise e
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=300132)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=300132)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=300132)     super().__init__(
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=300132)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=300132)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=300132)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=300132)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=300132)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=300132)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=300132)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=300132)     return func(*args, **kwargs)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=300132)     return func(*args, **kwargs)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=300132)     self.model_runner.profile_run()
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=300132)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=300132)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=300132)     return func(*args, **kwargs)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=300132)     outputs = self.model(
(EngineCore_DP0 pid=300132)               ^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=300132)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=300132)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=300132)     model_output = self.model(
(EngineCore_DP0 pid=300132)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=300132)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=300132)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=300132)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=300132)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=300132)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=300132)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=300132)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=300132)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=300132)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=300132)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=300132)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=300132)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=300132)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=300132)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=300132)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=300132)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 685, in apply_weights
(EngineCore_DP0 pid=300132)     return self.slidesparse_fp8_linear.apply(
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 471, in apply
(EngineCore_DP0 pid=300132)     return self._linear_fn(
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_FP8.py", line 256, in cuSPARSELt_FP8_linear
(EngineCore_DP0 pid=300132)     qinput, scale_a_pad = quant_slide_fp8_kernel(input, model_name, L)
(EngineCore_DP0 pid=300132)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/slidesparse/core/kernels.py", line 309, in quant_slide_fp8_kernel
(EngineCore_DP0 pid=300132)     return torch.ops.slidesparse.quant_slide_fp8(input, model_name, L)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=300132)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=300132)     return fn(input, L)
(EngineCore_DP0 pid=300132)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 259, in quant_slide_fp8_triton
(EngineCore_DP0 pid=300132)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=300132)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=300132)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=300132)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=300132)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=300132)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=300132)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=300132)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=300132)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=300132)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=300132)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=300132)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=300132)     raise PTXASError(error)
(EngineCore_DP0 pid=300132) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=300132) `ptxas` stderr:
(EngineCore_DP0 pid=300132) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=300132) 
(EngineCore_DP0 pid=300132) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp75xlnwzf.ptx -o /tmp/tmp75xlnwzf.ptx.o
(EngineCore_DP0 pid=300132) 
[rank0]:[W125 18:47:53.574404003 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=256

========== M=16 ==========
Time: 2026-01-26 02:37:09
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-FP8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:37:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:37:12 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=753758) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=753758) WARNING 01-26 02:37:31 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 6.51 requests/s, 1771.31 total tokens/s, 1667.11 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096

STDERR:
[2026-01-26 02:37:12] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:37:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:37:12] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:37:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:37:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:37:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:37:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:37:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:37:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:37:16] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:37:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:37:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:37:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:37:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:37:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:37:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:37:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:37:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=753758) [2026-01-26 02:37:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=753758) [2026-01-26 02:37:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=753758) [2026-01-26 02:37:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=753758) [2026-01-26 02:37:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=753758) [2026-01-26 02:37:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=753758) [2026-01-26 02:37:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=753758) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=753758) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.06s/it]
(EngineCore_DP0 pid=753758) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.06s/it]
(EngineCore_DP0 pid=753758) 
(EngineCore_DP0 pid=753758) [2026-01-26 02:37:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=753758) [2026-01-26 02:37:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=753758) [2026-01-26 02:37:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=753758) [2026-01-26 02:37:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=753758) [2026-01-26 02:37:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=753758) [2026-01-26 02:37:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=753758) [2026-01-26 02:37:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=753758) [2026-01-26 02:37:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=753758) 2026-01-26 02:37:30,685 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=753758) 2026-01-26 02:37:30,692 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|| 16/16 [00:00<00:00, 8361.43it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|         | 1/16 [00:02<00:36,  2.44s/it, est. speed input: 6.56 toks/s, output: 104.98 toks/s]
Processed prompts: 100%|| 16/16 [00:02<00:00,  2.44s/it, est. speed input: 104.31 toks/s, output: 1668.95 toks/s]
Processed prompts: 100%|| 16/16 [00:02<00:00,  6.52it/s, est. speed input: 104.31 toks/s, output: 1668.95 toks/s]
[rank0]:[W126 02:37:33.162784520 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 02:37:36
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:37:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:37:39 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=754324) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=754324) WARNING 01-26 02:37:58 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 25.55 requests/s, 6948.98 total tokens/s, 6540.22 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 02:37:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:37:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:37:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:37:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:37:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:37:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:37:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:37:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:37:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:37:43] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:37:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:37:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:37:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:37:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:37:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:37:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:37:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:37:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:37:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=754324) [2026-01-26 02:37:44] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=754324) [2026-01-26 02:37:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=754324) [2026-01-26 02:37:44] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=754324) [2026-01-26 02:37:44] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=754324) [2026-01-26 02:37:44] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=754324) [2026-01-26 02:37:44] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=754324) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=754324) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.10s/it]
(EngineCore_DP0 pid=754324) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.10s/it]
(EngineCore_DP0 pid=754324) 
(EngineCore_DP0 pid=754324) [2026-01-26 02:37:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=754324) [2026-01-26 02:37:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=754324) [2026-01-26 02:37:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=754324) [2026-01-26 02:37:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=754324) [2026-01-26 02:37:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=754324) [2026-01-26 02:37:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=754324) [2026-01-26 02:37:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=754324) [2026-01-26 02:37:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=754324) 2026-01-26 02:37:57,648 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=754324) 2026-01-26 02:37:57,655 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 12874.60it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:04<09:47,  4.63s/it, est. speed input: 3.46 toks/s, output: 55.33 toks/s]
Processed prompts:   2%|         | 2/128 [00:04<04:07,  1.97s/it, est. speed input: 6.76 toks/s, output: 108.22 toks/s]
Processed prompts:  48%|     | 62/128 [00:04<00:02, 24.74it/s, est. speed input: 203.93 toks/s, output: 3262.85 toks/s]
Processed prompts:  77%|  | 98/128 [00:04<00:00, 42.85it/s, est. speed input: 315.33 toks/s, output: 5045.21 toks/s]
Processed prompts: 100%|| 128/128 [00:04<00:00, 42.85it/s, est. speed input: 409.64 toks/s, output: 6554.24 toks/s]
Processed prompts: 100%|| 128/128 [00:04<00:00, 25.60it/s, est. speed input: 409.64 toks/s, output: 6554.24 toks/s]
[rank0]:[W126 02:38:03.714507603 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 02:38:05
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:38:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:38:09 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=754928) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=754928) WARNING 01-26 02:38:27 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 31.40 requests/s, 8539.87 total tokens/s, 8037.52 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 02:38:09] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:38:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:38:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:38:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:38:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:38:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:38:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:38:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:38:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:38:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:38:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:38:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:38:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:38:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:38:12] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:38:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 02:38:12] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 02:38:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:38:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:38:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:38:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:38:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 02:38:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 02:38:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:38:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:38:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:38:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:38:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=754928) [2026-01-26 02:38:13] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=754928) [2026-01-26 02:38:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=754928) [2026-01-26 02:38:13] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=754928) [2026-01-26 02:38:13] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=754928) [2026-01-26 02:38:13] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=754928) [2026-01-26 02:38:13] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=754928) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=754928) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.07s/it]
(EngineCore_DP0 pid=754928) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.07s/it]
(EngineCore_DP0 pid=754928) 
(EngineCore_DP0 pid=754928) [2026-01-26 02:38:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=754928) [2026-01-26 02:38:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=754928) [2026-01-26 02:38:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=754928) [2026-01-26 02:38:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=754928) [2026-01-26 02:38:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=754928) [2026-01-26 02:38:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=754928) [2026-01-26 02:38:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=754928) [2026-01-26 02:38:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=754928) 2026-01-26 02:38:27,124 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=754928) 2026-01-26 02:38:27,132 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 13151.03it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<29:55,  7.04s/it, est. speed input: 2.27 toks/s, output: 36.36 toks/s]
Processed prompts:  13%|        | 33/256 [00:07<00:34,  6.44it/s, est. speed input: 73.45 toks/s, output: 1175.15 toks/s]
Processed prompts:  29%|       | 75/256 [00:07<00:10, 17.66it/s, est. speed input: 164.18 toks/s, output: 2626.85 toks/s]
Processed prompts:  38%|      | 98/256 [00:07<00:06, 25.57it/s, est. speed input: 211.50 toks/s, output: 3384.02 toks/s]
Processed prompts:  53%|    | 136/256 [00:07<00:02, 42.97it/s, est. speed input: 288.62 toks/s, output: 4617.85 toks/s]
Processed prompts:  65%|   | 166/256 [00:07<00:01, 59.52it/s, est. speed input: 346.99 toks/s, output: 5551.87 toks/s]
Processed prompts:  75%|  | 193/256 [00:07<00:00, 76.42it/s, est. speed input: 397.26 toks/s, output: 6356.17 toks/s]
Processed prompts:  86%| | 219/256 [00:07<00:00, 95.35it/s, est. speed input: 444.56 toks/s, output: 7112.88 toks/s]
Processed prompts:  95%|| 244/256 [00:08<00:00, 109.55it/s, est. speed input: 486.45 toks/s, output: 7783.20 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 109.55it/s, est. speed input: 503.61 toks/s, output: 8057.69 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 31.47it/s, est. speed input: 503.61 toks/s, output: 8057.69 toks/s] 
[rank0]:[W126 02:38:36.368674399 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:03:22
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:03:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:03:26 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2844144) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2844144) WARNING 01-27 17:03:44 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.26 requests/s, 4967.14 total tokens/s, 4674.95 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:03:26] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:03:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:03:26] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:03:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:03:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:03:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:03:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:03:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:03:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:03:29] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:03:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:03:29] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:03:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:03:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:03:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:03:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:03:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:03:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2844144) [2026-01-27 17:03:30] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2844144) [2026-01-27 17:03:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2844144) [2026-01-27 17:03:30] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2844144) [2026-01-27 17:03:30] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2844144) [2026-01-27 17:03:30] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2844144) [2026-01-27 17:03:30] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2844144) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2844144) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.06s/it]
(EngineCore_DP0 pid=2844144) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.06s/it]
(EngineCore_DP0 pid=2844144) 
(EngineCore_DP0 pid=2844144) [2026-01-27 17:03:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=2844144) [2026-01-27 17:03:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=2844144) [2026-01-27 17:03:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=2844144) [2026-01-27 17:03:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=2844144) [2026-01-27 17:03:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=2844144) [2026-01-27 17:03:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=2844144) [2026-01-27 17:03:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=2844144) [2026-01-27 17:03:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=2844144) 2026-01-27 17:03:44,209 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2844144) 2026-01-27 17:03:44,218 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 11799.88it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:03<03:31,  3.36s/it, est. speed input: 4.76 toks/s, output: 76.09 toks/s]
Processed prompts:  78%|  | 50/64 [00:03<00:00, 20.03it/s, est. speed input: 229.17 toks/s, output: 3666.64 toks/s]
Processed prompts: 100%|| 64/64 [00:03<00:00, 20.03it/s, est. speed input: 292.70 toks/s, output: 4683.27 toks/s]
Processed prompts: 100%|| 64/64 [00:03<00:00, 18.29it/s, est. speed input: 292.70 toks/s, output: 4683.27 toks/s]
[rank0]:[W127 17:03:48.607105262 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:03:50
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:03:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:03:54 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2844714) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2844714) WARNING 01-27 17:04:13 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 25.76 requests/s, 7005.47 total tokens/s, 6593.38 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:03:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:03:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:03:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:03:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:03:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:03:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:03:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:03:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:03:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:03:57] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:03:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:03:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:03:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:03:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:03:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:03:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:03:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:03:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:03:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2844714) [2026-01-27 17:03:58] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2844714) [2026-01-27 17:03:58] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2844714) [2026-01-27 17:03:58] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2844714) [2026-01-27 17:03:58] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2844714) [2026-01-27 17:03:58] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2844714) [2026-01-27 17:03:58] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2844714) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2844714) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.16s/it]
(EngineCore_DP0 pid=2844714) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.16s/it]
(EngineCore_DP0 pid=2844714) 
(EngineCore_DP0 pid=2844714) [2026-01-27 17:04:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=2844714) [2026-01-27 17:04:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=2844714) [2026-01-27 17:04:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=2844714) [2026-01-27 17:04:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=2844714) [2026-01-27 17:04:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=2844714) [2026-01-27 17:04:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=2844714) [2026-01-27 17:04:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=2844714) [2026-01-27 17:04:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=2844714) 2026-01-27 17:04:12,816 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2844714) 2026-01-27 17:04:12,824 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 7535.14it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:04<09:44,  4.60s/it, est. speed input: 3.47 toks/s, output: 55.60 toks/s]
Processed prompts:  26%|       | 33/128 [00:04<00:09,  9.78it/s, est. speed input: 111.71 toks/s, output: 1787.39 toks/s]
Processed prompts:  59%|    | 75/128 [00:04<00:01, 26.54it/s, est. speed input: 248.46 toks/s, output: 3975.39 toks/s]
Processed prompts: 100%|| 128/128 [00:04<00:00, 26.54it/s, est. speed input: 415.55 toks/s, output: 6648.84 toks/s]
Processed prompts: 100%|| 128/128 [00:04<00:00, 25.97it/s, est. speed input: 415.55 toks/s, output: 6648.84 toks/s]
[rank0]:[W127 17:04:18.748308653 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:04:21
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:04:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:04:25 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2845343) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2845343) WARNING 01-27 17:04:44 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 31.81 requests/s, 8652.31 total tokens/s, 8143.35 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:04:24] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:04:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:04:24] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:04:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:04:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:04:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:04:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:04:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:04:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:04:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:04:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:04:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:04:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:04:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:04:28] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:04:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:04:28] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:04:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:04:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:04:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:04:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:04:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:04:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:04:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:04:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:04:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:04:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:04:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2845343) [2026-01-27 17:04:29] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2845343) [2026-01-27 17:04:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2845343) [2026-01-27 17:04:29] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2845343) [2026-01-27 17:04:29] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2845343) [2026-01-27 17:04:29] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2845343) [2026-01-27 17:04:29] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2845343) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2845343) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.14s/it]
(EngineCore_DP0 pid=2845343) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.14s/it]
(EngineCore_DP0 pid=2845343) 
(EngineCore_DP0 pid=2845343) [2026-01-27 17:04:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=2845343) [2026-01-27 17:04:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=2845343) [2026-01-27 17:04:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=2845343) [2026-01-27 17:04:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=2845343) [2026-01-27 17:04:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=2845343) [2026-01-27 17:04:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=2845343) [2026-01-27 17:04:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=2845343) [2026-01-27 17:04:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=2845343) 2026-01-27 17:04:43,563 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2845343) 2026-01-27 17:04:43,577 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 12686.59it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:06<29:05,  6.85s/it, est. speed input: 2.34 toks/s, output: 37.40 toks/s]
Processed prompts:  14%|        | 36/256 [00:06<00:30,  7.24it/s, est. speed input: 82.50 toks/s, output: 1320.06 toks/s]
Processed prompts:  30%|       | 76/256 [00:07<00:09, 18.20it/s, est. speed input: 171.19 toks/s, output: 2739.06 toks/s]
Processed prompts:  43%|     | 109/256 [00:07<00:04, 30.04it/s, est. speed input: 241.69 toks/s, output: 3867.02 toks/s]
Processed prompts:  54%|    | 137/256 [00:07<00:02, 42.93it/s, est. speed input: 299.48 toks/s, output: 4791.73 toks/s]
Processed prompts:  65%|   | 167/256 [00:07<00:01, 59.78it/s, est. speed input: 359.14 toks/s, output: 5746.25 toks/s]
Processed prompts:  77%|  | 198/256 [00:07<00:00, 81.17it/s, est. speed input: 419.60 toks/s, output: 6713.60 toks/s]
Processed prompts:  88%| | 225/256 [00:07<00:00, 96.33it/s, est. speed input: 467.25 toks/s, output: 7476.04 toks/s]
Processed prompts:  97%|| 249/256 [00:07<00:00, 104.93it/s, est. speed input: 505.68 toks/s, output: 8090.77 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 104.93it/s, est. speed input: 510.38 toks/s, output: 8165.98 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 31.90it/s, est. speed input: 510.38 toks/s, output: 8165.98 toks/s] 
[rank0]:[W127 17:04:52.561072267 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:04:55
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:05:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:05:01 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2846043) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2846043) WARNING 01-27 17:05:20 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 32.27 requests/s, 8778.57 total tokens/s, 8262.18 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:05:01] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:05:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:05:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:05:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:05:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:05:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:05:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:05:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:05:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:05:04] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:05:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-27 17:05:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-27 17:05:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-27 17:05:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-27 17:05:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:05:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:05:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:05:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:05:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2846043) [2026-01-27 17:05:05] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2846043) [2026-01-27 17:05:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2846043) [2026-01-27 17:05:05] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2846043) [2026-01-27 17:05:05] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2846043) [2026-01-27 17:05:05] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=2846043) [2026-01-27 17:05:05] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2846043) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2846043) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.21s/it]
(EngineCore_DP0 pid=2846043) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.21s/it]
(EngineCore_DP0 pid=2846043) 
(EngineCore_DP0 pid=2846043) [2026-01-27 17:05:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=2846043) [2026-01-27 17:05:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=2846043) [2026-01-27 17:05:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=2846043) [2026-01-27 17:05:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=2846043) [2026-01-27 17:05:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=2846043) [2026-01-27 17:05:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=2846043) [2026-01-27 17:05:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=2846043) [2026-01-27 17:05:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=2846043) 2026-01-27 17:05:19,654 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2846043) 2026-01-27 17:05:19,662 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 13548.19it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:12<1:42:32, 12.04s/it, est. speed input: 1.33 toks/s, output: 21.26 toks/s]
Processed prompts:   1%|          | 4/512 [00:12<19:31,  2.31s/it, est. speed input: 5.27 toks/s, output: 84.33 toks/s]  
Processed prompts:  12%|        | 64/512 [00:12<00:44, 10.18it/s, est. speed input: 83.10 toks/s, output: 1329.59 toks/s]
Processed prompts:  23%|       | 118/512 [00:12<00:18, 21.78it/s, est. speed input: 150.95 toks/s, output: 2415.13 toks/s]
Processed prompts:  32%|      | 165/512 [00:12<00:09, 35.34it/s, est. speed input: 209.00 toks/s, output: 3343.95 toks/s]
Processed prompts:  40%|      | 206/512 [00:12<00:06, 50.64it/s, est. speed input: 258.51 toks/s, output: 4136.12 toks/s]
Processed prompts:  47%|     | 243/512 [00:12<00:03, 67.47it/s, est. speed input: 301.90 toks/s, output: 4830.36 toks/s]
Processed prompts:  54%|    | 274/512 [00:12<00:02, 84.12it/s, est. speed input: 337.34 toks/s, output: 5397.44 toks/s]
Processed prompts:  62%|   | 315/512 [00:13<00:01, 112.10it/s, est. speed input: 384.16 toks/s, output: 6146.62 toks/s]
Processed prompts:  68%|   | 348/512 [00:13<00:01, 134.59it/s, est. speed input: 420.65 toks/s, output: 6730.40 toks/s]
Processed prompts:  74%|  | 378/512 [00:13<00:00, 156.24it/s, est. speed input: 453.27 toks/s, output: 7252.31 toks/s]
Processed prompts:  80%|  | 408/512 [00:13<00:00, 167.81it/s, est. speed input: 484.03 toks/s, output: 7744.43 toks/s]
Processed prompts:  85%| | 435/512 [00:13<00:00, 172.60it/s, est. speed input: 510.61 toks/s, output: 8169.84 toks/s]
Processed prompts:  90%| | 460/512 [00:13<00:00, 178.75it/s, est. speed input: 535.04 toks/s, output: 8560.59 toks/s]
Processed prompts:  94%|| 483/512 [00:13<00:00, 174.95it/s, est. speed input: 556.14 toks/s, output: 8898.21 toks/s]
Processed prompts:  99%|| 505/512 [00:15<00:00, 39.12it/s, est. speed input: 513.82 toks/s, output: 8221.18 toks/s] 
Processed prompts: 100%|| 512/512 [00:15<00:00, 39.12it/s, est. speed input: 517.66 toks/s, output: 8282.48 toks/s]
Processed prompts: 100%|| 512/512 [00:15<00:00, 32.35it/s, est. speed input: 517.66 toks/s, output: 8282.48 toks/s]
[rank0]:[W127 17:05:36.482355642 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:37:04
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-3B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:37:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:37:08 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2879857) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2879857) WARNING 01-27 17:37:35 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 9.14 requests/s, 2486.30 total tokens/s, 2340.04 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:37:07] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:37:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:37:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:37:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:37:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:37:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:37:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:37:11] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:37:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:37:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:37:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:37:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:37:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:37:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:12] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:12] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:12] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:12] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:12] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2879857) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2879857) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.06s/it]
(EngineCore_DP0 pid=2879857) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.06s/it]
(EngineCore_DP0 pid=2879857) 
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=2879857) 2026-01-27 17:37:35,220 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2879857) 2026-01-27 17:37:35,234 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 11988.54it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:06<07:09,  6.82s/it, est. speed input: 2.35 toks/s, output: 37.55 toks/s]
Processed prompts:  53%|    | 34/64 [00:06<00:04,  6.89it/s, est. speed input: 78.40 toks/s, output: 1254.41 toks/s]
Processed prompts: 100%|| 64/64 [00:06<00:00,  6.89it/s, est. speed input: 146.38 toks/s, output: 2342.10 toks/s]
Processed prompts: 100%|| 64/64 [00:06<00:00,  9.15it/s, est. speed input: 146.38 toks/s, output: 2342.10 toks/s]
[rank0]:[W127 17:37:43.045853400 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:37:45
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-3B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:37:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:37:49 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2880607) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2880607) WARNING 01-27 17:38:15 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 13.54 requests/s, 3682.43 total tokens/s, 3465.82 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:37:49] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:37:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:37:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:37:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:37:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:37:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:37:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:37:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:37:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:37:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:37:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:37:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:37:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:37:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2880607) [2026-01-27 17:37:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2880607) [2026-01-27 17:37:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2880607) [2026-01-27 17:37:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2880607) [2026-01-27 17:37:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2880607) [2026-01-27 17:37:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2880607) [2026-01-27 17:37:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2880607) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2880607) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.24s/it]
(EngineCore_DP0 pid=2880607) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.24s/it]
(EngineCore_DP0 pid=2880607) 
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=2880607) 2026-01-27 17:38:14,722 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2880607) 2026-01-27 17:38:14,733 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 12876.15it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:08<18:52,  8.92s/it, est. speed input: 1.79 toks/s, output: 28.71 toks/s]
Processed prompts:  16%|        | 21/128 [00:09<00:32,  3.26it/s, est. speed input: 37.26 toks/s, output: 596.09 toks/s]
Processed prompts:  39%|      | 50/128 [00:09<00:08,  9.55it/s, est. speed input: 87.61 toks/s, output: 1401.83 toks/s]
Processed prompts:  69%|   | 88/128 [00:09<00:01, 20.76it/s, est. speed input: 152.38 toks/s, output: 2438.06 toks/s]
Processed prompts:  93%|| 119/128 [00:09<00:00, 32.72it/s, est. speed input: 203.76 toks/s, output: 3260.09 toks/s]
Processed prompts: 100%|| 128/128 [00:09<00:00, 32.72it/s, est. speed input: 216.86 toks/s, output: 3469.76 toks/s]
Processed prompts: 100%|| 128/128 [00:09<00:00, 13.55it/s, est. speed input: 216.86 toks/s, output: 3469.76 toks/s]
[rank0]:[W127 17:38:25.145103462 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:38:27
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-3B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:38:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:38:31 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2881393) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2881393) WARNING 01-27 17:38:57 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.45 requests/s, 4475.54 total tokens/s, 4212.27 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:38:30] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:38:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:38:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:38:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:38:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:38:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:38:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:38:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:38:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:38:34] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:38:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:38:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:38:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:38:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:38:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:38:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:38:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:38:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:35] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2881393) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2881393) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.38s/it]
(EngineCore_DP0 pid=2881393) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.38s/it]
(EngineCore_DP0 pid=2881393) 
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=2881393) 2026-01-27 17:38:56,638 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2881393) 2026-01-27 17:38:56,650 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 4619.20it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:13<57:30, 13.53s/it, est. speed input: 1.18 toks/s, output: 18.92 toks/s]
Processed prompts:   7%|         | 18/256 [00:13<02:09,  1.84it/s, est. speed input: 21.03 toks/s, output: 336.53 toks/s]
Processed prompts:  19%|        | 48/256 [00:13<00:33,  6.15it/s, est. speed input: 55.50 toks/s, output: 887.98 toks/s]
Processed prompts:  29%|       | 75/256 [00:13<00:15, 11.39it/s, est. speed input: 85.76 toks/s, output: 1372.11 toks/s]
Processed prompts:  36%|      | 92/256 [00:14<00:10, 15.78it/s, est. speed input: 104.40 toks/s, output: 1670.38 toks/s]
Processed prompts:  43%|     | 109/256 [00:14<00:06, 21.54it/s, est. speed input: 122.67 toks/s, output: 1962.65 toks/s]
Processed prompts:  50%|     | 127/256 [00:14<00:04, 29.52it/s, est. speed input: 141.81 toks/s, output: 2268.95 toks/s]
Processed prompts:  56%|    | 144/256 [00:14<00:02, 38.94it/s, est. speed input: 159.61 toks/s, output: 2553.78 toks/s]
Processed prompts:  65%|   | 166/256 [00:14<00:01, 52.53it/s, est. speed input: 182.08 toks/s, output: 2913.30 toks/s]
Processed prompts:  71%|  | 183/256 [00:14<00:01, 63.84it/s, est. speed input: 199.10 toks/s, output: 3185.53 toks/s]
Processed prompts:  78%|  | 199/256 [00:14<00:00, 74.08it/s, est. speed input: 214.69 toks/s, output: 3435.02 toks/s]
Processed prompts:  84%| | 214/256 [00:14<00:00, 82.19it/s, est. speed input: 228.89 toks/s, output: 3662.30 toks/s]
Processed prompts:  89%| | 228/256 [00:15<00:00, 86.16it/s, est. speed input: 241.60 toks/s, output: 3865.58 toks/s]
Processed prompts:  94%|| 241/256 [00:15<00:00, 89.77it/s, est. speed input: 253.24 toks/s, output: 4051.77 toks/s]
Processed prompts:  99%|| 253/256 [00:15<00:00, 77.36it/s, est. speed input: 262.10 toks/s, output: 4193.52 toks/s]
Processed prompts: 100%|| 256/256 [00:15<00:00, 77.36it/s, est. speed input: 264.23 toks/s, output: 4227.65 toks/s]
Processed prompts: 100%|| 256/256 [00:15<00:00, 16.51it/s, est. speed input: 264.23 toks/s, output: 4227.65 toks/s]
[rank0]:[W127 17:39:13.119649523 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:39:15
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-3B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:39:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:39:18 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2882236) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2882236) WARNING 01-27 17:39:45 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.67 requests/s, 4534.54 total tokens/s, 4267.81 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:39:18] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:39:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:39:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:39:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:39:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:39:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:39:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:39:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:39:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:39:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:39:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:39:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:39:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:39:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:39:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:39:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:39:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:39:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:23] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2882236) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2882236) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.68s/it]
(EngineCore_DP0 pid=2882236) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.68s/it]
(EngineCore_DP0 pid=2882236) 
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=2882236) 2026-01-27 17:39:44,879 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2882236) 2026-01-27 17:39:44,890 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 13686.43it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:23<3:18:12, 23.27s/it, est. speed input: 0.69 toks/s, output: 11.00 toks/s]
Processed prompts:   1%|          | 6/512 [00:23<24:28,  2.90s/it, est. speed input: 4.08 toks/s, output: 65.31 toks/s]  
Processed prompts:   7%|         | 37/512 [00:23<02:40,  2.96it/s, est. speed input: 24.92 toks/s, output: 398.77 toks/s]
Processed prompts:  13%|        | 66/512 [00:23<01:10,  6.30it/s, est. speed input: 44.05 toks/s, output: 704.82 toks/s]
Processed prompts:  18%|        | 93/512 [00:24<00:39, 10.49it/s, est. speed input: 61.55 toks/s, output: 984.73 toks/s]
Processed prompts:  23%|       | 119/512 [00:24<00:24, 15.74it/s, est. speed input: 78.05 toks/s, output: 1248.85 toks/s]
Processed prompts:  28%|       | 143/512 [00:24<00:16, 22.51it/s, est. speed input: 93.36 toks/s, output: 1493.72 toks/s]
Processed prompts:  32%|      | 166/512 [00:24<00:11, 30.91it/s, est. speed input: 107.84 toks/s, output: 1725.45 toks/s]
Processed prompts:  37%|      | 187/512 [00:24<00:07, 40.79it/s, est. speed input: 120.97 toks/s, output: 1935.44 toks/s]
Processed prompts:  40%|      | 207/512 [00:24<00:05, 52.50it/s, est. speed input: 133.36 toks/s, output: 2133.76 toks/s]
Processed prompts:  47%|     | 243/512 [00:25<00:03, 75.42it/s, est. speed input: 155.37 toks/s, output: 2486.00 toks/s]
Processed prompts:  54%|    | 274/512 [00:25<00:02, 93.42it/s, est. speed input: 173.96 toks/s, output: 2783.31 toks/s]
Processed prompts:  59%|    | 302/512 [00:25<00:01, 112.05it/s, est. speed input: 190.70 toks/s, output: 3051.23 toks/s]
Processed prompts:  64%|   | 327/512 [00:25<00:01, 122.85it/s, est. speed input: 205.25 toks/s, output: 3283.97 toks/s]
Processed prompts:  68%|   | 348/512 [00:25<00:01, 132.18it/s, est. speed input: 217.38 toks/s, output: 3478.09 toks/s]
Processed prompts:  72%|  | 367/512 [00:25<00:01, 135.69it/s, est. speed input: 228.10 toks/s, output: 3649.66 toks/s]
Processed prompts:  75%|  | 384/512 [00:25<00:00, 140.28it/s, est. speed input: 237.67 toks/s, output: 3802.75 toks/s]
Processed prompts:  78%|  | 401/512 [00:25<00:00, 145.19it/s, est. speed input: 247.19 toks/s, output: 3955.02 toks/s]
Processed prompts:  82%| | 418/512 [00:26<00:00, 133.41it/s, est. speed input: 256.13 toks/s, output: 4098.08 toks/s]
Processed prompts:  86%| | 438/512 [00:26<00:00, 139.92it/s, est. speed input: 267.08 toks/s, output: 4273.32 toks/s]
Processed prompts:  89%| | 454/512 [00:26<00:00, 131.65it/s, est. speed input: 275.35 toks/s, output: 4405.60 toks/s]
Processed prompts:  91%|| 468/512 [00:26<00:00, 118.57it/s, est. speed input: 282.19 toks/s, output: 4515.10 toks/s]
Processed prompts:  94%|| 481/512 [00:26<00:00, 112.11it/s, est. speed input: 288.56 toks/s, output: 4616.97 toks/s]
Processed prompts:  96%|| 493/512 [00:26<00:00, 80.41it/s, est. speed input: 292.70 toks/s, output: 4683.13 toks/s] 
Processed prompts:  98%|| 503/512 [00:30<00:00, 10.97it/s, est. speed input: 263.34 toks/s, output: 4213.51 toks/s]
Processed prompts: 100%|| 512/512 [00:30<00:00, 13.61it/s, est. speed input: 267.07 toks/s, output: 4273.14 toks/s]
Processed prompts: 100%|| 512/512 [00:30<00:00, 13.61it/s, est. speed input: 267.07 toks/s, output: 4273.14 toks/s]
Processed prompts: 100%|| 512/512 [00:30<00:00, 16.69it/s, est. speed input: 267.07 toks/s, output: 4273.14 toks/s]
[rank0]:[W127 17:40:16.565079079 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 18:31:32
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-7B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:31:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:31:35 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2932753) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2932753) WARNING 01-27 18:32:29 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 6.16 requests/s, 1676.21 total tokens/s, 1577.61 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 18:31:35] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:31:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:31:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:31:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:31:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:31:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:31:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:31:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:31:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:31:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:31:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:31:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:31:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:31:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:31:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:31:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2932753) [2026-01-27 18:31:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2932753) [2026-01-27 18:31:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2932753) [2026-01-27 18:31:40] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2932753) [2026-01-27 18:31:40] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2932753) [2026-01-27 18:31:40] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2932753) [2026-01-27 18:31:40] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2932753) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2932753) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:20<00:20, 20.29s/it]
(EngineCore_DP0 pid=2932753) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:40<00:00, 20.47s/it]
(EngineCore_DP0 pid=2932753) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:40<00:00, 20.45s/it]
(EngineCore_DP0 pid=2932753) 
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=2932753) 2026-01-27 18:32:28,371 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2932753) 2026-01-27 18:32:28,388 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 9986.07it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:10<10:37, 10.12s/it, est. speed input: 1.58 toks/s, output: 25.30 toks/s]
Processed prompts:  33%|      | 21/64 [00:10<00:14,  2.88it/s, est. speed input: 32.83 toks/s, output: 525.28 toks/s]
Processed prompts:  78%|  | 50/64 [00:10<00:01,  8.45it/s, est. speed input: 77.35 toks/s, output: 1237.54 toks/s]
Processed prompts: 100%|| 64/64 [00:10<00:00,  8.45it/s, est. speed input: 98.72 toks/s, output: 1579.58 toks/s]
Processed prompts: 100%|| 64/64 [00:10<00:00,  6.17it/s, est. speed input: 98.72 toks/s, output: 1579.58 toks/s]
[rank0]:[W127 18:32:39.757796097 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 18:32:42
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-7B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:32:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:32:45 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2933925) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2933925) WARNING 01-27 18:33:37 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.01 requests/s, 2721.81 total tokens/s, 2561.70 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 18:32:45] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:32:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:32:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:32:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:32:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:32:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:32:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:32:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:32:49] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:32:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:32:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:32:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:32:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:32:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:32:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:32:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2933925) [2026-01-27 18:32:50] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2933925) [2026-01-27 18:32:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2933925) [2026-01-27 18:32:50] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2933925) [2026-01-27 18:32:50] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2933925) [2026-01-27 18:32:50] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2933925) [2026-01-27 18:32:50] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2933925) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2933925) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.74s/it]
(EngineCore_DP0 pid=2933925) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:40<00:00, 20.17s/it]
(EngineCore_DP0 pid=2933925) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:40<00:00, 20.11s/it]
(EngineCore_DP0 pid=2933925) 
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=2933925) 2026-01-27 18:33:37,049 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2933925) 2026-01-27 18:33:37,076 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 10719.62it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:12<25:46, 12.17s/it, est. speed input: 1.31 toks/s, output: 21.03 toks/s]
Processed prompts:  14%|        | 18/128 [00:12<00:53,  2.05it/s, est. speed input: 23.41 toks/s, output: 374.59 toks/s]
Processed prompts:  38%|      | 48/128 [00:12<00:11,  6.86it/s, est. speed input: 61.78 toks/s, output: 988.53 toks/s]
Processed prompts:  59%|    | 75/128 [00:12<00:04, 12.69it/s, est. speed input: 95.49 toks/s, output: 1527.76 toks/s]
Processed prompts:  77%|  | 98/128 [00:12<00:01, 19.34it/s, est. speed input: 123.64 toks/s, output: 1978.25 toks/s]
Processed prompts: 100%|| 128/128 [00:12<00:00, 19.34it/s, est. speed input: 160.27 toks/s, output: 2564.28 toks/s]
Processed prompts: 100%|| 128/128 [00:12<00:00, 10.02it/s, est. speed input: 160.27 toks/s, output: 2564.28 toks/s]
[rank0]:[W127 18:33:50.861647498 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 18:33:53
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-7B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:33:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:33:56 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2935102) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2935102) WARNING 01-27 18:34:48 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 12.85 requests/s, 3495.64 total tokens/s, 3290.02 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 18:33:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:33:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:33:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:33:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:33:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:33:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:33:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:33:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:33:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:33:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:33:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:33:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:33:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:33:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:34:00] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:34:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:34:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:34:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:34:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:34:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:34:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:34:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:34:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:34:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:34:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:34:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:34:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:34:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:01] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:01] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:01] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:01] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:01] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2935102) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2935102) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.94s/it]
(EngineCore_DP0 pid=2935102) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.92s/it]
(EngineCore_DP0 pid=2935102) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.93s/it]
(EngineCore_DP0 pid=2935102) 
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=2935102) 2026-01-27 18:34:47,509 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2935102) 2026-01-27 18:34:47,522 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 11788.09it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:17<1:13:28, 17.29s/it, est. speed input: 0.93 toks/s, output: 14.81 toks/s]
Processed prompts:   7%|         | 19/256 [00:17<02:35,  1.52it/s, est. speed input: 17.39 toks/s, output: 278.22 toks/s]
Processed prompts:  19%|        | 48/256 [00:17<00:43,  4.80it/s, est. speed input: 43.55 toks/s, output: 696.78 toks/s]
Processed prompts:  29%|       | 75/256 [00:17<00:20,  8.96it/s, est. speed input: 67.39 toks/s, output: 1078.21 toks/s]
Processed prompts:  38%|      | 98/256 [00:17<00:11, 13.67it/s, est. speed input: 87.26 toks/s, output: 1396.14 toks/s]
Processed prompts:  46%|     | 118/256 [00:18<00:07, 19.11it/s, est. speed input: 104.28 toks/s, output: 1668.56 toks/s]
Processed prompts:  53%|    | 136/256 [00:18<00:04, 25.41it/s, est. speed input: 119.35 toks/s, output: 1909.52 toks/s]
Processed prompts:  59%|    | 152/256 [00:18<00:03, 32.48it/s, est. speed input: 132.54 toks/s, output: 2120.56 toks/s]
Processed prompts:  65%|   | 167/256 [00:18<00:02, 40.64it/s, est. speed input: 144.74 toks/s, output: 2315.89 toks/s]
Processed prompts:  71%|   | 182/256 [00:18<00:01, 49.79it/s, est. speed input: 156.72 toks/s, output: 2507.47 toks/s]
Processed prompts:  77%|  | 196/256 [00:18<00:01, 57.06it/s, est. speed input: 167.44 toks/s, output: 2679.11 toks/s]
Processed prompts:  82%| | 209/256 [00:18<00:00, 64.96it/s, est. speed input: 177.40 toks/s, output: 2838.34 toks/s]
Processed prompts:  86%| | 221/256 [00:19<00:00, 66.60it/s, est. speed input: 185.93 toks/s, output: 2974.94 toks/s]
Processed prompts:  91%| | 232/256 [00:19<00:00, 66.20it/s, est. speed input: 193.47 toks/s, output: 3095.49 toks/s]
Processed prompts:  95%|| 242/256 [00:19<00:00, 62.77it/s, est. speed input: 199.88 toks/s, output: 3198.15 toks/s]
Processed prompts:  98%|| 251/256 [00:19<00:00, 51.18it/s, est. speed input: 204.38 toks/s, output: 3270.09 toks/s]
Processed prompts: 100%|| 256/256 [00:19<00:00, 51.18it/s, est. speed input: 205.86 toks/s, output: 3293.76 toks/s]
Processed prompts: 100%|| 256/256 [00:19<00:00, 12.87it/s, est. speed input: 205.86 toks/s, output: 3293.76 toks/s]
[rank0]:[W127 18:35:08.427917804 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 18:35:10
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-7B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:35:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:35:14 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2936377) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2936377) WARNING 01-27 18:36:04 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 12.90 requests/s, 3510.11 total tokens/s, 3303.64 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 18:35:14] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:35:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:35:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:35:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:35:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:35:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:35:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:35:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:35:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:35:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:35:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:35:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:35:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:35:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:35:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:35:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:18] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:18] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2936377) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2936377) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.76s/it]
(EngineCore_DP0 pid=2936377) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:38<00:00, 19.29s/it]
(EngineCore_DP0 pid=2936377) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:38<00:00, 19.36s/it]
(EngineCore_DP0 pid=2936377) 
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=2936377) 2026-01-27 18:36:03,976 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2936377) 2026-01-27 18:36:03,997 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 12019.88it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:28<4:06:32, 28.95s/it, est. speed input: 0.55 toks/s, output: 8.84 toks/s]
Processed prompts:   1%|          | 3/512 [00:29<1:04:39,  7.62s/it, est. speed input: 1.64 toks/s, output: 26.24 toks/s]
Processed prompts:   7%|         | 34/512 [00:29<03:33,  2.24it/s, est. speed input: 18.40 toks/s, output: 294.40 toks/s]
Processed prompts:  12%|        | 63/512 [00:29<01:31,  4.92it/s, est. speed input: 33.77 toks/s, output: 540.36 toks/s]
Processed prompts:  18%|        | 91/512 [00:30<00:50,  8.40it/s, est. speed input: 48.35 toks/s, output: 773.59 toks/s]
Processed prompts:  23%|       | 117/512 [00:30<00:31, 12.59it/s, est. speed input: 61.59 toks/s, output: 985.48 toks/s]
Processed prompts:  28%|       | 141/512 [00:30<00:20, 18.10it/s, est. speed input: 73.93 toks/s, output: 1182.95 toks/s]
Processed prompts:  32%|      | 164/512 [00:30<00:13, 25.16it/s, est. speed input: 85.68 toks/s, output: 1370.80 toks/s]
Processed prompts:  36%|      | 185/512 [00:30<00:09, 33.55it/s, est. speed input: 96.31 toks/s, output: 1540.94 toks/s]
Processed prompts:  40%|      | 205/512 [00:30<00:07, 43.58it/s, est. speed input: 106.35 toks/s, output: 1701.59 toks/s]
Processed prompts:  44%|     | 224/512 [00:30<00:05, 55.11it/s, est. speed input: 115.81 toks/s, output: 1852.97 toks/s]
Processed prompts:  50%|     | 258/512 [00:31<00:03, 75.38it/s, est. speed input: 132.47 toks/s, output: 2119.45 toks/s]
Processed prompts:  54%|    | 275/512 [00:31<00:02, 86.16it/s, est. speed input: 140.73 toks/s, output: 2251.76 toks/s]
Processed prompts:  59%|    | 302/512 [00:31<00:02, 104.37it/s, est. speed input: 153.80 toks/s, output: 2460.80 toks/s]
Processed prompts:  62%|   | 320/512 [00:31<00:01, 116.00it/s, est. speed input: 162.44 toks/s, output: 2599.03 toks/s]
Processed prompts:  66%|   | 338/512 [00:31<00:01, 113.64it/s, est. speed input: 170.67 toks/s, output: 2730.71 toks/s]
Processed prompts:  70%|   | 358/512 [00:31<00:01, 122.68it/s, est. speed input: 180.02 toks/s, output: 2880.33 toks/s]
Processed prompts:  73%|  | 376/512 [00:31<00:01, 126.70it/s, est. speed input: 188.31 toks/s, output: 3012.88 toks/s]
Processed prompts:  77%|  | 392/512 [00:32<00:00, 123.17it/s, est. speed input: 195.46 toks/s, output: 3127.35 toks/s]
Processed prompts:  79%|  | 406/512 [00:32<00:00, 122.73it/s, est. speed input: 201.72 toks/s, output: 3227.44 toks/s]
Processed prompts:  82%| | 420/512 [00:32<00:00, 123.43it/s, est. speed input: 207.95 toks/s, output: 3327.20 toks/s]
Processed prompts:  85%| | 434/512 [00:32<00:00, 112.33it/s, est. speed input: 213.86 toks/s, output: 3421.69 toks/s]
Processed prompts:  87%| | 446/512 [00:32<00:00, 107.73it/s, est. speed input: 218.92 toks/s, output: 3502.79 toks/s]
Processed prompts:  89%| | 458/512 [00:32<00:00, 98.04it/s, est. speed input: 223.76 toks/s, output: 3580.18 toks/s] 
Processed prompts:  92%|| 469/512 [00:32<00:00, 86.20it/s, est. speed input: 227.93 toks/s, output: 3646.94 toks/s]
Processed prompts:  94%|| 479/512 [00:33<00:00, 76.62it/s, est. speed input: 231.57 toks/s, output: 3705.09 toks/s]
Processed prompts:  95%|| 488/512 [00:33<00:00, 58.75it/s, est. speed input: 234.05 toks/s, output: 3744.74 toks/s]
Processed prompts:  97%|| 495/512 [00:33<00:00, 49.36it/s, est. speed input: 235.81 toks/s, output: 3772.90 toks/s]
Processed prompts:  98%|| 501/512 [00:39<00:02,  4.51it/s, est. speed input: 202.56 toks/s, output: 3241.02 toks/s]
Processed prompts: 100%|| 512/512 [00:39<00:00,  4.51it/s, est. speed input: 206.71 toks/s, output: 3307.28 toks/s]
Processed prompts: 100%|| 512/512 [00:39<00:00, 12.92it/s, est. speed input: 206.71 toks/s, output: 3307.28 toks/s]
[rank0]:[W127 18:36:44.696266674 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 20:12:32
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-14B-FP8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:12:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:12:39 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3036314) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3036314) WARNING 01-27 20:14:18 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 3.21 requests/s, 872.17 total tokens/s, 820.86 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 20:12:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:12:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:12:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:12:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:12:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:12:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:12:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:12:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:12:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:12:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:12:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:12:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:12:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:12:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:12:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:12:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3036314) [2026-01-27 20:12:44] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3036314) [2026-01-27 20:12:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3036314) [2026-01-27 20:12:44] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3036314) [2026-01-27 20:12:44] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3036314) [2026-01-27 20:12:44] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3036314) [2026-01-27 20:12:44] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3036314) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3036314) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.43s/it]
(EngineCore_DP0 pid=3036314) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:33<00:36, 18.38s/it]
(EngineCore_DP0 pid=3036314) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:54<00:19, 19.63s/it]
(EngineCore_DP0 pid=3036314) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:22<00:00, 22.67s/it]
(EngineCore_DP0 pid=3036314) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:22<00:00, 20.56s/it]
(EngineCore_DP0 pid=3036314) 
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=3036314) 2026-01-27 20:14:15,686 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3036314) 2026-01-27 20:14:15,740 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|         | 1/64 [00:00<00:10,  6.06it/s]
Adding requests:   5%|         | 3/64 [00:00<00:05, 11.09it/s]
Adding requests:   9%|         | 6/64 [00:00<00:03, 17.16it/s]
Adding requests:  16%|        | 10/64 [00:00<00:02, 23.85it/s]
Adding requests:  23%|       | 15/64 [00:00<00:01, 31.37it/s]
Adding requests:  36%|      | 23/64 [00:00<00:00, 46.20it/s]
Adding requests:  56%|    | 36/64 [00:00<00:00, 70.34it/s]
Adding requests:  81%| | 52/64 [00:00<00:00, 96.43it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 64.67it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:18<19:08, 18.23s/it, est. speed input: 0.88 toks/s, output: 14.04 toks/s]
Processed prompts:   8%|         | 5/64 [00:18<02:42,  2.75s/it, est. speed input: 4.35 toks/s, output: 69.64 toks/s]
Processed prompts:  16%|        | 10/64 [00:18<00:59,  1.11s/it, est. speed input: 8.63 toks/s, output: 138.15 toks/s]
Processed prompts:  27%|       | 17/64 [00:18<00:24,  1.95it/s, est. speed input: 14.56 toks/s, output: 233.02 toks/s]
Processed prompts:  75%|  | 48/64 [00:18<00:01,  8.41it/s, est. speed input: 40.79 toks/s, output: 652.70 toks/s]
Processed prompts: 100%|| 64/64 [00:18<00:00,  8.41it/s, est. speed input: 54.23 toks/s, output: 867.67 toks/s]
Processed prompts: 100%|| 64/64 [00:18<00:00,  3.39it/s, est. speed input: 54.23 toks/s, output: 867.67 toks/s]
[rank0]:[W127 20:14:39.414628487 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 20:14:54
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-14B-FP8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:14:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:14:59 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3038601) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3038601) WARNING 01-27 20:16:35 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.43 requests/s, 1477.59 total tokens/s, 1390.68 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 20:14:59] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:14:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:14:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:14:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:14:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:14:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:14:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:14:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:14:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:14:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:14:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:14:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:14:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:14:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:15:02] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:15:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:15:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:15:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:15:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:15:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:15:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:15:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:15:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:15:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:15:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:15:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:15:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:15:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3038601) [2026-01-27 20:15:03] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3038601) [2026-01-27 20:15:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3038601) [2026-01-27 20:15:03] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3038601) [2026-01-27 20:15:03] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3038601) [2026-01-27 20:15:03] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3038601) [2026-01-27 20:15:03] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3038601) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3038601) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.56s/it]
(EngineCore_DP0 pid=3038601) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.26s/it]
(EngineCore_DP0 pid=3038601) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.21s/it]
(EngineCore_DP0 pid=3038601) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.67s/it]
(EngineCore_DP0 pid=3038601) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.77s/it]
(EngineCore_DP0 pid=3038601) 
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=3038601) 2026-01-27 20:16:34,544 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3038601) 2026-01-27 20:16:34,572 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 9971.60it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:22<47:49, 22.60s/it, est. speed input: 0.71 toks/s, output: 11.33 toks/s]
Processed prompts:   5%|         | 6/128 [00:22<05:40,  2.79s/it, est. speed input: 4.23 toks/s, output: 67.65 toks/s]
Processed prompts:  17%|        | 22/128 [00:22<00:59,  1.77it/s, est. speed input: 15.42 toks/s, output: 246.76 toks/s]
Processed prompts:  29%|       | 37/128 [00:22<00:25,  3.63it/s, est. speed input: 25.82 toks/s, output: 413.06 toks/s]
Processed prompts:  50%|     | 64/128 [00:23<00:07,  8.21it/s, est. speed input: 44.34 toks/s, output: 709.48 toks/s]
Processed prompts:  61%|    | 78/128 [00:23<00:04, 11.36it/s, est. speed input: 53.77 toks/s, output: 860.39 toks/s]
Processed prompts:  78%|  | 100/128 [00:23<00:01, 17.83it/s, est. speed input: 68.41 toks/s, output: 1094.49 toks/s]
Processed prompts:  94%|| 120/128 [00:23<00:00, 25.24it/s, est. speed input: 81.53 toks/s, output: 1304.55 toks/s]
Processed prompts: 100%|| 128/128 [00:23<00:00, 25.24it/s, est. speed input: 86.97 toks/s, output: 1391.51 toks/s]
Processed prompts: 100%|| 128/128 [00:23<00:00,  5.44it/s, est. speed input: 86.97 toks/s, output: 1391.51 toks/s]
[rank0]:[W127 20:16:59.511859181 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 20:17:02
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-14B-FP8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:17:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:17:09 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3040609) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3040609) WARNING 01-27 20:18:53 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 6.81 requests/s, 1852.71 total tokens/s, 1743.73 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 20:17:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:17:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:17:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:17:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:17:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:17:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:17:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:17:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:17:12] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:17:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:17:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:17:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:17:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:17:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:17:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:17:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3040609) [2026-01-27 20:17:13] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3040609) [2026-01-27 20:17:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3040609) [2026-01-27 20:17:13] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3040609) [2026-01-27 20:17:13] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3040609) [2026-01-27 20:17:13] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3040609) [2026-01-27 20:17:13] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3040609) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3040609) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.39s/it]
(EngineCore_DP0 pid=3040609) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.11s/it]
(EngineCore_DP0 pid=3040609) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.13s/it]
(EngineCore_DP0 pid=3040609) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.92s/it]
(EngineCore_DP0 pid=3040609) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.89s/it]
(EngineCore_DP0 pid=3040609) 
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=3040609) 2026-01-27 20:18:46,240 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3040609) 2026-01-27 20:18:46,566 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:54,  4.70it/s]
Adding requests:   1%|          | 2/256 [00:00<00:38,  6.53it/s]
Adding requests:   2%|         | 5/256 [00:00<00:18, 13.63it/s]
Adding requests:   4%|         | 9/256 [00:00<00:12, 19.47it/s]
Adding requests:   5%|         | 13/256 [00:00<00:09, 24.76it/s]
Adding requests:   8%|         | 20/256 [00:00<00:06, 36.44it/s]
Adding requests:  12%|        | 30/256 [00:00<00:04, 53.64it/s]
Adding requests:  17%|        | 43/256 [00:01<00:02, 74.77it/s]
Adding requests:  23%|       | 59/256 [00:01<00:01, 98.97it/s]
Adding requests:  31%|       | 79/256 [00:01<00:01, 126.66it/s]
Adding requests:  41%|      | 104/256 [00:01<00:00, 161.31it/s]
Adding requests:  51%|     | 130/256 [00:01<00:00, 187.58it/s]
Adding requests:  61%|    | 156/256 [00:01<00:00, 141.59it/s]
Adding requests:  75%|  | 192/256 [00:01<00:00, 187.99it/s]
Adding requests:  91%|| 234/256 [00:01<00:00, 242.96it/s]
Adding requests: 100%|| 256/256 [00:01<00:00, 131.33it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:30<2:09:02, 30.36s/it, est. speed input: 0.53 toks/s, output: 8.43 toks/s]
Processed prompts:   1%|          | 2/256 [00:30<53:16, 12.59s/it, est. speed input: 1.05 toks/s, output: 16.78 toks/s] 
Processed prompts:   2%|         | 5/256 [00:30<15:05,  3.61s/it, est. speed input: 2.61 toks/s, output: 41.77 toks/s]
Processed prompts:   4%|         | 9/256 [00:30<06:22,  1.55s/it, est. speed input: 4.68 toks/s, output: 74.83 toks/s]
Processed prompts:   5%|         | 12/256 [00:30<03:57,  1.03it/s, est. speed input: 6.21 toks/s, output: 99.31 toks/s]
Processed prompts:   6%|         | 15/256 [00:31<02:36,  1.54it/s, est. speed input: 7.72 toks/s, output: 123.56 toks/s]
Processed prompts:   7%|         | 19/256 [00:31<01:35,  2.47it/s, est. speed input: 9.73 toks/s, output: 155.65 toks/s]
Processed prompts:  10%|         | 26/256 [00:31<00:48,  4.71it/s, est. speed input: 13.25 toks/s, output: 211.99 toks/s]
Processed prompts:  16%|        | 41/256 [00:31<00:19, 11.09it/s, est. speed input: 20.79 toks/s, output: 332.60 toks/s]
Processed prompts:  21%|       | 55/256 [00:31<00:10, 18.28it/s, est. speed input: 27.75 toks/s, output: 444.05 toks/s]
Processed prompts:  27%|       | 68/256 [00:31<00:07, 25.79it/s, est. speed input: 34.15 toks/s, output: 546.35 toks/s]
Processed prompts:  31%|      | 80/256 [00:31<00:05, 34.43it/s, est. speed input: 40.03 toks/s, output: 640.54 toks/s]
Processed prompts:  36%|      | 92/256 [00:32<00:03, 42.36it/s, est. speed input: 45.83 toks/s, output: 733.30 toks/s]
Processed prompts:  40%|      | 103/256 [00:32<00:03, 48.65it/s, est. speed input: 51.08 toks/s, output: 817.27 toks/s]
Processed prompts:  44%|     | 113/256 [00:32<00:02, 55.19it/s, est. speed input: 55.84 toks/s, output: 893.36 toks/s]
Processed prompts:  48%|     | 123/256 [00:32<00:02, 62.90it/s, est. speed input: 60.58 toks/s, output: 969.34 toks/s]
Processed prompts:  52%|    | 132/256 [00:32<00:01, 65.11it/s, est. speed input: 64.77 toks/s, output: 1036.28 toks/s]
Processed prompts:  58%|    | 148/256 [00:32<00:01, 66.75it/s, est. speed input: 72.11 toks/s, output: 1153.72 toks/s]
Processed prompts:  63%|   | 162/256 [00:33<00:01, 70.29it/s, est. speed input: 78.50 toks/s, output: 1256.04 toks/s]
Processed prompts:  68%|   | 174/256 [00:33<00:01, 70.09it/s, est. speed input: 83.88 toks/s, output: 1342.07 toks/s]
Processed prompts:  72%|  | 185/256 [00:33<00:01, 69.79it/s, est. speed input: 88.76 toks/s, output: 1420.10 toks/s]
Processed prompts:  76%|  | 195/256 [00:33<00:00, 68.04it/s, est. speed input: 93.11 toks/s, output: 1489.83 toks/s]
Processed prompts:  79%|  | 203/256 [00:33<00:00, 63.71it/s, est. speed input: 96.50 toks/s, output: 1543.95 toks/s]
Processed prompts:  82%| | 211/256 [00:33<00:00, 60.90it/s, est. speed input: 99.86 toks/s, output: 1597.73 toks/s]
Processed prompts:  85%| | 218/256 [00:33<00:00, 57.60it/s, est. speed input: 102.74 toks/s, output: 1643.79 toks/s]
Processed prompts:  88%| | 224/256 [00:34<00:00, 53.84it/s, est. speed input: 105.14 toks/s, output: 1682.27 toks/s]
Processed prompts:  90%| | 230/256 [00:34<00:00, 43.54it/s, est. speed input: 107.27 toks/s, output: 1716.27 toks/s]
Processed prompts:  92%|| 235/256 [00:34<00:00, 42.33it/s, est. speed input: 109.19 toks/s, output: 1746.98 toks/s]
Processed prompts:  94%|| 240/256 [00:34<00:00, 36.83it/s, est. speed input: 110.90 toks/s, output: 1774.32 toks/s]
Processed prompts:  95%|| 244/256 [00:34<00:00, 32.38it/s, est. speed input: 112.17 toks/s, output: 1794.71 toks/s]
Processed prompts:  97%|| 248/256 [00:35<00:00, 26.59it/s, est. speed input: 113.24 toks/s, output: 1811.79 toks/s]
Processed prompts:  98%|| 251/256 [00:35<00:00, 23.56it/s, est. speed input: 114.01 toks/s, output: 1824.09 toks/s]
Processed prompts:  99%|| 254/256 [00:35<00:00, 22.05it/s, est. speed input: 114.83 toks/s, output: 1837.23 toks/s]
Processed prompts: 100%|| 256/256 [00:35<00:00, 22.05it/s, est. speed input: 115.30 toks/s, output: 1844.87 toks/s]
Processed prompts: 100%|| 256/256 [00:35<00:00,  7.21it/s, est. speed input: 115.30 toks/s, output: 1844.87 toks/s]
[rank0]:[W127 20:19:31.838537969 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 20:19:48
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-14B-FP8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:19:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:19:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3043084) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3043084) WARNING 01-27 20:21:27 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 6.86 requests/s, 1865.01 total tokens/s, 1755.30 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 20:19:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:19:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:19:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:19:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:19:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:19:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:19:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:19:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:19:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:19:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:19:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:19:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:19:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:19:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:19:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:19:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3043084) [2026-01-27 20:19:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3043084) [2026-01-27 20:19:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3043084) [2026-01-27 20:19:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3043084) [2026-01-27 20:19:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3043084) [2026-01-27 20:19:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3043084) [2026-01-27 20:19:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3043084) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3043084) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.44s/it]
(EngineCore_DP0 pid=3043084) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.19s/it]
(EngineCore_DP0 pid=3043084) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.03s/it]
(EngineCore_DP0 pid=3043084) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.75s/it]
(EngineCore_DP0 pid=3043084) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.78s/it]
(EngineCore_DP0 pid=3043084) 
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=3043084) 2026-01-27 20:21:26,203 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3043084) 2026-01-27 20:21:26,240 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 11886.48it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:54<7:42:03, 54.25s/it, est. speed input: 0.29 toks/s, output: 4.72 toks/s]
Processed prompts:   0%|          | 2/512 [00:54<3:13:22, 22.75s/it, est. speed input: 0.58 toks/s, output: 9.32 toks/s]
Processed prompts:   6%|         | 33/512 [00:55<06:52,  1.16it/s, est. speed input: 9.50 toks/s, output: 151.92 toks/s]
Processed prompts:  12%|        | 63/512 [00:56<02:51,  2.62it/s, est. speed input: 17.93 toks/s, output: 286.81 toks/s]
Processed prompts:  18%|        | 91/512 [00:56<01:35,  4.43it/s, est. speed input: 25.62 toks/s, output: 409.97 toks/s]
Processed prompts:  23%|       | 117/512 [00:57<00:59,  6.60it/s, est. speed input: 32.61 toks/s, output: 521.84 toks/s]
Processed prompts:  28%|       | 141/512 [00:57<00:39,  9.50it/s, est. speed input: 39.16 toks/s, output: 626.52 toks/s]
Processed prompts:  32%|      | 164/512 [00:57<00:26, 13.22it/s, est. speed input: 45.38 toks/s, output: 726.07 toks/s]
Processed prompts:  36%|      | 185/512 [00:58<00:18, 17.63it/s, est. speed input: 51.01 toks/s, output: 816.20 toks/s]
Processed prompts:  40%|      | 205/512 [00:58<00:13, 22.98it/s, est. speed input: 56.34 toks/s, output: 901.46 toks/s]
Processed prompts:  44%|     | 224/512 [00:58<00:09, 29.27it/s, est. speed input: 61.37 toks/s, output: 981.94 toks/s]
Processed prompts:  47%|     | 242/512 [00:58<00:07, 36.28it/s, est. speed input: 66.10 toks/s, output: 1057.63 toks/s]
Processed prompts:  50%|     | 258/512 [00:58<00:05, 42.55it/s, est. speed input: 70.25 toks/s, output: 1123.96 toks/s]
Processed prompts:  53%|    | 273/512 [00:58<00:04, 49.02it/s, est. speed input: 74.12 toks/s, output: 1185.86 toks/s]
Processed prompts:  56%|    | 288/512 [00:59<00:03, 57.70it/s, est. speed input: 78.01 toks/s, output: 1248.18 toks/s]
Processed prompts:  59%|    | 302/512 [00:59<00:03, 63.87it/s, est. speed input: 81.59 toks/s, output: 1305.46 toks/s]
Processed prompts:  62%|   | 315/512 [00:59<00:02, 68.63it/s, est. speed input: 84.89 toks/s, output: 1358.24 toks/s]
Processed prompts:  64%|   | 327/512 [00:59<00:02, 72.10it/s, est. speed input: 87.91 toks/s, output: 1406.63 toks/s]
Processed prompts:  66%|   | 338/512 [00:59<00:02, 73.54it/s, est. speed input: 90.66 toks/s, output: 1450.52 toks/s]
Processed prompts:  68%|   | 348/512 [00:59<00:02, 76.08it/s, est. speed input: 93.16 toks/s, output: 1490.50 toks/s]
Processed prompts:  70%|   | 358/512 [00:59<00:02, 76.10it/s, est. speed input: 95.62 toks/s, output: 1529.97 toks/s]
Processed prompts:  72%|  | 367/512 [01:00<00:01, 78.24it/s, est. speed input: 97.85 toks/s, output: 1565.68 toks/s]
Processed prompts:  73%|  | 376/512 [01:00<00:01, 76.59it/s, est. speed input: 100.05 toks/s, output: 1600.75 toks/s]
Processed prompts:  77%|  | 392/512 [01:00<00:01, 73.10it/s, est. speed input: 103.90 toks/s, output: 1662.40 toks/s]
Processed prompts:  79%|  | 406/512 [01:00<00:01, 71.84it/s, est. speed input: 107.25 toks/s, output: 1716.04 toks/s]
Processed prompts:  82%| | 418/512 [01:00<00:01, 67.87it/s, est. speed input: 110.06 toks/s, output: 1760.93 toks/s]
Processed prompts:  84%| | 428/512 [01:00<00:01, 65.36it/s, est. speed input: 112.38 toks/s, output: 1798.05 toks/s]
Processed prompts:  85%| | 435/512 [01:01<00:01, 64.02it/s, est. speed input: 113.99 toks/s, output: 1823.92 toks/s]
Processed prompts:  86%| | 442/512 [01:01<00:01, 58.51it/s, est. speed input: 115.54 toks/s, output: 1848.58 toks/s]
Processed prompts:  88%| | 450/512 [01:01<00:01, 52.95it/s, est. speed input: 117.26 toks/s, output: 1876.21 toks/s]
Processed prompts:  89%| | 456/512 [01:01<00:01, 49.44it/s, est. speed input: 118.54 toks/s, output: 1896.62 toks/s]
Processed prompts:  90%| | 462/512 [01:01<00:01, 47.36it/s, est. speed input: 119.82 toks/s, output: 1917.11 toks/s]
Processed prompts:  91%|| 468/512 [01:01<00:01, 43.86it/s, est. speed input: 121.05 toks/s, output: 1936.79 toks/s]
Processed prompts:  92%|| 473/512 [01:01<00:00, 42.04it/s, est. speed input: 122.08 toks/s, output: 1953.22 toks/s]
Processed prompts:  93%|| 478/512 [01:02<00:00, 35.88it/s, est. speed input: 122.97 toks/s, output: 1967.51 toks/s]
Processed prompts:  94%|| 482/512 [01:02<00:00, 33.38it/s, est. speed input: 123.70 toks/s, output: 1979.24 toks/s]
Processed prompts:  95%|| 486/512 [01:02<00:00, 26.42it/s, est. speed input: 124.23 toks/s, output: 1987.67 toks/s]
Processed prompts:  96%|| 489/512 [01:02<00:00, 23.41it/s, est. speed input: 124.63 toks/s, output: 1994.06 toks/s]
Processed prompts:  96%|| 492/512 [01:02<00:00, 21.27it/s, est. speed input: 125.03 toks/s, output: 2000.40 toks/s]
Processed prompts:  97%|| 495/512 [01:03<00:00, 19.98it/s, est. speed input: 125.43 toks/s, output: 2006.92 toks/s]
Processed prompts:  97%|| 498/512 [01:14<00:14,  1.01s/it, est. speed input: 107.10 toks/s, output: 1713.66 toks/s]
Processed prompts:  98%|| 501/512 [01:14<00:08,  1.34it/s, est. speed input: 107.58 toks/s, output: 1721.32 toks/s]
Processed prompts: 100%|| 511/512 [01:14<00:00,  3.00it/s, est. speed input: 109.56 toks/s, output: 1752.94 toks/s]
Processed prompts: 100%|| 512/512 [01:14<00:00,  3.00it/s, est. speed input: 109.77 toks/s, output: 1756.36 toks/s]
Processed prompts: 100%|| 512/512 [01:14<00:00,  6.86it/s, est. speed input: 109.77 toks/s, output: 1756.36 toks/s]
[rank0]:[W127 20:22:42.366219832 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

