
========== M=16 ==========
Time: 2026-01-25 18:36:50
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-INT8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:36:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:36:53 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=285054) 
(EngineCore_DP0 pid=285054) 
(EngineCore_DP0 pid=285054) ================================================================
(EngineCore_DP0 pid=285054) Internal Triton PTX codegen error
(EngineCore_DP0 pid=285054) `ptxas` stderr:
(EngineCore_DP0 pid=285054) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=285054) 
(EngineCore_DP0 pid=285054) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp1wjb5lgj.ptx -o /tmp/tmp1wjb5lgj.ptx.o
(EngineCore_DP0 pid=285054) 
(EngineCore_DP0 pid=285054) 
(EngineCore_DP0 pid=285054) //
(EngineCore_DP0 pid=285054) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=285054) //
(EngineCore_DP0 pid=285054) 
(EngineCore_DP0 pid=285054) .version 8.7
(EngineCore_DP0 pid=285054) .target sm_121a
(EngineCore_DP0 pid=285054) .address_size 64
(EngineCore_DP0 pid=285054) 
(EngineCore_DP0 pid=285054) 	// .globl	_quant_only_int8_kernel // -- Begin function _quant_only_int8_kernel
(EngineCore_DP0 pid=285054) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=285054)                                         // @_quant_only_int8_kernel
(EngineCore_DP0 pid=285054) .visible .entry _quant_only_int8_kernel(
(EngineCore_DP0 pid=285054) 	.param .u64 .ptr .global .align 1 _quant_only_int8_kernel_param_0,
(EngineCore_DP0 pid=285054) 	.param .u64 .ptr .global .align 1 _quant_only_int8_kernel_param_1,
(EngineCore_DP0 pid=285054) 	.param .u64 .ptr .global .align 1 _quant_only_int8_kernel_param_2,
(EngineCore_DP0 pid=285054) 	.param .u32 _quant_only_int8_kernel_param_3,
(EngineCore_DP0 pid=285054) 	.param .u32 _quant_only_int8_kernel_param_4,
(EngineCore_DP0 pid=285054) 	.param .u32 _quant_only_int8_kernel_param_5,
(EngineCore_DP0 pid=285054) 	.param .u64 .ptr .global .align 1 _quant_only_int8_kernel_param_6,
(EngineCore_DP0 pid=285054) 	.param .u64 .ptr .global .align 1 _quant_only_int8_kernel_param_7
(EngineCore_DP0 pid=285054) )
(EngineCore_DP0 pid=285054) .reqntid 256
(EngineCore_DP0 pid=285054) {
(EngineCore_DP0 pid=285054) 	.reg .pred 	%p<10>;
(EngineCore_DP0 pid=285054) 	.reg .b16 	%rs<96>;
(EngineCore_DP0 pid=285054) 	.reg .b32 	%r<189>;
(EngineCore_DP0 pid=285054) 	.reg .b64 	%rd<14>;
(EngineCore_DP0 pid=285054) 	.loc	1 147 0                         // quant_only_tuned_Llama3.2-1B.py:147:0
(EngineCore_DP0 pid=285054) $L__func_begin0:
(EngineCore_DP0 pid=285054) 	.loc	1 147 0                         // quant_only_tuned_Llama3.2-1B.py:147:0
(EngineCore_DP0 pid=285054) 
(EngineCore_DP0 pid=285054) // %bb.0:
(EngineCore_DP0 pid=285054) 	ld.param.b64 	%rd7, [_quant_only_int8_kernel_param_0];
(EngineCore_DP0 pid=285054) 	ld.param.b64 	%rd8, [_quant_only_int8_kernel_param_1];
(EngineCore_DP0 pid=285054) $L__tmp0:
(EngineCore_DP0 pid=285054) 	.loc	1 154 24                        // quant_only_tuned_Llama3.2-1B.py:154:24
(EngineCore_DP0 pid=285054) 	mov.u32 	%r44, %ctaid.x;
(EngineCore_DP0 pid=285054) 	ld.param.b32 	%r45, [_quant_only_int8_kernel_param_4];
(EngineCore_DP0 pid=285054) 	.loc	1 159 30                        // quant_only_tuned_Llama3.2-1B.py:159:30
(EngineCore_DP0 pid=285054) 	mul.lo.s32 	%r46, %r45, %r44;
(EngineCore_DP0 pid=285054) 	ld.param.b64 	%rd9, [_quant_only_int8_kernel_param_2];
(EngineCore_DP0 pid=285054) 	.loc	1 159 24                        // quant_only_tuned_Llama3.2-1B.py:159:24
(EngineCore_DP0 pid=285054) 	mad.wide.s32 	%rd10, %r46, 2, %rd7;
(EngineCore_DP0 pid=285054) 	ld.param.b32 	%r47, [_quant_only_int8_kernel_param_5];
(EngineCore_DP0 pid=285054) 	.loc	1 160 34                        // quant_only_tuned_Llama3.2-1B.py:160:34
(EngineCore_DP0 pid=285054) 	mul.lo.s32 	%r48, %r47, %r44;
(EngineCore_DP0 pid=285054) 	.loc	1 160 28                        // quant_only_tuned_Llama3.2-1B.py:160:28
(EngineCore_DP0 pid=285054) 	cvt.s64.s32 	%rd11, %r48;
(EngineCore_DP0 pid=285054) 	add.s64 	%rd12, %rd8, %rd11;
(EngineCore_DP0 pid=285054) 	.loc	1 166 40                        // quant_only_tuned_Llama3.2-1B.py:166:40
(EngineCore_DP0 pid=285054) 	mov.u32 	%r49, %tid.x;
(EngineCore_DP0 pid=285054) 	and.b32 	%r50, %r49, 31;
(EngineCore_DP0 pid=285054) 	shl.b32 	%r51, %r49, 4;
(EngineCore_DP0 pid=285054) 	and.b32 	%r52, %r51, 4080;
(EngineCore_DP0 pid=285054) 	.loc	1 167 26                        // quant_only_tuned_Llama3.2-1B.py:167:26
(EngineCore_DP0 pid=285054) 	setp.lt.u32 	%p1, %r52, 2048;
(EngineCore_DP0 pid=285054) 	.loc	1 168 36                        // quant_only_tuned_Llama3.2-1B.py:168:36
(EngineCore_DP0 pid=285054) 	cvt.u64.u32 	%rd13, %r52;
(EngineCore_DP0 pid=285054) 	mad.wide.u32 	%rd1, %r52, 2, %rd10;
(EngineCore_DP0 pid=285054) 	add.s64 	%rd2, %rd1, 16;
(EngineCore_DP0 pid=285054) 	mov.b32 	%r5, 0;
(EngineCore_DP0 pid=285054) 	.loc	1 168 24                        // quant_only_tuned_Llama3.2-1B.py:168:24
(EngineCore_DP0 pid=285054) 	// begin inline asm
(EngineCore_DP0 pid=285054) 	mov.u32 %r1, %r5;
(EngineCore_DP0 pid=285054) 	mov.u32 %r2, %r5;
(EngineCore_DP0 pid=285054) 	mov.u32 %r3, %r5;
(EngineCore_DP0 pid=285054) 	mov.u32 %r4, %r5;
(EngineCore_DP0 pid=285054) 	@%p1 ld.global.v4.b32 { %r1, %r2, %r3, %r4 }, [ %rd1 + 0 ];
(EngineCore_DP0 pid=285054) 	// end inline asm
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs1, %rs2}, %r1;
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs3, %rs4}, %r2;
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs5, %rs6}, %r3;
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs7, %rs8}, %r4;
(EngineCore_DP0 pid=285054) 	// begin inline asm
(EngineCore_DP0 pid=285054) 	mov.u32 %r9, %r5;
(EngineCore_DP0 pid=285054) 	mov.u32 %r10, %r5;
(EngineCore_DP0 pid=285054) 	mov.u32 %r11, %r5;
(EngineCore_DP0 pid=285054) 	mov.u32 %r12, %r5;
(EngineCore_DP0 pid=285054) 	@%p1 ld.global.v4.b32 { %r9, %r10, %r11, %r12 }, [ %rd2 + 0 ];
(EngineCore_DP0 pid=285054) 	// end inline asm
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs9, %rs10}, %r9;
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs11, %rs12}, %r10;
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs13, %rs14}, %r11;
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs15, %rs16}, %r12;
(EngineCore_DP0 pid=285054) 	.loc	1 169 50                        // quant_only_tuned_Llama3.2-1B.py:169:50
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs17, %rs1;
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs18, %rs2;
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs19, %rs3;
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs20, %rs4;
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs21, %rs5;
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs22, %rs6;
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs23, %rs7;
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs24, %rs8;
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs25, %rs9;
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs26, %rs10;
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs27, %rs11;
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs28, %rs12;
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs29, %rs13;
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs30, %rs14;
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs31, %rs15;
(EngineCore_DP0 pid=285054) 	abs.bf16 	%rs32, %rs16;
(EngineCore_DP0 pid=285054) $L__tmp1:
(EngineCore_DP0 pid=285054) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	max.bf16 	%rs33, %rs17, %rs18;
(EngineCore_DP0 pid=285054) 	max.bf16 	%rs34, %rs33, %rs19;
(EngineCore_DP0 pid=285054) 	max.bf16 	%rs35, %rs34, %rs20;
(EngineCore_DP0 pid=285054) 	max.bf16 	%rs36, %rs35, %rs21;
(EngineCore_DP0 pid=285054) 	max.bf16 	%rs37, %rs36, %rs22;
(EngineCore_DP0 pid=285054) 	max.bf16 	%rs38, %rs37, %rs23;
(EngineCore_DP0 pid=285054) 	max.bf16 	%rs39, %rs38, %rs24;
(EngineCore_DP0 pid=285054) 	max.bf16 	%rs40, %rs39, %rs25;
(EngineCore_DP0 pid=285054) 	max.bf16 	%rs41, %rs40, %rs26;
(EngineCore_DP0 pid=285054) 	max.bf16 	%rs42, %rs41, %rs27;
(EngineCore_DP0 pid=285054) 	max.bf16 	%rs43, %rs42, %rs28;
(EngineCore_DP0 pid=285054) 	max.bf16 	%rs44, %rs43, %rs29;
(EngineCore_DP0 pid=285054) 	max.bf16 	%rs45, %rs44, %rs30;
(EngineCore_DP0 pid=285054) 	max.bf16 	%rs46, %rs45, %rs31;
(EngineCore_DP0 pid=285054) 	max.bf16 	%rs47, %rs46, %rs32;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r53, %rs47;
(EngineCore_DP0 pid=285054) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	shfl.sync.bfly.b32 	%r54, %r53, 16, 31, -1;
(EngineCore_DP0 pid=285054) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	max.f32 	%r55, %r53, %r54;
(EngineCore_DP0 pid=285054) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	shfl.sync.bfly.b32 	%r56, %r55, 8, 31, -1;
(EngineCore_DP0 pid=285054) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	max.f32 	%r57, %r55, %r56;
(EngineCore_DP0 pid=285054) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	shfl.sync.bfly.b32 	%r58, %r57, 4, 31, -1;
(EngineCore_DP0 pid=285054) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	max.f32 	%r59, %r57, %r58;
(EngineCore_DP0 pid=285054) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	shfl.sync.bfly.b32 	%r60, %r59, 2, 31, -1;
(EngineCore_DP0 pid=285054) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	max.f32 	%r61, %r59, %r60;
(EngineCore_DP0 pid=285054) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	shfl.sync.bfly.b32 	%r62, %r61, 1, 31, -1;
(EngineCore_DP0 pid=285054) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	max.f32 	%r18, %r61, %r62;
(EngineCore_DP0 pid=285054) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	setp.eq.b32 	%p3, %r50, 0;
(EngineCore_DP0 pid=285054) 	shr.u32 	%r63, %r49, 3;
(EngineCore_DP0 pid=285054) 	and.b32 	%r64, %r63, 28;
(EngineCore_DP0 pid=285054) 	mov.b32 	%r65, global_smem;
(EngineCore_DP0 pid=285054) 	add.s32 	%r17, %r65, %r64;
(EngineCore_DP0 pid=285054) 	// begin inline asm
(EngineCore_DP0 pid=285054) 	@%p3 st.shared.b32 [ %r17 + 0 ], %r18;
(EngineCore_DP0 pid=285054) 	// end inline asm
(EngineCore_DP0 pid=285054) 	bar.sync 	0;
(EngineCore_DP0 pid=285054) 	setp.lt.u32 	%p4, %r49, 8;
(EngineCore_DP0 pid=285054) 	shl.b32 	%r66, %r49, 2;
(EngineCore_DP0 pid=285054) 	add.s32 	%r20, %r65, %r66;
(EngineCore_DP0 pid=285054) 	// begin inline asm
(EngineCore_DP0 pid=285054) 	@%p4 ld.shared.b32 %r19, [ %r20 + 0 ];
(EngineCore_DP0 pid=285054) 	// end inline asm
(EngineCore_DP0 pid=285054) 	shfl.sync.bfly.b32 	%r67, %r19, 4, 31, -1;
(EngineCore_DP0 pid=285054) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	max.f32 	%r68, %r19, %r67;
(EngineCore_DP0 pid=285054) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	shfl.sync.bfly.b32 	%r69, %r68, 2, 31, -1;
(EngineCore_DP0 pid=285054) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	max.f32 	%r70, %r68, %r69;
(EngineCore_DP0 pid=285054) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	shfl.sync.bfly.b32 	%r71, %r70, 1, 31, -1;
(EngineCore_DP0 pid=285054) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	max.f32 	%r22, %r70, %r71;
(EngineCore_DP0 pid=285054) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285054) 	setp.eq.b32 	%p5, %r49, 0;
(EngineCore_DP0 pid=285054) 	// begin inline asm
(EngineCore_DP0 pid=285054) 	@%p5 st.shared.b32 [ %r20 + 0 ], %r22;
(EngineCore_DP0 pid=285054) 	// end inline asm
(EngineCore_DP0 pid=285054) 	bar.sync 	0;
(EngineCore_DP0 pid=285054) 	ld.shared.b32 	%r72, [global_smem];
(EngineCore_DP0 pid=285054) $L__tmp2:
(EngineCore_DP0 pid=285054) 	.loc	1 172 32                        // quant_only_tuned_Llama3.2-1B.py:172:32
(EngineCore_DP0 pid=285054) 	max.f32 	%r73, %r72, 0f2B8CBCCC;
(EngineCore_DP0 pid=285054) 	mov.b32 	%r74, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	.loc	1 173 32                        // quant_only_tuned_Llama3.2-1B.py:173:32
(EngineCore_DP0 pid=285054) 	div.full.f32 	%r75, %r73, %r74;
(EngineCore_DP0 pid=285054) 	.loc	1 173 42                        // quant_only_tuned_Llama3.2-1B.py:173:42
(EngineCore_DP0 pid=285054) 	max.f32 	%r23, %r75, 0f37810204;
(EngineCore_DP0 pid=285054) 	.loc	1 174 27                        // quant_only_tuned_Llama3.2-1B.py:174:27
(EngineCore_DP0 pid=285054) 	div.full.f32 	%r76, %r74, %r73;
(EngineCore_DP0 pid=285054) 	.loc	1 176 25                        // quant_only_tuned_Llama3.2-1B.py:176:25
(EngineCore_DP0 pid=285054) 	mad.wide.u32 	%rd3, %r44, 4, %rd9;
(EngineCore_DP0 pid=285054) 	.loc	1 176 30                        // quant_only_tuned_Llama3.2-1B.py:176:30
(EngineCore_DP0 pid=285054) 	// begin inline asm
(EngineCore_DP0 pid=285054) 	@%p5 st.global.b32 [ %rd3 + 0 ], { %r23 };
(EngineCore_DP0 pid=285054) 	// end inline asm
(EngineCore_DP0 pid=285054) 	.loc	1 182 24                        // quant_only_tuned_Llama3.2-1B.py:182:24
(EngineCore_DP0 pid=285054) 	// begin inline asm
(EngineCore_DP0 pid=285054) 	mov.u32 %r24, %r5;
(EngineCore_DP0 pid=285054) 	mov.u32 %r25, %r5;
(EngineCore_DP0 pid=285054) 	mov.u32 %r26, %r5;
(EngineCore_DP0 pid=285054) 	mov.u32 %r27, %r5;
(EngineCore_DP0 pid=285054) 	@%p1 ld.global.v4.b32 { %r24, %r25, %r26, %r27 }, [ %rd1 + 0 ];
(EngineCore_DP0 pid=285054) 	// end inline asm
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs48, %rs49}, %r24;
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs50, %rs51}, %r25;
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs52, %rs53}, %r26;
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs54, %rs55}, %r27;
(EngineCore_DP0 pid=285054) 	// begin inline asm
(EngineCore_DP0 pid=285054) 	mov.u32 %r32, %r5;
(EngineCore_DP0 pid=285054) 	mov.u32 %r33, %r5;
(EngineCore_DP0 pid=285054) 	mov.u32 %r34, %r5;
(EngineCore_DP0 pid=285054) 	mov.u32 %r35, %r5;
(EngineCore_DP0 pid=285054) 	@%p1 ld.global.v4.b32 { %r32, %r33, %r34, %r35 }, [ %rd2 + 0 ];
(EngineCore_DP0 pid=285054) 	// end inline asm
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs56, %rs57}, %r32;
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs58, %rs59}, %r33;
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs60, %rs61}, %r34;
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs62, %rs63}, %r35;
(EngineCore_DP0 pid=285054) 	.loc	1 182 71                        // quant_only_tuned_Llama3.2-1B.py:182:71
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r77, %rs48;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r78, %rs49;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r79, %rs50;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r80, %rs51;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r81, %rs52;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r82, %rs53;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r83, %rs54;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r84, %rs55;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r85, %rs56;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r86, %rs57;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r87, %rs58;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r88, %rs59;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r89, %rs60;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r90, %rs61;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r91, %rs62;
(EngineCore_DP0 pid=285054) 	cvt.f32.bf16 	%r92, %rs63;
(EngineCore_DP0 pid=285054) 	.loc	1 183 62                        // quant_only_tuned_Llama3.2-1B.py:183:62
(EngineCore_DP0 pid=285054) 	mul.f32 	%r93, %r76, %r77;
(EngineCore_DP0 pid=285054) 	mul.f32 	%r94, %r76, %r78;
(EngineCore_DP0 pid=285054) 	mul.f32 	%r95, %r76, %r79;
(EngineCore_DP0 pid=285054) 	mul.f32 	%r96, %r76, %r80;
(EngineCore_DP0 pid=285054) 	mul.f32 	%r97, %r76, %r81;
(EngineCore_DP0 pid=285054) 	mul.f32 	%r98, %r76, %r82;
(EngineCore_DP0 pid=285054) 	mul.f32 	%r99, %r76, %r83;
(EngineCore_DP0 pid=285054) 	mul.f32 	%r100, %r76, %r84;
(EngineCore_DP0 pid=285054) 	mul.f32 	%r101, %r76, %r85;
(EngineCore_DP0 pid=285054) 	mul.f32 	%r102, %r76, %r86;
(EngineCore_DP0 pid=285054) 	mul.f32 	%r103, %r76, %r87;
(EngineCore_DP0 pid=285054) 	mul.f32 	%r104, %r76, %r88;
(EngineCore_DP0 pid=285054) 	mul.f32 	%r105, %r76, %r89;
(EngineCore_DP0 pid=285054) 	mul.f32 	%r106, %r76, %r90;
(EngineCore_DP0 pid=285054) 	mul.f32 	%r107, %r76, %r91;
(EngineCore_DP0 pid=285054) 	mul.f32 	%r108, %r76, %r92;
(EngineCore_DP0 pid=285054) 	.loc	1 183 54                        // quant_only_tuned_Llama3.2-1B.py:183:54
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r109, %r93;
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r110, %r94;
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r111, %r95;
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r112, %r96;
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r113, %r97;
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r114, %r98;
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r115, %r99;
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r116, %r100;
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r117, %r101;
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r118, %r102;
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r119, %r103;
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r120, %r104;
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r121, %r105;
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r122, %r106;
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r123, %r107;
(EngineCore_DP0 pid=285054) 	cvt.rni.f32.f32 	%r124, %r108;
(EngineCore_DP0 pid=285054) 	.loc	1 184 31                        // quant_only_tuned_Llama3.2-1B.py:184:31
(EngineCore_DP0 pid=285054) 	add.s64 	%rd6, %rd12, %rd13;
(EngineCore_DP0 pid=285054) 	.loc	1 183 82                        // quant_only_tuned_Llama3.2-1B.py:183:82
(EngineCore_DP0 pid=285054) 	max.f32 	%r125, %r112, 0fC3000000;
(EngineCore_DP0 pid=285054) 	max.f32 	%r126, %r111, 0fC3000000;
(EngineCore_DP0 pid=285054) 	max.f32 	%r127, %r110, 0fC3000000;
(EngineCore_DP0 pid=285054) 	max.f32 	%r128, %r109, 0fC3000000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r129, %r128, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r130, %r127, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r131, %r126, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r132, %r125, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	.loc	1 184 48                        // quant_only_tuned_Llama3.2-1B.py:184:48
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs64, %r132;
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs65, %r131;
(EngineCore_DP0 pid=285054) 	mov.b32 	%r133, {%rs65, %rs64};
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs66, %rs67}, %r133;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r134, %rs67;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r135, %rs66;
(EngineCore_DP0 pid=285054) 	prmt.b32 	%r136, %r135, %r134, 0x3340U;
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs68, %r130;
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs69, %r129;
(EngineCore_DP0 pid=285054) 	mov.b32 	%r137, {%rs69, %rs68};
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs70, %rs71}, %r137;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r138, %rs71;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r139, %rs70;
(EngineCore_DP0 pid=285054) 	prmt.b32 	%r140, %r139, %r138, 0x3340U;
(EngineCore_DP0 pid=285054) 	prmt.b32 	%r40, %r140, %r136, 0x5410U;
(EngineCore_DP0 pid=285054) 	.loc	1 183 82                        // quant_only_tuned_Llama3.2-1B.py:183:82
(EngineCore_DP0 pid=285054) 	max.f32 	%r141, %r116, 0fC3000000;
(EngineCore_DP0 pid=285054) 	max.f32 	%r142, %r115, 0fC3000000;
(EngineCore_DP0 pid=285054) 	max.f32 	%r143, %r114, 0fC3000000;
(EngineCore_DP0 pid=285054) 	max.f32 	%r144, %r113, 0fC3000000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r145, %r144, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r146, %r143, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r147, %r142, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r148, %r141, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	.loc	1 184 48                        // quant_only_tuned_Llama3.2-1B.py:184:48
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs72, %r148;
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs73, %r147;
(EngineCore_DP0 pid=285054) 	mov.b32 	%r149, {%rs73, %rs72};
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs74, %rs75}, %r149;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r150, %rs75;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r151, %rs74;
(EngineCore_DP0 pid=285054) 	prmt.b32 	%r152, %r151, %r150, 0x3340U;
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs76, %r146;
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs77, %r145;
(EngineCore_DP0 pid=285054) 	mov.b32 	%r153, {%rs77, %rs76};
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs78, %rs79}, %r153;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r154, %rs79;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r155, %rs78;
(EngineCore_DP0 pid=285054) 	prmt.b32 	%r156, %r155, %r154, 0x3340U;
(EngineCore_DP0 pid=285054) 	prmt.b32 	%r41, %r156, %r152, 0x5410U;
(EngineCore_DP0 pid=285054) 	.loc	1 183 82                        // quant_only_tuned_Llama3.2-1B.py:183:82
(EngineCore_DP0 pid=285054) 	max.f32 	%r157, %r120, 0fC3000000;
(EngineCore_DP0 pid=285054) 	max.f32 	%r158, %r119, 0fC3000000;
(EngineCore_DP0 pid=285054) 	max.f32 	%r159, %r118, 0fC3000000;
(EngineCore_DP0 pid=285054) 	max.f32 	%r160, %r117, 0fC3000000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r161, %r160, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r162, %r159, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r163, %r158, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r164, %r157, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	.loc	1 184 48                        // quant_only_tuned_Llama3.2-1B.py:184:48
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs80, %r164;
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs81, %r163;
(EngineCore_DP0 pid=285054) 	mov.b32 	%r165, {%rs81, %rs80};
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs82, %rs83}, %r165;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r166, %rs83;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r167, %rs82;
(EngineCore_DP0 pid=285054) 	prmt.b32 	%r168, %r167, %r166, 0x3340U;
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs84, %r162;
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs85, %r161;
(EngineCore_DP0 pid=285054) 	mov.b32 	%r169, {%rs85, %rs84};
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs86, %rs87}, %r169;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r170, %rs87;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r171, %rs86;
(EngineCore_DP0 pid=285054) 	prmt.b32 	%r172, %r171, %r170, 0x3340U;
(EngineCore_DP0 pid=285054) 	prmt.b32 	%r42, %r172, %r168, 0x5410U;
(EngineCore_DP0 pid=285054) 	.loc	1 183 82                        // quant_only_tuned_Llama3.2-1B.py:183:82
(EngineCore_DP0 pid=285054) 	max.f32 	%r173, %r124, 0fC3000000;
(EngineCore_DP0 pid=285054) 	max.f32 	%r174, %r123, 0fC3000000;
(EngineCore_DP0 pid=285054) 	max.f32 	%r175, %r122, 0fC3000000;
(EngineCore_DP0 pid=285054) 	max.f32 	%r176, %r121, 0fC3000000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r177, %r176, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r178, %r175, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r179, %r174, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	min.f32 	%r180, %r173, 0f42FE0000;
(EngineCore_DP0 pid=285054) 	.loc	1 184 48                        // quant_only_tuned_Llama3.2-1B.py:184:48
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs88, %r180;
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs89, %r179;
(EngineCore_DP0 pid=285054) 	mov.b32 	%r181, {%rs89, %rs88};
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs90, %rs91}, %r181;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r182, %rs91;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r183, %rs90;
(EngineCore_DP0 pid=285054) 	prmt.b32 	%r184, %r183, %r182, 0x3340U;
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs92, %r178;
(EngineCore_DP0 pid=285054) 	cvt.rzi.s16.f32 	%rs93, %r177;
(EngineCore_DP0 pid=285054) 	mov.b32 	%r185, {%rs93, %rs92};
(EngineCore_DP0 pid=285054) 	mov.b32 	{%rs94, %rs95}, %r185;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r186, %rs95;
(EngineCore_DP0 pid=285054) 	cvt.u32.u16 	%r187, %rs94;
(EngineCore_DP0 pid=285054) 	prmt.b32 	%r188, %r187, %r186, 0x3340U;
(EngineCore_DP0 pid=285054) 	prmt.b32 	%r43, %r188, %r184, 0x5410U;
(EngineCore_DP0 pid=285054) 	.loc	1 184 39                        // quant_only_tuned_Llama3.2-1B.py:184:39
(EngineCore_DP0 pid=285054) 	// begin inline asm
(EngineCore_DP0 pid=285054) 	@%p1 st.global.v4.b32 [ %rd6 + 0 ], { %r40, %r41, %r42, %r43 };
(EngineCore_DP0 pid=285054) 	// end inline asm
(EngineCore_DP0 pid=285054) 	.loc	1 179 4                         // quant_only_tuned_Llama3.2-1B.py:179:4
(EngineCore_DP0 pid=285054) 	ret;
(EngineCore_DP0 pid=285054) $L__tmp3:
(EngineCore_DP0 pid=285054) $L__func_end0:
(EngineCore_DP0 pid=285054)                                         // -- End function
(EngineCore_DP0 pid=285054) }
(EngineCore_DP0 pid=285054) 	.file	1 "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=285054) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=285054) 	.section	.debug_abbrev
(EngineCore_DP0 pid=285054) 	{
(EngineCore_DP0 pid=285054) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=285054) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=285054) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=285054) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=285054) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=285054) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=285054) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=285054) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=285054) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=285054) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=285054) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=285054) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=285054) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=285054) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=285054) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=285054) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=285054) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=285054) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=285054) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=285054) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=285054) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=285054) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=285054) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=285054) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=285054) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=285054) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=285054) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=285054) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=285054) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=285054) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=285054) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=285054) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=285054) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=285054) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=285054) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=285054) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=285054) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=285054) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=285054) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=285054) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=285054) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=285054) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=285054) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=285054) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=285054) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=285054) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=285054) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=285054) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=285054) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=285054) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=285054) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=285054) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=285054) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=285054) 	}
(EngineCore_DP0 pid=285054) 	.section	.debug_info
(EngineCore_DP0 pid=285054) 	{
(EngineCore_DP0 pid=285054) .b32 214                                // Length of Unit
(EngineCore_DP0 pid=285054) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=285054) .b8 0
(EngineCore_DP0 pid=285054) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=285054) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=285054) .b8 1                                   // Abbrev [1] 0xb:0xcf DW_TAG_compile_unit
(EngineCore_DP0 pid=285054) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=285054) .b8 114
(EngineCore_DP0 pid=285054) .b8 105
(EngineCore_DP0 pid=285054) .b8 116
(EngineCore_DP0 pid=285054) .b8 111
(EngineCore_DP0 pid=285054) .b8 110
(EngineCore_DP0 pid=285054) .b8 0
(EngineCore_DP0 pid=285054) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=285054) .b8 0
(EngineCore_DP0 pid=285054) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=285054) .b8 117
(EngineCore_DP0 pid=285054) .b8 97
(EngineCore_DP0 pid=285054) .b8 110
(EngineCore_DP0 pid=285054) .b8 116
(EngineCore_DP0 pid=285054) .b8 95
(EngineCore_DP0 pid=285054) .b8 111
(EngineCore_DP0 pid=285054) .b8 110
(EngineCore_DP0 pid=285054) .b8 108
(EngineCore_DP0 pid=285054) .b8 121
(EngineCore_DP0 pid=285054) .b8 95
(EngineCore_DP0 pid=285054) .b8 116
(EngineCore_DP0 pid=285054) .b8 117
(EngineCore_DP0 pid=285054) .b8 110
(EngineCore_DP0 pid=285054) .b8 101
(EngineCore_DP0 pid=285054) .b8 100
(EngineCore_DP0 pid=285054) .b8 95
(EngineCore_DP0 pid=285054) .b8 76
(EngineCore_DP0 pid=285054) .b8 108
(EngineCore_DP0 pid=285054) .b8 97
(EngineCore_DP0 pid=285054) .b8 109
(EngineCore_DP0 pid=285054) .b8 97
(EngineCore_DP0 pid=285054) .b8 51
(EngineCore_DP0 pid=285054) .b8 46
(EngineCore_DP0 pid=285054) .b8 50
(EngineCore_DP0 pid=285054) .b8 45
(EngineCore_DP0 pid=285054) .b8 49
(EngineCore_DP0 pid=285054) .b8 66
(EngineCore_DP0 pid=285054) .b8 46
(EngineCore_DP0 pid=285054) .b8 112
(EngineCore_DP0 pid=285054) .b8 121
(EngineCore_DP0 pid=285054) .b8 0
(EngineCore_DP0 pid=285054) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=285054) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=285054) .b8 114
(EngineCore_DP0 pid=285054) .b8 111
(EngineCore_DP0 pid=285054) .b8 111
(EngineCore_DP0 pid=285054) .b8 116
(EngineCore_DP0 pid=285054) .b8 47
(EngineCore_DP0 pid=285054) .b8 118
(EngineCore_DP0 pid=285054) .b8 108
(EngineCore_DP0 pid=285054) .b8 108
(EngineCore_DP0 pid=285054) .b8 109
(EngineCore_DP0 pid=285054) .b8 98
(EngineCore_DP0 pid=285054) .b8 101
(EngineCore_DP0 pid=285054) .b8 110
(EngineCore_DP0 pid=285054) .b8 99
(EngineCore_DP0 pid=285054) .b8 104
(EngineCore_DP0 pid=285054) .b8 47
(EngineCore_DP0 pid=285054) .b8 115
(EngineCore_DP0 pid=285054) .b8 108
(EngineCore_DP0 pid=285054) .b8 105
(EngineCore_DP0 pid=285054) .b8 100
(EngineCore_DP0 pid=285054) .b8 101
(EngineCore_DP0 pid=285054) .b8 115
(EngineCore_DP0 pid=285054) .b8 112
(EngineCore_DP0 pid=285054) .b8 97
(EngineCore_DP0 pid=285054) .b8 114
(EngineCore_DP0 pid=285054) .b8 115
(EngineCore_DP0 pid=285054) .b8 101
(EngineCore_DP0 pid=285054) .b8 47
(EngineCore_DP0 pid=285054) .b8 99
(EngineCore_DP0 pid=285054) .b8 115
(EngineCore_DP0 pid=285054) .b8 114
(EngineCore_DP0 pid=285054) .b8 99
(EngineCore_DP0 pid=285054) .b8 47
(EngineCore_DP0 pid=285054) .b8 113
(EngineCore_DP0 pid=285054) .b8 117
(EngineCore_DP0 pid=285054) .b8 97
(EngineCore_DP0 pid=285054) .b8 110
(EngineCore_DP0 pid=285054) .b8 116
(EngineCore_DP0 pid=285054) .b8 95
(EngineCore_DP0 pid=285054) .b8 111
(EngineCore_DP0 pid=285054) .b8 110
(EngineCore_DP0 pid=285054) .b8 108
(EngineCore_DP0 pid=285054) .b8 121
(EngineCore_DP0 pid=285054) .b8 95
(EngineCore_DP0 pid=285054) .b8 116
(EngineCore_DP0 pid=285054) .b8 114
(EngineCore_DP0 pid=285054) .b8 105
(EngineCore_DP0 pid=285054) .b8 116
(EngineCore_DP0 pid=285054) .b8 111
(EngineCore_DP0 pid=285054) .b8 110
(EngineCore_DP0 pid=285054) .b8 47
(EngineCore_DP0 pid=285054) .b8 98
(EngineCore_DP0 pid=285054) .b8 117
(EngineCore_DP0 pid=285054) .b8 105
(EngineCore_DP0 pid=285054) .b8 108
(EngineCore_DP0 pid=285054) .b8 100
(EngineCore_DP0 pid=285054) .b8 47
(EngineCore_DP0 pid=285054) .b8 71
(EngineCore_DP0 pid=285054) .b8 66
(EngineCore_DP0 pid=285054) .b8 49
(EngineCore_DP0 pid=285054) .b8 48
(EngineCore_DP0 pid=285054) .b8 95
(EngineCore_DP0 pid=285054) .b8 99
(EngineCore_DP0 pid=285054) .b8 99
(EngineCore_DP0 pid=285054) .b8 49
(EngineCore_DP0 pid=285054) .b8 50
(EngineCore_DP0 pid=285054) .b8 49
(EngineCore_DP0 pid=285054) .b8 95
(EngineCore_DP0 pid=285054) .b8 112
(EngineCore_DP0 pid=285054) .b8 121
(EngineCore_DP0 pid=285054) .b8 51
(EngineCore_DP0 pid=285054) .b8 49
(EngineCore_DP0 pid=285054) .b8 50
(EngineCore_DP0 pid=285054) .b8 95
(EngineCore_DP0 pid=285054) .b8 99
(EngineCore_DP0 pid=285054) .b8 117
(EngineCore_DP0 pid=285054) .b8 49
(EngineCore_DP0 pid=285054) .b8 50
(EngineCore_DP0 pid=285054) .b8 57
(EngineCore_DP0 pid=285054) .b8 95
(EngineCore_DP0 pid=285054) .b8 97
(EngineCore_DP0 pid=285054) .b8 97
(EngineCore_DP0 pid=285054) .b8 114
(EngineCore_DP0 pid=285054) .b8 99
(EngineCore_DP0 pid=285054) .b8 104
(EngineCore_DP0 pid=285054) .b8 54
(EngineCore_DP0 pid=285054) .b8 52
(EngineCore_DP0 pid=285054) .b8 0
(EngineCore_DP0 pid=285054) .b8 2                                   // Abbrev [2] 0x91:0x1a DW_TAG_subprogram
(EngineCore_DP0 pid=285054) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=285054) .b8 113
(EngineCore_DP0 pid=285054) .b8 117
(EngineCore_DP0 pid=285054) .b8 97
(EngineCore_DP0 pid=285054) .b8 110
(EngineCore_DP0 pid=285054) .b8 116
(EngineCore_DP0 pid=285054) .b8 95
(EngineCore_DP0 pid=285054) .b8 111
(EngineCore_DP0 pid=285054) .b8 110
(EngineCore_DP0 pid=285054) .b8 108
(EngineCore_DP0 pid=285054) .b8 121
(EngineCore_DP0 pid=285054) .b8 95
(EngineCore_DP0 pid=285054) .b8 105
(EngineCore_DP0 pid=285054) .b8 110
(EngineCore_DP0 pid=285054) .b8 116
(EngineCore_DP0 pid=285054) .b8 56
(EngineCore_DP0 pid=285054) .b8 95
(EngineCore_DP0 pid=285054) .b8 107
(EngineCore_DP0 pid=285054) .b8 101
(EngineCore_DP0 pid=285054) .b8 114
(EngineCore_DP0 pid=285054) .b8 110
(EngineCore_DP0 pid=285054) .b8 101
(EngineCore_DP0 pid=285054) .b8 108
(EngineCore_DP0 pid=285054) .b8 0
(EngineCore_DP0 pid=285054) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=285054) .b8 3                                   // Abbrev [3] 0xab:0x2e DW_TAG_subprogram
(EngineCore_DP0 pid=285054) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=285054) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=285054) .b32 145                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=285054) .b8 4                                   // Abbrev [4] 0xc0:0x18 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=285054) .b32 145                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=285054) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=285054) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=285054) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=285054) .b8 169                                 // DW_AT_call_line
(EngineCore_DP0 pid=285054) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=285054) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=285054) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=285054) 	}
(EngineCore_DP0 pid=285054) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=285054) 
(EngineCore_DP0 pid=285054) ================================================================
(EngineCore_DP0 pid=285054) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=285054) 
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmp1wjb5lgj.ptx', '-o', '/tmp/tmp1wjb5lgj.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866] 
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866] 
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 760, in apply_weights
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 546, in apply
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 194, in cuBLASLt_INT8_linear
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     qinput, scale_a_pad = quant_only_int8_kernel(input, model_name)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 369, in quant_only_int8_kernel
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return torch.ops.slidesparse.quant_only_int8(input, model_name)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 544, in _quant_only_int8_impl
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return fn(input)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]            ^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py", line 206, in quant_only_int8_triton
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     _quant_only_int8_kernel[(M,)](
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866] 
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp1wjb5lgj.ptx -o /tmp/tmp1wjb5lgj.ptx.o
(EngineCore_DP0 pid=285054) ERROR 01-25 18:37:07 [core.py:866] 

STDERR:
[2026-01-25 18:36:53] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:36:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:36:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:36:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:36:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:36:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:36:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:36:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:36:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:36:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:36:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:36:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:36:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:36:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:36:57] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:36:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:36:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:36:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:36:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:36:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:36:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:36:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:36:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:36:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:36:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:36:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:36:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:36:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=285054) [2026-01-25 18:36:58] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=285054) [2026-01-25 18:36:58] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=285054) [2026-01-25 18:36:58] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=285054) [2026-01-25 18:36:58] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=285054) [2026-01-25 18:36:58] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=285054) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=285054) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.53s/it]
(EngineCore_DP0 pid=285054) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.53s/it]
(EngineCore_DP0 pid=285054) 
(EngineCore_DP0 pid=285054) Process EngineCore_DP0:
(EngineCore_DP0 pid=285054) Traceback (most recent call last):
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=285054)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=285054)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=285054)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=285054) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmp1wjb5lgj.ptx', '-o', '/tmp/tmp1wjb5lgj.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=285054) 
(EngineCore_DP0 pid=285054) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=285054) 
(EngineCore_DP0 pid=285054) Traceback (most recent call last):
(EngineCore_DP0 pid=285054)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=285054)     self.run()
(EngineCore_DP0 pid=285054)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=285054)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=285054)     raise e
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=285054)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=285054)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=285054)     super().__init__(
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=285054)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=285054)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=285054)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=285054)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=285054)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=285054)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=285054)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=285054)     return func(*args, **kwargs)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=285054)     return func(*args, **kwargs)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=285054)     self.model_runner.profile_run()
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=285054)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=285054)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=285054)     return func(*args, **kwargs)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=285054)     outputs = self.model(
(EngineCore_DP0 pid=285054)               ^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285054)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285054)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=285054)     model_output = self.model(
(EngineCore_DP0 pid=285054)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=285054)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=285054)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=285054)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285054)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285054)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=285054)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=285054)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285054)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285054)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=285054)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=285054)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285054)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285054)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=285054)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=285054)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=285054)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 760, in apply_weights
(EngineCore_DP0 pid=285054)     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 546, in apply
(EngineCore_DP0 pid=285054)     return self._linear_fn(
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 194, in cuBLASLt_INT8_linear
(EngineCore_DP0 pid=285054)     qinput, scale_a_pad = quant_only_int8_kernel(input, model_name)
(EngineCore_DP0 pid=285054)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/slidesparse/core/kernels.py", line 369, in quant_only_int8_kernel
(EngineCore_DP0 pid=285054)     return torch.ops.slidesparse.quant_only_int8(input, model_name)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=285054)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/slidesparse/core/kernels.py", line 544, in _quant_only_int8_impl
(EngineCore_DP0 pid=285054)     return fn(input)
(EngineCore_DP0 pid=285054)            ^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py", line 206, in quant_only_int8_triton
(EngineCore_DP0 pid=285054)     _quant_only_int8_kernel[(M,)](
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=285054)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=285054)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=285054)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=285054)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=285054)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=285054)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=285054)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=285054)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=285054)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=285054)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285054)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=285054)     raise PTXASError(error)
(EngineCore_DP0 pid=285054) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=285054) `ptxas` stderr:
(EngineCore_DP0 pid=285054) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=285054) 
(EngineCore_DP0 pid=285054) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp1wjb5lgj.ptx -o /tmp/tmp1wjb5lgj.ptx.o
(EngineCore_DP0 pid=285054) 
[rank0]:[W125 18:37:08.393760229 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=128 ==========
Time: 2026-01-25 18:37:09
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:37:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:37:13 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=285519) 
(EngineCore_DP0 pid=285519) 
(EngineCore_DP0 pid=285519) ================================================================
(EngineCore_DP0 pid=285519) Internal Triton PTX codegen error
(EngineCore_DP0 pid=285519) `ptxas` stderr:
(EngineCore_DP0 pid=285519) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=285519) 
(EngineCore_DP0 pid=285519) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpn3ugkz7v.ptx -o /tmp/tmpn3ugkz7v.ptx.o
(EngineCore_DP0 pid=285519) 
(EngineCore_DP0 pid=285519) 
(EngineCore_DP0 pid=285519) //
(EngineCore_DP0 pid=285519) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=285519) //
(EngineCore_DP0 pid=285519) 
(EngineCore_DP0 pid=285519) .version 8.7
(EngineCore_DP0 pid=285519) .target sm_121a
(EngineCore_DP0 pid=285519) .address_size 64
(EngineCore_DP0 pid=285519) 
(EngineCore_DP0 pid=285519) 	// .globl	_quant_only_int8_kernel // -- Begin function _quant_only_int8_kernel
(EngineCore_DP0 pid=285519) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=285519)                                         // @_quant_only_int8_kernel
(EngineCore_DP0 pid=285519) .visible .entry _quant_only_int8_kernel(
(EngineCore_DP0 pid=285519) 	.param .u64 .ptr .global .align 1 _quant_only_int8_kernel_param_0,
(EngineCore_DP0 pid=285519) 	.param .u64 .ptr .global .align 1 _quant_only_int8_kernel_param_1,
(EngineCore_DP0 pid=285519) 	.param .u64 .ptr .global .align 1 _quant_only_int8_kernel_param_2,
(EngineCore_DP0 pid=285519) 	.param .u32 _quant_only_int8_kernel_param_3,
(EngineCore_DP0 pid=285519) 	.param .u32 _quant_only_int8_kernel_param_4,
(EngineCore_DP0 pid=285519) 	.param .u32 _quant_only_int8_kernel_param_5,
(EngineCore_DP0 pid=285519) 	.param .u64 .ptr .global .align 1 _quant_only_int8_kernel_param_6,
(EngineCore_DP0 pid=285519) 	.param .u64 .ptr .global .align 1 _quant_only_int8_kernel_param_7
(EngineCore_DP0 pid=285519) )
(EngineCore_DP0 pid=285519) .reqntid 256
(EngineCore_DP0 pid=285519) {
(EngineCore_DP0 pid=285519) 	.reg .pred 	%p<10>;
(EngineCore_DP0 pid=285519) 	.reg .b16 	%rs<96>;
(EngineCore_DP0 pid=285519) 	.reg .b32 	%r<189>;
(EngineCore_DP0 pid=285519) 	.reg .b64 	%rd<14>;
(EngineCore_DP0 pid=285519) 	.loc	1 147 0                         // quant_only_tuned_Llama3.2-1B.py:147:0
(EngineCore_DP0 pid=285519) $L__func_begin0:
(EngineCore_DP0 pid=285519) 	.loc	1 147 0                         // quant_only_tuned_Llama3.2-1B.py:147:0
(EngineCore_DP0 pid=285519) 
(EngineCore_DP0 pid=285519) // %bb.0:
(EngineCore_DP0 pid=285519) 	ld.param.b64 	%rd7, [_quant_only_int8_kernel_param_0];
(EngineCore_DP0 pid=285519) 	ld.param.b64 	%rd8, [_quant_only_int8_kernel_param_1];
(EngineCore_DP0 pid=285519) $L__tmp0:
(EngineCore_DP0 pid=285519) 	.loc	1 154 24                        // quant_only_tuned_Llama3.2-1B.py:154:24
(EngineCore_DP0 pid=285519) 	mov.u32 	%r44, %ctaid.x;
(EngineCore_DP0 pid=285519) 	ld.param.b32 	%r45, [_quant_only_int8_kernel_param_4];
(EngineCore_DP0 pid=285519) 	.loc	1 159 30                        // quant_only_tuned_Llama3.2-1B.py:159:30
(EngineCore_DP0 pid=285519) 	mul.lo.s32 	%r46, %r45, %r44;
(EngineCore_DP0 pid=285519) 	ld.param.b64 	%rd9, [_quant_only_int8_kernel_param_2];
(EngineCore_DP0 pid=285519) 	.loc	1 159 24                        // quant_only_tuned_Llama3.2-1B.py:159:24
(EngineCore_DP0 pid=285519) 	mad.wide.s32 	%rd10, %r46, 2, %rd7;
(EngineCore_DP0 pid=285519) 	ld.param.b32 	%r47, [_quant_only_int8_kernel_param_5];
(EngineCore_DP0 pid=285519) 	.loc	1 160 34                        // quant_only_tuned_Llama3.2-1B.py:160:34
(EngineCore_DP0 pid=285519) 	mul.lo.s32 	%r48, %r47, %r44;
(EngineCore_DP0 pid=285519) 	.loc	1 160 28                        // quant_only_tuned_Llama3.2-1B.py:160:28
(EngineCore_DP0 pid=285519) 	cvt.s64.s32 	%rd11, %r48;
(EngineCore_DP0 pid=285519) 	add.s64 	%rd12, %rd8, %rd11;
(EngineCore_DP0 pid=285519) 	.loc	1 166 40                        // quant_only_tuned_Llama3.2-1B.py:166:40
(EngineCore_DP0 pid=285519) 	mov.u32 	%r49, %tid.x;
(EngineCore_DP0 pid=285519) 	and.b32 	%r50, %r49, 31;
(EngineCore_DP0 pid=285519) 	shl.b32 	%r51, %r49, 4;
(EngineCore_DP0 pid=285519) 	and.b32 	%r52, %r51, 4080;
(EngineCore_DP0 pid=285519) 	.loc	1 167 26                        // quant_only_tuned_Llama3.2-1B.py:167:26
(EngineCore_DP0 pid=285519) 	setp.lt.u32 	%p1, %r52, 2048;
(EngineCore_DP0 pid=285519) 	.loc	1 168 36                        // quant_only_tuned_Llama3.2-1B.py:168:36
(EngineCore_DP0 pid=285519) 	cvt.u64.u32 	%rd13, %r52;
(EngineCore_DP0 pid=285519) 	mad.wide.u32 	%rd1, %r52, 2, %rd10;
(EngineCore_DP0 pid=285519) 	add.s64 	%rd2, %rd1, 16;
(EngineCore_DP0 pid=285519) 	mov.b32 	%r5, 0;
(EngineCore_DP0 pid=285519) 	.loc	1 168 24                        // quant_only_tuned_Llama3.2-1B.py:168:24
(EngineCore_DP0 pid=285519) 	// begin inline asm
(EngineCore_DP0 pid=285519) 	mov.u32 %r1, %r5;
(EngineCore_DP0 pid=285519) 	mov.u32 %r2, %r5;
(EngineCore_DP0 pid=285519) 	mov.u32 %r3, %r5;
(EngineCore_DP0 pid=285519) 	mov.u32 %r4, %r5;
(EngineCore_DP0 pid=285519) 	@%p1 ld.global.v4.b32 { %r1, %r2, %r3, %r4 }, [ %rd1 + 0 ];
(EngineCore_DP0 pid=285519) 	// end inline asm
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs1, %rs2}, %r1;
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs3, %rs4}, %r2;
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs5, %rs6}, %r3;
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs7, %rs8}, %r4;
(EngineCore_DP0 pid=285519) 	// begin inline asm
(EngineCore_DP0 pid=285519) 	mov.u32 %r9, %r5;
(EngineCore_DP0 pid=285519) 	mov.u32 %r10, %r5;
(EngineCore_DP0 pid=285519) 	mov.u32 %r11, %r5;
(EngineCore_DP0 pid=285519) 	mov.u32 %r12, %r5;
(EngineCore_DP0 pid=285519) 	@%p1 ld.global.v4.b32 { %r9, %r10, %r11, %r12 }, [ %rd2 + 0 ];
(EngineCore_DP0 pid=285519) 	// end inline asm
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs9, %rs10}, %r9;
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs11, %rs12}, %r10;
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs13, %rs14}, %r11;
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs15, %rs16}, %r12;
(EngineCore_DP0 pid=285519) 	.loc	1 169 50                        // quant_only_tuned_Llama3.2-1B.py:169:50
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs17, %rs1;
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs18, %rs2;
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs19, %rs3;
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs20, %rs4;
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs21, %rs5;
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs22, %rs6;
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs23, %rs7;
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs24, %rs8;
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs25, %rs9;
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs26, %rs10;
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs27, %rs11;
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs28, %rs12;
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs29, %rs13;
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs30, %rs14;
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs31, %rs15;
(EngineCore_DP0 pid=285519) 	abs.bf16 	%rs32, %rs16;
(EngineCore_DP0 pid=285519) $L__tmp1:
(EngineCore_DP0 pid=285519) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	max.bf16 	%rs33, %rs17, %rs18;
(EngineCore_DP0 pid=285519) 	max.bf16 	%rs34, %rs33, %rs19;
(EngineCore_DP0 pid=285519) 	max.bf16 	%rs35, %rs34, %rs20;
(EngineCore_DP0 pid=285519) 	max.bf16 	%rs36, %rs35, %rs21;
(EngineCore_DP0 pid=285519) 	max.bf16 	%rs37, %rs36, %rs22;
(EngineCore_DP0 pid=285519) 	max.bf16 	%rs38, %rs37, %rs23;
(EngineCore_DP0 pid=285519) 	max.bf16 	%rs39, %rs38, %rs24;
(EngineCore_DP0 pid=285519) 	max.bf16 	%rs40, %rs39, %rs25;
(EngineCore_DP0 pid=285519) 	max.bf16 	%rs41, %rs40, %rs26;
(EngineCore_DP0 pid=285519) 	max.bf16 	%rs42, %rs41, %rs27;
(EngineCore_DP0 pid=285519) 	max.bf16 	%rs43, %rs42, %rs28;
(EngineCore_DP0 pid=285519) 	max.bf16 	%rs44, %rs43, %rs29;
(EngineCore_DP0 pid=285519) 	max.bf16 	%rs45, %rs44, %rs30;
(EngineCore_DP0 pid=285519) 	max.bf16 	%rs46, %rs45, %rs31;
(EngineCore_DP0 pid=285519) 	max.bf16 	%rs47, %rs46, %rs32;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r53, %rs47;
(EngineCore_DP0 pid=285519) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	shfl.sync.bfly.b32 	%r54, %r53, 16, 31, -1;
(EngineCore_DP0 pid=285519) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	max.f32 	%r55, %r53, %r54;
(EngineCore_DP0 pid=285519) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	shfl.sync.bfly.b32 	%r56, %r55, 8, 31, -1;
(EngineCore_DP0 pid=285519) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	max.f32 	%r57, %r55, %r56;
(EngineCore_DP0 pid=285519) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	shfl.sync.bfly.b32 	%r58, %r57, 4, 31, -1;
(EngineCore_DP0 pid=285519) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	max.f32 	%r59, %r57, %r58;
(EngineCore_DP0 pid=285519) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	shfl.sync.bfly.b32 	%r60, %r59, 2, 31, -1;
(EngineCore_DP0 pid=285519) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	max.f32 	%r61, %r59, %r60;
(EngineCore_DP0 pid=285519) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	shfl.sync.bfly.b32 	%r62, %r61, 1, 31, -1;
(EngineCore_DP0 pid=285519) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	max.f32 	%r18, %r61, %r62;
(EngineCore_DP0 pid=285519) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	setp.eq.b32 	%p3, %r50, 0;
(EngineCore_DP0 pid=285519) 	shr.u32 	%r63, %r49, 3;
(EngineCore_DP0 pid=285519) 	and.b32 	%r64, %r63, 28;
(EngineCore_DP0 pid=285519) 	mov.b32 	%r65, global_smem;
(EngineCore_DP0 pid=285519) 	add.s32 	%r17, %r65, %r64;
(EngineCore_DP0 pid=285519) 	// begin inline asm
(EngineCore_DP0 pid=285519) 	@%p3 st.shared.b32 [ %r17 + 0 ], %r18;
(EngineCore_DP0 pid=285519) 	// end inline asm
(EngineCore_DP0 pid=285519) 	bar.sync 	0;
(EngineCore_DP0 pid=285519) 	setp.lt.u32 	%p4, %r49, 8;
(EngineCore_DP0 pid=285519) 	shl.b32 	%r66, %r49, 2;
(EngineCore_DP0 pid=285519) 	add.s32 	%r20, %r65, %r66;
(EngineCore_DP0 pid=285519) 	// begin inline asm
(EngineCore_DP0 pid=285519) 	@%p4 ld.shared.b32 %r19, [ %r20 + 0 ];
(EngineCore_DP0 pid=285519) 	// end inline asm
(EngineCore_DP0 pid=285519) 	shfl.sync.bfly.b32 	%r67, %r19, 4, 31, -1;
(EngineCore_DP0 pid=285519) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	max.f32 	%r68, %r19, %r67;
(EngineCore_DP0 pid=285519) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	shfl.sync.bfly.b32 	%r69, %r68, 2, 31, -1;
(EngineCore_DP0 pid=285519) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	max.f32 	%r70, %r68, %r69;
(EngineCore_DP0 pid=285519) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	shfl.sync.bfly.b32 	%r71, %r70, 1, 31, -1;
(EngineCore_DP0 pid=285519) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	max.f32 	%r22, %r70, %r71;
(EngineCore_DP0 pid=285519) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285519) 	setp.eq.b32 	%p5, %r49, 0;
(EngineCore_DP0 pid=285519) 	// begin inline asm
(EngineCore_DP0 pid=285519) 	@%p5 st.shared.b32 [ %r20 + 0 ], %r22;
(EngineCore_DP0 pid=285519) 	// end inline asm
(EngineCore_DP0 pid=285519) 	bar.sync 	0;
(EngineCore_DP0 pid=285519) 	ld.shared.b32 	%r72, [global_smem];
(EngineCore_DP0 pid=285519) $L__tmp2:
(EngineCore_DP0 pid=285519) 	.loc	1 172 32                        // quant_only_tuned_Llama3.2-1B.py:172:32
(EngineCore_DP0 pid=285519) 	max.f32 	%r73, %r72, 0f2B8CBCCC;
(EngineCore_DP0 pid=285519) 	mov.b32 	%r74, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	.loc	1 173 32                        // quant_only_tuned_Llama3.2-1B.py:173:32
(EngineCore_DP0 pid=285519) 	div.full.f32 	%r75, %r73, %r74;
(EngineCore_DP0 pid=285519) 	.loc	1 173 42                        // quant_only_tuned_Llama3.2-1B.py:173:42
(EngineCore_DP0 pid=285519) 	max.f32 	%r23, %r75, 0f37810204;
(EngineCore_DP0 pid=285519) 	.loc	1 174 27                        // quant_only_tuned_Llama3.2-1B.py:174:27
(EngineCore_DP0 pid=285519) 	div.full.f32 	%r76, %r74, %r73;
(EngineCore_DP0 pid=285519) 	.loc	1 176 25                        // quant_only_tuned_Llama3.2-1B.py:176:25
(EngineCore_DP0 pid=285519) 	mad.wide.u32 	%rd3, %r44, 4, %rd9;
(EngineCore_DP0 pid=285519) 	.loc	1 176 30                        // quant_only_tuned_Llama3.2-1B.py:176:30
(EngineCore_DP0 pid=285519) 	// begin inline asm
(EngineCore_DP0 pid=285519) 	@%p5 st.global.b32 [ %rd3 + 0 ], { %r23 };
(EngineCore_DP0 pid=285519) 	// end inline asm
(EngineCore_DP0 pid=285519) 	.loc	1 182 24                        // quant_only_tuned_Llama3.2-1B.py:182:24
(EngineCore_DP0 pid=285519) 	// begin inline asm
(EngineCore_DP0 pid=285519) 	mov.u32 %r24, %r5;
(EngineCore_DP0 pid=285519) 	mov.u32 %r25, %r5;
(EngineCore_DP0 pid=285519) 	mov.u32 %r26, %r5;
(EngineCore_DP0 pid=285519) 	mov.u32 %r27, %r5;
(EngineCore_DP0 pid=285519) 	@%p1 ld.global.v4.b32 { %r24, %r25, %r26, %r27 }, [ %rd1 + 0 ];
(EngineCore_DP0 pid=285519) 	// end inline asm
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs48, %rs49}, %r24;
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs50, %rs51}, %r25;
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs52, %rs53}, %r26;
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs54, %rs55}, %r27;
(EngineCore_DP0 pid=285519) 	// begin inline asm
(EngineCore_DP0 pid=285519) 	mov.u32 %r32, %r5;
(EngineCore_DP0 pid=285519) 	mov.u32 %r33, %r5;
(EngineCore_DP0 pid=285519) 	mov.u32 %r34, %r5;
(EngineCore_DP0 pid=285519) 	mov.u32 %r35, %r5;
(EngineCore_DP0 pid=285519) 	@%p1 ld.global.v4.b32 { %r32, %r33, %r34, %r35 }, [ %rd2 + 0 ];
(EngineCore_DP0 pid=285519) 	// end inline asm
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs56, %rs57}, %r32;
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs58, %rs59}, %r33;
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs60, %rs61}, %r34;
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs62, %rs63}, %r35;
(EngineCore_DP0 pid=285519) 	.loc	1 182 71                        // quant_only_tuned_Llama3.2-1B.py:182:71
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r77, %rs48;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r78, %rs49;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r79, %rs50;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r80, %rs51;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r81, %rs52;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r82, %rs53;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r83, %rs54;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r84, %rs55;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r85, %rs56;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r86, %rs57;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r87, %rs58;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r88, %rs59;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r89, %rs60;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r90, %rs61;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r91, %rs62;
(EngineCore_DP0 pid=285519) 	cvt.f32.bf16 	%r92, %rs63;
(EngineCore_DP0 pid=285519) 	.loc	1 183 62                        // quant_only_tuned_Llama3.2-1B.py:183:62
(EngineCore_DP0 pid=285519) 	mul.f32 	%r93, %r76, %r77;
(EngineCore_DP0 pid=285519) 	mul.f32 	%r94, %r76, %r78;
(EngineCore_DP0 pid=285519) 	mul.f32 	%r95, %r76, %r79;
(EngineCore_DP0 pid=285519) 	mul.f32 	%r96, %r76, %r80;
(EngineCore_DP0 pid=285519) 	mul.f32 	%r97, %r76, %r81;
(EngineCore_DP0 pid=285519) 	mul.f32 	%r98, %r76, %r82;
(EngineCore_DP0 pid=285519) 	mul.f32 	%r99, %r76, %r83;
(EngineCore_DP0 pid=285519) 	mul.f32 	%r100, %r76, %r84;
(EngineCore_DP0 pid=285519) 	mul.f32 	%r101, %r76, %r85;
(EngineCore_DP0 pid=285519) 	mul.f32 	%r102, %r76, %r86;
(EngineCore_DP0 pid=285519) 	mul.f32 	%r103, %r76, %r87;
(EngineCore_DP0 pid=285519) 	mul.f32 	%r104, %r76, %r88;
(EngineCore_DP0 pid=285519) 	mul.f32 	%r105, %r76, %r89;
(EngineCore_DP0 pid=285519) 	mul.f32 	%r106, %r76, %r90;
(EngineCore_DP0 pid=285519) 	mul.f32 	%r107, %r76, %r91;
(EngineCore_DP0 pid=285519) 	mul.f32 	%r108, %r76, %r92;
(EngineCore_DP0 pid=285519) 	.loc	1 183 54                        // quant_only_tuned_Llama3.2-1B.py:183:54
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r109, %r93;
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r110, %r94;
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r111, %r95;
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r112, %r96;
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r113, %r97;
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r114, %r98;
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r115, %r99;
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r116, %r100;
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r117, %r101;
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r118, %r102;
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r119, %r103;
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r120, %r104;
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r121, %r105;
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r122, %r106;
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r123, %r107;
(EngineCore_DP0 pid=285519) 	cvt.rni.f32.f32 	%r124, %r108;
(EngineCore_DP0 pid=285519) 	.loc	1 184 31                        // quant_only_tuned_Llama3.2-1B.py:184:31
(EngineCore_DP0 pid=285519) 	add.s64 	%rd6, %rd12, %rd13;
(EngineCore_DP0 pid=285519) 	.loc	1 183 82                        // quant_only_tuned_Llama3.2-1B.py:183:82
(EngineCore_DP0 pid=285519) 	max.f32 	%r125, %r112, 0fC3000000;
(EngineCore_DP0 pid=285519) 	max.f32 	%r126, %r111, 0fC3000000;
(EngineCore_DP0 pid=285519) 	max.f32 	%r127, %r110, 0fC3000000;
(EngineCore_DP0 pid=285519) 	max.f32 	%r128, %r109, 0fC3000000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r129, %r128, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r130, %r127, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r131, %r126, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r132, %r125, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	.loc	1 184 48                        // quant_only_tuned_Llama3.2-1B.py:184:48
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs64, %r132;
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs65, %r131;
(EngineCore_DP0 pid=285519) 	mov.b32 	%r133, {%rs65, %rs64};
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs66, %rs67}, %r133;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r134, %rs67;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r135, %rs66;
(EngineCore_DP0 pid=285519) 	prmt.b32 	%r136, %r135, %r134, 0x3340U;
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs68, %r130;
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs69, %r129;
(EngineCore_DP0 pid=285519) 	mov.b32 	%r137, {%rs69, %rs68};
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs70, %rs71}, %r137;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r138, %rs71;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r139, %rs70;
(EngineCore_DP0 pid=285519) 	prmt.b32 	%r140, %r139, %r138, 0x3340U;
(EngineCore_DP0 pid=285519) 	prmt.b32 	%r40, %r140, %r136, 0x5410U;
(EngineCore_DP0 pid=285519) 	.loc	1 183 82                        // quant_only_tuned_Llama3.2-1B.py:183:82
(EngineCore_DP0 pid=285519) 	max.f32 	%r141, %r116, 0fC3000000;
(EngineCore_DP0 pid=285519) 	max.f32 	%r142, %r115, 0fC3000000;
(EngineCore_DP0 pid=285519) 	max.f32 	%r143, %r114, 0fC3000000;
(EngineCore_DP0 pid=285519) 	max.f32 	%r144, %r113, 0fC3000000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r145, %r144, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r146, %r143, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r147, %r142, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r148, %r141, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	.loc	1 184 48                        // quant_only_tuned_Llama3.2-1B.py:184:48
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs72, %r148;
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs73, %r147;
(EngineCore_DP0 pid=285519) 	mov.b32 	%r149, {%rs73, %rs72};
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs74, %rs75}, %r149;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r150, %rs75;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r151, %rs74;
(EngineCore_DP0 pid=285519) 	prmt.b32 	%r152, %r151, %r150, 0x3340U;
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs76, %r146;
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs77, %r145;
(EngineCore_DP0 pid=285519) 	mov.b32 	%r153, {%rs77, %rs76};
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs78, %rs79}, %r153;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r154, %rs79;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r155, %rs78;
(EngineCore_DP0 pid=285519) 	prmt.b32 	%r156, %r155, %r154, 0x3340U;
(EngineCore_DP0 pid=285519) 	prmt.b32 	%r41, %r156, %r152, 0x5410U;
(EngineCore_DP0 pid=285519) 	.loc	1 183 82                        // quant_only_tuned_Llama3.2-1B.py:183:82
(EngineCore_DP0 pid=285519) 	max.f32 	%r157, %r120, 0fC3000000;
(EngineCore_DP0 pid=285519) 	max.f32 	%r158, %r119, 0fC3000000;
(EngineCore_DP0 pid=285519) 	max.f32 	%r159, %r118, 0fC3000000;
(EngineCore_DP0 pid=285519) 	max.f32 	%r160, %r117, 0fC3000000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r161, %r160, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r162, %r159, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r163, %r158, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r164, %r157, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	.loc	1 184 48                        // quant_only_tuned_Llama3.2-1B.py:184:48
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs80, %r164;
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs81, %r163;
(EngineCore_DP0 pid=285519) 	mov.b32 	%r165, {%rs81, %rs80};
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs82, %rs83}, %r165;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r166, %rs83;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r167, %rs82;
(EngineCore_DP0 pid=285519) 	prmt.b32 	%r168, %r167, %r166, 0x3340U;
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs84, %r162;
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs85, %r161;
(EngineCore_DP0 pid=285519) 	mov.b32 	%r169, {%rs85, %rs84};
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs86, %rs87}, %r169;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r170, %rs87;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r171, %rs86;
(EngineCore_DP0 pid=285519) 	prmt.b32 	%r172, %r171, %r170, 0x3340U;
(EngineCore_DP0 pid=285519) 	prmt.b32 	%r42, %r172, %r168, 0x5410U;
(EngineCore_DP0 pid=285519) 	.loc	1 183 82                        // quant_only_tuned_Llama3.2-1B.py:183:82
(EngineCore_DP0 pid=285519) 	max.f32 	%r173, %r124, 0fC3000000;
(EngineCore_DP0 pid=285519) 	max.f32 	%r174, %r123, 0fC3000000;
(EngineCore_DP0 pid=285519) 	max.f32 	%r175, %r122, 0fC3000000;
(EngineCore_DP0 pid=285519) 	max.f32 	%r176, %r121, 0fC3000000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r177, %r176, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r178, %r175, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r179, %r174, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	min.f32 	%r180, %r173, 0f42FE0000;
(EngineCore_DP0 pid=285519) 	.loc	1 184 48                        // quant_only_tuned_Llama3.2-1B.py:184:48
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs88, %r180;
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs89, %r179;
(EngineCore_DP0 pid=285519) 	mov.b32 	%r181, {%rs89, %rs88};
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs90, %rs91}, %r181;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r182, %rs91;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r183, %rs90;
(EngineCore_DP0 pid=285519) 	prmt.b32 	%r184, %r183, %r182, 0x3340U;
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs92, %r178;
(EngineCore_DP0 pid=285519) 	cvt.rzi.s16.f32 	%rs93, %r177;
(EngineCore_DP0 pid=285519) 	mov.b32 	%r185, {%rs93, %rs92};
(EngineCore_DP0 pid=285519) 	mov.b32 	{%rs94, %rs95}, %r185;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r186, %rs95;
(EngineCore_DP0 pid=285519) 	cvt.u32.u16 	%r187, %rs94;
(EngineCore_DP0 pid=285519) 	prmt.b32 	%r188, %r187, %r186, 0x3340U;
(EngineCore_DP0 pid=285519) 	prmt.b32 	%r43, %r188, %r184, 0x5410U;
(EngineCore_DP0 pid=285519) 	.loc	1 184 39                        // quant_only_tuned_Llama3.2-1B.py:184:39
(EngineCore_DP0 pid=285519) 	// begin inline asm
(EngineCore_DP0 pid=285519) 	@%p1 st.global.v4.b32 [ %rd6 + 0 ], { %r40, %r41, %r42, %r43 };
(EngineCore_DP0 pid=285519) 	// end inline asm
(EngineCore_DP0 pid=285519) 	.loc	1 179 4                         // quant_only_tuned_Llama3.2-1B.py:179:4
(EngineCore_DP0 pid=285519) 	ret;
(EngineCore_DP0 pid=285519) $L__tmp3:
(EngineCore_DP0 pid=285519) $L__func_end0:
(EngineCore_DP0 pid=285519)                                         // -- End function
(EngineCore_DP0 pid=285519) }
(EngineCore_DP0 pid=285519) 	.file	1 "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=285519) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=285519) 	.section	.debug_abbrev
(EngineCore_DP0 pid=285519) 	{
(EngineCore_DP0 pid=285519) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=285519) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=285519) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=285519) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=285519) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=285519) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=285519) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=285519) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=285519) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=285519) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=285519) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=285519) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=285519) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=285519) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=285519) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=285519) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=285519) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=285519) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=285519) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=285519) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=285519) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=285519) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=285519) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=285519) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=285519) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=285519) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=285519) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=285519) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=285519) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=285519) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=285519) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=285519) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=285519) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=285519) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=285519) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=285519) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=285519) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=285519) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=285519) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=285519) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=285519) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=285519) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=285519) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=285519) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=285519) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=285519) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=285519) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=285519) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=285519) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=285519) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=285519) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=285519) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=285519) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=285519) 	}
(EngineCore_DP0 pid=285519) 	.section	.debug_info
(EngineCore_DP0 pid=285519) 	{
(EngineCore_DP0 pid=285519) .b32 214                                // Length of Unit
(EngineCore_DP0 pid=285519) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=285519) .b8 0
(EngineCore_DP0 pid=285519) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=285519) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=285519) .b8 1                                   // Abbrev [1] 0xb:0xcf DW_TAG_compile_unit
(EngineCore_DP0 pid=285519) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=285519) .b8 114
(EngineCore_DP0 pid=285519) .b8 105
(EngineCore_DP0 pid=285519) .b8 116
(EngineCore_DP0 pid=285519) .b8 111
(EngineCore_DP0 pid=285519) .b8 110
(EngineCore_DP0 pid=285519) .b8 0
(EngineCore_DP0 pid=285519) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=285519) .b8 0
(EngineCore_DP0 pid=285519) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=285519) .b8 117
(EngineCore_DP0 pid=285519) .b8 97
(EngineCore_DP0 pid=285519) .b8 110
(EngineCore_DP0 pid=285519) .b8 116
(EngineCore_DP0 pid=285519) .b8 95
(EngineCore_DP0 pid=285519) .b8 111
(EngineCore_DP0 pid=285519) .b8 110
(EngineCore_DP0 pid=285519) .b8 108
(EngineCore_DP0 pid=285519) .b8 121
(EngineCore_DP0 pid=285519) .b8 95
(EngineCore_DP0 pid=285519) .b8 116
(EngineCore_DP0 pid=285519) .b8 117
(EngineCore_DP0 pid=285519) .b8 110
(EngineCore_DP0 pid=285519) .b8 101
(EngineCore_DP0 pid=285519) .b8 100
(EngineCore_DP0 pid=285519) .b8 95
(EngineCore_DP0 pid=285519) .b8 76
(EngineCore_DP0 pid=285519) .b8 108
(EngineCore_DP0 pid=285519) .b8 97
(EngineCore_DP0 pid=285519) .b8 109
(EngineCore_DP0 pid=285519) .b8 97
(EngineCore_DP0 pid=285519) .b8 51
(EngineCore_DP0 pid=285519) .b8 46
(EngineCore_DP0 pid=285519) .b8 50
(EngineCore_DP0 pid=285519) .b8 45
(EngineCore_DP0 pid=285519) .b8 49
(EngineCore_DP0 pid=285519) .b8 66
(EngineCore_DP0 pid=285519) .b8 46
(EngineCore_DP0 pid=285519) .b8 112
(EngineCore_DP0 pid=285519) .b8 121
(EngineCore_DP0 pid=285519) .b8 0
(EngineCore_DP0 pid=285519) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=285519) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=285519) .b8 114
(EngineCore_DP0 pid=285519) .b8 111
(EngineCore_DP0 pid=285519) .b8 111
(EngineCore_DP0 pid=285519) .b8 116
(EngineCore_DP0 pid=285519) .b8 47
(EngineCore_DP0 pid=285519) .b8 118
(EngineCore_DP0 pid=285519) .b8 108
(EngineCore_DP0 pid=285519) .b8 108
(EngineCore_DP0 pid=285519) .b8 109
(EngineCore_DP0 pid=285519) .b8 98
(EngineCore_DP0 pid=285519) .b8 101
(EngineCore_DP0 pid=285519) .b8 110
(EngineCore_DP0 pid=285519) .b8 99
(EngineCore_DP0 pid=285519) .b8 104
(EngineCore_DP0 pid=285519) .b8 47
(EngineCore_DP0 pid=285519) .b8 115
(EngineCore_DP0 pid=285519) .b8 108
(EngineCore_DP0 pid=285519) .b8 105
(EngineCore_DP0 pid=285519) .b8 100
(EngineCore_DP0 pid=285519) .b8 101
(EngineCore_DP0 pid=285519) .b8 115
(EngineCore_DP0 pid=285519) .b8 112
(EngineCore_DP0 pid=285519) .b8 97
(EngineCore_DP0 pid=285519) .b8 114
(EngineCore_DP0 pid=285519) .b8 115
(EngineCore_DP0 pid=285519) .b8 101
(EngineCore_DP0 pid=285519) .b8 47
(EngineCore_DP0 pid=285519) .b8 99
(EngineCore_DP0 pid=285519) .b8 115
(EngineCore_DP0 pid=285519) .b8 114
(EngineCore_DP0 pid=285519) .b8 99
(EngineCore_DP0 pid=285519) .b8 47
(EngineCore_DP0 pid=285519) .b8 113
(EngineCore_DP0 pid=285519) .b8 117
(EngineCore_DP0 pid=285519) .b8 97
(EngineCore_DP0 pid=285519) .b8 110
(EngineCore_DP0 pid=285519) .b8 116
(EngineCore_DP0 pid=285519) .b8 95
(EngineCore_DP0 pid=285519) .b8 111
(EngineCore_DP0 pid=285519) .b8 110
(EngineCore_DP0 pid=285519) .b8 108
(EngineCore_DP0 pid=285519) .b8 121
(EngineCore_DP0 pid=285519) .b8 95
(EngineCore_DP0 pid=285519) .b8 116
(EngineCore_DP0 pid=285519) .b8 114
(EngineCore_DP0 pid=285519) .b8 105
(EngineCore_DP0 pid=285519) .b8 116
(EngineCore_DP0 pid=285519) .b8 111
(EngineCore_DP0 pid=285519) .b8 110
(EngineCore_DP0 pid=285519) .b8 47
(EngineCore_DP0 pid=285519) .b8 98
(EngineCore_DP0 pid=285519) .b8 117
(EngineCore_DP0 pid=285519) .b8 105
(EngineCore_DP0 pid=285519) .b8 108
(EngineCore_DP0 pid=285519) .b8 100
(EngineCore_DP0 pid=285519) .b8 47
(EngineCore_DP0 pid=285519) .b8 71
(EngineCore_DP0 pid=285519) .b8 66
(EngineCore_DP0 pid=285519) .b8 49
(EngineCore_DP0 pid=285519) .b8 48
(EngineCore_DP0 pid=285519) .b8 95
(EngineCore_DP0 pid=285519) .b8 99
(EngineCore_DP0 pid=285519) .b8 99
(EngineCore_DP0 pid=285519) .b8 49
(EngineCore_DP0 pid=285519) .b8 50
(EngineCore_DP0 pid=285519) .b8 49
(EngineCore_DP0 pid=285519) .b8 95
(EngineCore_DP0 pid=285519) .b8 112
(EngineCore_DP0 pid=285519) .b8 121
(EngineCore_DP0 pid=285519) .b8 51
(EngineCore_DP0 pid=285519) .b8 49
(EngineCore_DP0 pid=285519) .b8 50
(EngineCore_DP0 pid=285519) .b8 95
(EngineCore_DP0 pid=285519) .b8 99
(EngineCore_DP0 pid=285519) .b8 117
(EngineCore_DP0 pid=285519) .b8 49
(EngineCore_DP0 pid=285519) .b8 50
(EngineCore_DP0 pid=285519) .b8 57
(EngineCore_DP0 pid=285519) .b8 95
(EngineCore_DP0 pid=285519) .b8 97
(EngineCore_DP0 pid=285519) .b8 97
(EngineCore_DP0 pid=285519) .b8 114
(EngineCore_DP0 pid=285519) .b8 99
(EngineCore_DP0 pid=285519) .b8 104
(EngineCore_DP0 pid=285519) .b8 54
(EngineCore_DP0 pid=285519) .b8 52
(EngineCore_DP0 pid=285519) .b8 0
(EngineCore_DP0 pid=285519) .b8 2                                   // Abbrev [2] 0x91:0x1a DW_TAG_subprogram
(EngineCore_DP0 pid=285519) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=285519) .b8 113
(EngineCore_DP0 pid=285519) .b8 117
(EngineCore_DP0 pid=285519) .b8 97
(EngineCore_DP0 pid=285519) .b8 110
(EngineCore_DP0 pid=285519) .b8 116
(EngineCore_DP0 pid=285519) .b8 95
(EngineCore_DP0 pid=285519) .b8 111
(EngineCore_DP0 pid=285519) .b8 110
(EngineCore_DP0 pid=285519) .b8 108
(EngineCore_DP0 pid=285519) .b8 121
(EngineCore_DP0 pid=285519) .b8 95
(EngineCore_DP0 pid=285519) .b8 105
(EngineCore_DP0 pid=285519) .b8 110
(EngineCore_DP0 pid=285519) .b8 116
(EngineCore_DP0 pid=285519) .b8 56
(EngineCore_DP0 pid=285519) .b8 95
(EngineCore_DP0 pid=285519) .b8 107
(EngineCore_DP0 pid=285519) .b8 101
(EngineCore_DP0 pid=285519) .b8 114
(EngineCore_DP0 pid=285519) .b8 110
(EngineCore_DP0 pid=285519) .b8 101
(EngineCore_DP0 pid=285519) .b8 108
(EngineCore_DP0 pid=285519) .b8 0
(EngineCore_DP0 pid=285519) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=285519) .b8 3                                   // Abbrev [3] 0xab:0x2e DW_TAG_subprogram
(EngineCore_DP0 pid=285519) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=285519) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=285519) .b32 145                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=285519) .b8 4                                   // Abbrev [4] 0xc0:0x18 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=285519) .b32 145                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=285519) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=285519) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=285519) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=285519) .b8 169                                 // DW_AT_call_line
(EngineCore_DP0 pid=285519) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=285519) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=285519) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=285519) 	}
(EngineCore_DP0 pid=285519) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=285519) 
(EngineCore_DP0 pid=285519) ================================================================
(EngineCore_DP0 pid=285519) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=285519) 
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpn3ugkz7v.ptx', '-o', '/tmp/tmpn3ugkz7v.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866] 
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866] 
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 760, in apply_weights
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 546, in apply
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 194, in cuBLASLt_INT8_linear
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     qinput, scale_a_pad = quant_only_int8_kernel(input, model_name)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 369, in quant_only_int8_kernel
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return torch.ops.slidesparse.quant_only_int8(input, model_name)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 544, in _quant_only_int8_impl
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return fn(input)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]            ^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py", line 206, in quant_only_int8_triton
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     _quant_only_int8_kernel[(M,)](
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866] 
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpn3ugkz7v.ptx -o /tmp/tmpn3ugkz7v.ptx.o
(EngineCore_DP0 pid=285519) ERROR 01-25 18:37:27 [core.py:866] 

STDERR:
[2026-01-25 18:37:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:37:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:37:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:37:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:37:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:37:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:37:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:37:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:37:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:37:16] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:37:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:37:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:37:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:37:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:37:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:37:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:37:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:37:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=285519) [2026-01-25 18:37:17] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=285519) [2026-01-25 18:37:17] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=285519) [2026-01-25 18:37:17] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=285519) [2026-01-25 18:37:17] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=285519) [2026-01-25 18:37:17] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=285519) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=285519) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.38s/it]
(EngineCore_DP0 pid=285519) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.38s/it]
(EngineCore_DP0 pid=285519) 
(EngineCore_DP0 pid=285519) Process EngineCore_DP0:
(EngineCore_DP0 pid=285519) Traceback (most recent call last):
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=285519)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=285519)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=285519)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=285519) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpn3ugkz7v.ptx', '-o', '/tmp/tmpn3ugkz7v.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=285519) 
(EngineCore_DP0 pid=285519) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=285519) 
(EngineCore_DP0 pid=285519) Traceback (most recent call last):
(EngineCore_DP0 pid=285519)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=285519)     self.run()
(EngineCore_DP0 pid=285519)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=285519)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=285519)     raise e
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=285519)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=285519)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=285519)     super().__init__(
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=285519)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=285519)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=285519)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=285519)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=285519)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=285519)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=285519)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=285519)     return func(*args, **kwargs)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=285519)     return func(*args, **kwargs)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=285519)     self.model_runner.profile_run()
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=285519)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=285519)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=285519)     return func(*args, **kwargs)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=285519)     outputs = self.model(
(EngineCore_DP0 pid=285519)               ^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285519)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285519)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=285519)     model_output = self.model(
(EngineCore_DP0 pid=285519)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=285519)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=285519)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=285519)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285519)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285519)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=285519)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=285519)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285519)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285519)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=285519)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=285519)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285519)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285519)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=285519)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=285519)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=285519)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 760, in apply_weights
(EngineCore_DP0 pid=285519)     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 546, in apply
(EngineCore_DP0 pid=285519)     return self._linear_fn(
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 194, in cuBLASLt_INT8_linear
(EngineCore_DP0 pid=285519)     qinput, scale_a_pad = quant_only_int8_kernel(input, model_name)
(EngineCore_DP0 pid=285519)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/slidesparse/core/kernels.py", line 369, in quant_only_int8_kernel
(EngineCore_DP0 pid=285519)     return torch.ops.slidesparse.quant_only_int8(input, model_name)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=285519)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/slidesparse/core/kernels.py", line 544, in _quant_only_int8_impl
(EngineCore_DP0 pid=285519)     return fn(input)
(EngineCore_DP0 pid=285519)            ^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py", line 206, in quant_only_int8_triton
(EngineCore_DP0 pid=285519)     _quant_only_int8_kernel[(M,)](
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=285519)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=285519)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=285519)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=285519)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=285519)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=285519)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=285519)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=285519)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=285519)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=285519)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285519)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=285519)     raise PTXASError(error)
(EngineCore_DP0 pid=285519) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=285519) `ptxas` stderr:
(EngineCore_DP0 pid=285519) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=285519) 
(EngineCore_DP0 pid=285519) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpn3ugkz7v.ptx -o /tmp/tmpn3ugkz7v.ptx.o
(EngineCore_DP0 pid=285519) 
[rank0]:[W125 18:37:27.516738613 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=128

========== M=256 ==========
Time: 2026-01-25 18:37:28
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:37:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:37:32 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=285977) 
(EngineCore_DP0 pid=285977) 
(EngineCore_DP0 pid=285977) ================================================================
(EngineCore_DP0 pid=285977) Internal Triton PTX codegen error
(EngineCore_DP0 pid=285977) `ptxas` stderr:
(EngineCore_DP0 pid=285977) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=285977) 
(EngineCore_DP0 pid=285977) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpxvn3v08w.ptx -o /tmp/tmpxvn3v08w.ptx.o
(EngineCore_DP0 pid=285977) 
(EngineCore_DP0 pid=285977) 
(EngineCore_DP0 pid=285977) //
(EngineCore_DP0 pid=285977) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=285977) //
(EngineCore_DP0 pid=285977) 
(EngineCore_DP0 pid=285977) .version 8.7
(EngineCore_DP0 pid=285977) .target sm_121a
(EngineCore_DP0 pid=285977) .address_size 64
(EngineCore_DP0 pid=285977) 
(EngineCore_DP0 pid=285977) 	// .globl	_quant_only_int8_kernel // -- Begin function _quant_only_int8_kernel
(EngineCore_DP0 pid=285977) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=285977)                                         // @_quant_only_int8_kernel
(EngineCore_DP0 pid=285977) .visible .entry _quant_only_int8_kernel(
(EngineCore_DP0 pid=285977) 	.param .u64 .ptr .global .align 1 _quant_only_int8_kernel_param_0,
(EngineCore_DP0 pid=285977) 	.param .u64 .ptr .global .align 1 _quant_only_int8_kernel_param_1,
(EngineCore_DP0 pid=285977) 	.param .u64 .ptr .global .align 1 _quant_only_int8_kernel_param_2,
(EngineCore_DP0 pid=285977) 	.param .u32 _quant_only_int8_kernel_param_3,
(EngineCore_DP0 pid=285977) 	.param .u32 _quant_only_int8_kernel_param_4,
(EngineCore_DP0 pid=285977) 	.param .u32 _quant_only_int8_kernel_param_5,
(EngineCore_DP0 pid=285977) 	.param .u64 .ptr .global .align 1 _quant_only_int8_kernel_param_6,
(EngineCore_DP0 pid=285977) 	.param .u64 .ptr .global .align 1 _quant_only_int8_kernel_param_7
(EngineCore_DP0 pid=285977) )
(EngineCore_DP0 pid=285977) .reqntid 256
(EngineCore_DP0 pid=285977) {
(EngineCore_DP0 pid=285977) 	.reg .pred 	%p<10>;
(EngineCore_DP0 pid=285977) 	.reg .b16 	%rs<96>;
(EngineCore_DP0 pid=285977) 	.reg .b32 	%r<189>;
(EngineCore_DP0 pid=285977) 	.reg .b64 	%rd<14>;
(EngineCore_DP0 pid=285977) 	.loc	1 147 0                         // quant_only_tuned_Llama3.2-1B.py:147:0
(EngineCore_DP0 pid=285977) $L__func_begin0:
(EngineCore_DP0 pid=285977) 	.loc	1 147 0                         // quant_only_tuned_Llama3.2-1B.py:147:0
(EngineCore_DP0 pid=285977) 
(EngineCore_DP0 pid=285977) // %bb.0:
(EngineCore_DP0 pid=285977) 	ld.param.b64 	%rd7, [_quant_only_int8_kernel_param_0];
(EngineCore_DP0 pid=285977) 	ld.param.b64 	%rd8, [_quant_only_int8_kernel_param_1];
(EngineCore_DP0 pid=285977) $L__tmp0:
(EngineCore_DP0 pid=285977) 	.loc	1 154 24                        // quant_only_tuned_Llama3.2-1B.py:154:24
(EngineCore_DP0 pid=285977) 	mov.u32 	%r44, %ctaid.x;
(EngineCore_DP0 pid=285977) 	ld.param.b32 	%r45, [_quant_only_int8_kernel_param_4];
(EngineCore_DP0 pid=285977) 	.loc	1 159 30                        // quant_only_tuned_Llama3.2-1B.py:159:30
(EngineCore_DP0 pid=285977) 	mul.lo.s32 	%r46, %r45, %r44;
(EngineCore_DP0 pid=285977) 	ld.param.b64 	%rd9, [_quant_only_int8_kernel_param_2];
(EngineCore_DP0 pid=285977) 	.loc	1 159 24                        // quant_only_tuned_Llama3.2-1B.py:159:24
(EngineCore_DP0 pid=285977) 	mad.wide.s32 	%rd10, %r46, 2, %rd7;
(EngineCore_DP0 pid=285977) 	ld.param.b32 	%r47, [_quant_only_int8_kernel_param_5];
(EngineCore_DP0 pid=285977) 	.loc	1 160 34                        // quant_only_tuned_Llama3.2-1B.py:160:34
(EngineCore_DP0 pid=285977) 	mul.lo.s32 	%r48, %r47, %r44;
(EngineCore_DP0 pid=285977) 	.loc	1 160 28                        // quant_only_tuned_Llama3.2-1B.py:160:28
(EngineCore_DP0 pid=285977) 	cvt.s64.s32 	%rd11, %r48;
(EngineCore_DP0 pid=285977) 	add.s64 	%rd12, %rd8, %rd11;
(EngineCore_DP0 pid=285977) 	.loc	1 166 40                        // quant_only_tuned_Llama3.2-1B.py:166:40
(EngineCore_DP0 pid=285977) 	mov.u32 	%r49, %tid.x;
(EngineCore_DP0 pid=285977) 	and.b32 	%r50, %r49, 31;
(EngineCore_DP0 pid=285977) 	shl.b32 	%r51, %r49, 4;
(EngineCore_DP0 pid=285977) 	and.b32 	%r52, %r51, 4080;
(EngineCore_DP0 pid=285977) 	.loc	1 167 26                        // quant_only_tuned_Llama3.2-1B.py:167:26
(EngineCore_DP0 pid=285977) 	setp.lt.u32 	%p1, %r52, 2048;
(EngineCore_DP0 pid=285977) 	.loc	1 168 36                        // quant_only_tuned_Llama3.2-1B.py:168:36
(EngineCore_DP0 pid=285977) 	cvt.u64.u32 	%rd13, %r52;
(EngineCore_DP0 pid=285977) 	mad.wide.u32 	%rd1, %r52, 2, %rd10;
(EngineCore_DP0 pid=285977) 	add.s64 	%rd2, %rd1, 16;
(EngineCore_DP0 pid=285977) 	mov.b32 	%r5, 0;
(EngineCore_DP0 pid=285977) 	.loc	1 168 24                        // quant_only_tuned_Llama3.2-1B.py:168:24
(EngineCore_DP0 pid=285977) 	// begin inline asm
(EngineCore_DP0 pid=285977) 	mov.u32 %r1, %r5;
(EngineCore_DP0 pid=285977) 	mov.u32 %r2, %r5;
(EngineCore_DP0 pid=285977) 	mov.u32 %r3, %r5;
(EngineCore_DP0 pid=285977) 	mov.u32 %r4, %r5;
(EngineCore_DP0 pid=285977) 	@%p1 ld.global.v4.b32 { %r1, %r2, %r3, %r4 }, [ %rd1 + 0 ];
(EngineCore_DP0 pid=285977) 	// end inline asm
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs1, %rs2}, %r1;
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs3, %rs4}, %r2;
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs5, %rs6}, %r3;
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs7, %rs8}, %r4;
(EngineCore_DP0 pid=285977) 	// begin inline asm
(EngineCore_DP0 pid=285977) 	mov.u32 %r9, %r5;
(EngineCore_DP0 pid=285977) 	mov.u32 %r10, %r5;
(EngineCore_DP0 pid=285977) 	mov.u32 %r11, %r5;
(EngineCore_DP0 pid=285977) 	mov.u32 %r12, %r5;
(EngineCore_DP0 pid=285977) 	@%p1 ld.global.v4.b32 { %r9, %r10, %r11, %r12 }, [ %rd2 + 0 ];
(EngineCore_DP0 pid=285977) 	// end inline asm
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs9, %rs10}, %r9;
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs11, %rs12}, %r10;
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs13, %rs14}, %r11;
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs15, %rs16}, %r12;
(EngineCore_DP0 pid=285977) 	.loc	1 169 50                        // quant_only_tuned_Llama3.2-1B.py:169:50
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs17, %rs1;
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs18, %rs2;
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs19, %rs3;
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs20, %rs4;
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs21, %rs5;
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs22, %rs6;
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs23, %rs7;
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs24, %rs8;
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs25, %rs9;
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs26, %rs10;
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs27, %rs11;
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs28, %rs12;
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs29, %rs13;
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs30, %rs14;
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs31, %rs15;
(EngineCore_DP0 pid=285977) 	abs.bf16 	%rs32, %rs16;
(EngineCore_DP0 pid=285977) $L__tmp1:
(EngineCore_DP0 pid=285977) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	max.bf16 	%rs33, %rs17, %rs18;
(EngineCore_DP0 pid=285977) 	max.bf16 	%rs34, %rs33, %rs19;
(EngineCore_DP0 pid=285977) 	max.bf16 	%rs35, %rs34, %rs20;
(EngineCore_DP0 pid=285977) 	max.bf16 	%rs36, %rs35, %rs21;
(EngineCore_DP0 pid=285977) 	max.bf16 	%rs37, %rs36, %rs22;
(EngineCore_DP0 pid=285977) 	max.bf16 	%rs38, %rs37, %rs23;
(EngineCore_DP0 pid=285977) 	max.bf16 	%rs39, %rs38, %rs24;
(EngineCore_DP0 pid=285977) 	max.bf16 	%rs40, %rs39, %rs25;
(EngineCore_DP0 pid=285977) 	max.bf16 	%rs41, %rs40, %rs26;
(EngineCore_DP0 pid=285977) 	max.bf16 	%rs42, %rs41, %rs27;
(EngineCore_DP0 pid=285977) 	max.bf16 	%rs43, %rs42, %rs28;
(EngineCore_DP0 pid=285977) 	max.bf16 	%rs44, %rs43, %rs29;
(EngineCore_DP0 pid=285977) 	max.bf16 	%rs45, %rs44, %rs30;
(EngineCore_DP0 pid=285977) 	max.bf16 	%rs46, %rs45, %rs31;
(EngineCore_DP0 pid=285977) 	max.bf16 	%rs47, %rs46, %rs32;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r53, %rs47;
(EngineCore_DP0 pid=285977) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	shfl.sync.bfly.b32 	%r54, %r53, 16, 31, -1;
(EngineCore_DP0 pid=285977) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	max.f32 	%r55, %r53, %r54;
(EngineCore_DP0 pid=285977) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	shfl.sync.bfly.b32 	%r56, %r55, 8, 31, -1;
(EngineCore_DP0 pid=285977) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	max.f32 	%r57, %r55, %r56;
(EngineCore_DP0 pid=285977) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	shfl.sync.bfly.b32 	%r58, %r57, 4, 31, -1;
(EngineCore_DP0 pid=285977) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	max.f32 	%r59, %r57, %r58;
(EngineCore_DP0 pid=285977) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	shfl.sync.bfly.b32 	%r60, %r59, 2, 31, -1;
(EngineCore_DP0 pid=285977) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	max.f32 	%r61, %r59, %r60;
(EngineCore_DP0 pid=285977) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	shfl.sync.bfly.b32 	%r62, %r61, 1, 31, -1;
(EngineCore_DP0 pid=285977) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	max.f32 	%r18, %r61, %r62;
(EngineCore_DP0 pid=285977) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	setp.eq.b32 	%p3, %r50, 0;
(EngineCore_DP0 pid=285977) 	shr.u32 	%r63, %r49, 3;
(EngineCore_DP0 pid=285977) 	and.b32 	%r64, %r63, 28;
(EngineCore_DP0 pid=285977) 	mov.b32 	%r65, global_smem;
(EngineCore_DP0 pid=285977) 	add.s32 	%r17, %r65, %r64;
(EngineCore_DP0 pid=285977) 	// begin inline asm
(EngineCore_DP0 pid=285977) 	@%p3 st.shared.b32 [ %r17 + 0 ], %r18;
(EngineCore_DP0 pid=285977) 	// end inline asm
(EngineCore_DP0 pid=285977) 	bar.sync 	0;
(EngineCore_DP0 pid=285977) 	setp.lt.u32 	%p4, %r49, 8;
(EngineCore_DP0 pid=285977) 	shl.b32 	%r66, %r49, 2;
(EngineCore_DP0 pid=285977) 	add.s32 	%r20, %r65, %r66;
(EngineCore_DP0 pid=285977) 	// begin inline asm
(EngineCore_DP0 pid=285977) 	@%p4 ld.shared.b32 %r19, [ %r20 + 0 ];
(EngineCore_DP0 pid=285977) 	// end inline asm
(EngineCore_DP0 pid=285977) 	shfl.sync.bfly.b32 	%r67, %r19, 4, 31, -1;
(EngineCore_DP0 pid=285977) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	max.f32 	%r68, %r19, %r67;
(EngineCore_DP0 pid=285977) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	shfl.sync.bfly.b32 	%r69, %r68, 2, 31, -1;
(EngineCore_DP0 pid=285977) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	max.f32 	%r70, %r68, %r69;
(EngineCore_DP0 pid=285977) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	shfl.sync.bfly.b32 	%r71, %r70, 1, 31, -1;
(EngineCore_DP0 pid=285977) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	max.f32 	%r22, %r70, %r71;
(EngineCore_DP0 pid=285977) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_only_tuned_Llama3.2-1B.py:169:43 ]
(EngineCore_DP0 pid=285977) 	setp.eq.b32 	%p5, %r49, 0;
(EngineCore_DP0 pid=285977) 	// begin inline asm
(EngineCore_DP0 pid=285977) 	@%p5 st.shared.b32 [ %r20 + 0 ], %r22;
(EngineCore_DP0 pid=285977) 	// end inline asm
(EngineCore_DP0 pid=285977) 	bar.sync 	0;
(EngineCore_DP0 pid=285977) 	ld.shared.b32 	%r72, [global_smem];
(EngineCore_DP0 pid=285977) $L__tmp2:
(EngineCore_DP0 pid=285977) 	.loc	1 172 32                        // quant_only_tuned_Llama3.2-1B.py:172:32
(EngineCore_DP0 pid=285977) 	max.f32 	%r73, %r72, 0f2B8CBCCC;
(EngineCore_DP0 pid=285977) 	mov.b32 	%r74, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	.loc	1 173 32                        // quant_only_tuned_Llama3.2-1B.py:173:32
(EngineCore_DP0 pid=285977) 	div.full.f32 	%r75, %r73, %r74;
(EngineCore_DP0 pid=285977) 	.loc	1 173 42                        // quant_only_tuned_Llama3.2-1B.py:173:42
(EngineCore_DP0 pid=285977) 	max.f32 	%r23, %r75, 0f37810204;
(EngineCore_DP0 pid=285977) 	.loc	1 174 27                        // quant_only_tuned_Llama3.2-1B.py:174:27
(EngineCore_DP0 pid=285977) 	div.full.f32 	%r76, %r74, %r73;
(EngineCore_DP0 pid=285977) 	.loc	1 176 25                        // quant_only_tuned_Llama3.2-1B.py:176:25
(EngineCore_DP0 pid=285977) 	mad.wide.u32 	%rd3, %r44, 4, %rd9;
(EngineCore_DP0 pid=285977) 	.loc	1 176 30                        // quant_only_tuned_Llama3.2-1B.py:176:30
(EngineCore_DP0 pid=285977) 	// begin inline asm
(EngineCore_DP0 pid=285977) 	@%p5 st.global.b32 [ %rd3 + 0 ], { %r23 };
(EngineCore_DP0 pid=285977) 	// end inline asm
(EngineCore_DP0 pid=285977) 	.loc	1 182 24                        // quant_only_tuned_Llama3.2-1B.py:182:24
(EngineCore_DP0 pid=285977) 	// begin inline asm
(EngineCore_DP0 pid=285977) 	mov.u32 %r24, %r5;
(EngineCore_DP0 pid=285977) 	mov.u32 %r25, %r5;
(EngineCore_DP0 pid=285977) 	mov.u32 %r26, %r5;
(EngineCore_DP0 pid=285977) 	mov.u32 %r27, %r5;
(EngineCore_DP0 pid=285977) 	@%p1 ld.global.v4.b32 { %r24, %r25, %r26, %r27 }, [ %rd1 + 0 ];
(EngineCore_DP0 pid=285977) 	// end inline asm
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs48, %rs49}, %r24;
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs50, %rs51}, %r25;
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs52, %rs53}, %r26;
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs54, %rs55}, %r27;
(EngineCore_DP0 pid=285977) 	// begin inline asm
(EngineCore_DP0 pid=285977) 	mov.u32 %r32, %r5;
(EngineCore_DP0 pid=285977) 	mov.u32 %r33, %r5;
(EngineCore_DP0 pid=285977) 	mov.u32 %r34, %r5;
(EngineCore_DP0 pid=285977) 	mov.u32 %r35, %r5;
(EngineCore_DP0 pid=285977) 	@%p1 ld.global.v4.b32 { %r32, %r33, %r34, %r35 }, [ %rd2 + 0 ];
(EngineCore_DP0 pid=285977) 	// end inline asm
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs56, %rs57}, %r32;
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs58, %rs59}, %r33;
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs60, %rs61}, %r34;
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs62, %rs63}, %r35;
(EngineCore_DP0 pid=285977) 	.loc	1 182 71                        // quant_only_tuned_Llama3.2-1B.py:182:71
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r77, %rs48;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r78, %rs49;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r79, %rs50;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r80, %rs51;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r81, %rs52;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r82, %rs53;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r83, %rs54;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r84, %rs55;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r85, %rs56;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r86, %rs57;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r87, %rs58;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r88, %rs59;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r89, %rs60;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r90, %rs61;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r91, %rs62;
(EngineCore_DP0 pid=285977) 	cvt.f32.bf16 	%r92, %rs63;
(EngineCore_DP0 pid=285977) 	.loc	1 183 62                        // quant_only_tuned_Llama3.2-1B.py:183:62
(EngineCore_DP0 pid=285977) 	mul.f32 	%r93, %r76, %r77;
(EngineCore_DP0 pid=285977) 	mul.f32 	%r94, %r76, %r78;
(EngineCore_DP0 pid=285977) 	mul.f32 	%r95, %r76, %r79;
(EngineCore_DP0 pid=285977) 	mul.f32 	%r96, %r76, %r80;
(EngineCore_DP0 pid=285977) 	mul.f32 	%r97, %r76, %r81;
(EngineCore_DP0 pid=285977) 	mul.f32 	%r98, %r76, %r82;
(EngineCore_DP0 pid=285977) 	mul.f32 	%r99, %r76, %r83;
(EngineCore_DP0 pid=285977) 	mul.f32 	%r100, %r76, %r84;
(EngineCore_DP0 pid=285977) 	mul.f32 	%r101, %r76, %r85;
(EngineCore_DP0 pid=285977) 	mul.f32 	%r102, %r76, %r86;
(EngineCore_DP0 pid=285977) 	mul.f32 	%r103, %r76, %r87;
(EngineCore_DP0 pid=285977) 	mul.f32 	%r104, %r76, %r88;
(EngineCore_DP0 pid=285977) 	mul.f32 	%r105, %r76, %r89;
(EngineCore_DP0 pid=285977) 	mul.f32 	%r106, %r76, %r90;
(EngineCore_DP0 pid=285977) 	mul.f32 	%r107, %r76, %r91;
(EngineCore_DP0 pid=285977) 	mul.f32 	%r108, %r76, %r92;
(EngineCore_DP0 pid=285977) 	.loc	1 183 54                        // quant_only_tuned_Llama3.2-1B.py:183:54
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r109, %r93;
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r110, %r94;
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r111, %r95;
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r112, %r96;
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r113, %r97;
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r114, %r98;
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r115, %r99;
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r116, %r100;
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r117, %r101;
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r118, %r102;
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r119, %r103;
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r120, %r104;
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r121, %r105;
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r122, %r106;
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r123, %r107;
(EngineCore_DP0 pid=285977) 	cvt.rni.f32.f32 	%r124, %r108;
(EngineCore_DP0 pid=285977) 	.loc	1 184 31                        // quant_only_tuned_Llama3.2-1B.py:184:31
(EngineCore_DP0 pid=285977) 	add.s64 	%rd6, %rd12, %rd13;
(EngineCore_DP0 pid=285977) 	.loc	1 183 82                        // quant_only_tuned_Llama3.2-1B.py:183:82
(EngineCore_DP0 pid=285977) 	max.f32 	%r125, %r112, 0fC3000000;
(EngineCore_DP0 pid=285977) 	max.f32 	%r126, %r111, 0fC3000000;
(EngineCore_DP0 pid=285977) 	max.f32 	%r127, %r110, 0fC3000000;
(EngineCore_DP0 pid=285977) 	max.f32 	%r128, %r109, 0fC3000000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r129, %r128, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r130, %r127, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r131, %r126, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r132, %r125, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	.loc	1 184 48                        // quant_only_tuned_Llama3.2-1B.py:184:48
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs64, %r132;
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs65, %r131;
(EngineCore_DP0 pid=285977) 	mov.b32 	%r133, {%rs65, %rs64};
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs66, %rs67}, %r133;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r134, %rs67;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r135, %rs66;
(EngineCore_DP0 pid=285977) 	prmt.b32 	%r136, %r135, %r134, 0x3340U;
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs68, %r130;
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs69, %r129;
(EngineCore_DP0 pid=285977) 	mov.b32 	%r137, {%rs69, %rs68};
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs70, %rs71}, %r137;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r138, %rs71;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r139, %rs70;
(EngineCore_DP0 pid=285977) 	prmt.b32 	%r140, %r139, %r138, 0x3340U;
(EngineCore_DP0 pid=285977) 	prmt.b32 	%r40, %r140, %r136, 0x5410U;
(EngineCore_DP0 pid=285977) 	.loc	1 183 82                        // quant_only_tuned_Llama3.2-1B.py:183:82
(EngineCore_DP0 pid=285977) 	max.f32 	%r141, %r116, 0fC3000000;
(EngineCore_DP0 pid=285977) 	max.f32 	%r142, %r115, 0fC3000000;
(EngineCore_DP0 pid=285977) 	max.f32 	%r143, %r114, 0fC3000000;
(EngineCore_DP0 pid=285977) 	max.f32 	%r144, %r113, 0fC3000000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r145, %r144, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r146, %r143, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r147, %r142, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r148, %r141, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	.loc	1 184 48                        // quant_only_tuned_Llama3.2-1B.py:184:48
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs72, %r148;
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs73, %r147;
(EngineCore_DP0 pid=285977) 	mov.b32 	%r149, {%rs73, %rs72};
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs74, %rs75}, %r149;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r150, %rs75;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r151, %rs74;
(EngineCore_DP0 pid=285977) 	prmt.b32 	%r152, %r151, %r150, 0x3340U;
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs76, %r146;
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs77, %r145;
(EngineCore_DP0 pid=285977) 	mov.b32 	%r153, {%rs77, %rs76};
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs78, %rs79}, %r153;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r154, %rs79;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r155, %rs78;
(EngineCore_DP0 pid=285977) 	prmt.b32 	%r156, %r155, %r154, 0x3340U;
(EngineCore_DP0 pid=285977) 	prmt.b32 	%r41, %r156, %r152, 0x5410U;
(EngineCore_DP0 pid=285977) 	.loc	1 183 82                        // quant_only_tuned_Llama3.2-1B.py:183:82
(EngineCore_DP0 pid=285977) 	max.f32 	%r157, %r120, 0fC3000000;
(EngineCore_DP0 pid=285977) 	max.f32 	%r158, %r119, 0fC3000000;
(EngineCore_DP0 pid=285977) 	max.f32 	%r159, %r118, 0fC3000000;
(EngineCore_DP0 pid=285977) 	max.f32 	%r160, %r117, 0fC3000000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r161, %r160, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r162, %r159, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r163, %r158, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r164, %r157, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	.loc	1 184 48                        // quant_only_tuned_Llama3.2-1B.py:184:48
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs80, %r164;
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs81, %r163;
(EngineCore_DP0 pid=285977) 	mov.b32 	%r165, {%rs81, %rs80};
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs82, %rs83}, %r165;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r166, %rs83;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r167, %rs82;
(EngineCore_DP0 pid=285977) 	prmt.b32 	%r168, %r167, %r166, 0x3340U;
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs84, %r162;
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs85, %r161;
(EngineCore_DP0 pid=285977) 	mov.b32 	%r169, {%rs85, %rs84};
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs86, %rs87}, %r169;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r170, %rs87;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r171, %rs86;
(EngineCore_DP0 pid=285977) 	prmt.b32 	%r172, %r171, %r170, 0x3340U;
(EngineCore_DP0 pid=285977) 	prmt.b32 	%r42, %r172, %r168, 0x5410U;
(EngineCore_DP0 pid=285977) 	.loc	1 183 82                        // quant_only_tuned_Llama3.2-1B.py:183:82
(EngineCore_DP0 pid=285977) 	max.f32 	%r173, %r124, 0fC3000000;
(EngineCore_DP0 pid=285977) 	max.f32 	%r174, %r123, 0fC3000000;
(EngineCore_DP0 pid=285977) 	max.f32 	%r175, %r122, 0fC3000000;
(EngineCore_DP0 pid=285977) 	max.f32 	%r176, %r121, 0fC3000000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r177, %r176, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r178, %r175, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r179, %r174, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	min.f32 	%r180, %r173, 0f42FE0000;
(EngineCore_DP0 pid=285977) 	.loc	1 184 48                        // quant_only_tuned_Llama3.2-1B.py:184:48
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs88, %r180;
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs89, %r179;
(EngineCore_DP0 pid=285977) 	mov.b32 	%r181, {%rs89, %rs88};
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs90, %rs91}, %r181;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r182, %rs91;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r183, %rs90;
(EngineCore_DP0 pid=285977) 	prmt.b32 	%r184, %r183, %r182, 0x3340U;
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs92, %r178;
(EngineCore_DP0 pid=285977) 	cvt.rzi.s16.f32 	%rs93, %r177;
(EngineCore_DP0 pid=285977) 	mov.b32 	%r185, {%rs93, %rs92};
(EngineCore_DP0 pid=285977) 	mov.b32 	{%rs94, %rs95}, %r185;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r186, %rs95;
(EngineCore_DP0 pid=285977) 	cvt.u32.u16 	%r187, %rs94;
(EngineCore_DP0 pid=285977) 	prmt.b32 	%r188, %r187, %r186, 0x3340U;
(EngineCore_DP0 pid=285977) 	prmt.b32 	%r43, %r188, %r184, 0x5410U;
(EngineCore_DP0 pid=285977) 	.loc	1 184 39                        // quant_only_tuned_Llama3.2-1B.py:184:39
(EngineCore_DP0 pid=285977) 	// begin inline asm
(EngineCore_DP0 pid=285977) 	@%p1 st.global.v4.b32 [ %rd6 + 0 ], { %r40, %r41, %r42, %r43 };
(EngineCore_DP0 pid=285977) 	// end inline asm
(EngineCore_DP0 pid=285977) 	.loc	1 179 4                         // quant_only_tuned_Llama3.2-1B.py:179:4
(EngineCore_DP0 pid=285977) 	ret;
(EngineCore_DP0 pid=285977) $L__tmp3:
(EngineCore_DP0 pid=285977) $L__func_end0:
(EngineCore_DP0 pid=285977)                                         // -- End function
(EngineCore_DP0 pid=285977) }
(EngineCore_DP0 pid=285977) 	.file	1 "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=285977) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=285977) 	.section	.debug_abbrev
(EngineCore_DP0 pid=285977) 	{
(EngineCore_DP0 pid=285977) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=285977) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=285977) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=285977) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=285977) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=285977) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=285977) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=285977) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=285977) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=285977) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=285977) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=285977) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=285977) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=285977) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=285977) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=285977) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=285977) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=285977) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=285977) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=285977) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=285977) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=285977) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=285977) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=285977) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=285977) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=285977) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=285977) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=285977) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=285977) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=285977) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=285977) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=285977) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=285977) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=285977) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=285977) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=285977) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=285977) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=285977) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=285977) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=285977) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=285977) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=285977) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=285977) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=285977) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=285977) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=285977) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=285977) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=285977) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=285977) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=285977) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=285977) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=285977) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=285977) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=285977) 	}
(EngineCore_DP0 pid=285977) 	.section	.debug_info
(EngineCore_DP0 pid=285977) 	{
(EngineCore_DP0 pid=285977) .b32 214                                // Length of Unit
(EngineCore_DP0 pid=285977) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=285977) .b8 0
(EngineCore_DP0 pid=285977) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=285977) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=285977) .b8 1                                   // Abbrev [1] 0xb:0xcf DW_TAG_compile_unit
(EngineCore_DP0 pid=285977) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=285977) .b8 114
(EngineCore_DP0 pid=285977) .b8 105
(EngineCore_DP0 pid=285977) .b8 116
(EngineCore_DP0 pid=285977) .b8 111
(EngineCore_DP0 pid=285977) .b8 110
(EngineCore_DP0 pid=285977) .b8 0
(EngineCore_DP0 pid=285977) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=285977) .b8 0
(EngineCore_DP0 pid=285977) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=285977) .b8 117
(EngineCore_DP0 pid=285977) .b8 97
(EngineCore_DP0 pid=285977) .b8 110
(EngineCore_DP0 pid=285977) .b8 116
(EngineCore_DP0 pid=285977) .b8 95
(EngineCore_DP0 pid=285977) .b8 111
(EngineCore_DP0 pid=285977) .b8 110
(EngineCore_DP0 pid=285977) .b8 108
(EngineCore_DP0 pid=285977) .b8 121
(EngineCore_DP0 pid=285977) .b8 95
(EngineCore_DP0 pid=285977) .b8 116
(EngineCore_DP0 pid=285977) .b8 117
(EngineCore_DP0 pid=285977) .b8 110
(EngineCore_DP0 pid=285977) .b8 101
(EngineCore_DP0 pid=285977) .b8 100
(EngineCore_DP0 pid=285977) .b8 95
(EngineCore_DP0 pid=285977) .b8 76
(EngineCore_DP0 pid=285977) .b8 108
(EngineCore_DP0 pid=285977) .b8 97
(EngineCore_DP0 pid=285977) .b8 109
(EngineCore_DP0 pid=285977) .b8 97
(EngineCore_DP0 pid=285977) .b8 51
(EngineCore_DP0 pid=285977) .b8 46
(EngineCore_DP0 pid=285977) .b8 50
(EngineCore_DP0 pid=285977) .b8 45
(EngineCore_DP0 pid=285977) .b8 49
(EngineCore_DP0 pid=285977) .b8 66
(EngineCore_DP0 pid=285977) .b8 46
(EngineCore_DP0 pid=285977) .b8 112
(EngineCore_DP0 pid=285977) .b8 121
(EngineCore_DP0 pid=285977) .b8 0
(EngineCore_DP0 pid=285977) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=285977) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=285977) .b8 114
(EngineCore_DP0 pid=285977) .b8 111
(EngineCore_DP0 pid=285977) .b8 111
(EngineCore_DP0 pid=285977) .b8 116
(EngineCore_DP0 pid=285977) .b8 47
(EngineCore_DP0 pid=285977) .b8 118
(EngineCore_DP0 pid=285977) .b8 108
(EngineCore_DP0 pid=285977) .b8 108
(EngineCore_DP0 pid=285977) .b8 109
(EngineCore_DP0 pid=285977) .b8 98
(EngineCore_DP0 pid=285977) .b8 101
(EngineCore_DP0 pid=285977) .b8 110
(EngineCore_DP0 pid=285977) .b8 99
(EngineCore_DP0 pid=285977) .b8 104
(EngineCore_DP0 pid=285977) .b8 47
(EngineCore_DP0 pid=285977) .b8 115
(EngineCore_DP0 pid=285977) .b8 108
(EngineCore_DP0 pid=285977) .b8 105
(EngineCore_DP0 pid=285977) .b8 100
(EngineCore_DP0 pid=285977) .b8 101
(EngineCore_DP0 pid=285977) .b8 115
(EngineCore_DP0 pid=285977) .b8 112
(EngineCore_DP0 pid=285977) .b8 97
(EngineCore_DP0 pid=285977) .b8 114
(EngineCore_DP0 pid=285977) .b8 115
(EngineCore_DP0 pid=285977) .b8 101
(EngineCore_DP0 pid=285977) .b8 47
(EngineCore_DP0 pid=285977) .b8 99
(EngineCore_DP0 pid=285977) .b8 115
(EngineCore_DP0 pid=285977) .b8 114
(EngineCore_DP0 pid=285977) .b8 99
(EngineCore_DP0 pid=285977) .b8 47
(EngineCore_DP0 pid=285977) .b8 113
(EngineCore_DP0 pid=285977) .b8 117
(EngineCore_DP0 pid=285977) .b8 97
(EngineCore_DP0 pid=285977) .b8 110
(EngineCore_DP0 pid=285977) .b8 116
(EngineCore_DP0 pid=285977) .b8 95
(EngineCore_DP0 pid=285977) .b8 111
(EngineCore_DP0 pid=285977) .b8 110
(EngineCore_DP0 pid=285977) .b8 108
(EngineCore_DP0 pid=285977) .b8 121
(EngineCore_DP0 pid=285977) .b8 95
(EngineCore_DP0 pid=285977) .b8 116
(EngineCore_DP0 pid=285977) .b8 114
(EngineCore_DP0 pid=285977) .b8 105
(EngineCore_DP0 pid=285977) .b8 116
(EngineCore_DP0 pid=285977) .b8 111
(EngineCore_DP0 pid=285977) .b8 110
(EngineCore_DP0 pid=285977) .b8 47
(EngineCore_DP0 pid=285977) .b8 98
(EngineCore_DP0 pid=285977) .b8 117
(EngineCore_DP0 pid=285977) .b8 105
(EngineCore_DP0 pid=285977) .b8 108
(EngineCore_DP0 pid=285977) .b8 100
(EngineCore_DP0 pid=285977) .b8 47
(EngineCore_DP0 pid=285977) .b8 71
(EngineCore_DP0 pid=285977) .b8 66
(EngineCore_DP0 pid=285977) .b8 49
(EngineCore_DP0 pid=285977) .b8 48
(EngineCore_DP0 pid=285977) .b8 95
(EngineCore_DP0 pid=285977) .b8 99
(EngineCore_DP0 pid=285977) .b8 99
(EngineCore_DP0 pid=285977) .b8 49
(EngineCore_DP0 pid=285977) .b8 50
(EngineCore_DP0 pid=285977) .b8 49
(EngineCore_DP0 pid=285977) .b8 95
(EngineCore_DP0 pid=285977) .b8 112
(EngineCore_DP0 pid=285977) .b8 121
(EngineCore_DP0 pid=285977) .b8 51
(EngineCore_DP0 pid=285977) .b8 49
(EngineCore_DP0 pid=285977) .b8 50
(EngineCore_DP0 pid=285977) .b8 95
(EngineCore_DP0 pid=285977) .b8 99
(EngineCore_DP0 pid=285977) .b8 117
(EngineCore_DP0 pid=285977) .b8 49
(EngineCore_DP0 pid=285977) .b8 50
(EngineCore_DP0 pid=285977) .b8 57
(EngineCore_DP0 pid=285977) .b8 95
(EngineCore_DP0 pid=285977) .b8 97
(EngineCore_DP0 pid=285977) .b8 97
(EngineCore_DP0 pid=285977) .b8 114
(EngineCore_DP0 pid=285977) .b8 99
(EngineCore_DP0 pid=285977) .b8 104
(EngineCore_DP0 pid=285977) .b8 54
(EngineCore_DP0 pid=285977) .b8 52
(EngineCore_DP0 pid=285977) .b8 0
(EngineCore_DP0 pid=285977) .b8 2                                   // Abbrev [2] 0x91:0x1a DW_TAG_subprogram
(EngineCore_DP0 pid=285977) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=285977) .b8 113
(EngineCore_DP0 pid=285977) .b8 117
(EngineCore_DP0 pid=285977) .b8 97
(EngineCore_DP0 pid=285977) .b8 110
(EngineCore_DP0 pid=285977) .b8 116
(EngineCore_DP0 pid=285977) .b8 95
(EngineCore_DP0 pid=285977) .b8 111
(EngineCore_DP0 pid=285977) .b8 110
(EngineCore_DP0 pid=285977) .b8 108
(EngineCore_DP0 pid=285977) .b8 121
(EngineCore_DP0 pid=285977) .b8 95
(EngineCore_DP0 pid=285977) .b8 105
(EngineCore_DP0 pid=285977) .b8 110
(EngineCore_DP0 pid=285977) .b8 116
(EngineCore_DP0 pid=285977) .b8 56
(EngineCore_DP0 pid=285977) .b8 95
(EngineCore_DP0 pid=285977) .b8 107
(EngineCore_DP0 pid=285977) .b8 101
(EngineCore_DP0 pid=285977) .b8 114
(EngineCore_DP0 pid=285977) .b8 110
(EngineCore_DP0 pid=285977) .b8 101
(EngineCore_DP0 pid=285977) .b8 108
(EngineCore_DP0 pid=285977) .b8 0
(EngineCore_DP0 pid=285977) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=285977) .b8 3                                   // Abbrev [3] 0xab:0x2e DW_TAG_subprogram
(EngineCore_DP0 pid=285977) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=285977) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=285977) .b32 145                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=285977) .b8 4                                   // Abbrev [4] 0xc0:0x18 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=285977) .b32 145                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=285977) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=285977) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=285977) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=285977) .b8 169                                 // DW_AT_call_line
(EngineCore_DP0 pid=285977) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=285977) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=285977) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=285977) 	}
(EngineCore_DP0 pid=285977) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=285977) 
(EngineCore_DP0 pid=285977) ================================================================
(EngineCore_DP0 pid=285977) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=285977) 
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpxvn3v08w.ptx', '-o', '/tmp/tmpxvn3v08w.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866] 
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866] 
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 760, in apply_weights
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 546, in apply
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 194, in cuBLASLt_INT8_linear
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     qinput, scale_a_pad = quant_only_int8_kernel(input, model_name)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 369, in quant_only_int8_kernel
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return torch.ops.slidesparse.quant_only_int8(input, model_name)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 544, in _quant_only_int8_impl
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return fn(input)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]            ^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py", line 206, in quant_only_int8_triton
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     _quant_only_int8_kernel[(M,)](
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866] 
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpxvn3v08w.ptx -o /tmp/tmpxvn3v08w.ptx.o
(EngineCore_DP0 pid=285977) ERROR 01-25 18:37:46 [core.py:866] 

STDERR:
[2026-01-25 18:37:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:37:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:37:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:37:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:37:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:37:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:37:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:37:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:37:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:37:35] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:37:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:37:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:37:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:37:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:37:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:37:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:37:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:37:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:37:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=285977) [2026-01-25 18:37:36] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=285977) [2026-01-25 18:37:36] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=285977) [2026-01-25 18:37:36] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=285977) [2026-01-25 18:37:36] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=285977) [2026-01-25 18:37:36] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=285977) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=285977) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.52s/it]
(EngineCore_DP0 pid=285977) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.52s/it]
(EngineCore_DP0 pid=285977) 
(EngineCore_DP0 pid=285977) Process EngineCore_DP0:
(EngineCore_DP0 pid=285977) Traceback (most recent call last):
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=285977)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=285977)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=285977)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=285977) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpxvn3v08w.ptx', '-o', '/tmp/tmpxvn3v08w.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=285977) 
(EngineCore_DP0 pid=285977) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=285977) 
(EngineCore_DP0 pid=285977) Traceback (most recent call last):
(EngineCore_DP0 pid=285977)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=285977)     self.run()
(EngineCore_DP0 pid=285977)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=285977)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=285977)     raise e
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=285977)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=285977)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=285977)     super().__init__(
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=285977)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=285977)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=285977)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=285977)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=285977)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=285977)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=285977)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=285977)     return func(*args, **kwargs)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=285977)     return func(*args, **kwargs)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=285977)     self.model_runner.profile_run()
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=285977)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=285977)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=285977)     return func(*args, **kwargs)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=285977)     outputs = self.model(
(EngineCore_DP0 pid=285977)               ^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285977)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285977)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=285977)     model_output = self.model(
(EngineCore_DP0 pid=285977)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=285977)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=285977)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=285977)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285977)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285977)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=285977)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=285977)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285977)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285977)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=285977)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=285977)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=285977)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=285977)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=285977)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=285977)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=285977)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 760, in apply_weights
(EngineCore_DP0 pid=285977)     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 546, in apply
(EngineCore_DP0 pid=285977)     return self._linear_fn(
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 194, in cuBLASLt_INT8_linear
(EngineCore_DP0 pid=285977)     qinput, scale_a_pad = quant_only_int8_kernel(input, model_name)
(EngineCore_DP0 pid=285977)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/slidesparse/core/kernels.py", line 369, in quant_only_int8_kernel
(EngineCore_DP0 pid=285977)     return torch.ops.slidesparse.quant_only_int8(input, model_name)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=285977)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/slidesparse/core/kernels.py", line 544, in _quant_only_int8_impl
(EngineCore_DP0 pid=285977)     return fn(input)
(EngineCore_DP0 pid=285977)            ^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/root/vllmbench/slidesparse/csrc/quant_only_triton/build/GB10_cc121_py312_cu129_aarch64/quant_only_tuned_Llama3.2-1B.py", line 206, in quant_only_int8_triton
(EngineCore_DP0 pid=285977)     _quant_only_int8_kernel[(M,)](
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=285977)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=285977)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=285977)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=285977)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=285977)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=285977)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=285977)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=285977)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=285977)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=285977)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=285977)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=285977)     raise PTXASError(error)
(EngineCore_DP0 pid=285977) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=285977) `ptxas` stderr:
(EngineCore_DP0 pid=285977) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=285977) 
(EngineCore_DP0 pid=285977) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpxvn3v08w.ptx -o /tmp/tmpxvn3v08w.ptx.o
(EngineCore_DP0 pid=285977) 
[rank0]:[W125 18:37:46.686472309 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=256

========== M=16 ==========
Time: 2026-01-25 22:41:28
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-INT8_M16.json --enforce-eager


========== M=16 ==========
Time: 2026-01-26 02:22:33
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-INT8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:22:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:22:36 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=735693) WARNING 01-26 02:22:54 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.29 requests/s, 1438.88 total tokens/s, 1354.24 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096

STDERR:
[2026-01-26 02:22:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:22:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:22:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:22:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:22:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:22:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:22:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:22:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:22:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:22:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:22:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:22:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:22:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:22:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:22:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:22:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:22:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:22:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=735693) [2026-01-26 02:22:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=735693) [2026-01-26 02:22:41] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=735693) [2026-01-26 02:22:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=735693) [2026-01-26 02:22:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=735693) [2026-01-26 02:22:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=735693) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=735693) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.89s/it]
(EngineCore_DP0 pid=735693) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.89s/it]
(EngineCore_DP0 pid=735693) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=735693) 2026-01-26 02:22:54,273 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=735693) 2026-01-26 02:22:54,283 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|| 16/16 [00:00<00:00, 8557.62it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|         | 1/16 [00:03<00:45,  3.01s/it, est. speed input: 5.32 toks/s, output: 85.10 toks/s]
Processed prompts: 100%|| 16/16 [00:03<00:00,  3.01s/it, est. speed input: 84.71 toks/s, output: 1355.42 toks/s]
Processed prompts: 100%|| 16/16 [00:03<00:00,  5.29it/s, est. speed input: 84.71 toks/s, output: 1355.42 toks/s]
[rank0]:[W126 02:22:58.211402805 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 02:23:00
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:23:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:23:03 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=736276) WARNING 01-26 02:23:21 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 23.28 requests/s, 6332.62 total tokens/s, 5960.11 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 02:23:03] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:23:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:23:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:23:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:23:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:23:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:23:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:23:06] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:23:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:23:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:23:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:23:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:23:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:23:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=736276) [2026-01-26 02:23:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=736276) [2026-01-26 02:23:07] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=736276) [2026-01-26 02:23:07] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=736276) [2026-01-26 02:23:07] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=736276) [2026-01-26 02:23:07] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=736276) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=736276) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.12s/it]
(EngineCore_DP0 pid=736276) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.12s/it]
(EngineCore_DP0 pid=736276) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=736276) 2026-01-26 02:23:21,363 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=736276) 2026-01-26 02:23:21,376 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 13084.20it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:05<11:06,  5.25s/it, est. speed input: 3.05 toks/s, output: 48.79 toks/s]
Processed prompts:  59%|    | 76/128 [00:05<00:02, 20.00it/s, est. speed input: 226.87 toks/s, output: 3629.91 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 37.80it/s, est. speed input: 373.22 toks/s, output: 5971.58 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 37.80it/s, est. speed input: 373.22 toks/s, output: 5971.58 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 23.33it/s, est. speed input: 373.22 toks/s, output: 5971.58 toks/s]
[rank0]:[W126 02:23:27.810189298 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 02:23:29
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:23:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:23:33 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=736868) WARNING 01-26 02:23:51 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 28.71 requests/s, 7807.80 total tokens/s, 7348.52 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 02:23:33] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:23:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:23:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:23:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:23:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:23:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:23:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:23:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:23:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:23:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:23:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:23:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:23:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:23:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=736868) [2026-01-26 02:23:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=736868) [2026-01-26 02:23:37] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=736868) [2026-01-26 02:23:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=736868) [2026-01-26 02:23:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=736868) [2026-01-26 02:23:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=736868) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=736868) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.12s/it]
(EngineCore_DP0 pid=736868) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.12s/it]
(EngineCore_DP0 pid=736868) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=736868) 2026-01-26 02:23:50,831 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=736868) 2026-01-26 02:23:50,844 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 13585.99it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<33:49,  7.96s/it, est. speed input: 2.01 toks/s, output: 32.17 toks/s]
Processed prompts:  14%|        | 35/256 [00:08<00:36,  6.12it/s, est. speed input: 69.48 toks/s, output: 1111.74 toks/s]
Processed prompts:  34%|      | 88/256 [00:08<00:08, 18.93it/s, est. speed input: 172.29 toks/s, output: 2756.67 toks/s]
Processed prompts:  54%|    | 137/256 [00:08<00:03, 34.62it/s, est. speed input: 264.36 toks/s, output: 4229.80 toks/s]
Processed prompts:  70%|   | 179/256 [00:08<00:01, 51.87it/s, est. speed input: 340.61 toks/s, output: 5449.79 toks/s]
Processed prompts:  84%| | 215/256 [00:08<00:00, 69.67it/s, est. speed input: 403.21 toks/s, output: 6451.40 toks/s]
Processed prompts:  97%|| 249/256 [00:08<00:00, 82.83it/s, est. speed input: 454.96 toks/s, output: 7279.34 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 82.83it/s, est. speed input: 460.30 toks/s, output: 7364.86 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 28.77it/s, est. speed input: 460.30 toks/s, output: 7364.86 toks/s]
[rank0]:[W126 02:24:00.741122194 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 16:49:16
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:49:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:49:20 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2827769) WARNING 01-27 16:49:40 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.52 requests/s, 4221.63 total tokens/s, 3973.30 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 16:49:20] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:49:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:49:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:49:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:49:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:49:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:49:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:49:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:49:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:49:23] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:49:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:49:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:49:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:49:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:49:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:49:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:49:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:49:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2827769) [2026-01-27 16:49:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2827769) [2026-01-27 16:49:25] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2827769) [2026-01-27 16:49:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2827769) [2026-01-27 16:49:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2827769) [2026-01-27 16:49:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2827769) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2827769) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.26s/it]
(EngineCore_DP0 pid=2827769) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.26s/it]
(EngineCore_DP0 pid=2827769) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2827769) 2026-01-27 16:49:39,780 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2827769) 2026-01-27 16:49:39,793 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 11656.41it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:04<04:14,  4.04s/it, est. speed input: 3.96 toks/s, output: 63.33 toks/s]
Processed prompts: 100%|| 64/64 [00:04<00:00,  4.04s/it, est. speed input: 248.72 toks/s, output: 3979.48 toks/s]
Processed prompts: 100%|| 64/64 [00:04<00:00, 15.54it/s, est. speed input: 248.72 toks/s, output: 3979.48 toks/s]
[rank0]:[W127 16:49:44.728843553 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 16:49:47
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:49:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:49:52 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2828398) WARNING 01-27 16:50:11 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 22.90 requests/s, 6229.46 total tokens/s, 5863.02 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 16:49:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:49:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:49:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:49:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:49:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:49:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:49:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:49:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:49:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:49:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:49:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:49:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:49:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:49:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:49:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:49:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:49:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:49:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:49:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2828398) [2026-01-27 16:49:57] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2828398) [2026-01-27 16:49:57] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2828398) [2026-01-27 16:49:57] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2828398) [2026-01-27 16:49:57] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2828398) [2026-01-27 16:49:57] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2828398) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2828398) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.08s/it]
(EngineCore_DP0 pid=2828398) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.08s/it]
(EngineCore_DP0 pid=2828398) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2828398) 2026-01-27 16:50:11,084 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2828398) 2026-01-27 16:50:11,097 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 13357.66it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:05<11:12,  5.30s/it, est. speed input: 3.02 toks/s, output: 48.32 toks/s]
Processed prompts:  59%|    | 76/128 [00:05<00:02, 19.82it/s, est. speed input: 224.79 toks/s, output: 3596.57 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 36.93it/s, est. speed input: 367.12 toks/s, output: 5873.93 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 36.93it/s, est. speed input: 367.12 toks/s, output: 5873.93 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 22.94it/s, est. speed input: 367.12 toks/s, output: 5873.93 toks/s]
[rank0]:[W127 16:50:17.551425487 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 16:50:19
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:50:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:50:23 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2829012) WARNING 01-27 16:50:41 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 28.89 requests/s, 7858.77 total tokens/s, 7396.49 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 16:50:23] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:50:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:50:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:50:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:50:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:50:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:50:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:50:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:50:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:50:26] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:50:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:50:26] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:50:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:50:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:50:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:50:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:50:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:50:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2829012) [2026-01-27 16:50:27] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2829012) [2026-01-27 16:50:27] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2829012) [2026-01-27 16:50:27] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2829012) [2026-01-27 16:50:27] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2829012) [2026-01-27 16:50:27] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2829012) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2829012) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.01s/it]
(EngineCore_DP0 pid=2829012) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.01s/it]
(EngineCore_DP0 pid=2829012) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2829012) 2026-01-27 16:50:41,082 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2829012) 2026-01-27 16:50:41,095 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 13316.57it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<33:35,  7.90s/it, est. speed input: 2.02 toks/s, output: 32.39 toks/s]
Processed prompts:  21%|        | 53/256 [00:08<00:21,  9.31it/s, est. speed input: 105.59 toks/s, output: 1689.37 toks/s]
Processed prompts:  39%|      | 101/256 [00:08<00:07, 20.87it/s, est. speed input: 198.61 toks/s, output: 3177.79 toks/s]
Processed prompts:  57%|    | 146/256 [00:08<00:03, 35.27it/s, est. speed input: 283.16 toks/s, output: 4530.60 toks/s]
Processed prompts:  72%|  | 184/256 [00:08<00:01, 50.91it/s, est. speed input: 352.18 toks/s, output: 5634.85 toks/s]
Processed prompts:  86%| | 220/256 [00:08<00:00, 68.43it/s, est. speed input: 414.39 toks/s, output: 6630.20 toks/s]
Processed prompts:  99%|| 253/256 [00:08<00:00, 79.96it/s, est. speed input: 463.20 toks/s, output: 7411.11 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 79.96it/s, est. speed input: 463.34 toks/s, output: 7413.44 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 28.96it/s, est. speed input: 463.34 toks/s, output: 7413.44 toks/s]
[rank0]:[W127 16:50:50.775577342 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 16:50:53
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Llama3.2-1B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:50:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:50:56 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2829669) WARNING 01-27 16:51:15 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 29.39 requests/s, 7992.80 total tokens/s, 7522.64 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 16:50:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:50:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:50:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:50:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:50:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:50:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:50:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:50:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:50:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:50:59] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:50:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:50:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:50:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:50:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:50:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:50:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:50:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:50:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:50:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2829669) [2026-01-27 16:51:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2829669) [2026-01-27 16:51:00] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2829669) [2026-01-27 16:51:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2829669) [2026-01-27 16:51:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2829669) [2026-01-27 16:51:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2829669) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2829669) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.02s/it]
(EngineCore_DP0 pid=2829669) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.02s/it]
(EngineCore_DP0 pid=2829669) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2829669) 2026-01-27 16:51:14,543 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2829669) 2026-01-27 16:51:14,586 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 14216.94it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:13<1:55:43, 13.59s/it, est. speed input: 1.18 toks/s, output: 18.84 toks/s]
Processed prompts:   1%|          | 6/512 [00:13<14:14,  1.69s/it, est. speed input: 7.01 toks/s, output: 112.12 toks/s] 
Processed prompts:  13%|        | 66/512 [00:13<00:48,  9.24it/s, est. speed input: 76.02 toks/s, output: 1216.33 toks/s]
Processed prompts:  23%|       | 119/512 [00:14<00:20, 19.52it/s, est. speed input: 135.51 toks/s, output: 2168.18 toks/s]
Processed prompts:  37%|      | 187/512 [00:14<00:08, 37.52it/s, est. speed input: 210.98 toks/s, output: 3375.64 toks/s]
Processed prompts:  47%|     | 243/512 [00:14<00:04, 57.10it/s, est. speed input: 272.09 toks/s, output: 4353.50 toks/s]
Processed prompts:  56%|    | 288/512 [00:14<00:02, 77.05it/s, est. speed input: 320.17 toks/s, output: 5122.74 toks/s]
Processed prompts:  66%|   | 338/512 [00:14<00:01, 104.74it/s, est. speed input: 372.79 toks/s, output: 5964.65 toks/s]
Processed prompts:  75%|  | 384/512 [00:14<00:00, 133.48it/s, est. speed input: 420.04 toks/s, output: 6720.59 toks/s]
Processed prompts:  83%| | 426/512 [00:14<00:00, 161.53it/s, est. speed input: 462.22 toks/s, output: 7395.56 toks/s]
Processed prompts:  91%| | 466/512 [00:14<00:00, 176.27it/s, est. speed input: 499.78 toks/s, output: 7996.49 toks/s]
Processed prompts:  98%|| 501/512 [00:17<00:00, 46.58it/s, est. speed input: 463.97 toks/s, output: 7423.57 toks/s] 
Processed prompts: 100%|| 512/512 [00:17<00:00, 46.58it/s, est. speed input: 471.17 toks/s, output: 7538.68 toks/s]
Processed prompts: 100%|| 512/512 [00:17<00:00, 29.45it/s, est. speed input: 471.17 toks/s, output: 7538.68 toks/s]
[rank0]:[W127 16:51:32.853423265 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:13:10
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Llama3.2-3B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:13:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:13:13 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2855389) WARNING 01-27 17:13:44 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 1.04 requests/s, 283.47 total tokens/s, 266.80 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:13:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:13:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:13:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:13:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:13:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:13:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:13:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:13:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:13:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:13:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:13:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:13:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:13:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:13:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:13:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:13:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:13:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:13:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:13:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:13:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:13:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:13:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:13:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:13:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:13:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:13:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:13:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:13:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2855389) [2026-01-27 17:13:18] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2855389) [2026-01-27 17:13:18] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2855389) [2026-01-27 17:13:18] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2855389) [2026-01-27 17:13:18] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2855389) [2026-01-27 17:13:18] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2855389) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2855389) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.86s/it]
(EngineCore_DP0 pid=2855389) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.86s/it]
(EngineCore_DP0 pid=2855389) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2855389) 2026-01-27 17:13:43,894 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2855389) 2026-01-27 17:13:43,908 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 4457.14it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [01:01<1:04:18, 61.25s/it, est. speed input: 0.26 toks/s, output: 4.18 toks/s]
Processed prompts:  75%|  | 48/64 [01:01<00:14,  1.11it/s, est. speed input: 12.51 toks/s, output: 200.22 toks/s]
Processed prompts: 100%|| 64/64 [01:01<00:00,  1.11it/s, est. speed input: 16.68 toks/s, output: 266.87 toks/s]
Processed prompts: 100%|| 64/64 [01:01<00:00,  1.04it/s, est. speed input: 16.68 toks/s, output: 266.87 toks/s]
[rank0]:[W127 17:14:46.043632817 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:14:48
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Llama3.2-3B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:14:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:14:52 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2856945) WARNING 01-27 17:15:21 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 11.30 requests/s, 3074.05 total tokens/s, 2893.22 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:14:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:14:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:14:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:14:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:14:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:14:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:14:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:14:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:14:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:14:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:14:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:14:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:14:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:14:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:14:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:14:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:14:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:14:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:14:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:14:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:14:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:14:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:14:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:14:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:14:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:14:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:14:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:14:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2856945) [2026-01-27 17:14:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2856945) [2026-01-27 17:14:56] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2856945) [2026-01-27 17:14:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2856945) [2026-01-27 17:14:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2856945) [2026-01-27 17:14:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2856945) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2856945) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.27s/it]
(EngineCore_DP0 pid=2856945) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.27s/it]
(EngineCore_DP0 pid=2856945) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2856945) 2026-01-27 17:15:21,193 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2856945) 2026-01-27 17:15:21,206 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 4991.97it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:10<22:54, 10.82s/it, est. speed input: 1.48 toks/s, output: 23.66 toks/s]
Processed prompts:  16%|        | 20/128 [00:10<00:42,  2.57it/s, est. speed input: 29.30 toks/s, output: 468.85 toks/s]
Processed prompts:  49%|     | 63/128 [00:11<00:06, 10.34it/s, est. speed input: 91.30 toks/s, output: 1460.76 toks/s]
Processed prompts:  85%| | 109/128 [00:11<00:00, 21.63it/s, est. speed input: 156.27 toks/s, output: 2500.37 toks/s]
Processed prompts: 100%|| 128/128 [00:11<00:00, 21.63it/s, est. speed input: 181.26 toks/s, output: 2900.08 toks/s]
Processed prompts: 100%|| 128/128 [00:11<00:00, 11.33it/s, est. speed input: 181.26 toks/s, output: 2900.08 toks/s]
[rank0]:[W127 17:15:33.356989378 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:15:35
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Llama3.2-3B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:15:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:15:39 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2857780) WARNING 01-27 17:16:08 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.58 requests/s, 3965.06 total tokens/s, 3731.82 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:15:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:15:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:15:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:15:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:15:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:15:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:15:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:15:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:15:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:15:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:15:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:15:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:15:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:15:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:15:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:15:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:15:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:15:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:15:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:15:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:15:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:15:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:15:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:15:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:15:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:15:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:15:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:15:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2857780) [2026-01-27 17:15:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2857780) [2026-01-27 17:15:43] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2857780) [2026-01-27 17:15:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2857780) [2026-01-27 17:15:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2857780) [2026-01-27 17:15:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2857780) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2857780) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.43s/it]
(EngineCore_DP0 pid=2857780) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.43s/it]
(EngineCore_DP0 pid=2857780) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2857780) 2026-01-27 17:16:08,325 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2857780) 2026-01-27 17:16:08,339 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 4737.32it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:15<1:06:09, 15.57s/it, est. speed input: 1.03 toks/s, output: 16.45 toks/s]
Processed prompts:   7%|         | 18/256 [00:15<02:28,  1.60it/s, est. speed input: 18.33 toks/s, output: 293.21 toks/s]
Processed prompts:  19%|        | 48/256 [00:15<00:38,  5.38it/s, est. speed input: 48.44 toks/s, output: 774.97 toks/s]
Processed prompts:  29%|       | 75/256 [00:15<00:17, 10.06it/s, est. speed input: 75.11 toks/s, output: 1201.70 toks/s]
Processed prompts:  38%|      | 98/256 [00:16<00:10, 15.46it/s, est. speed input: 97.45 toks/s, output: 1559.23 toks/s]
Processed prompts:  46%|     | 118/256 [00:16<00:06, 21.71it/s, est. speed input: 116.59 toks/s, output: 1865.41 toks/s]
Processed prompts:  56%|    | 144/256 [00:16<00:03, 32.06it/s, est. speed input: 141.00 toks/s, output: 2256.07 toks/s]
Processed prompts:  65%|   | 166/256 [00:16<00:02, 42.75it/s, est. speed input: 161.28 toks/s, output: 2580.45 toks/s]
Processed prompts:  72%|  | 185/256 [00:16<00:01, 53.59it/s, est. speed input: 178.46 toks/s, output: 2855.41 toks/s]
Processed prompts:  79%|  | 203/256 [00:16<00:00, 63.14it/s, est. speed input: 194.09 toks/s, output: 3105.40 toks/s]
Processed prompts:  86%| | 219/256 [00:16<00:00, 69.87it/s, est. speed input: 207.43 toks/s, output: 3318.80 toks/s]
Processed prompts:  91%|| 234/256 [00:17<00:00, 73.79it/s, est. speed input: 219.42 toks/s, output: 3510.76 toks/s]
Processed prompts:  96%|| 247/256 [00:17<00:00, 68.74it/s, est. speed input: 228.53 toks/s, output: 3656.51 toks/s]
Processed prompts: 100%|| 256/256 [00:17<00:00, 68.74it/s, est. speed input: 233.98 toks/s, output: 3743.61 toks/s]
Processed prompts: 100%|| 256/256 [00:17<00:00, 14.62it/s, est. speed input: 233.98 toks/s, output: 3743.61 toks/s]
[rank0]:[W127 17:16:26.664716056 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:16:28
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Llama3.2-3B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:16:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:16:32 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2858716) WARNING 01-27 17:17:02 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.75 requests/s, 4012.55 total tokens/s, 3776.52 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:16:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:16:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:16:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:16:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:16:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:16:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:16:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:16:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:16:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:16:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:16:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:16:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:16:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:16:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:16:35] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:16:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:16:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:16:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:16:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:16:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:16:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:16:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:16:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:16:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:16:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:16:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:16:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:16:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2858716) [2026-01-27 17:16:36] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2858716) [2026-01-27 17:16:36] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2858716) [2026-01-27 17:16:36] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2858716) [2026-01-27 17:16:36] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2858716) [2026-01-27 17:16:36] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2858716) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2858716) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.32s/it]
(EngineCore_DP0 pid=2858716) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.32s/it]
(EngineCore_DP0 pid=2858716) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2858716) 2026-01-27 17:17:01,770 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2858716) 2026-01-27 17:17:01,793 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 14402.78it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:26<3:43:03, 26.19s/it, est. speed input: 0.61 toks/s, output: 9.77 toks/s]
Processed prompts:   1%|          | 6/512 [00:26<27:29,  3.26s/it, est. speed input: 3.63 toks/s, output: 58.09 toks/s] 
Processed prompts:   7%|         | 37/512 [00:26<02:59,  2.64it/s, est. speed input: 22.18 toks/s, output: 354.95 toks/s]
Processed prompts:  13%|        | 66/512 [00:26<01:19,  5.63it/s, est. speed input: 39.23 toks/s, output: 627.71 toks/s]
Processed prompts:  18%|        | 93/512 [00:27<00:44,  9.38it/s, est. speed input: 54.84 toks/s, output: 877.42 toks/s]
Processed prompts:  23%|       | 119/512 [00:27<00:27, 14.18it/s, est. speed input: 69.65 toks/s, output: 1114.37 toks/s]
Processed prompts:  32%|      | 166/512 [00:27<00:13, 26.12it/s, est. speed input: 96.47 toks/s, output: 1543.51 toks/s]
Processed prompts:  40%|      | 207/512 [00:27<00:07, 39.28it/s, est. speed input: 119.54 toks/s, output: 1912.56 toks/s]
Processed prompts:  47%|     | 243/512 [00:27<00:05, 53.26it/s, est. speed input: 139.48 toks/s, output: 2231.72 toks/s]
Processed prompts:  54%|    | 274/512 [00:28<00:03, 67.46it/s, est. speed input: 156.44 toks/s, output: 2503.00 toks/s]
Processed prompts:  59%|    | 302/512 [00:28<00:02, 82.39it/s, est. speed input: 171.60 toks/s, output: 2745.66 toks/s]
Processed prompts:  64%|   | 327/512 [00:28<00:01, 97.01it/s, est. speed input: 185.00 toks/s, output: 2959.95 toks/s]
Processed prompts:  68%|   | 348/512 [00:28<00:01, 108.67it/s, est. speed input: 196.06 toks/s, output: 3137.04 toks/s]
Processed prompts:  72%|  | 368/512 [00:28<00:01, 121.07it/s, est. speed input: 206.56 toks/s, output: 3304.99 toks/s]
Processed prompts:  77%|  | 392/512 [00:28<00:00, 131.30it/s, est. speed input: 218.92 toks/s, output: 3502.66 toks/s]
Processed prompts:  80%|  | 412/512 [00:28<00:00, 135.79it/s, est. speed input: 229.02 toks/s, output: 3664.38 toks/s]
Processed prompts:  84%| | 430/512 [00:28<00:00, 137.88it/s, est. speed input: 238.00 toks/s, output: 3808.00 toks/s]
Processed prompts:  87%| | 447/512 [00:29<00:00, 131.62it/s, est. speed input: 246.16 toks/s, output: 3938.58 toks/s]
Processed prompts:  90%| | 463/512 [00:29<00:00, 120.39it/s, est. speed input: 253.52 toks/s, output: 4056.33 toks/s]
Processed prompts:  93%|| 477/512 [00:29<00:00, 106.19it/s, est. speed input: 259.58 toks/s, output: 4153.25 toks/s]
Processed prompts:  96%|| 489/512 [00:29<00:00, 80.47it/s, est. speed input: 263.67 toks/s, output: 4218.72 toks/s] 
Processed prompts:  97%|| 499/512 [00:34<00:01,  8.84it/s, est. speed input: 231.25 toks/s, output: 3699.99 toks/s]
Processed prompts: 100%|| 512/512 [00:34<00:00, 11.88it/s, est. speed input: 236.29 toks/s, output: 3780.57 toks/s]
Processed prompts: 100%|| 512/512 [00:34<00:00, 11.88it/s, est. speed input: 236.29 toks/s, output: 3780.57 toks/s]
Processed prompts: 100%|| 512/512 [00:34<00:00, 14.77it/s, est. speed input: 236.29 toks/s, output: 3780.57 toks/s]
[rank0]:[W127 17:17:37.334330475 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:52:01
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Qwen2.5-7B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:52:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:52:04 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2895394) WARNING 01-27 17:53:01 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.18 requests/s, 1136.84 total tokens/s, 1069.96 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:52:04] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:52:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:52:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 17:52:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:52:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:52:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:52:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:52:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:52:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:52:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:52:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:52:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:52:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:52:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:52:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:52:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:52:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 17:52:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:52:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:52:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:52:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:52:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:52:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:52:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:52:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:52:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:52:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:52:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2895394) [2026-01-27 17:52:09] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2895394) [2026-01-27 17:52:09] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2895394) [2026-01-27 17:52:09] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2895394) [2026-01-27 17:52:09] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2895394) [2026-01-27 17:52:09] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2895394) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2895394) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:20<00:20, 20.54s/it]
(EngineCore_DP0 pid=2895394) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:41<00:00, 20.70s/it]
(EngineCore_DP0 pid=2895394) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:41<00:00, 20.68s/it]
(EngineCore_DP0 pid=2895394) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2895394) 2026-01-27 17:52:57,264 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2895394) 2026-01-27 17:52:57,315 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|         | 1/64 [00:00<00:11,  5.69it/s]
Adding requests:   5%|         | 3/64 [00:00<00:05, 11.88it/s]
Adding requests:  11%|         | 7/64 [00:00<00:02, 20.63it/s]
Adding requests:  19%|        | 12/64 [00:00<00:01, 28.54it/s]
Adding requests:  30%|       | 19/64 [00:00<00:01, 40.00it/s]
Adding requests:  45%|     | 29/64 [00:00<00:00, 57.91it/s]
Adding requests:  66%|   | 42/64 [00:00<00:00, 78.76it/s]
Adding requests:  92%|| 59/64 [00:00<00:00, 104.53it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 66.08it/s] 

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:13<14:14, 13.56s/it, est. speed input: 1.18 toks/s, output: 18.88 toks/s]
Processed prompts:   6%|         | 4/64 [00:13<02:35,  2.60s/it, est. speed input: 4.68 toks/s, output: 74.90 toks/s]
Processed prompts:  12%|        | 8/64 [00:13<00:58,  1.04s/it, est. speed input: 9.28 toks/s, output: 148.50 toks/s]
Processed prompts:  19%|        | 12/64 [00:13<00:29,  1.76it/s, est. speed input: 13.82 toks/s, output: 221.13 toks/s]
Processed prompts:  28%|       | 18/64 [00:14<00:13,  3.41it/s, est. speed input: 20.57 toks/s, output: 329.12 toks/s]
Processed prompts:  44%|     | 28/64 [00:14<00:05,  7.16it/s, est. speed input: 31.77 toks/s, output: 508.28 toks/s]
Processed prompts:  88%| | 56/64 [00:14<00:00, 21.01it/s, est. speed input: 62.89 toks/s, output: 1006.26 toks/s]
Processed prompts: 100%|| 64/64 [00:14<00:00, 21.01it/s, est. speed input: 71.87 toks/s, output: 1149.99 toks/s]
Processed prompts: 100%|| 64/64 [00:14<00:00,  4.49it/s, est. speed input: 71.87 toks/s, output: 1149.99 toks/s]
[rank0]:[W127 17:53:17.819287295 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:53:33
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Qwen2.5-7B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:53:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:53:37 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2896867) WARNING 01-27 17:54:27 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.14 requests/s, 1942.11 total tokens/s, 1827.87 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:53:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:53:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:53:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 17:53:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:53:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:53:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:53:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:53:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:53:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:53:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:53:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:53:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:53:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:53:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:53:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:53:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:53:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 17:53:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:53:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:53:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:53:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:53:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:53:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:53:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:53:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:53:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:53:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:53:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2896867) [2026-01-27 17:53:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2896867) [2026-01-27 17:53:41] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2896867) [2026-01-27 17:53:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2896867) [2026-01-27 17:53:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2896867) [2026-01-27 17:53:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2896867) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2896867) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:20<00:20, 20.22s/it]
(EngineCore_DP0 pid=2896867) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.95s/it]
(EngineCore_DP0 pid=2896867) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.99s/it]
(EngineCore_DP0 pid=2896867) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2896867) 2026-01-27 17:54:27,088 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2896867) 2026-01-27 17:54:27,112 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 4427.07it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:17<36:34, 17.28s/it, est. speed input: 0.93 toks/s, output: 14.82 toks/s]
Processed prompts:  14%|        | 18/128 [00:17<01:16,  1.45it/s, est. speed input: 16.53 toks/s, output: 264.54 toks/s]
Processed prompts:  38%|      | 48/128 [00:17<00:16,  4.88it/s, est. speed input: 43.77 toks/s, output: 700.40 toks/s]
Processed prompts:  59%|    | 75/128 [00:17<00:05,  9.15it/s, est. speed input: 67.97 toks/s, output: 1087.46 toks/s]
Processed prompts:  77%|  | 98/128 [00:17<00:02, 14.12it/s, est. speed input: 88.30 toks/s, output: 1412.80 toks/s]
Processed prompts:  99%|| 127/128 [00:17<00:00, 22.45it/s, est. speed input: 113.54 toks/s, output: 1816.66 toks/s]
Processed prompts: 100%|| 128/128 [00:17<00:00, 22.45it/s, est. speed input: 114.43 toks/s, output: 1830.95 toks/s]
Processed prompts: 100%|| 128/128 [00:17<00:00,  7.15it/s, est. speed input: 114.43 toks/s, output: 1830.95 toks/s]
[rank0]:[W127 17:54:45.964046756 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:54:48
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Qwen2.5-7B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:54:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:54:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2898089) WARNING 01-27 17:55:43 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 9.39 requests/s, 2553.21 total tokens/s, 2403.03 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:54:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:54:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:54:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 17:54:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:54:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:54:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:54:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:54:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:54:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:54:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:54:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:54:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:54:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:54:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:54:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:54:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:54:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 17:54:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:54:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:54:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:54:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:54:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:54:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:54:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:54:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:54:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:54:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:54:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2898089) [2026-01-27 17:54:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2898089) [2026-01-27 17:54:56] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2898089) [2026-01-27 17:54:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2898089) [2026-01-27 17:54:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2898089) [2026-01-27 17:54:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2898089) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2898089) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:20<00:20, 20.55s/it]
(EngineCore_DP0 pid=2898089) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:40<00:00, 20.19s/it]
(EngineCore_DP0 pid=2898089) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:40<00:00, 20.25s/it]
(EngineCore_DP0 pid=2898089) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2898089) 2026-01-27 17:55:42,810 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2898089) 2026-01-27 17:55:42,841 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 4117.50it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:24<1:42:28, 24.11s/it, est. speed input: 0.66 toks/s, output: 10.62 toks/s]
Processed prompts:   1%|          | 2/256 [00:24<42:17,  9.99s/it, est. speed input: 1.32 toks/s, output: 21.14 toks/s]  
Processed prompts:  13%|        | 33/256 [00:24<01:23,  2.66it/s, est. speed input: 21.63 toks/s, output: 346.05 toks/s]
Processed prompts:  24%|       | 62/256 [00:24<00:32,  5.96it/s, est. speed input: 40.33 toks/s, output: 645.35 toks/s]
Processed prompts:  34%|      | 87/256 [00:24<00:17,  9.83it/s, est. speed input: 56.22 toks/s, output: 899.50 toks/s]
Processed prompts:  42%|     | 108/256 [00:24<00:10, 14.16it/s, est. speed input: 69.36 toks/s, output: 1109.78 toks/s]
Processed prompts:  50%|     | 127/256 [00:25<00:06, 19.31it/s, est. speed input: 81.10 toks/s, output: 1297.57 toks/s]
Processed prompts:  56%|    | 144/256 [00:25<00:04, 25.16it/s, est. speed input: 91.44 toks/s, output: 1463.10 toks/s]
Processed prompts:  62%|   | 159/256 [00:25<00:03, 31.51it/s, est. speed input: 100.45 toks/s, output: 1607.15 toks/s]
Processed prompts:  67%|   | 172/256 [00:25<00:02, 37.95it/s, est. speed input: 108.13 toks/s, output: 1730.11 toks/s]
Processed prompts:  72%|  | 184/256 [00:25<00:01, 44.55it/s, est. speed input: 115.12 toks/s, output: 1841.92 toks/s]
Processed prompts:  77%|  | 196/256 [00:25<00:01, 52.18it/s, est. speed input: 122.07 toks/s, output: 1953.19 toks/s]
Processed prompts:  81%|  | 207/256 [00:25<00:00, 55.25it/s, est. speed input: 128.11 toks/s, output: 2049.70 toks/s]
Processed prompts:  85%| | 217/256 [00:26<00:00, 57.06it/s, est. speed input: 133.48 toks/s, output: 2135.68 toks/s]
Processed prompts:  88%| | 226/256 [00:26<00:00, 57.48it/s, est. speed input: 138.20 toks/s, output: 2211.26 toks/s]
Processed prompts:  91%|| 234/256 [00:26<00:00, 52.37it/s, est. speed input: 142.03 toks/s, output: 2272.41 toks/s]
Processed prompts:  94%|| 241/256 [00:26<00:00, 51.51it/s, est. speed input: 145.48 toks/s, output: 2327.71 toks/s]
Processed prompts:  97%|| 248/256 [00:26<00:00, 38.45it/s, est. speed input: 147.90 toks/s, output: 2366.32 toks/s]
Processed prompts:  99%|| 253/256 [00:27<00:00, 33.73it/s, est. speed input: 149.64 toks/s, output: 2394.28 toks/s]
Processed prompts: 100%|| 256/256 [00:27<00:00, 33.73it/s, est. speed input: 150.54 toks/s, output: 2408.63 toks/s]
Processed prompts: 100%|| 256/256 [00:27<00:00,  9.41it/s, est. speed input: 150.54 toks/s, output: 2408.63 toks/s]
[rank0]:[W127 17:56:11.195961845 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:56:13
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Qwen2.5-7B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:56:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:56:17 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2899460) WARNING 01-27 17:56:58 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 9.96 requests/s, 2707.94 total tokens/s, 2548.65 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:56:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:56:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:56:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 17:56:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:56:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:56:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:56:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:56:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:56:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:56:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:56:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:56:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:56:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:56:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:56:20] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:56:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:56:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 17:56:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:56:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:56:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:56:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:56:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:56:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:56:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:56:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:56:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:56:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:56:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2899460) [2026-01-27 17:56:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2899460) [2026-01-27 17:56:21] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2899460) [2026-01-27 17:56:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2899460) [2026-01-27 17:56:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2899460) [2026-01-27 17:56:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2899460) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2899460) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:14<00:14, 14.85s/it]
(EngineCore_DP0 pid=2899460) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:29<00:00, 14.99s/it]
(EngineCore_DP0 pid=2899460) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:29<00:00, 14.97s/it]
(EngineCore_DP0 pid=2899460) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2899460) 2026-01-27 17:56:57,265 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2899460) 2026-01-27 17:56:57,293 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 12063.70it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:37<5:15:39, 37.06s/it, est. speed input: 0.43 toks/s, output: 6.91 toks/s]
Processed prompts:   1%|          | 4/512 [00:37<1:00:09,  7.11s/it, est. speed input: 1.71 toks/s, output: 27.37 toks/s]
Processed prompts:   7%|         | 35/512 [00:37<04:26,  1.79it/s, est. speed input: 14.84 toks/s, output: 237.41 toks/s]
Processed prompts:  12%|        | 64/512 [00:38<01:54,  3.91it/s, est. speed input: 26.91 toks/s, output: 430.54 toks/s]
Processed prompts:  18%|        | 92/512 [00:38<01:02,  6.68it/s, est. speed input: 38.38 toks/s, output: 614.03 toks/s]
Processed prompts:  23%|       | 118/512 [00:38<00:39, 10.09it/s, est. speed input: 48.87 toks/s, output: 781.88 toks/s]
Processed prompts:  28%|       | 142/512 [00:38<00:25, 14.59it/s, est. speed input: 58.62 toks/s, output: 937.99 toks/s]
Processed prompts:  32%|      | 165/512 [00:38<00:17, 20.40it/s, est. speed input: 67.92 toks/s, output: 1086.68 toks/s]
Processed prompts:  36%|      | 186/512 [00:38<00:11, 27.43it/s, est. speed input: 76.35 toks/s, output: 1221.57 toks/s]
Processed prompts:  40%|      | 206/512 [00:39<00:08, 36.14it/s, est. speed input: 84.34 toks/s, output: 1349.37 toks/s]
Processed prompts:  44%|     | 225/512 [00:39<00:06, 46.38it/s, est. speed input: 91.87 toks/s, output: 1469.89 toks/s]
Processed prompts:  51%|     | 259/512 [00:39<00:03, 66.32it/s, est. speed input: 105.21 toks/s, output: 1683.42 toks/s]
Processed prompts:  56%|    | 288/512 [00:39<00:02, 82.37it/s, est. speed input: 116.45 toks/s, output: 1863.26 toks/s]
Processed prompts:  62%|   | 315/512 [00:39<00:02, 95.85it/s, est. speed input: 126.80 toks/s, output: 2028.80 toks/s]
Processed prompts:  66%|   | 338/512 [00:39<00:01, 105.05it/s, est. speed input: 135.50 toks/s, output: 2168.05 toks/s]
Processed prompts:  70%|   | 358/512 [00:40<00:01, 110.28it/s, est. speed input: 142.97 toks/s, output: 2287.47 toks/s]
Processed prompts:  73%|  | 376/512 [00:40<00:01, 113.39it/s, est. speed input: 149.61 toks/s, output: 2393.82 toks/s]
Processed prompts:  77%|  | 392/512 [00:40<00:01, 113.51it/s, est. speed input: 155.44 toks/s, output: 2487.00 toks/s]
Processed prompts:  79%|  | 406/512 [00:40<00:00, 111.41it/s, est. speed input: 160.46 toks/s, output: 2567.32 toks/s]
Processed prompts:  82%| | 419/512 [00:40<00:00, 109.25it/s, est. speed input: 165.08 toks/s, output: 2641.27 toks/s]
Processed prompts:  84%| | 431/512 [00:40<00:00, 106.52it/s, est. speed input: 169.30 toks/s, output: 2708.79 toks/s]
Processed prompts:  87%| | 443/512 [00:40<00:00, 92.30it/s, est. speed input: 173.24 toks/s, output: 2771.85 toks/s] 
Processed prompts:  88%| | 453/512 [00:41<00:00, 81.90it/s, est. speed input: 176.43 toks/s, output: 2822.82 toks/s]
Processed prompts:  90%| | 462/512 [00:41<00:00, 73.83it/s, est. speed input: 179.22 toks/s, output: 2867.56 toks/s]
Processed prompts:  92%|| 470/512 [00:41<00:00, 67.14it/s, est. speed input: 181.64 toks/s, output: 2906.20 toks/s]
Processed prompts:  93%|| 477/512 [00:41<00:00, 61.06it/s, est. speed input: 183.67 toks/s, output: 2938.72 toks/s]
Processed prompts:  95%|| 484/512 [00:41<00:00, 47.89it/s, est. speed input: 185.27 toks/s, output: 2964.25 toks/s]
Processed prompts:  96%|| 490/512 [00:42<00:00, 36.58it/s, est. speed input: 186.24 toks/s, output: 2979.85 toks/s]
Processed prompts:  97%|| 495/512 [00:42<00:00, 31.68it/s, est. speed input: 187.09 toks/s, output: 2993.44 toks/s]
Processed prompts:  97%|| 499/512 [00:51<00:05,  2.19it/s, est. speed input: 155.96 toks/s, output: 2495.43 toks/s]
Processed prompts: 100%|| 512/512 [00:51<00:00,  4.05it/s, est. speed input: 159.43 toks/s, output: 2550.80 toks/s]
Processed prompts: 100%|| 512/512 [00:51<00:00,  4.05it/s, est. speed input: 159.43 toks/s, output: 2550.80 toks/s]
Processed prompts: 100%|| 512/512 [00:51<00:00,  9.96it/s, est. speed input: 159.43 toks/s, output: 2550.80 toks/s]
[rank0]:[W127 17:57:49.791741687 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 18:58:34
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-INT8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Qwen2.5-14B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:58:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:58:38 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2963737) WARNING 01-27 19:00:14 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.27 requests/s, 617.16 total tokens/s, 580.86 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 18:58:38] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:58:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 18:58:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 18:58:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:58:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:58:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:58:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:58:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:58:41] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:58:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 18:58:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 18:58:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:58:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:58:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:58:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:58:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2963737) [2026-01-27 18:58:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2963737) [2026-01-27 18:58:43] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2963737) [2026-01-27 18:58:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2963737) [2026-01-27 18:58:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2963737) [2026-01-27 18:58:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2963737) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2963737) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.41s/it]
(EngineCore_DP0 pid=2963737) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:34<00:37, 18.59s/it]
(EngineCore_DP0 pid=2963737) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:55<00:19, 19.81s/it]
(EngineCore_DP0 pid=2963737) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:22<00:00, 22.71s/it]
(EngineCore_DP0 pid=2963737) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:22<00:00, 20.64s/it]
(EngineCore_DP0 pid=2963737) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2963737) 2026-01-27 19:00:13,251 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2963737) 2026-01-27 19:00:13,299 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 4736.65it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:27<29:03, 27.68s/it, est. speed input: 0.58 toks/s, output: 9.25 toks/s]
Processed prompts:   6%|         | 4/64 [00:27<05:15,  5.27s/it, est. speed input: 2.30 toks/s, output: 36.85 toks/s]
Processed prompts:  31%|      | 20/64 [00:27<00:32,  1.35it/s, est. speed input: 11.47 toks/s, output: 183.55 toks/s]
Processed prompts:  77%|  | 49/64 [00:28<00:03,  4.28it/s, est. speed input: 27.92 toks/s, output: 446.65 toks/s]
Processed prompts: 100%|| 64/64 [00:28<00:00,  4.28it/s, est. speed input: 36.35 toks/s, output: 581.61 toks/s]
Processed prompts: 100%|| 64/64 [00:28<00:00,  2.27it/s, est. speed input: 36.35 toks/s, output: 581.61 toks/s]
[rank0]:[W127 19:00:42.476042597 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 19:00:46
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-INT8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Qwen2.5-14B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:00:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:00:52 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2966183) WARNING 01-27 19:02:28 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 3.76 requests/s, 1022.55 total tokens/s, 962.40 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 19:00:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:00:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:00:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:00:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:00:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:00:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:00:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:00:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:00:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:00:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:00:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:00:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:00:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:00:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:00:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:00:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2966183) [2026-01-27 19:00:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2966183) [2026-01-27 19:00:56] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2966183) [2026-01-27 19:00:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2966183) [2026-01-27 19:00:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2966183) [2026-01-27 19:00:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2966183) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2966183) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.45s/it]
(EngineCore_DP0 pid=2966183) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.44s/it]
(EngineCore_DP0 pid=2966183) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:57<00:20, 20.45s/it]
(EngineCore_DP0 pid=2966183) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.88s/it]
(EngineCore_DP0 pid=2966183) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.96s/it]
(EngineCore_DP0 pid=2966183) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2966183) 2026-01-27 19:02:27,340 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2966183) 2026-01-27 19:02:27,396 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 3093.52it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:32<1:09:20, 32.76s/it, est. speed input: 0.49 toks/s, output: 7.82 toks/s]
Processed prompts:   2%|         | 2/128 [00:32<28:29, 13.57s/it, est. speed input: 0.97 toks/s, output: 15.57 toks/s] 
Processed prompts:  14%|        | 18/128 [00:33<01:44,  1.06it/s, est. speed input: 8.72 toks/s, output: 139.53 toks/s]
Processed prompts:  26%|       | 33/128 [00:33<00:40,  2.35it/s, est. speed input: 15.93 toks/s, output: 254.81 toks/s]
Processed prompts:  38%|      | 48/128 [00:33<00:19,  4.14it/s, est. speed input: 23.08 toks/s, output: 369.30 toks/s]
Processed prompts:  48%|     | 62/128 [00:33<00:10,  6.46it/s, est. speed input: 29.71 toks/s, output: 475.35 toks/s]
Processed prompts:  59%|    | 75/128 [00:33<00:05,  9.38it/s, est. speed input: 35.82 toks/s, output: 573.16 toks/s]
Processed prompts:  68%|   | 87/128 [00:33<00:03, 13.01it/s, est. speed input: 41.43 toks/s, output: 662.82 toks/s]
Processed prompts:  84%| | 108/128 [00:33<00:00, 20.96it/s, est. speed input: 51.13 toks/s, output: 818.12 toks/s]
Processed prompts:  99%|| 127/128 [00:33<00:00, 29.36it/s, est. speed input: 59.81 toks/s, output: 956.89 toks/s]
Processed prompts: 100%|| 128/128 [00:33<00:00, 29.36it/s, est. speed input: 60.28 toks/s, output: 964.42 toks/s]
Processed prompts: 100%|| 128/128 [00:33<00:00,  3.77it/s, est. speed input: 60.28 toks/s, output: 964.42 toks/s]
[rank0]:[W127 19:03:02.485216076 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 19:03:05
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-INT8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Qwen2.5-14B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:03:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:03:08 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2968633) WARNING 01-27 19:04:44 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.08 requests/s, 1381.97 total tokens/s, 1300.68 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 19:03:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:03:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:03:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:03:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:03:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:03:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:03:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:03:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:03:11] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:03:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:03:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:03:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:03:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:03:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:03:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:03:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2968633) [2026-01-27 19:03:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2968633) [2026-01-27 19:03:12] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2968633) [2026-01-27 19:03:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2968633) [2026-01-27 19:03:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2968633) [2026-01-27 19:03:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2968633) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2968633) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.52s/it]
(EngineCore_DP0 pid=2968633) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:34<00:38, 19.08s/it]
(EngineCore_DP0 pid=2968633) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.04s/it]
(EngineCore_DP0 pid=2968633) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.90s/it]
(EngineCore_DP0 pid=2968633) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.87s/it]
(EngineCore_DP0 pid=2968633) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2968633) 2026-01-27 19:04:42,826 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2968633) 2026-01-27 19:04:42,875 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:52,  4.90it/s]
Adding requests:   1%|          | 3/256 [00:00<00:25,  9.94it/s]
Adding requests:   2%|         | 6/256 [00:00<00:15, 15.90it/s]
Adding requests:   4%|         | 10/256 [00:00<00:11, 22.36it/s]
Adding requests:   5%|         | 14/256 [00:00<00:09, 26.88it/s]
Adding requests:   8%|         | 21/256 [00:00<00:06, 38.67it/s]
Adding requests:  12%|        | 31/256 [00:00<00:04, 55.97it/s]
Adding requests:  17%|        | 44/256 [00:00<00:02, 77.50it/s]
Adding requests:  25%|       | 63/256 [00:01<00:01, 109.40it/s]
Adding requests:  32%|      | 83/256 [00:01<00:01, 134.54it/s]
Adding requests:  41%|     | 106/256 [00:01<00:00, 161.35it/s]
Adding requests:  51%|     | 130/256 [00:01<00:00, 182.57it/s]
Adding requests:  61%|    | 156/256 [00:01<00:00, 205.23it/s]
Adding requests:  69%|   | 177/256 [00:01<00:00, 143.11it/s]
Adding requests:  84%| | 214/256 [00:01<00:00, 193.29it/s]
Adding requests: 100%|| 256/256 [00:01<00:00, 246.94it/s]
Adding requests: 100%|| 256/256 [00:01<00:00, 130.76it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:41<2:56:51, 41.61s/it, est. speed input: 0.38 toks/s, output: 6.15 toks/s]
Processed prompts:   1%|          | 2/256 [00:41<1:13:00, 17.25s/it, est. speed input: 0.77 toks/s, output: 12.25 toks/s]
Processed prompts:   2%|         | 6/256 [00:41<16:37,  3.99s/it, est. speed input: 2.29 toks/s, output: 36.58 toks/s]  
Processed prompts:   4%|         | 9/256 [00:42<09:07,  2.21s/it, est. speed input: 3.41 toks/s, output: 54.62 toks/s]
Processed prompts:   5%|         | 13/256 [00:42<04:54,  1.21s/it, est. speed input: 4.91 toks/s, output: 78.54 toks/s]
Processed prompts:   7%|         | 18/256 [00:42<02:42,  1.47it/s, est. speed input: 6.77 toks/s, output: 108.25 toks/s]
Processed prompts:  10%|         | 26/256 [00:42<01:19,  2.89it/s, est. speed input: 9.73 toks/s, output: 155.68 toks/s]
Processed prompts:  15%|        | 39/256 [00:42<00:36,  6.02it/s, est. speed input: 14.54 toks/s, output: 232.59 toks/s]
Processed prompts:  21%|        | 53/256 [00:43<00:19, 10.40it/s, est. speed input: 19.68 toks/s, output: 314.85 toks/s]
Processed prompts:  26%|       | 66/256 [00:43<00:12, 15.45it/s, est. speed input: 24.41 toks/s, output: 390.58 toks/s]
Processed prompts:  30%|       | 78/256 [00:43<00:08, 20.95it/s, est. speed input: 28.74 toks/s, output: 459.88 toks/s]
Processed prompts:  35%|      | 90/256 [00:43<00:06, 27.35it/s, est. speed input: 33.05 toks/s, output: 528.73 toks/s]
Processed prompts:  39%|      | 101/256 [00:43<00:04, 33.49it/s, est. speed input: 36.95 toks/s, output: 591.28 toks/s]
Processed prompts:  43%|     | 111/256 [00:43<00:03, 38.79it/s, est. speed input: 40.47 toks/s, output: 647.57 toks/s]
Processed prompts:  47%|     | 121/256 [00:44<00:03, 44.33it/s, est. speed input: 43.97 toks/s, output: 703.60 toks/s]
Processed prompts:  51%|     | 130/256 [00:44<00:02, 48.42it/s, est. speed input: 47.10 toks/s, output: 753.55 toks/s]
Processed prompts:  54%|    | 138/256 [00:44<00:02, 50.71it/s, est. speed input: 49.84 toks/s, output: 797.47 toks/s]
Processed prompts:  57%|    | 146/256 [00:44<00:02, 53.03it/s, est. speed input: 52.57 toks/s, output: 841.20 toks/s]
Processed prompts:  60%|    | 153/256 [00:44<00:01, 53.41it/s, est. speed input: 54.94 toks/s, output: 878.99 toks/s]
Processed prompts:  62%|   | 160/256 [00:44<00:01, 53.90it/s, est. speed input: 57.29 toks/s, output: 916.61 toks/s]
Processed prompts:  65%|   | 167/256 [00:44<00:01, 53.59it/s, est. speed input: 59.62 toks/s, output: 953.88 toks/s]
Processed prompts:  68%|   | 173/256 [00:44<00:01, 53.27it/s, est. speed input: 61.60 toks/s, output: 985.63 toks/s]
Processed prompts:  70%|   | 179/256 [00:45<00:01, 52.63it/s, est. speed input: 63.57 toks/s, output: 1017.14 toks/s]
Processed prompts:  72%|  | 185/256 [00:45<00:01, 52.53it/s, est. speed input: 65.54 toks/s, output: 1048.56 toks/s]
Processed prompts:  75%|  | 191/256 [00:45<00:01, 52.57it/s, est. speed input: 67.49 toks/s, output: 1079.85 toks/s]
Processed prompts:  77%|  | 197/256 [00:45<00:01, 52.69it/s, est. speed input: 69.44 toks/s, output: 1110.99 toks/s]
Processed prompts:  79%|  | 203/256 [00:45<00:01, 41.63it/s, est. speed input: 71.21 toks/s, output: 1139.36 toks/s]
Processed prompts:  81%| | 208/256 [00:45<00:01, 42.80it/s, est. speed input: 72.79 toks/s, output: 1164.68 toks/s]
Processed prompts:  83%| | 213/256 [00:45<00:01, 35.47it/s, est. speed input: 74.21 toks/s, output: 1187.31 toks/s]
Processed prompts:  85%| | 217/256 [00:46<00:01, 35.90it/s, est. speed input: 75.42 toks/s, output: 1206.79 toks/s]
Processed prompts:  86%| | 221/256 [00:46<00:00, 36.70it/s, est. speed input: 76.65 toks/s, output: 1226.32 toks/s]
Processed prompts:  88%| | 225/256 [00:46<00:01, 30.23it/s, est. speed input: 77.70 toks/s, output: 1243.20 toks/s]
Processed prompts:  90%| | 230/256 [00:46<00:00, 28.60it/s, est. speed input: 79.09 toks/s, output: 1265.51 toks/s]
Processed prompts:  91%|| 234/256 [00:46<00:00, 26.08it/s, est. speed input: 80.14 toks/s, output: 1282.27 toks/s]
Processed prompts:  93%|| 238/256 [00:46<00:00, 24.65it/s, est. speed input: 81.19 toks/s, output: 1299.03 toks/s]
Processed prompts:  95%|| 242/256 [00:47<00:00, 23.66it/s, est. speed input: 82.23 toks/s, output: 1315.65 toks/s]
Processed prompts:  96%|| 245/256 [00:47<00:00, 18.88it/s, est. speed input: 82.78 toks/s, output: 1324.51 toks/s]
Processed prompts:  97%|| 248/256 [00:47<00:00, 16.26it/s, est. speed input: 83.34 toks/s, output: 1333.37 toks/s]
Processed prompts:  98%|| 250/256 [00:47<00:00, 15.06it/s, est. speed input: 83.70 toks/s, output: 1339.23 toks/s]
Processed prompts:  98%|| 252/256 [00:47<00:00, 14.19it/s, est. speed input: 84.07 toks/s, output: 1345.14 toks/s]
Processed prompts:  99%|| 254/256 [00:48<00:00, 13.42it/s, est. speed input: 84.43 toks/s, output: 1350.87 toks/s]
Processed prompts: 100%|| 256/256 [00:48<00:00, 12.52it/s, est. speed input: 84.76 toks/s, output: 1356.11 toks/s]
Processed prompts: 100%|| 256/256 [00:48<00:00, 12.52it/s, est. speed input: 84.76 toks/s, output: 1356.11 toks/s]
Processed prompts: 100%|| 256/256 [00:48<00:00,  5.30it/s, est. speed input: 84.76 toks/s, output: 1356.11 toks/s]
[rank0]:[W127 19:05:35.035372462 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 19:05:49
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-INT8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/Qwen2.5-14B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:05:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:05:54 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2971578) WARNING 01-27 19:07:31 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.31 requests/s, 1443.27 total tokens/s, 1358.37 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 19:05:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:05:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:05:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:05:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:05:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:05:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:05:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:05:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:05:58] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:05:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:05:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:05:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:05:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:05:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:05:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:05:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2971578) [2026-01-27 19:05:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2971578) [2026-01-27 19:05:59] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2971578) [2026-01-27 19:05:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2971578) [2026-01-27 19:05:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2971578) [2026-01-27 19:05:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2971578) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2971578) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.53s/it]
(EngineCore_DP0 pid=2971578) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.38s/it]
(EngineCore_DP0 pid=2971578) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.25s/it]
(EngineCore_DP0 pid=2971578) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.85s/it]
(EngineCore_DP0 pid=2971578) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.91s/it]
(EngineCore_DP0 pid=2971578) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2971578) 2026-01-27 19:07:29,833 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2971578) 2026-01-27 19:07:29,939 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 6024.30it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [01:08<9:46:31, 68.87s/it, est. speed input: 0.23 toks/s, output: 3.72 toks/s]
Processed prompts:   3%|         | 17/512 [01:09<24:10,  2.93s/it, est. speed input: 3.91 toks/s, output: 62.53 toks/s]
Processed prompts:   9%|         | 48/512 [01:10<06:19,  1.22it/s, est. speed input: 10.93 toks/s, output: 174.81 toks/s]
Processed prompts:  15%|        | 77/512 [01:10<03:05,  2.34it/s, est. speed input: 17.36 toks/s, output: 277.79 toks/s]
Processed prompts:  20%|        | 104/512 [01:11<01:48,  3.74it/s, est. speed input: 23.24 toks/s, output: 371.91 toks/s]
Processed prompts:  25%|       | 129/512 [01:11<01:08,  5.59it/s, est. speed input: 28.73 toks/s, output: 459.70 toks/s]
Processed prompts:  30%|       | 153/512 [01:12<00:44,  8.03it/s, est. speed input: 33.97 toks/s, output: 543.47 toks/s]
Processed prompts:  34%|      | 175/512 [01:12<00:30, 11.01it/s, est. speed input: 38.73 toks/s, output: 619.65 toks/s]
Processed prompts:  38%|      | 196/512 [01:12<00:21, 14.77it/s, est. speed input: 43.25 toks/s, output: 691.95 toks/s]
Processed prompts:  42%|     | 215/512 [01:12<00:15, 19.08it/s, est. speed input: 47.30 toks/s, output: 756.80 toks/s]
Processed prompts:  46%|     | 233/512 [01:12<00:11, 24.18it/s, est. speed input: 51.12 toks/s, output: 817.89 toks/s]
Processed prompts:  49%|     | 250/512 [01:13<00:08, 29.98it/s, est. speed input: 54.70 toks/s, output: 875.27 toks/s]
Processed prompts:  52%|    | 266/512 [01:13<00:06, 35.52it/s, est. speed input: 58.03 toks/s, output: 928.53 toks/s]
Processed prompts:  55%|    | 281/512 [01:13<00:05, 41.69it/s, est. speed input: 61.15 toks/s, output: 978.46 toks/s]
Processed prompts:  58%|    | 295/512 [01:13<00:04, 47.49it/s, est. speed input: 64.05 toks/s, output: 1024.74 toks/s]
Processed prompts:  60%|    | 308/512 [01:13<00:03, 52.24it/s, est. speed input: 66.71 toks/s, output: 1067.35 toks/s]
Processed prompts:  62%|   | 320/512 [01:14<00:03, 56.24it/s, est. speed input: 69.15 toks/s, output: 1106.47 toks/s]
Processed prompts:  65%|   | 332/512 [01:14<00:02, 60.53it/s, est. speed input: 71.60 toks/s, output: 1145.54 toks/s]
Processed prompts:  67%|   | 343/512 [01:14<00:02, 62.87it/s, est. speed input: 73.81 toks/s, output: 1181.02 toks/s]
Processed prompts:  69%|   | 353/512 [01:14<00:02, 63.02it/s, est. speed input: 75.81 toks/s, output: 1212.88 toks/s]
Processed prompts:  71%|   | 363/512 [01:14<00:02, 64.17it/s, est. speed input: 77.80 toks/s, output: 1244.77 toks/s]
Processed prompts:  73%|  | 372/512 [01:14<00:02, 63.94it/s, est. speed input: 79.58 toks/s, output: 1273.21 toks/s]
Processed prompts:  74%|  | 380/512 [01:14<00:02, 61.57it/s, est. speed input: 81.13 toks/s, output: 1298.07 toks/s]
Processed prompts:  76%|  | 388/512 [01:15<00:02, 60.22it/s, est. speed input: 82.68 toks/s, output: 1322.91 toks/s]
Processed prompts:  77%|  | 395/512 [01:15<00:02, 58.38it/s, est. speed input: 84.03 toks/s, output: 1344.43 toks/s]
Processed prompts:  79%|  | 402/512 [01:15<00:01, 57.32it/s, est. speed input: 85.37 toks/s, output: 1365.92 toks/s]
Processed prompts:  80%|  | 408/512 [01:15<00:01, 54.81it/s, est. speed input: 86.50 toks/s, output: 1384.01 toks/s]
Processed prompts:  81%|  | 414/512 [01:15<00:01, 52.90it/s, est. speed input: 87.63 toks/s, output: 1402.05 toks/s]
Processed prompts:  82%| | 420/512 [01:15<00:01, 51.98it/s, est. speed input: 88.76 toks/s, output: 1420.09 toks/s]
Processed prompts:  83%| | 426/512 [01:15<00:01, 51.48it/s, est. speed input: 89.88 toks/s, output: 1438.11 toks/s]
Processed prompts:  84%| | 432/512 [01:15<00:01, 50.55it/s, est. speed input: 91.00 toks/s, output: 1455.98 toks/s]
Processed prompts:  86%| | 438/512 [01:16<00:01, 51.09it/s, est. speed input: 92.12 toks/s, output: 1473.99 toks/s]
Processed prompts:  87%| | 444/512 [01:16<00:01, 40.33it/s, est. speed input: 93.11 toks/s, output: 1489.77 toks/s]
Processed prompts:  88%| | 449/512 [01:16<00:01, 41.36it/s, est. speed input: 94.02 toks/s, output: 1504.34 toks/s]
Processed prompts:  89%| | 454/512 [01:16<00:01, 34.13it/s, est. speed input: 94.80 toks/s, output: 1516.81 toks/s]
Processed prompts:  89%| | 458/512 [01:16<00:01, 34.90it/s, est. speed input: 95.50 toks/s, output: 1528.05 toks/s]
Processed prompts:  90%| | 462/512 [01:16<00:01, 35.54it/s, est. speed input: 96.20 toks/s, output: 1539.26 toks/s]
Processed prompts:  91%| | 466/512 [01:17<00:01, 28.80it/s, est. speed input: 96.77 toks/s, output: 1548.31 toks/s]
Processed prompts:  92%|| 470/512 [01:17<00:01, 25.54it/s, est. speed input: 97.34 toks/s, output: 1557.49 toks/s]
Processed prompts:  92%|| 473/512 [01:17<00:01, 26.43it/s, est. speed input: 97.84 toks/s, output: 1565.40 toks/s]
Processed prompts:  93%|| 476/512 [01:17<00:01, 22.42it/s, est. speed input: 98.21 toks/s, output: 1571.34 toks/s]
Processed prompts:  94%|| 480/512 [01:17<00:01, 21.81it/s, est. speed input: 98.79 toks/s, output: 1580.59 toks/s]
Processed prompts:  94%|| 483/512 [01:17<00:01, 19.93it/s, est. speed input: 99.16 toks/s, output: 1586.62 toks/s]
Processed prompts:  95%|| 486/512 [01:18<00:01, 16.10it/s, est. speed input: 99.42 toks/s, output: 1590.66 toks/s]
Processed prompts:  95%|| 488/512 [01:18<00:01, 14.65it/s, est. speed input: 99.59 toks/s, output: 1593.43 toks/s]
Processed prompts:  96%|| 490/512 [01:18<00:01, 13.58it/s, est. speed input: 99.76 toks/s, output: 1596.21 toks/s]
Processed prompts:  96%|| 492/512 [01:18<00:01, 12.82it/s, est. speed input: 99.94 toks/s, output: 1598.99 toks/s]
Processed prompts:  96%|| 494/512 [01:18<00:01, 12.30it/s, est. speed input: 100.11 toks/s, output: 1601.80 toks/s]
Processed prompts:  97%|| 496/512 [01:19<00:01, 11.93it/s, est. speed input: 100.29 toks/s, output: 1604.60 toks/s]
Processed prompts:  97%|| 497/512 [01:29<00:01, 11.93it/s, est. speed input: 100.37 toks/s, output: 1605.91 toks/s]
Processed prompts:  97%|| 498/512 [01:36<00:33,  2.42s/it, est. speed input: 82.90 toks/s, output: 1326.35 toks/s] 
Processed prompts:  98%|| 502/512 [01:36<00:13,  1.37s/it, est. speed input: 83.41 toks/s, output: 1334.52 toks/s]
Processed prompts: 100%|| 512/512 [01:36<00:00,  1.37s/it, est. speed input: 84.99 toks/s, output: 1359.88 toks/s]
Processed prompts: 100%|| 512/512 [01:36<00:00,  5.31it/s, est. speed input: 84.99 toks/s, output: 1359.88 toks/s]
[rank0]:[W127 19:09:08.125132910 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-28 09:45:26
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-INT8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/BitNet-2B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/BitNet-2B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:45:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:45:30 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3763201) WARNING 01-28 09:45:53 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 9.05 requests/s, 2462.92 total tokens/s, 2318.05 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-28 09:45:30] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:45:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:45:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:45:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:45:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:45:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:45:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:45:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:45:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:45:33] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:45:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:45:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:45:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:45:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:45:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:45:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:45:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:45:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:45:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3763201) [2026-01-28 09:45:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3763201) [2026-01-28 09:45:34] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3763201) [2026-01-28 09:45:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3763201) [2026-01-28 09:45:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3763201) [2026-01-28 09:45:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3763201) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3763201) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.20s/it]
(EngineCore_DP0 pid=3763201) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.20s/it]
(EngineCore_DP0 pid=3763201) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3763201) 2026-01-28 09:45:52,741 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3763201) 2026-01-28 09:45:52,760 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 11644.78it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:06<07:17,  6.94s/it, est. speed input: 2.31 toks/s, output: 36.88 toks/s]
Processed prompts:  75%|  | 48/64 [00:07<00:01,  9.62it/s, est. speed input: 109.06 toks/s, output: 1744.98 toks/s]
Processed prompts: 100%|| 64/64 [00:07<00:00,  9.62it/s, est. speed input: 145.01 toks/s, output: 2320.18 toks/s]
Processed prompts: 100%|| 64/64 [00:07<00:00,  9.06it/s, est. speed input: 145.01 toks/s, output: 2320.18 toks/s]
[rank0]:[W128 09:46:00.632105800 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-28 09:46:02
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-INT8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/BitNet-2B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/BitNet-2B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:46:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:46:06 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3763892) WARNING 01-28 09:46:28 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 13.64 requests/s, 3709.03 total tokens/s, 3490.85 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-28 09:46:06] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:46:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:46:06] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:46:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:06] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:06] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:46:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:46:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:46:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:46:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:46:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:46:09] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:46:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:46:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:46:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:46:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:46:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:46:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:46:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:46:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3763892) [2026-01-28 09:46:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3763892) [2026-01-28 09:46:10] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3763892) [2026-01-28 09:46:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3763892) [2026-01-28 09:46:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3763892) [2026-01-28 09:46:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3763892) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3763892) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.57s/it]
(EngineCore_DP0 pid=3763892) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.57s/it]
(EngineCore_DP0 pid=3763892) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3763892) 2026-01-28 09:46:28,121 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3763892) 2026-01-28 09:46:28,139 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 13110.08it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:09<19:11,  9.06s/it, est. speed input: 1.77 toks/s, output: 28.25 toks/s]
Processed prompts:  26%|       | 33/128 [00:09<00:18,  5.06it/s, est. speed input: 57.54 toks/s, output: 920.70 toks/s]
Processed prompts:  68%|   | 87/128 [00:09<00:02, 16.59it/s, est. speed input: 149.83 toks/s, output: 2397.26 toks/s]
Processed prompts: 100%|| 128/128 [00:09<00:00, 16.59it/s, est. speed input: 218.42 toks/s, output: 3494.78 toks/s]
Processed prompts: 100%|| 128/128 [00:09<00:00, 13.65it/s, est. speed input: 218.42 toks/s, output: 3494.78 toks/s]
[rank0]:[W128 09:46:38.318305961 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-28 09:46:40
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-INT8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/BitNet-2B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/BitNet-2B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:46:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:46:43 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3764729) WARNING 01-28 09:47:05 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 17.70 requests/s, 4814.98 total tokens/s, 4531.75 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-28 09:46:43] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:46:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:46:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:46:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:46:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:46:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:46:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:46:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:46:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:46:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:46:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:46:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:46:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:46:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:46:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:46:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:46:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:46:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:46:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3764729) [2026-01-28 09:46:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3764729) [2026-01-28 09:46:48] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3764729) [2026-01-28 09:46:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3764729) [2026-01-28 09:46:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3764729) [2026-01-28 09:46:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3764729) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3764729) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.46s/it]
(EngineCore_DP0 pid=3764729) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.46s/it]
(EngineCore_DP0 pid=3764729) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3764729) 2026-01-28 09:47:05,263 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3764729) 2026-01-28 09:47:05,280 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 12691.83it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:12<54:21, 12.79s/it, est. speed input: 1.25 toks/s, output: 20.02 toks/s]
Processed prompts:   7%|         | 19/256 [00:12<01:54,  2.06it/s, est. speed input: 23.55 toks/s, output: 376.86 toks/s]
Processed prompts:  19%|        | 48/256 [00:13<00:31,  6.52it/s, est. speed input: 59.03 toks/s, output: 944.40 toks/s]
Processed prompts:  34%|      | 87/256 [00:13<00:11, 14.68it/s, est. speed input: 105.80 toks/s, output: 1692.83 toks/s]
Processed prompts:  46%|     | 118/256 [00:13<00:05, 23.28it/s, est. speed input: 142.13 toks/s, output: 2274.12 toks/s]
Processed prompts:  56%|    | 144/256 [00:13<00:03, 32.64it/s, est. speed input: 171.95 toks/s, output: 2751.19 toks/s]
Processed prompts:  66%|   | 168/256 [00:13<00:02, 43.83it/s, est. speed input: 199.08 toks/s, output: 3185.30 toks/s]
Processed prompts:  75%|  | 192/256 [00:13<00:01, 56.88it/s, est. speed input: 225.43 toks/s, output: 3606.81 toks/s]
Processed prompts:  84%| | 215/256 [00:13<00:00, 68.53it/s, est. speed input: 249.37 toks/s, output: 3989.97 toks/s]
Processed prompts:  92%|| 235/256 [00:13<00:00, 75.77it/s, est. speed input: 268.94 toks/s, output: 4302.98 toks/s]
Processed prompts:  98%|| 252/256 [00:14<00:00, 70.76it/s, est. speed input: 282.54 toks/s, output: 4520.62 toks/s]
Processed prompts: 100%|| 256/256 [00:14<00:00, 70.76it/s, est. speed input: 283.65 toks/s, output: 4538.35 toks/s]
Processed prompts: 100%|| 256/256 [00:14<00:00, 17.73it/s, est. speed input: 283.65 toks/s, output: 4538.35 toks/s]
[rank0]:[W128 09:47:20.509213472 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-28 09:47:22
Backend: cuBLASLt [INT32 output]
Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-INT8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/BitNet-2B-INT8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/json/BitNet-2B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:47:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:47:26 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3765508) WARNING 01-28 09:47:48 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 17.54 requests/s, 4772.12 total tokens/s, 4491.41 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-28 09:47:26] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:47:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:47:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:47:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:47:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:47:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:47:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:47:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:47:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:47:29] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:47:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:47:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:47:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:47:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:47:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:47:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:47:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:47:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:47:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3765508) [2026-01-28 09:47:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3765508) [2026-01-28 09:47:30] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3765508) [2026-01-28 09:47:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3765508) [2026-01-28 09:47:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3765508) [2026-01-28 09:47:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3765508) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3765508) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.80s/it]
(EngineCore_DP0 pid=3765508) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.80s/it]
(EngineCore_DP0 pid=3765508) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3765508) 2026-01-28 09:47:48,131 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3765508) 2026-01-28 09:47:48,161 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 13762.30it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:22<3:08:12, 22.10s/it, est. speed input: 0.72 toks/s, output: 11.58 toks/s]
Processed prompts:   1%|          | 5/512 [00:22<28:13,  3.34s/it, est. speed input: 3.58 toks/s, output: 57.33 toks/s]  
Processed prompts:   7%|         | 36/512 [00:22<02:35,  3.06it/s, est. speed input: 25.54 toks/s, output: 408.68 toks/s]
Processed prompts:  13%|        | 65/512 [00:22<01:07,  6.58it/s, est. speed input: 45.70 toks/s, output: 731.19 toks/s]
Processed prompts:  18%|        | 93/512 [00:22<00:37, 11.16it/s, est. speed input: 64.83 toks/s, output: 1037.22 toks/s]
Processed prompts:  23%|       | 119/512 [00:23<00:23, 16.78it/s, est. speed input: 82.30 toks/s, output: 1316.77 toks/s]
Processed prompts:  32%|      | 166/512 [00:23<00:11, 30.90it/s, est. speed input: 114.03 toks/s, output: 1824.49 toks/s]
Processed prompts:  40%|      | 207/512 [00:23<00:06, 46.53it/s, est. speed input: 141.33 toks/s, output: 2261.30 toks/s]
Processed prompts:  47%|     | 243/512 [00:23<00:04, 63.38it/s, est. speed input: 164.98 toks/s, output: 2639.70 toks/s]
Processed prompts:  54%|    | 274/512 [00:23<00:02, 80.64it/s, est. speed input: 185.09 toks/s, output: 2961.49 toks/s]
Processed prompts:  59%|    | 302/512 [00:23<00:02, 99.03it/s, est. speed input: 203.10 toks/s, output: 3249.57 toks/s]
Processed prompts:  66%|   | 338/512 [00:23<00:01, 123.46it/s, est. speed input: 225.93 toks/s, output: 3614.89 toks/s]
Processed prompts:  72%|  | 367/512 [00:24<00:01, 141.29it/s, est. speed input: 244.02 toks/s, output: 3904.33 toks/s]
Processed prompts:  77%|  | 392/512 [00:24<00:00, 154.68it/s, est. speed input: 259.39 toks/s, output: 4150.22 toks/s]
Processed prompts:  81%| | 416/512 [00:24<00:00, 168.37it/s, est. speed input: 274.07 toks/s, output: 4385.06 toks/s]
Processed prompts:  86%| | 440/512 [00:24<00:00, 164.17it/s, est. speed input: 288.03 toks/s, output: 4608.42 toks/s]
Processed prompts:  90%| | 461/512 [00:24<00:00, 151.21it/s, est. speed input: 299.67 toks/s, output: 4794.72 toks/s]
Processed prompts:  94%|| 480/512 [00:24<00:00, 127.10it/s, est. speed input: 309.22 toks/s, output: 4947.55 toks/s]
Processed prompts:  97%|| 496/512 [00:25<00:00, 93.72it/s, est. speed input: 315.43 toks/s, output: 5046.93 toks/s] 
Processed prompts:  99%|| 509/512 [00:29<00:00, 14.07it/s, est. speed input: 280.34 toks/s, output: 4485.46 toks/s]
Processed prompts: 100%|| 512/512 [00:29<00:00, 14.07it/s, est. speed input: 281.08 toks/s, output: 4497.32 toks/s]
Processed prompts: 100%|| 512/512 [00:29<00:00, 17.57it/s, est. speed input: 281.08 toks/s, output: 4497.32 toks/s]
[rank0]:[W128 09:48:18.179915597 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

