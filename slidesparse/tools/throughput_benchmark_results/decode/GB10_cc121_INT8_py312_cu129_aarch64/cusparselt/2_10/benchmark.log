
========== M=16 ==========
Time: 2026-01-25 18:42:53
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-INT8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:42:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:42:57 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=293530) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=293530) 
(EngineCore_DP0 pid=293530) 
(EngineCore_DP0 pid=293530) ================================================================
(EngineCore_DP0 pid=293530) Internal Triton PTX codegen error
(EngineCore_DP0 pid=293530) `ptxas` stderr:
(EngineCore_DP0 pid=293530) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=293530) 
(EngineCore_DP0 pid=293530) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpehrptw85.ptx -o /tmp/tmpehrptw85.ptx.o
(EngineCore_DP0 pid=293530) 
(EngineCore_DP0 pid=293530) 
(EngineCore_DP0 pid=293530) //
(EngineCore_DP0 pid=293530) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=293530) //
(EngineCore_DP0 pid=293530) 
(EngineCore_DP0 pid=293530) .version 8.7
(EngineCore_DP0 pid=293530) .target sm_121a
(EngineCore_DP0 pid=293530) .address_size 64
(EngineCore_DP0 pid=293530) 
(EngineCore_DP0 pid=293530) 	// .globl	_quant_slide_int8_kernel // -- Begin function _quant_slide_int8_kernel
(EngineCore_DP0 pid=293530) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=293530)                                         // @_quant_slide_int8_kernel
(EngineCore_DP0 pid=293530) .visible .entry _quant_slide_int8_kernel(
(EngineCore_DP0 pid=293530) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_0,
(EngineCore_DP0 pid=293530) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_1,
(EngineCore_DP0 pid=293530) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_2,
(EngineCore_DP0 pid=293530) 	.param .u32 _quant_slide_int8_kernel_param_3,
(EngineCore_DP0 pid=293530) 	.param .u32 _quant_slide_int8_kernel_param_4,
(EngineCore_DP0 pid=293530) 	.param .u32 _quant_slide_int8_kernel_param_5,
(EngineCore_DP0 pid=293530) 	.param .u32 _quant_slide_int8_kernel_param_6,
(EngineCore_DP0 pid=293530) 	.param .u32 _quant_slide_int8_kernel_param_7,
(EngineCore_DP0 pid=293530) 	.param .u32 _quant_slide_int8_kernel_param_8,
(EngineCore_DP0 pid=293530) 	.param .u32 _quant_slide_int8_kernel_param_9,
(EngineCore_DP0 pid=293530) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_10,
(EngineCore_DP0 pid=293530) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_11
(EngineCore_DP0 pid=293530) )
(EngineCore_DP0 pid=293530) .reqntid 1024
(EngineCore_DP0 pid=293530) {
(EngineCore_DP0 pid=293530) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=293530) 	.reg .b16 	%rs<20>;
(EngineCore_DP0 pid=293530) 	.reg .b32 	%r<120>;
(EngineCore_DP0 pid=293530) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=293530) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=293530) $L__func_begin0:
(EngineCore_DP0 pid=293530) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=293530) 
(EngineCore_DP0 pid=293530) // %bb.0:
(EngineCore_DP0 pid=293530) 	ld.param.b32 	%r19, [_quant_slide_int8_kernel_param_7];
(EngineCore_DP0 pid=293530) 	ld.param.b32 	%r18, [_quant_slide_int8_kernel_param_5];
(EngineCore_DP0 pid=293530) 	ld.param.b32 	%r17, [_quant_slide_int8_kernel_param_4];
(EngineCore_DP0 pid=293530) 	ld.param.b64 	%rd3, [_quant_slide_int8_kernel_param_2];
(EngineCore_DP0 pid=293530) 	ld.param.b64 	%rd4, [_quant_slide_int8_kernel_param_0];
(EngineCore_DP0 pid=293530) $L__tmp0:
(EngineCore_DP0 pid=293530) 	.loc	1 288 24                        // quant_slide_tuned_Llama3.2-1B.py:288:24
(EngineCore_DP0 pid=293530) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=293530) 	ld.param.b32 	%r21, [_quant_slide_int8_kernel_param_8];
(EngineCore_DP0 pid=293530) 	.loc	1 293 26                        // quant_slide_tuned_Llama3.2-1B.py:293:26
(EngineCore_DP0 pid=293530) 	mul.lo.s32 	%r22, %r21, %r1;
(EngineCore_DP0 pid=293530) 	.loc	1 293 20                        // quant_slide_tuned_Llama3.2-1B.py:293:20
(EngineCore_DP0 pid=293530) 	mad.wide.s32 	%rd1, %r22, 2, %rd4;
(EngineCore_DP0 pid=293530) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=293530) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=293530) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=293530) 	setp.lt.s32 	%p1, %r18, 1;
(EngineCore_DP0 pid=293530) 	mov.b32 	%r118, 0f2B8CBCCC;
(EngineCore_DP0 pid=293530) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=293530) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=293530) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=293530) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=293530) 	shl.b32 	%r3, %r2, 2;
(EngineCore_DP0 pid=293530) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=293530) 	shr.u32 	%r31, %r2, 3;
(EngineCore_DP0 pid=293530) 	and.b32 	%r32, %r31, 124;
(EngineCore_DP0 pid=293530) 	mov.b32 	%r33, global_smem;
(EngineCore_DP0 pid=293530) 	add.s32 	%r39, %r33, %r32;
(EngineCore_DP0 pid=293530) 	add.s32 	%r42, %r33, %r3;
(EngineCore_DP0 pid=293530) 	mov.b32 	%r37, 0;
(EngineCore_DP0 pid=293530) 	mov.b32 	%r116, 0f00000000;
(EngineCore_DP0 pid=293530) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=293530) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=293530) 	mov.b32 	%r117, %r37;
(EngineCore_DP0 pid=293530) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=293530) 	.loc	1 300 22                        // quant_slide_tuned_Llama3.2-1B.py:300:22
(EngineCore_DP0 pid=293530) 	add.s32 	%r45, %r3, %r117;
(EngineCore_DP0 pid=293530) 	setp.lt.s32 	%p2, %r45, %r17;
(EngineCore_DP0 pid=293530) 	.loc	1 301 29                        // quant_slide_tuned_Llama3.2-1B.py:301:29
(EngineCore_DP0 pid=293530) 	mad.wide.s32 	%rd6, %r45, 2, %rd1;
(EngineCore_DP0 pid=293530) 	.loc	1 301 21                        // quant_slide_tuned_Llama3.2-1B.py:301:21
(EngineCore_DP0 pid=293530) 	// begin inline asm
(EngineCore_DP0 pid=293530) 	mov.u32 %r35, %r37;
(EngineCore_DP0 pid=293530) 	mov.u32 %r36, %r37;
(EngineCore_DP0 pid=293530) 	@%p2 ld.global.v2.b32 { %r35, %r36 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=293530) 	// end inline asm
(EngineCore_DP0 pid=293530) 	mov.b32 	{%rs1, %rs2}, %r35;
(EngineCore_DP0 pid=293530) 	mov.b32 	{%rs3, %rs4}, %r36;
(EngineCore_DP0 pid=293530) 	.loc	1 302 50                        // quant_slide_tuned_Llama3.2-1B.py:302:50
(EngineCore_DP0 pid=293530) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=293530) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=293530) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=293530) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=293530) $L__tmp1:
(EngineCore_DP0 pid=293530) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	bar.sync 	0;
(EngineCore_DP0 pid=293530) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=293530) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=293530) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=293530) 	cvt.f32.bf16 	%r46, %rs11;
(EngineCore_DP0 pid=293530) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	shfl.sync.bfly.b32 	%r47, %r46, 16, 31, -1;
(EngineCore_DP0 pid=293530) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	max.f32 	%r48, %r46, %r47;
(EngineCore_DP0 pid=293530) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	shfl.sync.bfly.b32 	%r49, %r48, 8, 31, -1;
(EngineCore_DP0 pid=293530) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	max.f32 	%r50, %r48, %r49;
(EngineCore_DP0 pid=293530) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	shfl.sync.bfly.b32 	%r51, %r50, 4, 31, -1;
(EngineCore_DP0 pid=293530) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	max.f32 	%r52, %r50, %r51;
(EngineCore_DP0 pid=293530) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	shfl.sync.bfly.b32 	%r53, %r52, 2, 31, -1;
(EngineCore_DP0 pid=293530) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	max.f32 	%r54, %r52, %r53;
(EngineCore_DP0 pid=293530) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	shfl.sync.bfly.b32 	%r55, %r54, 1, 31, -1;
(EngineCore_DP0 pid=293530) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	max.f32 	%r40, %r54, %r55;
(EngineCore_DP0 pid=293530) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	// begin inline asm
(EngineCore_DP0 pid=293530) 	@%p3 st.shared.b32 [ %r39 + 0 ], %r40;
(EngineCore_DP0 pid=293530) 	// end inline asm
(EngineCore_DP0 pid=293530) 	bar.sync 	0;
(EngineCore_DP0 pid=293530) 	// begin inline asm
(EngineCore_DP0 pid=293530) 	@%p4 ld.shared.b32 %r41, [ %r42 + 0 ];
(EngineCore_DP0 pid=293530) 	// end inline asm
(EngineCore_DP0 pid=293530) 	shfl.sync.bfly.b32 	%r56, %r41, 16, 31, -1;
(EngineCore_DP0 pid=293530) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	max.f32 	%r57, %r41, %r56;
(EngineCore_DP0 pid=293530) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	shfl.sync.bfly.b32 	%r58, %r57, 8, 31, -1;
(EngineCore_DP0 pid=293530) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	max.f32 	%r59, %r57, %r58;
(EngineCore_DP0 pid=293530) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	shfl.sync.bfly.b32 	%r60, %r59, 4, 31, -1;
(EngineCore_DP0 pid=293530) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	max.f32 	%r61, %r59, %r60;
(EngineCore_DP0 pid=293530) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	shfl.sync.bfly.b32 	%r62, %r61, 2, 31, -1;
(EngineCore_DP0 pid=293530) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	max.f32 	%r63, %r61, %r62;
(EngineCore_DP0 pid=293530) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	shfl.sync.bfly.b32 	%r64, %r63, 1, 31, -1;
(EngineCore_DP0 pid=293530) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	max.f32 	%r44, %r63, %r64;
(EngineCore_DP0 pid=293530) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=293530) 	// begin inline asm
(EngineCore_DP0 pid=293530) 	@%p19 st.shared.b32 [ %r42 + 0 ], %r44;
(EngineCore_DP0 pid=293530) 	// end inline asm
(EngineCore_DP0 pid=293530) 	bar.sync 	0;
(EngineCore_DP0 pid=293530) 	ld.shared.b32 	%r65, [global_smem];
(EngineCore_DP0 pid=293530) $L__tmp2:
(EngineCore_DP0 pid=293530) 	.loc	1 302 36                        // quant_slide_tuned_Llama3.2-1B.py:302:36
(EngineCore_DP0 pid=293530) 	max.f32 	%r116, %r116, %r65;
(EngineCore_DP0 pid=293530) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=293530) 	add.s32 	%r117, %r117, 4096;
(EngineCore_DP0 pid=293530) 	setp.lt.s32 	%p6, %r117, %r18;
(EngineCore_DP0 pid=293530) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=293530) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=293530) 	.loc	1 304 32                        // quant_slide_tuned_Llama3.2-1B.py:304:32
(EngineCore_DP0 pid=293530) 	max.f32 	%r118, %r116, 0f2B8CBCCC;
(EngineCore_DP0 pid=293530) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=293530) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=293530) 	mov.b32 	%r67, 0f42FE0000;
(EngineCore_DP0 pid=293530) 	.loc	1 305 32                        // quant_slide_tuned_Llama3.2-1B.py:305:32
(EngineCore_DP0 pid=293530) 	div.full.f32 	%r68, %r118, %r67;
(EngineCore_DP0 pid=293530) 	.loc	1 305 42                        // quant_slide_tuned_Llama3.2-1B.py:305:42
(EngineCore_DP0 pid=293530) 	max.f32 	%r66, %r68, 0f37810204;
(EngineCore_DP0 pid=293530) 	.loc	1 307 25                        // quant_slide_tuned_Llama3.2-1B.py:307:25
(EngineCore_DP0 pid=293530) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=293530) 	.loc	1 307 30                        // quant_slide_tuned_Llama3.2-1B.py:307:30
(EngineCore_DP0 pid=293530) 	// begin inline asm
(EngineCore_DP0 pid=293530) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r66 };
(EngineCore_DP0 pid=293530) 	// end inline asm
(EngineCore_DP0 pid=293530) 	.loc	1 310 29                        // quant_slide_tuned_Llama3.2-1B.py:310:29
(EngineCore_DP0 pid=293530) 	shl.b32 	%r14, %r19, 2;
(EngineCore_DP0 pid=293530) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=293530) 	setp.lt.s32 	%p8, %r14, 1;
(EngineCore_DP0 pid=293530) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=293530) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=293530) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=293530) 	ld.param.b32 	%r23, [_quant_slide_int8_kernel_param_9];
(EngineCore_DP0 pid=293530) 	shr.s32 	%r24, %r23, 31;
(EngineCore_DP0 pid=293530) 	shr.u32 	%r25, %r24, 30;
(EngineCore_DP0 pid=293530) 	add.s32 	%r26, %r23, %r25;
(EngineCore_DP0 pid=293530) 	shr.s32 	%r27, %r26, 2;
(EngineCore_DP0 pid=293530) 	ld.param.b64 	%rd5, [_quant_slide_int8_kernel_param_1];
(EngineCore_DP0 pid=293530) 	mul.lo.s32 	%r28, %r27, %r1;
(EngineCore_DP0 pid=293530) 	mad.wide.s32 	%rd2, %r28, 4, %rd5;
(EngineCore_DP0 pid=293530) 	div.full.f32 	%r13, %r67, %r118;
(EngineCore_DP0 pid=293530) 	mov.b32 	%r119, 0;
(EngineCore_DP0 pid=293530) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=293530)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=293530) 	.loc	1 314 30                        // quant_slide_tuned_Llama3.2-1B.py:314:30
(EngineCore_DP0 pid=293530) 	add.s32 	%r72, %r2, %r119;
(EngineCore_DP0 pid=293530) 	setp.lt.s32 	%p13, %r72, %r14;
(EngineCore_DP0 pid=293530) 	.loc	1 317 24                        // quant_slide_tuned_Llama3.2-1B.py:317:24
(EngineCore_DP0 pid=293530) 	shr.s32 	%r73, %r72, 31;
(EngineCore_DP0 pid=293530) 	shr.u32 	%r74, %r73, 30;
(EngineCore_DP0 pid=293530) 	add.s32 	%r75, %r72, %r74;
(EngineCore_DP0 pid=293530) 	shr.s32 	%r76, %r75, 2;
(EngineCore_DP0 pid=293530) 	.loc	1 318 23                        // quant_slide_tuned_Llama3.2-1B.py:318:23
(EngineCore_DP0 pid=293530) 	and.b32 	%r77, %r75, 2147483644;
(EngineCore_DP0 pid=293530) 	sub.s32 	%r78, %r72, %r77;
(EngineCore_DP0 pid=293530) 	.loc	1 319 30                        // quant_slide_tuned_Llama3.2-1B.py:319:30
(EngineCore_DP0 pid=293530) 	shl.b32 	%r79, %r78, 1;
(EngineCore_DP0 pid=293530) 	.loc	1 319 26                        // quant_slide_tuned_Llama3.2-1B.py:319:26
(EngineCore_DP0 pid=293530) 	mad.lo.s32 	%r80, %r76, 10, %r79;
(EngineCore_DP0 pid=293530) 	.loc	1 322 53                        // quant_slide_tuned_Llama3.2-1B.py:322:53
(EngineCore_DP0 pid=293530) 	setp.lt.s32 	%p14, %r80, %r17;
(EngineCore_DP0 pid=293530) 	.loc	1 322 37                        // quant_slide_tuned_Llama3.2-1B.py:322:37
(EngineCore_DP0 pid=293530) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=293530) 	.loc	1 321 29                        // quant_slide_tuned_Llama3.2-1B.py:321:29
(EngineCore_DP0 pid=293530) 	mad.wide.s32 	%rd8, %r80, 2, %rd1;
(EngineCore_DP0 pid=293530) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=293530) 	.loc	1 321 21                        // quant_slide_tuned_Llama3.2-1B.py:321:21
(EngineCore_DP0 pid=293530) 	// begin inline asm
(EngineCore_DP0 pid=293530) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=293530) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=293530) 	// end inline asm
(EngineCore_DP0 pid=293530) 	.loc	1 322 79                        // quant_slide_tuned_Llama3.2-1B.py:322:79
(EngineCore_DP0 pid=293530) 	cvt.f32.bf16 	%r81, %rs12;
(EngineCore_DP0 pid=293530) 	.loc	1 324 48                        // quant_slide_tuned_Llama3.2-1B.py:324:48
(EngineCore_DP0 pid=293530) 	or.b32 	%r82, %r80, 1;
(EngineCore_DP0 pid=293530) 	.loc	1 324 53                        // quant_slide_tuned_Llama3.2-1B.py:324:53
(EngineCore_DP0 pid=293530) 	setp.lt.s32 	%p15, %r82, %r17;
(EngineCore_DP0 pid=293530) 	.loc	1 324 37                        // quant_slide_tuned_Llama3.2-1B.py:324:37
(EngineCore_DP0 pid=293530) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=293530) 	.loc	1 323 39                        // quant_slide_tuned_Llama3.2-1B.py:323:39
(EngineCore_DP0 pid=293530) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=293530) 	.loc	1 323 21                        // quant_slide_tuned_Llama3.2-1B.py:323:21
(EngineCore_DP0 pid=293530) 	// begin inline asm
(EngineCore_DP0 pid=293530) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=293530) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=293530) 	// end inline asm
(EngineCore_DP0 pid=293530) 	.loc	1 324 79                        // quant_slide_tuned_Llama3.2-1B.py:324:79
(EngineCore_DP0 pid=293530) 	cvt.f32.bf16 	%r83, %rs14;
(EngineCore_DP0 pid=293530) 	.loc	1 326 48                        // quant_slide_tuned_Llama3.2-1B.py:326:48
(EngineCore_DP0 pid=293530) 	add.s32 	%r84, %r80, 2;
(EngineCore_DP0 pid=293530) 	.loc	1 326 53                        // quant_slide_tuned_Llama3.2-1B.py:326:53
(EngineCore_DP0 pid=293530) 	setp.lt.s32 	%p16, %r84, %r17;
(EngineCore_DP0 pid=293530) 	.loc	1 326 37                        // quant_slide_tuned_Llama3.2-1B.py:326:37
(EngineCore_DP0 pid=293530) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=293530) 	.loc	1 325 39                        // quant_slide_tuned_Llama3.2-1B.py:325:39
(EngineCore_DP0 pid=293530) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=293530) 	.loc	1 325 21                        // quant_slide_tuned_Llama3.2-1B.py:325:21
(EngineCore_DP0 pid=293530) 	// begin inline asm
(EngineCore_DP0 pid=293530) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=293530) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=293530) 	// end inline asm
(EngineCore_DP0 pid=293530) 	.loc	1 326 79                        // quant_slide_tuned_Llama3.2-1B.py:326:79
(EngineCore_DP0 pid=293530) 	cvt.f32.bf16 	%r85, %rs16;
(EngineCore_DP0 pid=293530) 	.loc	1 328 48                        // quant_slide_tuned_Llama3.2-1B.py:328:48
(EngineCore_DP0 pid=293530) 	add.s32 	%r86, %r80, 3;
(EngineCore_DP0 pid=293530) 	.loc	1 328 53                        // quant_slide_tuned_Llama3.2-1B.py:328:53
(EngineCore_DP0 pid=293530) 	setp.lt.s32 	%p17, %r86, %r17;
(EngineCore_DP0 pid=293530) 	.loc	1 328 37                        // quant_slide_tuned_Llama3.2-1B.py:328:37
(EngineCore_DP0 pid=293530) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=293530) 	.loc	1 327 39                        // quant_slide_tuned_Llama3.2-1B.py:327:39
(EngineCore_DP0 pid=293530) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=293530) 	.loc	1 327 21                        // quant_slide_tuned_Llama3.2-1B.py:327:21
(EngineCore_DP0 pid=293530) 	// begin inline asm
(EngineCore_DP0 pid=293530) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=293530) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=293530) 	// end inline asm
(EngineCore_DP0 pid=293530) 	.loc	1 328 79                        // quant_slide_tuned_Llama3.2-1B.py:328:79
(EngineCore_DP0 pid=293530) 	cvt.f32.bf16 	%r87, %rs18;
(EngineCore_DP0 pid=293530) 	.loc	1 330 56                        // quant_slide_tuned_Llama3.2-1B.py:330:56
(EngineCore_DP0 pid=293530) 	mul.f32 	%r88, %r13, %r81;
(EngineCore_DP0 pid=293530) 	.loc	1 330 51                        // quant_slide_tuned_Llama3.2-1B.py:330:51
(EngineCore_DP0 pid=293530) 	cvt.rni.f32.f32 	%r89, %r88;
(EngineCore_DP0 pid=293530) 	.loc	1 330 76                        // quant_slide_tuned_Llama3.2-1B.py:330:76
(EngineCore_DP0 pid=293530) 	max.f32 	%r90, %r89, 0fC3000000;
(EngineCore_DP0 pid=293530) 	min.f32 	%r91, %r90, 0f42FE0000;
(EngineCore_DP0 pid=293530) 	.loc	1 330 86                        // quant_slide_tuned_Llama3.2-1B.py:330:86
(EngineCore_DP0 pid=293530) 	cvt.rzi.s32.f32 	%r92, %r91;
(EngineCore_DP0 pid=293530) 	.loc	1 330 98                        // quant_slide_tuned_Llama3.2-1B.py:330:98
(EngineCore_DP0 pid=293530) 	and.b32 	%r93, %r92, 255;
(EngineCore_DP0 pid=293530) 	.loc	1 331 56                        // quant_slide_tuned_Llama3.2-1B.py:331:56
(EngineCore_DP0 pid=293530) 	mul.f32 	%r94, %r13, %r83;
(EngineCore_DP0 pid=293530) 	.loc	1 331 51                        // quant_slide_tuned_Llama3.2-1B.py:331:51
(EngineCore_DP0 pid=293530) 	cvt.rni.f32.f32 	%r95, %r94;
(EngineCore_DP0 pid=293530) 	.loc	1 332 56                        // quant_slide_tuned_Llama3.2-1B.py:332:56
(EngineCore_DP0 pid=293530) 	mul.f32 	%r96, %r13, %r85;
(EngineCore_DP0 pid=293530) 	.loc	1 332 51                        // quant_slide_tuned_Llama3.2-1B.py:332:51
(EngineCore_DP0 pid=293530) 	cvt.rni.f32.f32 	%r97, %r96;
(EngineCore_DP0 pid=293530) 	.loc	1 333 56                        // quant_slide_tuned_Llama3.2-1B.py:333:56
(EngineCore_DP0 pid=293530) 	mul.f32 	%r98, %r13, %r87;
(EngineCore_DP0 pid=293530) 	.loc	1 333 51                        // quant_slide_tuned_Llama3.2-1B.py:333:51
(EngineCore_DP0 pid=293530) 	cvt.rni.f32.f32 	%r99, %r98;
(EngineCore_DP0 pid=293530) 	.loc	1 333 76                        // quant_slide_tuned_Llama3.2-1B.py:333:76
(EngineCore_DP0 pid=293530) 	max.f32 	%r100, %r99, 0fC3000000;
(EngineCore_DP0 pid=293530) 	min.f32 	%r101, %r100, 0f42FE0000;
(EngineCore_DP0 pid=293530) 	.loc	1 333 86                        // quant_slide_tuned_Llama3.2-1B.py:333:86
(EngineCore_DP0 pid=293530) 	cvt.rzi.s32.f32 	%r102, %r101;
(EngineCore_DP0 pid=293530) 	.loc	1 331 76                        // quant_slide_tuned_Llama3.2-1B.py:331:76
(EngineCore_DP0 pid=293530) 	max.f32 	%r103, %r97, 0fC3000000;
(EngineCore_DP0 pid=293530) 	max.f32 	%r104, %r95, 0fC3000000;
(EngineCore_DP0 pid=293530) 	min.f32 	%r105, %r104, 0f42FE0000;
(EngineCore_DP0 pid=293530) 	min.f32 	%r106, %r103, 0f42FE0000;
(EngineCore_DP0 pid=293530) 	.loc	1 331 86                        // quant_slide_tuned_Llama3.2-1B.py:331:86
(EngineCore_DP0 pid=293530) 	cvt.rzi.s32.f32 	%r107, %r106;
(EngineCore_DP0 pid=293530) 	cvt.rzi.s32.f32 	%r108, %r105;
(EngineCore_DP0 pid=293530) 	.loc	1 335 30                        // quant_slide_tuned_Llama3.2-1B.py:335:30
(EngineCore_DP0 pid=293530) 	shl.b32 	%r109, %r108, 8;
(EngineCore_DP0 pid=293530) 	shl.b32 	%r110, %r107, 16;
(EngineCore_DP0 pid=293530) 	and.b32 	%r111, %r110, 16711680;
(EngineCore_DP0 pid=293530) 	and.b32 	%r112, %r109, 65280;
(EngineCore_DP0 pid=293530) 	.loc	1 335 24                        // quant_slide_tuned_Llama3.2-1B.py:335:24
(EngineCore_DP0 pid=293530) 	or.b32 	%r113, %r112, %r93;
(EngineCore_DP0 pid=293530) 	.loc	1 335 36                        // quant_slide_tuned_Llama3.2-1B.py:335:36
(EngineCore_DP0 pid=293530) 	or.b32 	%r114, %r113, %r111;
(EngineCore_DP0 pid=293530) 	.loc	1 335 55                        // quant_slide_tuned_Llama3.2-1B.py:335:55
(EngineCore_DP0 pid=293530) 	shl.b32 	%r115, %r102, 24;
(EngineCore_DP0 pid=293530) 	.loc	1 335 49                        // quant_slide_tuned_Llama3.2-1B.py:335:49
(EngineCore_DP0 pid=293530) 	or.b32 	%r70, %r114, %r115;
(EngineCore_DP0 pid=293530) 	.loc	1 336 29                        // quant_slide_tuned_Llama3.2-1B.py:336:29
(EngineCore_DP0 pid=293530) 	mad.wide.s32 	%rd12, %r72, 4, %rd2;
(EngineCore_DP0 pid=293530) 	.loc	1 336 39                        // quant_slide_tuned_Llama3.2-1B.py:336:39
(EngineCore_DP0 pid=293530) 	// begin inline asm
(EngineCore_DP0 pid=293530) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r70 };
(EngineCore_DP0 pid=293530) 	// end inline asm
(EngineCore_DP0 pid=293530) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=293530) 	add.s32 	%r119, %r119, 1024;
(EngineCore_DP0 pid=293530) 	setp.lt.s32 	%p18, %r119, %r14;
(EngineCore_DP0 pid=293530) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=293530) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=293530) 	.loc	1 312 4                         // quant_slide_tuned_Llama3.2-1B.py:312:4
(EngineCore_DP0 pid=293530) 	ret;
(EngineCore_DP0 pid=293530) $L__tmp3:
(EngineCore_DP0 pid=293530) $L__func_end0:
(EngineCore_DP0 pid=293530)                                         // -- End function
(EngineCore_DP0 pid=293530) }
(EngineCore_DP0 pid=293530) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=293530) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=293530) 	.section	.debug_abbrev
(EngineCore_DP0 pid=293530) 	{
(EngineCore_DP0 pid=293530) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=293530) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=293530) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=293530) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=293530) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=293530) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=293530) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=293530) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=293530) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=293530) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=293530) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=293530) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=293530) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=293530) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=293530) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=293530) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=293530) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=293530) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=293530) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=293530) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=293530) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=293530) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=293530) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=293530) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=293530) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=293530) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=293530) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=293530) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=293530) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=293530) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=293530) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=293530) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=293530) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=293530) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=293530) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=293530) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=293530) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=293530) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=293530) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=293530) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=293530) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=293530) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=293530) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=293530) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=293530) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=293530) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=293530) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=293530) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=293530) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=293530) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=293530) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=293530) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=293530) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=293530) 	}
(EngineCore_DP0 pid=293530) 	.section	.debug_info
(EngineCore_DP0 pid=293530) 	{
(EngineCore_DP0 pid=293530) .b32 224                                // Length of Unit
(EngineCore_DP0 pid=293530) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=293530) .b8 0
(EngineCore_DP0 pid=293530) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=293530) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=293530) .b8 1                                   // Abbrev [1] 0xb:0xd9 DW_TAG_compile_unit
(EngineCore_DP0 pid=293530) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=293530) .b8 114
(EngineCore_DP0 pid=293530) .b8 105
(EngineCore_DP0 pid=293530) .b8 116
(EngineCore_DP0 pid=293530) .b8 111
(EngineCore_DP0 pid=293530) .b8 110
(EngineCore_DP0 pid=293530) .b8 0
(EngineCore_DP0 pid=293530) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=293530) .b8 0
(EngineCore_DP0 pid=293530) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=293530) .b8 117
(EngineCore_DP0 pid=293530) .b8 97
(EngineCore_DP0 pid=293530) .b8 110
(EngineCore_DP0 pid=293530) .b8 116
(EngineCore_DP0 pid=293530) .b8 95
(EngineCore_DP0 pid=293530) .b8 115
(EngineCore_DP0 pid=293530) .b8 108
(EngineCore_DP0 pid=293530) .b8 105
(EngineCore_DP0 pid=293530) .b8 100
(EngineCore_DP0 pid=293530) .b8 101
(EngineCore_DP0 pid=293530) .b8 95
(EngineCore_DP0 pid=293530) .b8 116
(EngineCore_DP0 pid=293530) .b8 117
(EngineCore_DP0 pid=293530) .b8 110
(EngineCore_DP0 pid=293530) .b8 101
(EngineCore_DP0 pid=293530) .b8 100
(EngineCore_DP0 pid=293530) .b8 95
(EngineCore_DP0 pid=293530) .b8 76
(EngineCore_DP0 pid=293530) .b8 108
(EngineCore_DP0 pid=293530) .b8 97
(EngineCore_DP0 pid=293530) .b8 109
(EngineCore_DP0 pid=293530) .b8 97
(EngineCore_DP0 pid=293530) .b8 51
(EngineCore_DP0 pid=293530) .b8 46
(EngineCore_DP0 pid=293530) .b8 50
(EngineCore_DP0 pid=293530) .b8 45
(EngineCore_DP0 pid=293530) .b8 49
(EngineCore_DP0 pid=293530) .b8 66
(EngineCore_DP0 pid=293530) .b8 46
(EngineCore_DP0 pid=293530) .b8 112
(EngineCore_DP0 pid=293530) .b8 121
(EngineCore_DP0 pid=293530) .b8 0
(EngineCore_DP0 pid=293530) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=293530) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=293530) .b8 114
(EngineCore_DP0 pid=293530) .b8 111
(EngineCore_DP0 pid=293530) .b8 111
(EngineCore_DP0 pid=293530) .b8 116
(EngineCore_DP0 pid=293530) .b8 47
(EngineCore_DP0 pid=293530) .b8 118
(EngineCore_DP0 pid=293530) .b8 108
(EngineCore_DP0 pid=293530) .b8 108
(EngineCore_DP0 pid=293530) .b8 109
(EngineCore_DP0 pid=293530) .b8 98
(EngineCore_DP0 pid=293530) .b8 101
(EngineCore_DP0 pid=293530) .b8 110
(EngineCore_DP0 pid=293530) .b8 99
(EngineCore_DP0 pid=293530) .b8 104
(EngineCore_DP0 pid=293530) .b8 47
(EngineCore_DP0 pid=293530) .b8 115
(EngineCore_DP0 pid=293530) .b8 108
(EngineCore_DP0 pid=293530) .b8 105
(EngineCore_DP0 pid=293530) .b8 100
(EngineCore_DP0 pid=293530) .b8 101
(EngineCore_DP0 pid=293530) .b8 115
(EngineCore_DP0 pid=293530) .b8 112
(EngineCore_DP0 pid=293530) .b8 97
(EngineCore_DP0 pid=293530) .b8 114
(EngineCore_DP0 pid=293530) .b8 115
(EngineCore_DP0 pid=293530) .b8 101
(EngineCore_DP0 pid=293530) .b8 47
(EngineCore_DP0 pid=293530) .b8 99
(EngineCore_DP0 pid=293530) .b8 115
(EngineCore_DP0 pid=293530) .b8 114
(EngineCore_DP0 pid=293530) .b8 99
(EngineCore_DP0 pid=293530) .b8 47
(EngineCore_DP0 pid=293530) .b8 102
(EngineCore_DP0 pid=293530) .b8 117
(EngineCore_DP0 pid=293530) .b8 115
(EngineCore_DP0 pid=293530) .b8 101
(EngineCore_DP0 pid=293530) .b8 100
(EngineCore_DP0 pid=293530) .b8 95
(EngineCore_DP0 pid=293530) .b8 113
(EngineCore_DP0 pid=293530) .b8 117
(EngineCore_DP0 pid=293530) .b8 97
(EngineCore_DP0 pid=293530) .b8 110
(EngineCore_DP0 pid=293530) .b8 116
(EngineCore_DP0 pid=293530) .b8 95
(EngineCore_DP0 pid=293530) .b8 115
(EngineCore_DP0 pid=293530) .b8 108
(EngineCore_DP0 pid=293530) .b8 105
(EngineCore_DP0 pid=293530) .b8 100
(EngineCore_DP0 pid=293530) .b8 101
(EngineCore_DP0 pid=293530) .b8 95
(EngineCore_DP0 pid=293530) .b8 116
(EngineCore_DP0 pid=293530) .b8 114
(EngineCore_DP0 pid=293530) .b8 105
(EngineCore_DP0 pid=293530) .b8 116
(EngineCore_DP0 pid=293530) .b8 111
(EngineCore_DP0 pid=293530) .b8 110
(EngineCore_DP0 pid=293530) .b8 47
(EngineCore_DP0 pid=293530) .b8 98
(EngineCore_DP0 pid=293530) .b8 117
(EngineCore_DP0 pid=293530) .b8 105
(EngineCore_DP0 pid=293530) .b8 108
(EngineCore_DP0 pid=293530) .b8 100
(EngineCore_DP0 pid=293530) .b8 47
(EngineCore_DP0 pid=293530) .b8 71
(EngineCore_DP0 pid=293530) .b8 66
(EngineCore_DP0 pid=293530) .b8 49
(EngineCore_DP0 pid=293530) .b8 48
(EngineCore_DP0 pid=293530) .b8 95
(EngineCore_DP0 pid=293530) .b8 99
(EngineCore_DP0 pid=293530) .b8 99
(EngineCore_DP0 pid=293530) .b8 49
(EngineCore_DP0 pid=293530) .b8 50
(EngineCore_DP0 pid=293530) .b8 49
(EngineCore_DP0 pid=293530) .b8 95
(EngineCore_DP0 pid=293530) .b8 112
(EngineCore_DP0 pid=293530) .b8 121
(EngineCore_DP0 pid=293530) .b8 51
(EngineCore_DP0 pid=293530) .b8 49
(EngineCore_DP0 pid=293530) .b8 50
(EngineCore_DP0 pid=293530) .b8 95
(EngineCore_DP0 pid=293530) .b8 99
(EngineCore_DP0 pid=293530) .b8 117
(EngineCore_DP0 pid=293530) .b8 49
(EngineCore_DP0 pid=293530) .b8 50
(EngineCore_DP0 pid=293530) .b8 57
(EngineCore_DP0 pid=293530) .b8 95
(EngineCore_DP0 pid=293530) .b8 97
(EngineCore_DP0 pid=293530) .b8 97
(EngineCore_DP0 pid=293530) .b8 114
(EngineCore_DP0 pid=293530) .b8 99
(EngineCore_DP0 pid=293530) .b8 104
(EngineCore_DP0 pid=293530) .b8 54
(EngineCore_DP0 pid=293530) .b8 52
(EngineCore_DP0 pid=293530) .b8 0
(EngineCore_DP0 pid=293530) .b8 2                                   // Abbrev [2] 0x99:0x1b DW_TAG_subprogram
(EngineCore_DP0 pid=293530) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=293530) .b8 113
(EngineCore_DP0 pid=293530) .b8 117
(EngineCore_DP0 pid=293530) .b8 97
(EngineCore_DP0 pid=293530) .b8 110
(EngineCore_DP0 pid=293530) .b8 116
(EngineCore_DP0 pid=293530) .b8 95
(EngineCore_DP0 pid=293530) .b8 115
(EngineCore_DP0 pid=293530) .b8 108
(EngineCore_DP0 pid=293530) .b8 105
(EngineCore_DP0 pid=293530) .b8 100
(EngineCore_DP0 pid=293530) .b8 101
(EngineCore_DP0 pid=293530) .b8 95
(EngineCore_DP0 pid=293530) .b8 105
(EngineCore_DP0 pid=293530) .b8 110
(EngineCore_DP0 pid=293530) .b8 116
(EngineCore_DP0 pid=293530) .b8 56
(EngineCore_DP0 pid=293530) .b8 95
(EngineCore_DP0 pid=293530) .b8 107
(EngineCore_DP0 pid=293530) .b8 101
(EngineCore_DP0 pid=293530) .b8 114
(EngineCore_DP0 pid=293530) .b8 110
(EngineCore_DP0 pid=293530) .b8 101
(EngineCore_DP0 pid=293530) .b8 108
(EngineCore_DP0 pid=293530) .b8 0
(EngineCore_DP0 pid=293530) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=293530) .b8 3                                   // Abbrev [3] 0xb4:0x2f DW_TAG_subprogram
(EngineCore_DP0 pid=293530) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=293530) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=293530) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=293530) .b8 4                                   // Abbrev [4] 0xc9:0x19 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=293530) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=293530) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=293530) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=293530) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=293530) .b8 46                                  // DW_AT_call_line
(EngineCore_DP0 pid=293530) .b8 1
(EngineCore_DP0 pid=293530) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=293530) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=293530) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=293530) 	}
(EngineCore_DP0 pid=293530) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=293530) 
(EngineCore_DP0 pid=293530) ================================================================
(EngineCore_DP0 pid=293530) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=293530) 
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpehrptw85.ptx', '-o', '/tmp/tmpehrptw85.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866] 
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866] 
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866] 
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpehrptw85.ptx -o /tmp/tmpehrptw85.ptx.o
(EngineCore_DP0 pid=293530) ERROR 01-25 18:43:13 [core.py:866] 

STDERR:
[2026-01-25 18:42:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:42:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:42:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:42:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:42:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:42:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:42:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:42:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:42:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:42:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:42:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:42:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:42:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:42:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:43:00] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:43:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:43:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:43:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:43:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:43:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:43:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:43:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:43:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=293530) [2026-01-25 18:43:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=293530) [2026-01-25 18:43:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=293530) [2026-01-25 18:43:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=293530) [2026-01-25 18:43:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=293530) [2026-01-25 18:43:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=293530) [2026-01-25 18:43:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=293530) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=293530) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.93s/it]
(EngineCore_DP0 pid=293530) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.93s/it]
(EngineCore_DP0 pid=293530) 
(EngineCore_DP0 pid=293530) [2026-01-25 18:43:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=293530) [2026-01-25 18:43:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=293530) [2026-01-25 18:43:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=293530) [2026-01-25 18:43:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=293530) [2026-01-25 18:43:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=293530) [2026-01-25 18:43:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=293530) [2026-01-25 18:43:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=293530) [2026-01-25 18:43:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=293530) Process EngineCore_DP0:
(EngineCore_DP0 pid=293530) Traceback (most recent call last):
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=293530)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=293530)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=293530)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=293530) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpehrptw85.ptx', '-o', '/tmp/tmpehrptw85.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=293530) 
(EngineCore_DP0 pid=293530) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=293530) 
(EngineCore_DP0 pid=293530) Traceback (most recent call last):
(EngineCore_DP0 pid=293530)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=293530)     self.run()
(EngineCore_DP0 pid=293530)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=293530)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=293530)     raise e
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=293530)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=293530)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=293530)     super().__init__(
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=293530)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=293530)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=293530)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=293530)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=293530)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=293530)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=293530)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=293530)     return func(*args, **kwargs)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=293530)     return func(*args, **kwargs)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=293530)     self.model_runner.profile_run()
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=293530)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=293530)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=293530)     return func(*args, **kwargs)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=293530)     outputs = self.model(
(EngineCore_DP0 pid=293530)               ^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=293530)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=293530)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=293530)     model_output = self.model(
(EngineCore_DP0 pid=293530)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=293530)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=293530)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=293530)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=293530)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=293530)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=293530)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=293530)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=293530)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=293530)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=293530)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=293530)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=293530)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=293530)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=293530)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=293530)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=293530)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=293530)     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=293530)     return self._linear_fn(
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=293530)     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=293530)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=293530)     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=293530)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=293530)     return fn(input, L)
(EngineCore_DP0 pid=293530)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=293530)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=293530)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=293530)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=293530)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=293530)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=293530)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=293530)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=293530)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=293530)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=293530)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=293530)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=293530)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=293530)     raise PTXASError(error)
(EngineCore_DP0 pid=293530) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=293530) `ptxas` stderr:
(EngineCore_DP0 pid=293530) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=293530) 
(EngineCore_DP0 pid=293530) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpehrptw85.ptx -o /tmp/tmpehrptw85.ptx.o
(EngineCore_DP0 pid=293530) 
[rank0]:[W125 18:43:14.425014199 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=128 ==========
Time: 2026-01-25 18:43:15
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:43:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:43:19 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=294009) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=294009) 
(EngineCore_DP0 pid=294009) 
(EngineCore_DP0 pid=294009) ================================================================
(EngineCore_DP0 pid=294009) Internal Triton PTX codegen error
(EngineCore_DP0 pid=294009) `ptxas` stderr:
(EngineCore_DP0 pid=294009) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=294009) 
(EngineCore_DP0 pid=294009) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpp9qs5646.ptx -o /tmp/tmpp9qs5646.ptx.o
(EngineCore_DP0 pid=294009) 
(EngineCore_DP0 pid=294009) 
(EngineCore_DP0 pid=294009) //
(EngineCore_DP0 pid=294009) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=294009) //
(EngineCore_DP0 pid=294009) 
(EngineCore_DP0 pid=294009) .version 8.7
(EngineCore_DP0 pid=294009) .target sm_121a
(EngineCore_DP0 pid=294009) .address_size 64
(EngineCore_DP0 pid=294009) 
(EngineCore_DP0 pid=294009) 	// .globl	_quant_slide_int8_kernel // -- Begin function _quant_slide_int8_kernel
(EngineCore_DP0 pid=294009) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=294009)                                         // @_quant_slide_int8_kernel
(EngineCore_DP0 pid=294009) .visible .entry _quant_slide_int8_kernel(
(EngineCore_DP0 pid=294009) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_0,
(EngineCore_DP0 pid=294009) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_1,
(EngineCore_DP0 pid=294009) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_2,
(EngineCore_DP0 pid=294009) 	.param .u32 _quant_slide_int8_kernel_param_3,
(EngineCore_DP0 pid=294009) 	.param .u32 _quant_slide_int8_kernel_param_4,
(EngineCore_DP0 pid=294009) 	.param .u32 _quant_slide_int8_kernel_param_5,
(EngineCore_DP0 pid=294009) 	.param .u32 _quant_slide_int8_kernel_param_6,
(EngineCore_DP0 pid=294009) 	.param .u32 _quant_slide_int8_kernel_param_7,
(EngineCore_DP0 pid=294009) 	.param .u32 _quant_slide_int8_kernel_param_8,
(EngineCore_DP0 pid=294009) 	.param .u32 _quant_slide_int8_kernel_param_9,
(EngineCore_DP0 pid=294009) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_10,
(EngineCore_DP0 pid=294009) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_11
(EngineCore_DP0 pid=294009) )
(EngineCore_DP0 pid=294009) .reqntid 1024
(EngineCore_DP0 pid=294009) {
(EngineCore_DP0 pid=294009) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=294009) 	.reg .b16 	%rs<20>;
(EngineCore_DP0 pid=294009) 	.reg .b32 	%r<120>;
(EngineCore_DP0 pid=294009) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=294009) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=294009) $L__func_begin0:
(EngineCore_DP0 pid=294009) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=294009) 
(EngineCore_DP0 pid=294009) // %bb.0:
(EngineCore_DP0 pid=294009) 	ld.param.b32 	%r19, [_quant_slide_int8_kernel_param_7];
(EngineCore_DP0 pid=294009) 	ld.param.b32 	%r18, [_quant_slide_int8_kernel_param_5];
(EngineCore_DP0 pid=294009) 	ld.param.b32 	%r17, [_quant_slide_int8_kernel_param_4];
(EngineCore_DP0 pid=294009) 	ld.param.b64 	%rd3, [_quant_slide_int8_kernel_param_2];
(EngineCore_DP0 pid=294009) 	ld.param.b64 	%rd4, [_quant_slide_int8_kernel_param_0];
(EngineCore_DP0 pid=294009) $L__tmp0:
(EngineCore_DP0 pid=294009) 	.loc	1 288 24                        // quant_slide_tuned_Llama3.2-1B.py:288:24
(EngineCore_DP0 pid=294009) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=294009) 	ld.param.b32 	%r21, [_quant_slide_int8_kernel_param_8];
(EngineCore_DP0 pid=294009) 	.loc	1 293 26                        // quant_slide_tuned_Llama3.2-1B.py:293:26
(EngineCore_DP0 pid=294009) 	mul.lo.s32 	%r22, %r21, %r1;
(EngineCore_DP0 pid=294009) 	.loc	1 293 20                        // quant_slide_tuned_Llama3.2-1B.py:293:20
(EngineCore_DP0 pid=294009) 	mad.wide.s32 	%rd1, %r22, 2, %rd4;
(EngineCore_DP0 pid=294009) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=294009) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=294009) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=294009) 	setp.lt.s32 	%p1, %r18, 1;
(EngineCore_DP0 pid=294009) 	mov.b32 	%r118, 0f2B8CBCCC;
(EngineCore_DP0 pid=294009) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=294009) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=294009) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=294009) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=294009) 	shl.b32 	%r3, %r2, 2;
(EngineCore_DP0 pid=294009) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=294009) 	shr.u32 	%r31, %r2, 3;
(EngineCore_DP0 pid=294009) 	and.b32 	%r32, %r31, 124;
(EngineCore_DP0 pid=294009) 	mov.b32 	%r33, global_smem;
(EngineCore_DP0 pid=294009) 	add.s32 	%r39, %r33, %r32;
(EngineCore_DP0 pid=294009) 	add.s32 	%r42, %r33, %r3;
(EngineCore_DP0 pid=294009) 	mov.b32 	%r37, 0;
(EngineCore_DP0 pid=294009) 	mov.b32 	%r116, 0f00000000;
(EngineCore_DP0 pid=294009) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=294009) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=294009) 	mov.b32 	%r117, %r37;
(EngineCore_DP0 pid=294009) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=294009) 	.loc	1 300 22                        // quant_slide_tuned_Llama3.2-1B.py:300:22
(EngineCore_DP0 pid=294009) 	add.s32 	%r45, %r3, %r117;
(EngineCore_DP0 pid=294009) 	setp.lt.s32 	%p2, %r45, %r17;
(EngineCore_DP0 pid=294009) 	.loc	1 301 29                        // quant_slide_tuned_Llama3.2-1B.py:301:29
(EngineCore_DP0 pid=294009) 	mad.wide.s32 	%rd6, %r45, 2, %rd1;
(EngineCore_DP0 pid=294009) 	.loc	1 301 21                        // quant_slide_tuned_Llama3.2-1B.py:301:21
(EngineCore_DP0 pid=294009) 	// begin inline asm
(EngineCore_DP0 pid=294009) 	mov.u32 %r35, %r37;
(EngineCore_DP0 pid=294009) 	mov.u32 %r36, %r37;
(EngineCore_DP0 pid=294009) 	@%p2 ld.global.v2.b32 { %r35, %r36 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=294009) 	// end inline asm
(EngineCore_DP0 pid=294009) 	mov.b32 	{%rs1, %rs2}, %r35;
(EngineCore_DP0 pid=294009) 	mov.b32 	{%rs3, %rs4}, %r36;
(EngineCore_DP0 pid=294009) 	.loc	1 302 50                        // quant_slide_tuned_Llama3.2-1B.py:302:50
(EngineCore_DP0 pid=294009) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=294009) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=294009) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=294009) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=294009) $L__tmp1:
(EngineCore_DP0 pid=294009) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	bar.sync 	0;
(EngineCore_DP0 pid=294009) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=294009) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=294009) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=294009) 	cvt.f32.bf16 	%r46, %rs11;
(EngineCore_DP0 pid=294009) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	shfl.sync.bfly.b32 	%r47, %r46, 16, 31, -1;
(EngineCore_DP0 pid=294009) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	max.f32 	%r48, %r46, %r47;
(EngineCore_DP0 pid=294009) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	shfl.sync.bfly.b32 	%r49, %r48, 8, 31, -1;
(EngineCore_DP0 pid=294009) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	max.f32 	%r50, %r48, %r49;
(EngineCore_DP0 pid=294009) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	shfl.sync.bfly.b32 	%r51, %r50, 4, 31, -1;
(EngineCore_DP0 pid=294009) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	max.f32 	%r52, %r50, %r51;
(EngineCore_DP0 pid=294009) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	shfl.sync.bfly.b32 	%r53, %r52, 2, 31, -1;
(EngineCore_DP0 pid=294009) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	max.f32 	%r54, %r52, %r53;
(EngineCore_DP0 pid=294009) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	shfl.sync.bfly.b32 	%r55, %r54, 1, 31, -1;
(EngineCore_DP0 pid=294009) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	max.f32 	%r40, %r54, %r55;
(EngineCore_DP0 pid=294009) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	// begin inline asm
(EngineCore_DP0 pid=294009) 	@%p3 st.shared.b32 [ %r39 + 0 ], %r40;
(EngineCore_DP0 pid=294009) 	// end inline asm
(EngineCore_DP0 pid=294009) 	bar.sync 	0;
(EngineCore_DP0 pid=294009) 	// begin inline asm
(EngineCore_DP0 pid=294009) 	@%p4 ld.shared.b32 %r41, [ %r42 + 0 ];
(EngineCore_DP0 pid=294009) 	// end inline asm
(EngineCore_DP0 pid=294009) 	shfl.sync.bfly.b32 	%r56, %r41, 16, 31, -1;
(EngineCore_DP0 pid=294009) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	max.f32 	%r57, %r41, %r56;
(EngineCore_DP0 pid=294009) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	shfl.sync.bfly.b32 	%r58, %r57, 8, 31, -1;
(EngineCore_DP0 pid=294009) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	max.f32 	%r59, %r57, %r58;
(EngineCore_DP0 pid=294009) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	shfl.sync.bfly.b32 	%r60, %r59, 4, 31, -1;
(EngineCore_DP0 pid=294009) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	max.f32 	%r61, %r59, %r60;
(EngineCore_DP0 pid=294009) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	shfl.sync.bfly.b32 	%r62, %r61, 2, 31, -1;
(EngineCore_DP0 pid=294009) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	max.f32 	%r63, %r61, %r62;
(EngineCore_DP0 pid=294009) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	shfl.sync.bfly.b32 	%r64, %r63, 1, 31, -1;
(EngineCore_DP0 pid=294009) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	max.f32 	%r44, %r63, %r64;
(EngineCore_DP0 pid=294009) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294009) 	// begin inline asm
(EngineCore_DP0 pid=294009) 	@%p19 st.shared.b32 [ %r42 + 0 ], %r44;
(EngineCore_DP0 pid=294009) 	// end inline asm
(EngineCore_DP0 pid=294009) 	bar.sync 	0;
(EngineCore_DP0 pid=294009) 	ld.shared.b32 	%r65, [global_smem];
(EngineCore_DP0 pid=294009) $L__tmp2:
(EngineCore_DP0 pid=294009) 	.loc	1 302 36                        // quant_slide_tuned_Llama3.2-1B.py:302:36
(EngineCore_DP0 pid=294009) 	max.f32 	%r116, %r116, %r65;
(EngineCore_DP0 pid=294009) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=294009) 	add.s32 	%r117, %r117, 4096;
(EngineCore_DP0 pid=294009) 	setp.lt.s32 	%p6, %r117, %r18;
(EngineCore_DP0 pid=294009) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=294009) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=294009) 	.loc	1 304 32                        // quant_slide_tuned_Llama3.2-1B.py:304:32
(EngineCore_DP0 pid=294009) 	max.f32 	%r118, %r116, 0f2B8CBCCC;
(EngineCore_DP0 pid=294009) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=294009) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=294009) 	mov.b32 	%r67, 0f42FE0000;
(EngineCore_DP0 pid=294009) 	.loc	1 305 32                        // quant_slide_tuned_Llama3.2-1B.py:305:32
(EngineCore_DP0 pid=294009) 	div.full.f32 	%r68, %r118, %r67;
(EngineCore_DP0 pid=294009) 	.loc	1 305 42                        // quant_slide_tuned_Llama3.2-1B.py:305:42
(EngineCore_DP0 pid=294009) 	max.f32 	%r66, %r68, 0f37810204;
(EngineCore_DP0 pid=294009) 	.loc	1 307 25                        // quant_slide_tuned_Llama3.2-1B.py:307:25
(EngineCore_DP0 pid=294009) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=294009) 	.loc	1 307 30                        // quant_slide_tuned_Llama3.2-1B.py:307:30
(EngineCore_DP0 pid=294009) 	// begin inline asm
(EngineCore_DP0 pid=294009) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r66 };
(EngineCore_DP0 pid=294009) 	// end inline asm
(EngineCore_DP0 pid=294009) 	.loc	1 310 29                        // quant_slide_tuned_Llama3.2-1B.py:310:29
(EngineCore_DP0 pid=294009) 	shl.b32 	%r14, %r19, 2;
(EngineCore_DP0 pid=294009) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=294009) 	setp.lt.s32 	%p8, %r14, 1;
(EngineCore_DP0 pid=294009) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=294009) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=294009) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=294009) 	ld.param.b32 	%r23, [_quant_slide_int8_kernel_param_9];
(EngineCore_DP0 pid=294009) 	shr.s32 	%r24, %r23, 31;
(EngineCore_DP0 pid=294009) 	shr.u32 	%r25, %r24, 30;
(EngineCore_DP0 pid=294009) 	add.s32 	%r26, %r23, %r25;
(EngineCore_DP0 pid=294009) 	shr.s32 	%r27, %r26, 2;
(EngineCore_DP0 pid=294009) 	ld.param.b64 	%rd5, [_quant_slide_int8_kernel_param_1];
(EngineCore_DP0 pid=294009) 	mul.lo.s32 	%r28, %r27, %r1;
(EngineCore_DP0 pid=294009) 	mad.wide.s32 	%rd2, %r28, 4, %rd5;
(EngineCore_DP0 pid=294009) 	div.full.f32 	%r13, %r67, %r118;
(EngineCore_DP0 pid=294009) 	mov.b32 	%r119, 0;
(EngineCore_DP0 pid=294009) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=294009)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=294009) 	.loc	1 314 30                        // quant_slide_tuned_Llama3.2-1B.py:314:30
(EngineCore_DP0 pid=294009) 	add.s32 	%r72, %r2, %r119;
(EngineCore_DP0 pid=294009) 	setp.lt.s32 	%p13, %r72, %r14;
(EngineCore_DP0 pid=294009) 	.loc	1 317 24                        // quant_slide_tuned_Llama3.2-1B.py:317:24
(EngineCore_DP0 pid=294009) 	shr.s32 	%r73, %r72, 31;
(EngineCore_DP0 pid=294009) 	shr.u32 	%r74, %r73, 30;
(EngineCore_DP0 pid=294009) 	add.s32 	%r75, %r72, %r74;
(EngineCore_DP0 pid=294009) 	shr.s32 	%r76, %r75, 2;
(EngineCore_DP0 pid=294009) 	.loc	1 318 23                        // quant_slide_tuned_Llama3.2-1B.py:318:23
(EngineCore_DP0 pid=294009) 	and.b32 	%r77, %r75, 2147483644;
(EngineCore_DP0 pid=294009) 	sub.s32 	%r78, %r72, %r77;
(EngineCore_DP0 pid=294009) 	.loc	1 319 30                        // quant_slide_tuned_Llama3.2-1B.py:319:30
(EngineCore_DP0 pid=294009) 	shl.b32 	%r79, %r78, 1;
(EngineCore_DP0 pid=294009) 	.loc	1 319 26                        // quant_slide_tuned_Llama3.2-1B.py:319:26
(EngineCore_DP0 pid=294009) 	mad.lo.s32 	%r80, %r76, 10, %r79;
(EngineCore_DP0 pid=294009) 	.loc	1 322 53                        // quant_slide_tuned_Llama3.2-1B.py:322:53
(EngineCore_DP0 pid=294009) 	setp.lt.s32 	%p14, %r80, %r17;
(EngineCore_DP0 pid=294009) 	.loc	1 322 37                        // quant_slide_tuned_Llama3.2-1B.py:322:37
(EngineCore_DP0 pid=294009) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=294009) 	.loc	1 321 29                        // quant_slide_tuned_Llama3.2-1B.py:321:29
(EngineCore_DP0 pid=294009) 	mad.wide.s32 	%rd8, %r80, 2, %rd1;
(EngineCore_DP0 pid=294009) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=294009) 	.loc	1 321 21                        // quant_slide_tuned_Llama3.2-1B.py:321:21
(EngineCore_DP0 pid=294009) 	// begin inline asm
(EngineCore_DP0 pid=294009) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=294009) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=294009) 	// end inline asm
(EngineCore_DP0 pid=294009) 	.loc	1 322 79                        // quant_slide_tuned_Llama3.2-1B.py:322:79
(EngineCore_DP0 pid=294009) 	cvt.f32.bf16 	%r81, %rs12;
(EngineCore_DP0 pid=294009) 	.loc	1 324 48                        // quant_slide_tuned_Llama3.2-1B.py:324:48
(EngineCore_DP0 pid=294009) 	or.b32 	%r82, %r80, 1;
(EngineCore_DP0 pid=294009) 	.loc	1 324 53                        // quant_slide_tuned_Llama3.2-1B.py:324:53
(EngineCore_DP0 pid=294009) 	setp.lt.s32 	%p15, %r82, %r17;
(EngineCore_DP0 pid=294009) 	.loc	1 324 37                        // quant_slide_tuned_Llama3.2-1B.py:324:37
(EngineCore_DP0 pid=294009) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=294009) 	.loc	1 323 39                        // quant_slide_tuned_Llama3.2-1B.py:323:39
(EngineCore_DP0 pid=294009) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=294009) 	.loc	1 323 21                        // quant_slide_tuned_Llama3.2-1B.py:323:21
(EngineCore_DP0 pid=294009) 	// begin inline asm
(EngineCore_DP0 pid=294009) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=294009) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=294009) 	// end inline asm
(EngineCore_DP0 pid=294009) 	.loc	1 324 79                        // quant_slide_tuned_Llama3.2-1B.py:324:79
(EngineCore_DP0 pid=294009) 	cvt.f32.bf16 	%r83, %rs14;
(EngineCore_DP0 pid=294009) 	.loc	1 326 48                        // quant_slide_tuned_Llama3.2-1B.py:326:48
(EngineCore_DP0 pid=294009) 	add.s32 	%r84, %r80, 2;
(EngineCore_DP0 pid=294009) 	.loc	1 326 53                        // quant_slide_tuned_Llama3.2-1B.py:326:53
(EngineCore_DP0 pid=294009) 	setp.lt.s32 	%p16, %r84, %r17;
(EngineCore_DP0 pid=294009) 	.loc	1 326 37                        // quant_slide_tuned_Llama3.2-1B.py:326:37
(EngineCore_DP0 pid=294009) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=294009) 	.loc	1 325 39                        // quant_slide_tuned_Llama3.2-1B.py:325:39
(EngineCore_DP0 pid=294009) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=294009) 	.loc	1 325 21                        // quant_slide_tuned_Llama3.2-1B.py:325:21
(EngineCore_DP0 pid=294009) 	// begin inline asm
(EngineCore_DP0 pid=294009) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=294009) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=294009) 	// end inline asm
(EngineCore_DP0 pid=294009) 	.loc	1 326 79                        // quant_slide_tuned_Llama3.2-1B.py:326:79
(EngineCore_DP0 pid=294009) 	cvt.f32.bf16 	%r85, %rs16;
(EngineCore_DP0 pid=294009) 	.loc	1 328 48                        // quant_slide_tuned_Llama3.2-1B.py:328:48
(EngineCore_DP0 pid=294009) 	add.s32 	%r86, %r80, 3;
(EngineCore_DP0 pid=294009) 	.loc	1 328 53                        // quant_slide_tuned_Llama3.2-1B.py:328:53
(EngineCore_DP0 pid=294009) 	setp.lt.s32 	%p17, %r86, %r17;
(EngineCore_DP0 pid=294009) 	.loc	1 328 37                        // quant_slide_tuned_Llama3.2-1B.py:328:37
(EngineCore_DP0 pid=294009) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=294009) 	.loc	1 327 39                        // quant_slide_tuned_Llama3.2-1B.py:327:39
(EngineCore_DP0 pid=294009) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=294009) 	.loc	1 327 21                        // quant_slide_tuned_Llama3.2-1B.py:327:21
(EngineCore_DP0 pid=294009) 	// begin inline asm
(EngineCore_DP0 pid=294009) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=294009) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=294009) 	// end inline asm
(EngineCore_DP0 pid=294009) 	.loc	1 328 79                        // quant_slide_tuned_Llama3.2-1B.py:328:79
(EngineCore_DP0 pid=294009) 	cvt.f32.bf16 	%r87, %rs18;
(EngineCore_DP0 pid=294009) 	.loc	1 330 56                        // quant_slide_tuned_Llama3.2-1B.py:330:56
(EngineCore_DP0 pid=294009) 	mul.f32 	%r88, %r13, %r81;
(EngineCore_DP0 pid=294009) 	.loc	1 330 51                        // quant_slide_tuned_Llama3.2-1B.py:330:51
(EngineCore_DP0 pid=294009) 	cvt.rni.f32.f32 	%r89, %r88;
(EngineCore_DP0 pid=294009) 	.loc	1 330 76                        // quant_slide_tuned_Llama3.2-1B.py:330:76
(EngineCore_DP0 pid=294009) 	max.f32 	%r90, %r89, 0fC3000000;
(EngineCore_DP0 pid=294009) 	min.f32 	%r91, %r90, 0f42FE0000;
(EngineCore_DP0 pid=294009) 	.loc	1 330 86                        // quant_slide_tuned_Llama3.2-1B.py:330:86
(EngineCore_DP0 pid=294009) 	cvt.rzi.s32.f32 	%r92, %r91;
(EngineCore_DP0 pid=294009) 	.loc	1 330 98                        // quant_slide_tuned_Llama3.2-1B.py:330:98
(EngineCore_DP0 pid=294009) 	and.b32 	%r93, %r92, 255;
(EngineCore_DP0 pid=294009) 	.loc	1 331 56                        // quant_slide_tuned_Llama3.2-1B.py:331:56
(EngineCore_DP0 pid=294009) 	mul.f32 	%r94, %r13, %r83;
(EngineCore_DP0 pid=294009) 	.loc	1 331 51                        // quant_slide_tuned_Llama3.2-1B.py:331:51
(EngineCore_DP0 pid=294009) 	cvt.rni.f32.f32 	%r95, %r94;
(EngineCore_DP0 pid=294009) 	.loc	1 332 56                        // quant_slide_tuned_Llama3.2-1B.py:332:56
(EngineCore_DP0 pid=294009) 	mul.f32 	%r96, %r13, %r85;
(EngineCore_DP0 pid=294009) 	.loc	1 332 51                        // quant_slide_tuned_Llama3.2-1B.py:332:51
(EngineCore_DP0 pid=294009) 	cvt.rni.f32.f32 	%r97, %r96;
(EngineCore_DP0 pid=294009) 	.loc	1 333 56                        // quant_slide_tuned_Llama3.2-1B.py:333:56
(EngineCore_DP0 pid=294009) 	mul.f32 	%r98, %r13, %r87;
(EngineCore_DP0 pid=294009) 	.loc	1 333 51                        // quant_slide_tuned_Llama3.2-1B.py:333:51
(EngineCore_DP0 pid=294009) 	cvt.rni.f32.f32 	%r99, %r98;
(EngineCore_DP0 pid=294009) 	.loc	1 333 76                        // quant_slide_tuned_Llama3.2-1B.py:333:76
(EngineCore_DP0 pid=294009) 	max.f32 	%r100, %r99, 0fC3000000;
(EngineCore_DP0 pid=294009) 	min.f32 	%r101, %r100, 0f42FE0000;
(EngineCore_DP0 pid=294009) 	.loc	1 333 86                        // quant_slide_tuned_Llama3.2-1B.py:333:86
(EngineCore_DP0 pid=294009) 	cvt.rzi.s32.f32 	%r102, %r101;
(EngineCore_DP0 pid=294009) 	.loc	1 331 76                        // quant_slide_tuned_Llama3.2-1B.py:331:76
(EngineCore_DP0 pid=294009) 	max.f32 	%r103, %r97, 0fC3000000;
(EngineCore_DP0 pid=294009) 	max.f32 	%r104, %r95, 0fC3000000;
(EngineCore_DP0 pid=294009) 	min.f32 	%r105, %r104, 0f42FE0000;
(EngineCore_DP0 pid=294009) 	min.f32 	%r106, %r103, 0f42FE0000;
(EngineCore_DP0 pid=294009) 	.loc	1 331 86                        // quant_slide_tuned_Llama3.2-1B.py:331:86
(EngineCore_DP0 pid=294009) 	cvt.rzi.s32.f32 	%r107, %r106;
(EngineCore_DP0 pid=294009) 	cvt.rzi.s32.f32 	%r108, %r105;
(EngineCore_DP0 pid=294009) 	.loc	1 335 30                        // quant_slide_tuned_Llama3.2-1B.py:335:30
(EngineCore_DP0 pid=294009) 	shl.b32 	%r109, %r108, 8;
(EngineCore_DP0 pid=294009) 	shl.b32 	%r110, %r107, 16;
(EngineCore_DP0 pid=294009) 	and.b32 	%r111, %r110, 16711680;
(EngineCore_DP0 pid=294009) 	and.b32 	%r112, %r109, 65280;
(EngineCore_DP0 pid=294009) 	.loc	1 335 24                        // quant_slide_tuned_Llama3.2-1B.py:335:24
(EngineCore_DP0 pid=294009) 	or.b32 	%r113, %r112, %r93;
(EngineCore_DP0 pid=294009) 	.loc	1 335 36                        // quant_slide_tuned_Llama3.2-1B.py:335:36
(EngineCore_DP0 pid=294009) 	or.b32 	%r114, %r113, %r111;
(EngineCore_DP0 pid=294009) 	.loc	1 335 55                        // quant_slide_tuned_Llama3.2-1B.py:335:55
(EngineCore_DP0 pid=294009) 	shl.b32 	%r115, %r102, 24;
(EngineCore_DP0 pid=294009) 	.loc	1 335 49                        // quant_slide_tuned_Llama3.2-1B.py:335:49
(EngineCore_DP0 pid=294009) 	or.b32 	%r70, %r114, %r115;
(EngineCore_DP0 pid=294009) 	.loc	1 336 29                        // quant_slide_tuned_Llama3.2-1B.py:336:29
(EngineCore_DP0 pid=294009) 	mad.wide.s32 	%rd12, %r72, 4, %rd2;
(EngineCore_DP0 pid=294009) 	.loc	1 336 39                        // quant_slide_tuned_Llama3.2-1B.py:336:39
(EngineCore_DP0 pid=294009) 	// begin inline asm
(EngineCore_DP0 pid=294009) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r70 };
(EngineCore_DP0 pid=294009) 	// end inline asm
(EngineCore_DP0 pid=294009) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=294009) 	add.s32 	%r119, %r119, 1024;
(EngineCore_DP0 pid=294009) 	setp.lt.s32 	%p18, %r119, %r14;
(EngineCore_DP0 pid=294009) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=294009) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=294009) 	.loc	1 312 4                         // quant_slide_tuned_Llama3.2-1B.py:312:4
(EngineCore_DP0 pid=294009) 	ret;
(EngineCore_DP0 pid=294009) $L__tmp3:
(EngineCore_DP0 pid=294009) $L__func_end0:
(EngineCore_DP0 pid=294009)                                         // -- End function
(EngineCore_DP0 pid=294009) }
(EngineCore_DP0 pid=294009) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=294009) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=294009) 	.section	.debug_abbrev
(EngineCore_DP0 pid=294009) 	{
(EngineCore_DP0 pid=294009) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=294009) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=294009) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=294009) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=294009) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=294009) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=294009) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=294009) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=294009) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=294009) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=294009) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=294009) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=294009) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=294009) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=294009) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=294009) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=294009) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=294009) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=294009) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=294009) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=294009) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=294009) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=294009) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=294009) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=294009) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=294009) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=294009) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=294009) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=294009) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=294009) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=294009) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=294009) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=294009) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=294009) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=294009) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=294009) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=294009) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=294009) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=294009) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=294009) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=294009) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=294009) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=294009) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=294009) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=294009) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=294009) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=294009) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=294009) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=294009) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=294009) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=294009) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=294009) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=294009) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=294009) 	}
(EngineCore_DP0 pid=294009) 	.section	.debug_info
(EngineCore_DP0 pid=294009) 	{
(EngineCore_DP0 pid=294009) .b32 224                                // Length of Unit
(EngineCore_DP0 pid=294009) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=294009) .b8 0
(EngineCore_DP0 pid=294009) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=294009) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=294009) .b8 1                                   // Abbrev [1] 0xb:0xd9 DW_TAG_compile_unit
(EngineCore_DP0 pid=294009) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=294009) .b8 114
(EngineCore_DP0 pid=294009) .b8 105
(EngineCore_DP0 pid=294009) .b8 116
(EngineCore_DP0 pid=294009) .b8 111
(EngineCore_DP0 pid=294009) .b8 110
(EngineCore_DP0 pid=294009) .b8 0
(EngineCore_DP0 pid=294009) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=294009) .b8 0
(EngineCore_DP0 pid=294009) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=294009) .b8 117
(EngineCore_DP0 pid=294009) .b8 97
(EngineCore_DP0 pid=294009) .b8 110
(EngineCore_DP0 pid=294009) .b8 116
(EngineCore_DP0 pid=294009) .b8 95
(EngineCore_DP0 pid=294009) .b8 115
(EngineCore_DP0 pid=294009) .b8 108
(EngineCore_DP0 pid=294009) .b8 105
(EngineCore_DP0 pid=294009) .b8 100
(EngineCore_DP0 pid=294009) .b8 101
(EngineCore_DP0 pid=294009) .b8 95
(EngineCore_DP0 pid=294009) .b8 116
(EngineCore_DP0 pid=294009) .b8 117
(EngineCore_DP0 pid=294009) .b8 110
(EngineCore_DP0 pid=294009) .b8 101
(EngineCore_DP0 pid=294009) .b8 100
(EngineCore_DP0 pid=294009) .b8 95
(EngineCore_DP0 pid=294009) .b8 76
(EngineCore_DP0 pid=294009) .b8 108
(EngineCore_DP0 pid=294009) .b8 97
(EngineCore_DP0 pid=294009) .b8 109
(EngineCore_DP0 pid=294009) .b8 97
(EngineCore_DP0 pid=294009) .b8 51
(EngineCore_DP0 pid=294009) .b8 46
(EngineCore_DP0 pid=294009) .b8 50
(EngineCore_DP0 pid=294009) .b8 45
(EngineCore_DP0 pid=294009) .b8 49
(EngineCore_DP0 pid=294009) .b8 66
(EngineCore_DP0 pid=294009) .b8 46
(EngineCore_DP0 pid=294009) .b8 112
(EngineCore_DP0 pid=294009) .b8 121
(EngineCore_DP0 pid=294009) .b8 0
(EngineCore_DP0 pid=294009) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=294009) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=294009) .b8 114
(EngineCore_DP0 pid=294009) .b8 111
(EngineCore_DP0 pid=294009) .b8 111
(EngineCore_DP0 pid=294009) .b8 116
(EngineCore_DP0 pid=294009) .b8 47
(EngineCore_DP0 pid=294009) .b8 118
(EngineCore_DP0 pid=294009) .b8 108
(EngineCore_DP0 pid=294009) .b8 108
(EngineCore_DP0 pid=294009) .b8 109
(EngineCore_DP0 pid=294009) .b8 98
(EngineCore_DP0 pid=294009) .b8 101
(EngineCore_DP0 pid=294009) .b8 110
(EngineCore_DP0 pid=294009) .b8 99
(EngineCore_DP0 pid=294009) .b8 104
(EngineCore_DP0 pid=294009) .b8 47
(EngineCore_DP0 pid=294009) .b8 115
(EngineCore_DP0 pid=294009) .b8 108
(EngineCore_DP0 pid=294009) .b8 105
(EngineCore_DP0 pid=294009) .b8 100
(EngineCore_DP0 pid=294009) .b8 101
(EngineCore_DP0 pid=294009) .b8 115
(EngineCore_DP0 pid=294009) .b8 112
(EngineCore_DP0 pid=294009) .b8 97
(EngineCore_DP0 pid=294009) .b8 114
(EngineCore_DP0 pid=294009) .b8 115
(EngineCore_DP0 pid=294009) .b8 101
(EngineCore_DP0 pid=294009) .b8 47
(EngineCore_DP0 pid=294009) .b8 99
(EngineCore_DP0 pid=294009) .b8 115
(EngineCore_DP0 pid=294009) .b8 114
(EngineCore_DP0 pid=294009) .b8 99
(EngineCore_DP0 pid=294009) .b8 47
(EngineCore_DP0 pid=294009) .b8 102
(EngineCore_DP0 pid=294009) .b8 117
(EngineCore_DP0 pid=294009) .b8 115
(EngineCore_DP0 pid=294009) .b8 101
(EngineCore_DP0 pid=294009) .b8 100
(EngineCore_DP0 pid=294009) .b8 95
(EngineCore_DP0 pid=294009) .b8 113
(EngineCore_DP0 pid=294009) .b8 117
(EngineCore_DP0 pid=294009) .b8 97
(EngineCore_DP0 pid=294009) .b8 110
(EngineCore_DP0 pid=294009) .b8 116
(EngineCore_DP0 pid=294009) .b8 95
(EngineCore_DP0 pid=294009) .b8 115
(EngineCore_DP0 pid=294009) .b8 108
(EngineCore_DP0 pid=294009) .b8 105
(EngineCore_DP0 pid=294009) .b8 100
(EngineCore_DP0 pid=294009) .b8 101
(EngineCore_DP0 pid=294009) .b8 95
(EngineCore_DP0 pid=294009) .b8 116
(EngineCore_DP0 pid=294009) .b8 114
(EngineCore_DP0 pid=294009) .b8 105
(EngineCore_DP0 pid=294009) .b8 116
(EngineCore_DP0 pid=294009) .b8 111
(EngineCore_DP0 pid=294009) .b8 110
(EngineCore_DP0 pid=294009) .b8 47
(EngineCore_DP0 pid=294009) .b8 98
(EngineCore_DP0 pid=294009) .b8 117
(EngineCore_DP0 pid=294009) .b8 105
(EngineCore_DP0 pid=294009) .b8 108
(EngineCore_DP0 pid=294009) .b8 100
(EngineCore_DP0 pid=294009) .b8 47
(EngineCore_DP0 pid=294009) .b8 71
(EngineCore_DP0 pid=294009) .b8 66
(EngineCore_DP0 pid=294009) .b8 49
(EngineCore_DP0 pid=294009) .b8 48
(EngineCore_DP0 pid=294009) .b8 95
(EngineCore_DP0 pid=294009) .b8 99
(EngineCore_DP0 pid=294009) .b8 99
(EngineCore_DP0 pid=294009) .b8 49
(EngineCore_DP0 pid=294009) .b8 50
(EngineCore_DP0 pid=294009) .b8 49
(EngineCore_DP0 pid=294009) .b8 95
(EngineCore_DP0 pid=294009) .b8 112
(EngineCore_DP0 pid=294009) .b8 121
(EngineCore_DP0 pid=294009) .b8 51
(EngineCore_DP0 pid=294009) .b8 49
(EngineCore_DP0 pid=294009) .b8 50
(EngineCore_DP0 pid=294009) .b8 95
(EngineCore_DP0 pid=294009) .b8 99
(EngineCore_DP0 pid=294009) .b8 117
(EngineCore_DP0 pid=294009) .b8 49
(EngineCore_DP0 pid=294009) .b8 50
(EngineCore_DP0 pid=294009) .b8 57
(EngineCore_DP0 pid=294009) .b8 95
(EngineCore_DP0 pid=294009) .b8 97
(EngineCore_DP0 pid=294009) .b8 97
(EngineCore_DP0 pid=294009) .b8 114
(EngineCore_DP0 pid=294009) .b8 99
(EngineCore_DP0 pid=294009) .b8 104
(EngineCore_DP0 pid=294009) .b8 54
(EngineCore_DP0 pid=294009) .b8 52
(EngineCore_DP0 pid=294009) .b8 0
(EngineCore_DP0 pid=294009) .b8 2                                   // Abbrev [2] 0x99:0x1b DW_TAG_subprogram
(EngineCore_DP0 pid=294009) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=294009) .b8 113
(EngineCore_DP0 pid=294009) .b8 117
(EngineCore_DP0 pid=294009) .b8 97
(EngineCore_DP0 pid=294009) .b8 110
(EngineCore_DP0 pid=294009) .b8 116
(EngineCore_DP0 pid=294009) .b8 95
(EngineCore_DP0 pid=294009) .b8 115
(EngineCore_DP0 pid=294009) .b8 108
(EngineCore_DP0 pid=294009) .b8 105
(EngineCore_DP0 pid=294009) .b8 100
(EngineCore_DP0 pid=294009) .b8 101
(EngineCore_DP0 pid=294009) .b8 95
(EngineCore_DP0 pid=294009) .b8 105
(EngineCore_DP0 pid=294009) .b8 110
(EngineCore_DP0 pid=294009) .b8 116
(EngineCore_DP0 pid=294009) .b8 56
(EngineCore_DP0 pid=294009) .b8 95
(EngineCore_DP0 pid=294009) .b8 107
(EngineCore_DP0 pid=294009) .b8 101
(EngineCore_DP0 pid=294009) .b8 114
(EngineCore_DP0 pid=294009) .b8 110
(EngineCore_DP0 pid=294009) .b8 101
(EngineCore_DP0 pid=294009) .b8 108
(EngineCore_DP0 pid=294009) .b8 0
(EngineCore_DP0 pid=294009) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=294009) .b8 3                                   // Abbrev [3] 0xb4:0x2f DW_TAG_subprogram
(EngineCore_DP0 pid=294009) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=294009) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=294009) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=294009) .b8 4                                   // Abbrev [4] 0xc9:0x19 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=294009) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=294009) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=294009) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=294009) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=294009) .b8 46                                  // DW_AT_call_line
(EngineCore_DP0 pid=294009) .b8 1
(EngineCore_DP0 pid=294009) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=294009) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=294009) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=294009) 	}
(EngineCore_DP0 pid=294009) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=294009) 
(EngineCore_DP0 pid=294009) ================================================================
(EngineCore_DP0 pid=294009) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=294009) 
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpp9qs5646.ptx', '-o', '/tmp/tmpp9qs5646.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866] 
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866] 
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866] 
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpp9qs5646.ptx -o /tmp/tmpp9qs5646.ptx.o
(EngineCore_DP0 pid=294009) ERROR 01-25 18:43:36 [core.py:866] 

STDERR:
[2026-01-25 18:43:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:43:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:43:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:43:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:43:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:43:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:43:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:43:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:43:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:43:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:43:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:43:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:43:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:43:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:43:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:43:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:43:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:43:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=294009) [2026-01-25 18:43:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=294009) [2026-01-25 18:43:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=294009) [2026-01-25 18:43:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=294009) [2026-01-25 18:43:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=294009) [2026-01-25 18:43:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=294009) [2026-01-25 18:43:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=294009) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=294009) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.99s/it]
(EngineCore_DP0 pid=294009) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.99s/it]
(EngineCore_DP0 pid=294009) 
(EngineCore_DP0 pid=294009) [2026-01-25 18:43:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=294009) [2026-01-25 18:43:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=294009) [2026-01-25 18:43:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=294009) [2026-01-25 18:43:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=294009) [2026-01-25 18:43:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=294009) [2026-01-25 18:43:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=294009) [2026-01-25 18:43:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=294009) [2026-01-25 18:43:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=294009) Process EngineCore_DP0:
(EngineCore_DP0 pid=294009) Traceback (most recent call last):
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=294009)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=294009)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=294009)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=294009) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpp9qs5646.ptx', '-o', '/tmp/tmpp9qs5646.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=294009) 
(EngineCore_DP0 pid=294009) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=294009) 
(EngineCore_DP0 pid=294009) Traceback (most recent call last):
(EngineCore_DP0 pid=294009)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=294009)     self.run()
(EngineCore_DP0 pid=294009)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=294009)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=294009)     raise e
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=294009)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=294009)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=294009)     super().__init__(
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=294009)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=294009)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=294009)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=294009)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=294009)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=294009)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=294009)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=294009)     return func(*args, **kwargs)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=294009)     return func(*args, **kwargs)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=294009)     self.model_runner.profile_run()
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=294009)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=294009)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=294009)     return func(*args, **kwargs)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=294009)     outputs = self.model(
(EngineCore_DP0 pid=294009)               ^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294009)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294009)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=294009)     model_output = self.model(
(EngineCore_DP0 pid=294009)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=294009)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=294009)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=294009)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294009)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294009)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=294009)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=294009)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294009)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294009)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=294009)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=294009)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294009)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294009)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=294009)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=294009)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=294009)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=294009)     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=294009)     return self._linear_fn(
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=294009)     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=294009)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=294009)     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=294009)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=294009)     return fn(input, L)
(EngineCore_DP0 pid=294009)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=294009)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=294009)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=294009)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=294009)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=294009)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=294009)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=294009)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=294009)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=294009)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=294009)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=294009)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294009)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=294009)     raise PTXASError(error)
(EngineCore_DP0 pid=294009) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=294009) `ptxas` stderr:
(EngineCore_DP0 pid=294009) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=294009) 
(EngineCore_DP0 pid=294009) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpp9qs5646.ptx -o /tmp/tmpp9qs5646.ptx.o
(EngineCore_DP0 pid=294009) 
[rank0]:[W125 18:43:36.621558456 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=128

========== M=256 ==========
Time: 2026-01-25 18:43:37
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:43:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:43:41 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=294524) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=294524) 
(EngineCore_DP0 pid=294524) 
(EngineCore_DP0 pid=294524) ================================================================
(EngineCore_DP0 pid=294524) Internal Triton PTX codegen error
(EngineCore_DP0 pid=294524) `ptxas` stderr:
(EngineCore_DP0 pid=294524) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=294524) 
(EngineCore_DP0 pid=294524) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpkfdnpyjh.ptx -o /tmp/tmpkfdnpyjh.ptx.o
(EngineCore_DP0 pid=294524) 
(EngineCore_DP0 pid=294524) 
(EngineCore_DP0 pid=294524) //
(EngineCore_DP0 pid=294524) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=294524) //
(EngineCore_DP0 pid=294524) 
(EngineCore_DP0 pid=294524) .version 8.7
(EngineCore_DP0 pid=294524) .target sm_121a
(EngineCore_DP0 pid=294524) .address_size 64
(EngineCore_DP0 pid=294524) 
(EngineCore_DP0 pid=294524) 	// .globl	_quant_slide_int8_kernel // -- Begin function _quant_slide_int8_kernel
(EngineCore_DP0 pid=294524) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=294524)                                         // @_quant_slide_int8_kernel
(EngineCore_DP0 pid=294524) .visible .entry _quant_slide_int8_kernel(
(EngineCore_DP0 pid=294524) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_0,
(EngineCore_DP0 pid=294524) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_1,
(EngineCore_DP0 pid=294524) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_2,
(EngineCore_DP0 pid=294524) 	.param .u32 _quant_slide_int8_kernel_param_3,
(EngineCore_DP0 pid=294524) 	.param .u32 _quant_slide_int8_kernel_param_4,
(EngineCore_DP0 pid=294524) 	.param .u32 _quant_slide_int8_kernel_param_5,
(EngineCore_DP0 pid=294524) 	.param .u32 _quant_slide_int8_kernel_param_6,
(EngineCore_DP0 pid=294524) 	.param .u32 _quant_slide_int8_kernel_param_7,
(EngineCore_DP0 pid=294524) 	.param .u32 _quant_slide_int8_kernel_param_8,
(EngineCore_DP0 pid=294524) 	.param .u32 _quant_slide_int8_kernel_param_9,
(EngineCore_DP0 pid=294524) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_10,
(EngineCore_DP0 pid=294524) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_11
(EngineCore_DP0 pid=294524) )
(EngineCore_DP0 pid=294524) .reqntid 1024
(EngineCore_DP0 pid=294524) {
(EngineCore_DP0 pid=294524) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=294524) 	.reg .b16 	%rs<20>;
(EngineCore_DP0 pid=294524) 	.reg .b32 	%r<120>;
(EngineCore_DP0 pid=294524) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=294524) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=294524) $L__func_begin0:
(EngineCore_DP0 pid=294524) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=294524) 
(EngineCore_DP0 pid=294524) // %bb.0:
(EngineCore_DP0 pid=294524) 	ld.param.b32 	%r19, [_quant_slide_int8_kernel_param_7];
(EngineCore_DP0 pid=294524) 	ld.param.b32 	%r18, [_quant_slide_int8_kernel_param_5];
(EngineCore_DP0 pid=294524) 	ld.param.b32 	%r17, [_quant_slide_int8_kernel_param_4];
(EngineCore_DP0 pid=294524) 	ld.param.b64 	%rd3, [_quant_slide_int8_kernel_param_2];
(EngineCore_DP0 pid=294524) 	ld.param.b64 	%rd4, [_quant_slide_int8_kernel_param_0];
(EngineCore_DP0 pid=294524) $L__tmp0:
(EngineCore_DP0 pid=294524) 	.loc	1 288 24                        // quant_slide_tuned_Llama3.2-1B.py:288:24
(EngineCore_DP0 pid=294524) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=294524) 	ld.param.b32 	%r21, [_quant_slide_int8_kernel_param_8];
(EngineCore_DP0 pid=294524) 	.loc	1 293 26                        // quant_slide_tuned_Llama3.2-1B.py:293:26
(EngineCore_DP0 pid=294524) 	mul.lo.s32 	%r22, %r21, %r1;
(EngineCore_DP0 pid=294524) 	.loc	1 293 20                        // quant_slide_tuned_Llama3.2-1B.py:293:20
(EngineCore_DP0 pid=294524) 	mad.wide.s32 	%rd1, %r22, 2, %rd4;
(EngineCore_DP0 pid=294524) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=294524) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=294524) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=294524) 	setp.lt.s32 	%p1, %r18, 1;
(EngineCore_DP0 pid=294524) 	mov.b32 	%r118, 0f2B8CBCCC;
(EngineCore_DP0 pid=294524) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=294524) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=294524) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=294524) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=294524) 	shl.b32 	%r3, %r2, 2;
(EngineCore_DP0 pid=294524) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=294524) 	shr.u32 	%r31, %r2, 3;
(EngineCore_DP0 pid=294524) 	and.b32 	%r32, %r31, 124;
(EngineCore_DP0 pid=294524) 	mov.b32 	%r33, global_smem;
(EngineCore_DP0 pid=294524) 	add.s32 	%r39, %r33, %r32;
(EngineCore_DP0 pid=294524) 	add.s32 	%r42, %r33, %r3;
(EngineCore_DP0 pid=294524) 	mov.b32 	%r37, 0;
(EngineCore_DP0 pid=294524) 	mov.b32 	%r116, 0f00000000;
(EngineCore_DP0 pid=294524) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=294524) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=294524) 	mov.b32 	%r117, %r37;
(EngineCore_DP0 pid=294524) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=294524) 	.loc	1 300 22                        // quant_slide_tuned_Llama3.2-1B.py:300:22
(EngineCore_DP0 pid=294524) 	add.s32 	%r45, %r3, %r117;
(EngineCore_DP0 pid=294524) 	setp.lt.s32 	%p2, %r45, %r17;
(EngineCore_DP0 pid=294524) 	.loc	1 301 29                        // quant_slide_tuned_Llama3.2-1B.py:301:29
(EngineCore_DP0 pid=294524) 	mad.wide.s32 	%rd6, %r45, 2, %rd1;
(EngineCore_DP0 pid=294524) 	.loc	1 301 21                        // quant_slide_tuned_Llama3.2-1B.py:301:21
(EngineCore_DP0 pid=294524) 	// begin inline asm
(EngineCore_DP0 pid=294524) 	mov.u32 %r35, %r37;
(EngineCore_DP0 pid=294524) 	mov.u32 %r36, %r37;
(EngineCore_DP0 pid=294524) 	@%p2 ld.global.v2.b32 { %r35, %r36 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=294524) 	// end inline asm
(EngineCore_DP0 pid=294524) 	mov.b32 	{%rs1, %rs2}, %r35;
(EngineCore_DP0 pid=294524) 	mov.b32 	{%rs3, %rs4}, %r36;
(EngineCore_DP0 pid=294524) 	.loc	1 302 50                        // quant_slide_tuned_Llama3.2-1B.py:302:50
(EngineCore_DP0 pid=294524) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=294524) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=294524) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=294524) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=294524) $L__tmp1:
(EngineCore_DP0 pid=294524) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	bar.sync 	0;
(EngineCore_DP0 pid=294524) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=294524) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=294524) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=294524) 	cvt.f32.bf16 	%r46, %rs11;
(EngineCore_DP0 pid=294524) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	shfl.sync.bfly.b32 	%r47, %r46, 16, 31, -1;
(EngineCore_DP0 pid=294524) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	max.f32 	%r48, %r46, %r47;
(EngineCore_DP0 pid=294524) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	shfl.sync.bfly.b32 	%r49, %r48, 8, 31, -1;
(EngineCore_DP0 pid=294524) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	max.f32 	%r50, %r48, %r49;
(EngineCore_DP0 pid=294524) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	shfl.sync.bfly.b32 	%r51, %r50, 4, 31, -1;
(EngineCore_DP0 pid=294524) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	max.f32 	%r52, %r50, %r51;
(EngineCore_DP0 pid=294524) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	shfl.sync.bfly.b32 	%r53, %r52, 2, 31, -1;
(EngineCore_DP0 pid=294524) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	max.f32 	%r54, %r52, %r53;
(EngineCore_DP0 pid=294524) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	shfl.sync.bfly.b32 	%r55, %r54, 1, 31, -1;
(EngineCore_DP0 pid=294524) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	max.f32 	%r40, %r54, %r55;
(EngineCore_DP0 pid=294524) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	// begin inline asm
(EngineCore_DP0 pid=294524) 	@%p3 st.shared.b32 [ %r39 + 0 ], %r40;
(EngineCore_DP0 pid=294524) 	// end inline asm
(EngineCore_DP0 pid=294524) 	bar.sync 	0;
(EngineCore_DP0 pid=294524) 	// begin inline asm
(EngineCore_DP0 pid=294524) 	@%p4 ld.shared.b32 %r41, [ %r42 + 0 ];
(EngineCore_DP0 pid=294524) 	// end inline asm
(EngineCore_DP0 pid=294524) 	shfl.sync.bfly.b32 	%r56, %r41, 16, 31, -1;
(EngineCore_DP0 pid=294524) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	max.f32 	%r57, %r41, %r56;
(EngineCore_DP0 pid=294524) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	shfl.sync.bfly.b32 	%r58, %r57, 8, 31, -1;
(EngineCore_DP0 pid=294524) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	max.f32 	%r59, %r57, %r58;
(EngineCore_DP0 pid=294524) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	shfl.sync.bfly.b32 	%r60, %r59, 4, 31, -1;
(EngineCore_DP0 pid=294524) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	max.f32 	%r61, %r59, %r60;
(EngineCore_DP0 pid=294524) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	shfl.sync.bfly.b32 	%r62, %r61, 2, 31, -1;
(EngineCore_DP0 pid=294524) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	max.f32 	%r63, %r61, %r62;
(EngineCore_DP0 pid=294524) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	shfl.sync.bfly.b32 	%r64, %r63, 1, 31, -1;
(EngineCore_DP0 pid=294524) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	max.f32 	%r44, %r63, %r64;
(EngineCore_DP0 pid=294524) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=294524) 	// begin inline asm
(EngineCore_DP0 pid=294524) 	@%p19 st.shared.b32 [ %r42 + 0 ], %r44;
(EngineCore_DP0 pid=294524) 	// end inline asm
(EngineCore_DP0 pid=294524) 	bar.sync 	0;
(EngineCore_DP0 pid=294524) 	ld.shared.b32 	%r65, [global_smem];
(EngineCore_DP0 pid=294524) $L__tmp2:
(EngineCore_DP0 pid=294524) 	.loc	1 302 36                        // quant_slide_tuned_Llama3.2-1B.py:302:36
(EngineCore_DP0 pid=294524) 	max.f32 	%r116, %r116, %r65;
(EngineCore_DP0 pid=294524) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=294524) 	add.s32 	%r117, %r117, 4096;
(EngineCore_DP0 pid=294524) 	setp.lt.s32 	%p6, %r117, %r18;
(EngineCore_DP0 pid=294524) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=294524) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=294524) 	.loc	1 304 32                        // quant_slide_tuned_Llama3.2-1B.py:304:32
(EngineCore_DP0 pid=294524) 	max.f32 	%r118, %r116, 0f2B8CBCCC;
(EngineCore_DP0 pid=294524) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=294524) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=294524) 	mov.b32 	%r67, 0f42FE0000;
(EngineCore_DP0 pid=294524) 	.loc	1 305 32                        // quant_slide_tuned_Llama3.2-1B.py:305:32
(EngineCore_DP0 pid=294524) 	div.full.f32 	%r68, %r118, %r67;
(EngineCore_DP0 pid=294524) 	.loc	1 305 42                        // quant_slide_tuned_Llama3.2-1B.py:305:42
(EngineCore_DP0 pid=294524) 	max.f32 	%r66, %r68, 0f37810204;
(EngineCore_DP0 pid=294524) 	.loc	1 307 25                        // quant_slide_tuned_Llama3.2-1B.py:307:25
(EngineCore_DP0 pid=294524) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=294524) 	.loc	1 307 30                        // quant_slide_tuned_Llama3.2-1B.py:307:30
(EngineCore_DP0 pid=294524) 	// begin inline asm
(EngineCore_DP0 pid=294524) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r66 };
(EngineCore_DP0 pid=294524) 	// end inline asm
(EngineCore_DP0 pid=294524) 	.loc	1 310 29                        // quant_slide_tuned_Llama3.2-1B.py:310:29
(EngineCore_DP0 pid=294524) 	shl.b32 	%r14, %r19, 2;
(EngineCore_DP0 pid=294524) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=294524) 	setp.lt.s32 	%p8, %r14, 1;
(EngineCore_DP0 pid=294524) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=294524) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=294524) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=294524) 	ld.param.b32 	%r23, [_quant_slide_int8_kernel_param_9];
(EngineCore_DP0 pid=294524) 	shr.s32 	%r24, %r23, 31;
(EngineCore_DP0 pid=294524) 	shr.u32 	%r25, %r24, 30;
(EngineCore_DP0 pid=294524) 	add.s32 	%r26, %r23, %r25;
(EngineCore_DP0 pid=294524) 	shr.s32 	%r27, %r26, 2;
(EngineCore_DP0 pid=294524) 	ld.param.b64 	%rd5, [_quant_slide_int8_kernel_param_1];
(EngineCore_DP0 pid=294524) 	mul.lo.s32 	%r28, %r27, %r1;
(EngineCore_DP0 pid=294524) 	mad.wide.s32 	%rd2, %r28, 4, %rd5;
(EngineCore_DP0 pid=294524) 	div.full.f32 	%r13, %r67, %r118;
(EngineCore_DP0 pid=294524) 	mov.b32 	%r119, 0;
(EngineCore_DP0 pid=294524) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=294524)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=294524) 	.loc	1 314 30                        // quant_slide_tuned_Llama3.2-1B.py:314:30
(EngineCore_DP0 pid=294524) 	add.s32 	%r72, %r2, %r119;
(EngineCore_DP0 pid=294524) 	setp.lt.s32 	%p13, %r72, %r14;
(EngineCore_DP0 pid=294524) 	.loc	1 317 24                        // quant_slide_tuned_Llama3.2-1B.py:317:24
(EngineCore_DP0 pid=294524) 	shr.s32 	%r73, %r72, 31;
(EngineCore_DP0 pid=294524) 	shr.u32 	%r74, %r73, 30;
(EngineCore_DP0 pid=294524) 	add.s32 	%r75, %r72, %r74;
(EngineCore_DP0 pid=294524) 	shr.s32 	%r76, %r75, 2;
(EngineCore_DP0 pid=294524) 	.loc	1 318 23                        // quant_slide_tuned_Llama3.2-1B.py:318:23
(EngineCore_DP0 pid=294524) 	and.b32 	%r77, %r75, 2147483644;
(EngineCore_DP0 pid=294524) 	sub.s32 	%r78, %r72, %r77;
(EngineCore_DP0 pid=294524) 	.loc	1 319 30                        // quant_slide_tuned_Llama3.2-1B.py:319:30
(EngineCore_DP0 pid=294524) 	shl.b32 	%r79, %r78, 1;
(EngineCore_DP0 pid=294524) 	.loc	1 319 26                        // quant_slide_tuned_Llama3.2-1B.py:319:26
(EngineCore_DP0 pid=294524) 	mad.lo.s32 	%r80, %r76, 10, %r79;
(EngineCore_DP0 pid=294524) 	.loc	1 322 53                        // quant_slide_tuned_Llama3.2-1B.py:322:53
(EngineCore_DP0 pid=294524) 	setp.lt.s32 	%p14, %r80, %r17;
(EngineCore_DP0 pid=294524) 	.loc	1 322 37                        // quant_slide_tuned_Llama3.2-1B.py:322:37
(EngineCore_DP0 pid=294524) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=294524) 	.loc	1 321 29                        // quant_slide_tuned_Llama3.2-1B.py:321:29
(EngineCore_DP0 pid=294524) 	mad.wide.s32 	%rd8, %r80, 2, %rd1;
(EngineCore_DP0 pid=294524) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=294524) 	.loc	1 321 21                        // quant_slide_tuned_Llama3.2-1B.py:321:21
(EngineCore_DP0 pid=294524) 	// begin inline asm
(EngineCore_DP0 pid=294524) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=294524) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=294524) 	// end inline asm
(EngineCore_DP0 pid=294524) 	.loc	1 322 79                        // quant_slide_tuned_Llama3.2-1B.py:322:79
(EngineCore_DP0 pid=294524) 	cvt.f32.bf16 	%r81, %rs12;
(EngineCore_DP0 pid=294524) 	.loc	1 324 48                        // quant_slide_tuned_Llama3.2-1B.py:324:48
(EngineCore_DP0 pid=294524) 	or.b32 	%r82, %r80, 1;
(EngineCore_DP0 pid=294524) 	.loc	1 324 53                        // quant_slide_tuned_Llama3.2-1B.py:324:53
(EngineCore_DP0 pid=294524) 	setp.lt.s32 	%p15, %r82, %r17;
(EngineCore_DP0 pid=294524) 	.loc	1 324 37                        // quant_slide_tuned_Llama3.2-1B.py:324:37
(EngineCore_DP0 pid=294524) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=294524) 	.loc	1 323 39                        // quant_slide_tuned_Llama3.2-1B.py:323:39
(EngineCore_DP0 pid=294524) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=294524) 	.loc	1 323 21                        // quant_slide_tuned_Llama3.2-1B.py:323:21
(EngineCore_DP0 pid=294524) 	// begin inline asm
(EngineCore_DP0 pid=294524) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=294524) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=294524) 	// end inline asm
(EngineCore_DP0 pid=294524) 	.loc	1 324 79                        // quant_slide_tuned_Llama3.2-1B.py:324:79
(EngineCore_DP0 pid=294524) 	cvt.f32.bf16 	%r83, %rs14;
(EngineCore_DP0 pid=294524) 	.loc	1 326 48                        // quant_slide_tuned_Llama3.2-1B.py:326:48
(EngineCore_DP0 pid=294524) 	add.s32 	%r84, %r80, 2;
(EngineCore_DP0 pid=294524) 	.loc	1 326 53                        // quant_slide_tuned_Llama3.2-1B.py:326:53
(EngineCore_DP0 pid=294524) 	setp.lt.s32 	%p16, %r84, %r17;
(EngineCore_DP0 pid=294524) 	.loc	1 326 37                        // quant_slide_tuned_Llama3.2-1B.py:326:37
(EngineCore_DP0 pid=294524) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=294524) 	.loc	1 325 39                        // quant_slide_tuned_Llama3.2-1B.py:325:39
(EngineCore_DP0 pid=294524) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=294524) 	.loc	1 325 21                        // quant_slide_tuned_Llama3.2-1B.py:325:21
(EngineCore_DP0 pid=294524) 	// begin inline asm
(EngineCore_DP0 pid=294524) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=294524) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=294524) 	// end inline asm
(EngineCore_DP0 pid=294524) 	.loc	1 326 79                        // quant_slide_tuned_Llama3.2-1B.py:326:79
(EngineCore_DP0 pid=294524) 	cvt.f32.bf16 	%r85, %rs16;
(EngineCore_DP0 pid=294524) 	.loc	1 328 48                        // quant_slide_tuned_Llama3.2-1B.py:328:48
(EngineCore_DP0 pid=294524) 	add.s32 	%r86, %r80, 3;
(EngineCore_DP0 pid=294524) 	.loc	1 328 53                        // quant_slide_tuned_Llama3.2-1B.py:328:53
(EngineCore_DP0 pid=294524) 	setp.lt.s32 	%p17, %r86, %r17;
(EngineCore_DP0 pid=294524) 	.loc	1 328 37                        // quant_slide_tuned_Llama3.2-1B.py:328:37
(EngineCore_DP0 pid=294524) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=294524) 	.loc	1 327 39                        // quant_slide_tuned_Llama3.2-1B.py:327:39
(EngineCore_DP0 pid=294524) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=294524) 	.loc	1 327 21                        // quant_slide_tuned_Llama3.2-1B.py:327:21
(EngineCore_DP0 pid=294524) 	// begin inline asm
(EngineCore_DP0 pid=294524) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=294524) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=294524) 	// end inline asm
(EngineCore_DP0 pid=294524) 	.loc	1 328 79                        // quant_slide_tuned_Llama3.2-1B.py:328:79
(EngineCore_DP0 pid=294524) 	cvt.f32.bf16 	%r87, %rs18;
(EngineCore_DP0 pid=294524) 	.loc	1 330 56                        // quant_slide_tuned_Llama3.2-1B.py:330:56
(EngineCore_DP0 pid=294524) 	mul.f32 	%r88, %r13, %r81;
(EngineCore_DP0 pid=294524) 	.loc	1 330 51                        // quant_slide_tuned_Llama3.2-1B.py:330:51
(EngineCore_DP0 pid=294524) 	cvt.rni.f32.f32 	%r89, %r88;
(EngineCore_DP0 pid=294524) 	.loc	1 330 76                        // quant_slide_tuned_Llama3.2-1B.py:330:76
(EngineCore_DP0 pid=294524) 	max.f32 	%r90, %r89, 0fC3000000;
(EngineCore_DP0 pid=294524) 	min.f32 	%r91, %r90, 0f42FE0000;
(EngineCore_DP0 pid=294524) 	.loc	1 330 86                        // quant_slide_tuned_Llama3.2-1B.py:330:86
(EngineCore_DP0 pid=294524) 	cvt.rzi.s32.f32 	%r92, %r91;
(EngineCore_DP0 pid=294524) 	.loc	1 330 98                        // quant_slide_tuned_Llama3.2-1B.py:330:98
(EngineCore_DP0 pid=294524) 	and.b32 	%r93, %r92, 255;
(EngineCore_DP0 pid=294524) 	.loc	1 331 56                        // quant_slide_tuned_Llama3.2-1B.py:331:56
(EngineCore_DP0 pid=294524) 	mul.f32 	%r94, %r13, %r83;
(EngineCore_DP0 pid=294524) 	.loc	1 331 51                        // quant_slide_tuned_Llama3.2-1B.py:331:51
(EngineCore_DP0 pid=294524) 	cvt.rni.f32.f32 	%r95, %r94;
(EngineCore_DP0 pid=294524) 	.loc	1 332 56                        // quant_slide_tuned_Llama3.2-1B.py:332:56
(EngineCore_DP0 pid=294524) 	mul.f32 	%r96, %r13, %r85;
(EngineCore_DP0 pid=294524) 	.loc	1 332 51                        // quant_slide_tuned_Llama3.2-1B.py:332:51
(EngineCore_DP0 pid=294524) 	cvt.rni.f32.f32 	%r97, %r96;
(EngineCore_DP0 pid=294524) 	.loc	1 333 56                        // quant_slide_tuned_Llama3.2-1B.py:333:56
(EngineCore_DP0 pid=294524) 	mul.f32 	%r98, %r13, %r87;
(EngineCore_DP0 pid=294524) 	.loc	1 333 51                        // quant_slide_tuned_Llama3.2-1B.py:333:51
(EngineCore_DP0 pid=294524) 	cvt.rni.f32.f32 	%r99, %r98;
(EngineCore_DP0 pid=294524) 	.loc	1 333 76                        // quant_slide_tuned_Llama3.2-1B.py:333:76
(EngineCore_DP0 pid=294524) 	max.f32 	%r100, %r99, 0fC3000000;
(EngineCore_DP0 pid=294524) 	min.f32 	%r101, %r100, 0f42FE0000;
(EngineCore_DP0 pid=294524) 	.loc	1 333 86                        // quant_slide_tuned_Llama3.2-1B.py:333:86
(EngineCore_DP0 pid=294524) 	cvt.rzi.s32.f32 	%r102, %r101;
(EngineCore_DP0 pid=294524) 	.loc	1 331 76                        // quant_slide_tuned_Llama3.2-1B.py:331:76
(EngineCore_DP0 pid=294524) 	max.f32 	%r103, %r97, 0fC3000000;
(EngineCore_DP0 pid=294524) 	max.f32 	%r104, %r95, 0fC3000000;
(EngineCore_DP0 pid=294524) 	min.f32 	%r105, %r104, 0f42FE0000;
(EngineCore_DP0 pid=294524) 	min.f32 	%r106, %r103, 0f42FE0000;
(EngineCore_DP0 pid=294524) 	.loc	1 331 86                        // quant_slide_tuned_Llama3.2-1B.py:331:86
(EngineCore_DP0 pid=294524) 	cvt.rzi.s32.f32 	%r107, %r106;
(EngineCore_DP0 pid=294524) 	cvt.rzi.s32.f32 	%r108, %r105;
(EngineCore_DP0 pid=294524) 	.loc	1 335 30                        // quant_slide_tuned_Llama3.2-1B.py:335:30
(EngineCore_DP0 pid=294524) 	shl.b32 	%r109, %r108, 8;
(EngineCore_DP0 pid=294524) 	shl.b32 	%r110, %r107, 16;
(EngineCore_DP0 pid=294524) 	and.b32 	%r111, %r110, 16711680;
(EngineCore_DP0 pid=294524) 	and.b32 	%r112, %r109, 65280;
(EngineCore_DP0 pid=294524) 	.loc	1 335 24                        // quant_slide_tuned_Llama3.2-1B.py:335:24
(EngineCore_DP0 pid=294524) 	or.b32 	%r113, %r112, %r93;
(EngineCore_DP0 pid=294524) 	.loc	1 335 36                        // quant_slide_tuned_Llama3.2-1B.py:335:36
(EngineCore_DP0 pid=294524) 	or.b32 	%r114, %r113, %r111;
(EngineCore_DP0 pid=294524) 	.loc	1 335 55                        // quant_slide_tuned_Llama3.2-1B.py:335:55
(EngineCore_DP0 pid=294524) 	shl.b32 	%r115, %r102, 24;
(EngineCore_DP0 pid=294524) 	.loc	1 335 49                        // quant_slide_tuned_Llama3.2-1B.py:335:49
(EngineCore_DP0 pid=294524) 	or.b32 	%r70, %r114, %r115;
(EngineCore_DP0 pid=294524) 	.loc	1 336 29                        // quant_slide_tuned_Llama3.2-1B.py:336:29
(EngineCore_DP0 pid=294524) 	mad.wide.s32 	%rd12, %r72, 4, %rd2;
(EngineCore_DP0 pid=294524) 	.loc	1 336 39                        // quant_slide_tuned_Llama3.2-1B.py:336:39
(EngineCore_DP0 pid=294524) 	// begin inline asm
(EngineCore_DP0 pid=294524) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r70 };
(EngineCore_DP0 pid=294524) 	// end inline asm
(EngineCore_DP0 pid=294524) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=294524) 	add.s32 	%r119, %r119, 1024;
(EngineCore_DP0 pid=294524) 	setp.lt.s32 	%p18, %r119, %r14;
(EngineCore_DP0 pid=294524) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=294524) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=294524) 	.loc	1 312 4                         // quant_slide_tuned_Llama3.2-1B.py:312:4
(EngineCore_DP0 pid=294524) 	ret;
(EngineCore_DP0 pid=294524) $L__tmp3:
(EngineCore_DP0 pid=294524) $L__func_end0:
(EngineCore_DP0 pid=294524)                                         // -- End function
(EngineCore_DP0 pid=294524) }
(EngineCore_DP0 pid=294524) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=294524) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=294524) 	.section	.debug_abbrev
(EngineCore_DP0 pid=294524) 	{
(EngineCore_DP0 pid=294524) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=294524) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=294524) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=294524) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=294524) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=294524) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=294524) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=294524) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=294524) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=294524) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=294524) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=294524) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=294524) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=294524) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=294524) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=294524) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=294524) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=294524) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=294524) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=294524) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=294524) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=294524) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=294524) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=294524) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=294524) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=294524) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=294524) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=294524) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=294524) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=294524) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=294524) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=294524) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=294524) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=294524) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=294524) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=294524) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=294524) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=294524) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=294524) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=294524) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=294524) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=294524) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=294524) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=294524) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=294524) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=294524) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=294524) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=294524) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=294524) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=294524) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=294524) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=294524) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=294524) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=294524) 	}
(EngineCore_DP0 pid=294524) 	.section	.debug_info
(EngineCore_DP0 pid=294524) 	{
(EngineCore_DP0 pid=294524) .b32 224                                // Length of Unit
(EngineCore_DP0 pid=294524) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=294524) .b8 0
(EngineCore_DP0 pid=294524) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=294524) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=294524) .b8 1                                   // Abbrev [1] 0xb:0xd9 DW_TAG_compile_unit
(EngineCore_DP0 pid=294524) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=294524) .b8 114
(EngineCore_DP0 pid=294524) .b8 105
(EngineCore_DP0 pid=294524) .b8 116
(EngineCore_DP0 pid=294524) .b8 111
(EngineCore_DP0 pid=294524) .b8 110
(EngineCore_DP0 pid=294524) .b8 0
(EngineCore_DP0 pid=294524) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=294524) .b8 0
(EngineCore_DP0 pid=294524) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=294524) .b8 117
(EngineCore_DP0 pid=294524) .b8 97
(EngineCore_DP0 pid=294524) .b8 110
(EngineCore_DP0 pid=294524) .b8 116
(EngineCore_DP0 pid=294524) .b8 95
(EngineCore_DP0 pid=294524) .b8 115
(EngineCore_DP0 pid=294524) .b8 108
(EngineCore_DP0 pid=294524) .b8 105
(EngineCore_DP0 pid=294524) .b8 100
(EngineCore_DP0 pid=294524) .b8 101
(EngineCore_DP0 pid=294524) .b8 95
(EngineCore_DP0 pid=294524) .b8 116
(EngineCore_DP0 pid=294524) .b8 117
(EngineCore_DP0 pid=294524) .b8 110
(EngineCore_DP0 pid=294524) .b8 101
(EngineCore_DP0 pid=294524) .b8 100
(EngineCore_DP0 pid=294524) .b8 95
(EngineCore_DP0 pid=294524) .b8 76
(EngineCore_DP0 pid=294524) .b8 108
(EngineCore_DP0 pid=294524) .b8 97
(EngineCore_DP0 pid=294524) .b8 109
(EngineCore_DP0 pid=294524) .b8 97
(EngineCore_DP0 pid=294524) .b8 51
(EngineCore_DP0 pid=294524) .b8 46
(EngineCore_DP0 pid=294524) .b8 50
(EngineCore_DP0 pid=294524) .b8 45
(EngineCore_DP0 pid=294524) .b8 49
(EngineCore_DP0 pid=294524) .b8 66
(EngineCore_DP0 pid=294524) .b8 46
(EngineCore_DP0 pid=294524) .b8 112
(EngineCore_DP0 pid=294524) .b8 121
(EngineCore_DP0 pid=294524) .b8 0
(EngineCore_DP0 pid=294524) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=294524) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=294524) .b8 114
(EngineCore_DP0 pid=294524) .b8 111
(EngineCore_DP0 pid=294524) .b8 111
(EngineCore_DP0 pid=294524) .b8 116
(EngineCore_DP0 pid=294524) .b8 47
(EngineCore_DP0 pid=294524) .b8 118
(EngineCore_DP0 pid=294524) .b8 108
(EngineCore_DP0 pid=294524) .b8 108
(EngineCore_DP0 pid=294524) .b8 109
(EngineCore_DP0 pid=294524) .b8 98
(EngineCore_DP0 pid=294524) .b8 101
(EngineCore_DP0 pid=294524) .b8 110
(EngineCore_DP0 pid=294524) .b8 99
(EngineCore_DP0 pid=294524) .b8 104
(EngineCore_DP0 pid=294524) .b8 47
(EngineCore_DP0 pid=294524) .b8 115
(EngineCore_DP0 pid=294524) .b8 108
(EngineCore_DP0 pid=294524) .b8 105
(EngineCore_DP0 pid=294524) .b8 100
(EngineCore_DP0 pid=294524) .b8 101
(EngineCore_DP0 pid=294524) .b8 115
(EngineCore_DP0 pid=294524) .b8 112
(EngineCore_DP0 pid=294524) .b8 97
(EngineCore_DP0 pid=294524) .b8 114
(EngineCore_DP0 pid=294524) .b8 115
(EngineCore_DP0 pid=294524) .b8 101
(EngineCore_DP0 pid=294524) .b8 47
(EngineCore_DP0 pid=294524) .b8 99
(EngineCore_DP0 pid=294524) .b8 115
(EngineCore_DP0 pid=294524) .b8 114
(EngineCore_DP0 pid=294524) .b8 99
(EngineCore_DP0 pid=294524) .b8 47
(EngineCore_DP0 pid=294524) .b8 102
(EngineCore_DP0 pid=294524) .b8 117
(EngineCore_DP0 pid=294524) .b8 115
(EngineCore_DP0 pid=294524) .b8 101
(EngineCore_DP0 pid=294524) .b8 100
(EngineCore_DP0 pid=294524) .b8 95
(EngineCore_DP0 pid=294524) .b8 113
(EngineCore_DP0 pid=294524) .b8 117
(EngineCore_DP0 pid=294524) .b8 97
(EngineCore_DP0 pid=294524) .b8 110
(EngineCore_DP0 pid=294524) .b8 116
(EngineCore_DP0 pid=294524) .b8 95
(EngineCore_DP0 pid=294524) .b8 115
(EngineCore_DP0 pid=294524) .b8 108
(EngineCore_DP0 pid=294524) .b8 105
(EngineCore_DP0 pid=294524) .b8 100
(EngineCore_DP0 pid=294524) .b8 101
(EngineCore_DP0 pid=294524) .b8 95
(EngineCore_DP0 pid=294524) .b8 116
(EngineCore_DP0 pid=294524) .b8 114
(EngineCore_DP0 pid=294524) .b8 105
(EngineCore_DP0 pid=294524) .b8 116
(EngineCore_DP0 pid=294524) .b8 111
(EngineCore_DP0 pid=294524) .b8 110
(EngineCore_DP0 pid=294524) .b8 47
(EngineCore_DP0 pid=294524) .b8 98
(EngineCore_DP0 pid=294524) .b8 117
(EngineCore_DP0 pid=294524) .b8 105
(EngineCore_DP0 pid=294524) .b8 108
(EngineCore_DP0 pid=294524) .b8 100
(EngineCore_DP0 pid=294524) .b8 47
(EngineCore_DP0 pid=294524) .b8 71
(EngineCore_DP0 pid=294524) .b8 66
(EngineCore_DP0 pid=294524) .b8 49
(EngineCore_DP0 pid=294524) .b8 48
(EngineCore_DP0 pid=294524) .b8 95
(EngineCore_DP0 pid=294524) .b8 99
(EngineCore_DP0 pid=294524) .b8 99
(EngineCore_DP0 pid=294524) .b8 49
(EngineCore_DP0 pid=294524) .b8 50
(EngineCore_DP0 pid=294524) .b8 49
(EngineCore_DP0 pid=294524) .b8 95
(EngineCore_DP0 pid=294524) .b8 112
(EngineCore_DP0 pid=294524) .b8 121
(EngineCore_DP0 pid=294524) .b8 51
(EngineCore_DP0 pid=294524) .b8 49
(EngineCore_DP0 pid=294524) .b8 50
(EngineCore_DP0 pid=294524) .b8 95
(EngineCore_DP0 pid=294524) .b8 99
(EngineCore_DP0 pid=294524) .b8 117
(EngineCore_DP0 pid=294524) .b8 49
(EngineCore_DP0 pid=294524) .b8 50
(EngineCore_DP0 pid=294524) .b8 57
(EngineCore_DP0 pid=294524) .b8 95
(EngineCore_DP0 pid=294524) .b8 97
(EngineCore_DP0 pid=294524) .b8 97
(EngineCore_DP0 pid=294524) .b8 114
(EngineCore_DP0 pid=294524) .b8 99
(EngineCore_DP0 pid=294524) .b8 104
(EngineCore_DP0 pid=294524) .b8 54
(EngineCore_DP0 pid=294524) .b8 52
(EngineCore_DP0 pid=294524) .b8 0
(EngineCore_DP0 pid=294524) .b8 2                                   // Abbrev [2] 0x99:0x1b DW_TAG_subprogram
(EngineCore_DP0 pid=294524) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=294524) .b8 113
(EngineCore_DP0 pid=294524) .b8 117
(EngineCore_DP0 pid=294524) .b8 97
(EngineCore_DP0 pid=294524) .b8 110
(EngineCore_DP0 pid=294524) .b8 116
(EngineCore_DP0 pid=294524) .b8 95
(EngineCore_DP0 pid=294524) .b8 115
(EngineCore_DP0 pid=294524) .b8 108
(EngineCore_DP0 pid=294524) .b8 105
(EngineCore_DP0 pid=294524) .b8 100
(EngineCore_DP0 pid=294524) .b8 101
(EngineCore_DP0 pid=294524) .b8 95
(EngineCore_DP0 pid=294524) .b8 105
(EngineCore_DP0 pid=294524) .b8 110
(EngineCore_DP0 pid=294524) .b8 116
(EngineCore_DP0 pid=294524) .b8 56
(EngineCore_DP0 pid=294524) .b8 95
(EngineCore_DP0 pid=294524) .b8 107
(EngineCore_DP0 pid=294524) .b8 101
(EngineCore_DP0 pid=294524) .b8 114
(EngineCore_DP0 pid=294524) .b8 110
(EngineCore_DP0 pid=294524) .b8 101
(EngineCore_DP0 pid=294524) .b8 108
(EngineCore_DP0 pid=294524) .b8 0
(EngineCore_DP0 pid=294524) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=294524) .b8 3                                   // Abbrev [3] 0xb4:0x2f DW_TAG_subprogram
(EngineCore_DP0 pid=294524) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=294524) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=294524) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=294524) .b8 4                                   // Abbrev [4] 0xc9:0x19 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=294524) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=294524) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=294524) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=294524) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=294524) .b8 46                                  // DW_AT_call_line
(EngineCore_DP0 pid=294524) .b8 1
(EngineCore_DP0 pid=294524) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=294524) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=294524) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=294524) 	}
(EngineCore_DP0 pid=294524) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=294524) 
(EngineCore_DP0 pid=294524) ================================================================
(EngineCore_DP0 pid=294524) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=294524) 
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpkfdnpyjh.ptx', '-o', '/tmp/tmpkfdnpyjh.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866] 
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866] 
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866] 
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpkfdnpyjh.ptx -o /tmp/tmpkfdnpyjh.ptx.o
(EngineCore_DP0 pid=294524) ERROR 01-25 18:43:58 [core.py:866] 

STDERR:
[2026-01-25 18:43:41] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:43:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:43:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:43:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:43:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:43:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:43:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:43:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:43:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:43:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:43:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:43:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:43:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:43:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:43:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:43:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:43:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:43:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:43:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=294524) [2026-01-25 18:43:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=294524) [2026-01-25 18:43:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=294524) [2026-01-25 18:43:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=294524) [2026-01-25 18:43:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=294524) [2026-01-25 18:43:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=294524) [2026-01-25 18:43:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=294524) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=294524) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.84s/it]
(EngineCore_DP0 pid=294524) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.84s/it]
(EngineCore_DP0 pid=294524) 
(EngineCore_DP0 pid=294524) [2026-01-25 18:43:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=294524) [2026-01-25 18:43:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=294524) [2026-01-25 18:43:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=294524) [2026-01-25 18:43:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=294524) [2026-01-25 18:43:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=294524) [2026-01-25 18:43:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=294524) [2026-01-25 18:43:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=294524) [2026-01-25 18:43:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=294524) Process EngineCore_DP0:
(EngineCore_DP0 pid=294524) Traceback (most recent call last):
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=294524)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=294524)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=294524)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=294524) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpkfdnpyjh.ptx', '-o', '/tmp/tmpkfdnpyjh.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=294524) 
(EngineCore_DP0 pid=294524) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=294524) 
(EngineCore_DP0 pid=294524) Traceback (most recent call last):
(EngineCore_DP0 pid=294524)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=294524)     self.run()
(EngineCore_DP0 pid=294524)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=294524)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=294524)     raise e
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=294524)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=294524)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=294524)     super().__init__(
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=294524)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=294524)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=294524)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=294524)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=294524)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=294524)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=294524)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=294524)     return func(*args, **kwargs)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=294524)     return func(*args, **kwargs)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=294524)     self.model_runner.profile_run()
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=294524)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=294524)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=294524)     return func(*args, **kwargs)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=294524)     outputs = self.model(
(EngineCore_DP0 pid=294524)               ^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294524)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294524)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=294524)     model_output = self.model(
(EngineCore_DP0 pid=294524)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=294524)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=294524)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=294524)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294524)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294524)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=294524)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=294524)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294524)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294524)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=294524)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=294524)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=294524)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=294524)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=294524)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=294524)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=294524)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=294524)     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=294524)     return self._linear_fn(
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=294524)     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=294524)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=294524)     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=294524)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=294524)     return fn(input, L)
(EngineCore_DP0 pid=294524)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=294524)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=294524)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=294524)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=294524)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=294524)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=294524)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=294524)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=294524)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=294524)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=294524)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=294524)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=294524)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=294524)     raise PTXASError(error)
(EngineCore_DP0 pid=294524) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=294524) `ptxas` stderr:
(EngineCore_DP0 pid=294524) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=294524) 
(EngineCore_DP0 pid=294524) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpkfdnpyjh.ptx -o /tmp/tmpkfdnpyjh.ptx.o
(EngineCore_DP0 pid=294524) 
[rank0]:[W125 18:43:58.732316787 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=256

========== M=16 ==========
Time: 2026-01-26 02:31:15
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-INT8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:31:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:31:19 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=746470) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=746470) WARNING 01-26 02:31:40 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.06 requests/s, 1376.49 total tokens/s, 1295.52 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096

STDERR:
[2026-01-26 02:31:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:31:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:31:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:31:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:31:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:31:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:31:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:31:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:31:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:31:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:31:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:31:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:31:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:31:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=746470) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=746470) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.86s/it]
(EngineCore_DP0 pid=746470) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.86s/it]
(EngineCore_DP0 pid=746470) 
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=746470) 2026-01-26 02:31:40,236 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=746470) 2026-01-26 02:31:40,243 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|| 16/16 [00:00<00:00, 8315.84it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|         | 1/16 [00:03<00:47,  3.14s/it, est. speed input: 5.09 toks/s, output: 81.47 toks/s]
Processed prompts: 100%|| 16/16 [00:03<00:00,  3.14s/it, est. speed input: 81.04 toks/s, output: 1296.63 toks/s]
Processed prompts: 100%|| 16/16 [00:03<00:00,  5.06it/s, est. speed input: 81.04 toks/s, output: 1296.63 toks/s]
[rank0]:[W126 02:31:44.315266429 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 02:31:46
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:31:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:31:49 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=747091) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=747091) WARNING 01-26 02:32:10 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 22.36 requests/s, 6081.47 total tokens/s, 5723.74 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 02:31:49] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:31:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:31:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:31:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:31:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:31:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:31:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:31:53] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:31:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:31:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:31:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:31:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:31:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:31:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=747091) [2026-01-26 02:31:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=747091) [2026-01-26 02:31:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=747091) [2026-01-26 02:31:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=747091) [2026-01-26 02:31:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=747091) [2026-01-26 02:31:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=747091) [2026-01-26 02:31:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=747091) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=747091) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.67s/it]
(EngineCore_DP0 pid=747091) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.67s/it]
(EngineCore_DP0 pid=747091) 
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=747091) 2026-01-26 02:32:10,419 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=747091) 2026-01-26 02:32:10,427 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 13277.05it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:05<11:20,  5.36s/it, est. speed input: 2.98 toks/s, output: 47.76 toks/s]
Processed prompts:   2%|         | 3/128 [00:05<02:59,  1.43s/it, est. speed input: 8.76 toks/s, output: 140.17 toks/s]
Processed prompts:  48%|     | 62/128 [00:05<00:03, 21.56it/s, est. speed input: 177.34 toks/s, output: 2837.46 toks/s]
Processed prompts:  92%|| 118/128 [00:05<00:00, 47.25it/s, est. speed input: 330.99 toks/s, output: 5295.77 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 47.25it/s, est. speed input: 358.39 toks/s, output: 5734.26 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 22.40it/s, est. speed input: 358.39 toks/s, output: 5734.26 toks/s]
[rank0]:[W126 02:32:16.075848243 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 02:32:19
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:32:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:32:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=747742) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=747742) WARNING 01-26 02:32:43 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 28.18 requests/s, 7665.91 total tokens/s, 7214.98 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 02:32:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:32:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:32:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:32:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:32:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:32:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:32:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:32:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:32:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:32:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:32:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:32:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:32:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:32:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:32:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:32:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:32:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:32:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:26] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:26] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:26] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:26] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:26] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=747742) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=747742) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.69s/it]
(EngineCore_DP0 pid=747742) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.69s/it]
(EngineCore_DP0 pid=747742) 
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=747742) 2026-01-26 02:32:43,132 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=747742) 2026-01-26 02:32:43,140 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 13455.92it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<33:36,  7.91s/it, est. speed input: 2.02 toks/s, output: 32.37 toks/s]
Processed prompts:  14%|        | 36/256 [00:08<00:34,  6.31it/s, est. speed input: 71.69 toks/s, output: 1147.10 toks/s]
Processed prompts:  30%|       | 76/256 [00:08<00:11, 15.95it/s, est. speed input: 149.27 toks/s, output: 2388.29 toks/s]
Processed prompts:  43%|     | 109/256 [00:08<00:05, 26.57it/s, est. speed input: 211.46 toks/s, output: 3383.40 toks/s]
Processed prompts:  54%|    | 138/256 [00:08<00:03, 38.71it/s, est. speed input: 264.49 toks/s, output: 4231.78 toks/s]
Processed prompts:  65%|   | 167/256 [00:08<00:01, 54.06it/s, est. speed input: 316.08 toks/s, output: 5057.29 toks/s]
Processed prompts:  77%|  | 196/256 [00:08<00:00, 72.81it/s, est. speed input: 366.39 toks/s, output: 5862.19 toks/s]
Processed prompts:  88%| | 224/256 [00:08<00:00, 91.12it/s, est. speed input: 412.34 toks/s, output: 6597.42 toks/s]
Processed prompts:  98%|| 250/256 [00:08<00:00, 95.96it/s, est. speed input: 448.06 toks/s, output: 7168.95 toks/s]
Processed prompts: 100%|| 256/256 [00:09<00:00, 95.96it/s, est. speed input: 451.93 toks/s, output: 7230.84 toks/s]
Processed prompts: 100%|| 256/256 [00:09<00:00, 28.24it/s, est. speed input: 451.93 toks/s, output: 7230.84 toks/s]
[rank0]:[W126 02:32:53.189851079 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 16:58:30
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:58:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:58:34 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2838510) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2838510) WARNING 01-27 16:58:56 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.78 requests/s, 4019.97 total tokens/s, 3783.50 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 16:58:34] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:58:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:58:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:58:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:58:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:58:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:58:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:58:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:58:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:58:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:58:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:58:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:58:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:58:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:58:37] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:58:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:58:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:58:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:58:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:58:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:58:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:58:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:58:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:58:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:58:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:58:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:58:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:58:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2838510) [2026-01-27 16:58:38] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2838510) [2026-01-27 16:58:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2838510) [2026-01-27 16:58:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2838510) [2026-01-27 16:58:38] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2838510) [2026-01-27 16:58:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2838510) [2026-01-27 16:58:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2838510) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2838510) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.66s/it]
(EngineCore_DP0 pid=2838510) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.66s/it]
(EngineCore_DP0 pid=2838510) 
(EngineCore_DP0 pid=2838510) [2026-01-27 16:58:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=2838510) [2026-01-27 16:58:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=2838510) [2026-01-27 16:58:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=2838510) [2026-01-27 16:58:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=2838510) [2026-01-27 16:58:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=2838510) [2026-01-27 16:58:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=2838510) [2026-01-27 16:58:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=2838510) [2026-01-27 16:58:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=2838510) 2026-01-27 16:58:55,482 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2838510) 2026-01-27 16:58:55,499 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 3240.29it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:04<04:23,  4.19s/it, est. speed input: 3.82 toks/s, output: 61.17 toks/s]
Processed prompts: 100%|| 64/64 [00:04<00:00,  4.19s/it, est. speed input: 239.01 toks/s, output: 3824.08 toks/s]
Processed prompts: 100%|| 64/64 [00:04<00:00, 14.94it/s, est. speed input: 239.01 toks/s, output: 3824.08 toks/s]
[rank0]:[W127 16:59:00.605797599 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 16:59:03
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:59:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:59:07 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2839168) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2839168) WARNING 01-27 16:59:28 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 23.05 requests/s, 6270.75 total tokens/s, 5901.88 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 16:59:07] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:59:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:59:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:59:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:59:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:59:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:59:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:59:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:59:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:59:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:59:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:59:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:59:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:59:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:59:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:59:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:59:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:59:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2839168) [2026-01-27 16:59:11] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2839168) [2026-01-27 16:59:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2839168) [2026-01-27 16:59:11] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2839168) [2026-01-27 16:59:11] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2839168) [2026-01-27 16:59:11] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2839168) [2026-01-27 16:59:11] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2839168) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2839168) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.78s/it]
(EngineCore_DP0 pid=2839168) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.78s/it]
(EngineCore_DP0 pid=2839168) 
(EngineCore_DP0 pid=2839168) [2026-01-27 16:59:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=2839168) [2026-01-27 16:59:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=2839168) [2026-01-27 16:59:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=2839168) [2026-01-27 16:59:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=2839168) [2026-01-27 16:59:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=2839168) [2026-01-27 16:59:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=2839168) [2026-01-27 16:59:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=2839168) [2026-01-27 16:59:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=2839168) 2026-01-27 16:59:28,336 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2839168) 2026-01-27 16:59:28,343 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 13676.15it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:05<11:12,  5.29s/it, est. speed input: 3.02 toks/s, output: 48.36 toks/s]
Processed prompts:  40%|      | 51/128 [00:05<00:05, 13.29it/s, est. speed input: 151.00 toks/s, output: 2415.94 toks/s]
Processed prompts:  86%| | 110/128 [00:05<00:00, 33.96it/s, est. speed input: 319.10 toks/s, output: 5105.59 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 33.96it/s, est. speed input: 369.54 toks/s, output: 5912.71 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 23.10it/s, est. speed input: 369.54 toks/s, output: 5912.71 toks/s]
[rank0]:[W127 16:59:34.678543074 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 16:59:36
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:59:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:59:40 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2839824) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2839824) WARNING 01-27 17:00:02 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 27.96 requests/s, 7605.95 total tokens/s, 7158.54 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 16:59:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:59:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:59:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:59:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:59:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:59:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:59:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:59:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:59:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:59:43] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:59:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:59:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:59:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:59:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:59:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:59:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:59:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:59:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:59:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2839824) [2026-01-27 16:59:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2839824) [2026-01-27 16:59:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2839824) [2026-01-27 16:59:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2839824) [2026-01-27 16:59:44] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2839824) [2026-01-27 16:59:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2839824) [2026-01-27 16:59:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2839824) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2839824) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.89s/it]
(EngineCore_DP0 pid=2839824) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.89s/it]
(EngineCore_DP0 pid=2839824) 
(EngineCore_DP0 pid=2839824) [2026-01-27 16:59:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=2839824) [2026-01-27 16:59:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=2839824) [2026-01-27 16:59:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=2839824) [2026-01-27 16:59:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=2839824) [2026-01-27 16:59:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=2839824) [2026-01-27 16:59:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=2839824) [2026-01-27 16:59:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=2839824) [2026-01-27 16:59:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=2839824) 2026-01-27 17:00:01,716 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2839824) 2026-01-27 17:00:01,724 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 13508.91it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<33:34,  7.90s/it, est. speed input: 2.03 toks/s, output: 32.40 toks/s]
Processed prompts:  13%|        | 34/256 [00:08<00:37,  5.95it/s, est. speed input: 67.68 toks/s, output: 1082.94 toks/s]
Processed prompts:  29%|       | 75/256 [00:08<00:11, 15.83it/s, est. speed input: 147.23 toks/s, output: 2355.60 toks/s]
Processed prompts:  42%|     | 108/256 [00:08<00:05, 26.42it/s, est. speed input: 209.32 toks/s, output: 3349.16 toks/s]
Processed prompts:  53%|    | 136/256 [00:08<00:03, 37.75it/s, est. speed input: 259.84 toks/s, output: 4157.37 toks/s]
Processed prompts:  65%|   | 166/256 [00:08<00:01, 53.56it/s, est. speed input: 313.14 toks/s, output: 5010.26 toks/s]
Processed prompts:  75%|  | 193/256 [00:08<00:00, 70.05it/s, est. speed input: 359.20 toks/s, output: 5747.22 toks/s]
Processed prompts:  86%| | 219/256 [00:08<00:00, 85.97it/s, est. speed input: 401.13 toks/s, output: 6418.05 toks/s]
Processed prompts:  95%|| 243/256 [00:08<00:00, 98.49it/s, est. speed input: 437.45 toks/s, output: 6999.11 toks/s]
Processed prompts: 100%|| 256/256 [00:09<00:00, 98.49it/s, est. speed input: 448.38 toks/s, output: 7174.07 toks/s]
Processed prompts: 100%|| 256/256 [00:09<00:00, 28.02it/s, est. speed input: 448.38 toks/s, output: 7174.07 toks/s]
[rank0]:[W127 17:00:11.702666481 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:00:14
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-1B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:00:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:00:17 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2840532) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2840532) WARNING 01-27 17:00:39 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 29.18 requests/s, 7938.19 total tokens/s, 7471.23 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:00:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:00:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 17:00:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 17:00:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 17:00:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 17:00:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 17:00:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 17:00:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 17:00:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 17:00:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:00:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:00:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:00:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:00:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:00:20] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:00:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 17:00:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 17:00:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 17:00:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 17:00:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 17:00:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 17:00:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 17:00:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 17:00:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:00:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:00:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:00:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:00:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2840532) [2026-01-27 17:00:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2840532) [2026-01-27 17:00:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2840532) [2026-01-27 17:00:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2840532) [2026-01-27 17:00:21] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2840532) [2026-01-27 17:00:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2840532) [2026-01-27 17:00:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2840532) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2840532) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.57s/it]
(EngineCore_DP0 pid=2840532) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.57s/it]
(EngineCore_DP0 pid=2840532) 
(EngineCore_DP0 pid=2840532) [2026-01-27 17:00:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=2840532) [2026-01-27 17:00:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=2840532) [2026-01-27 17:00:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=2840532) [2026-01-27 17:00:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=2840532) [2026-01-27 17:00:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=2840532) [2026-01-27 17:00:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=2840532) [2026-01-27 17:00:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=2840532) [2026-01-27 17:00:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=2840532) 2026-01-27 17:00:38,432 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2840532) 2026-01-27 17:00:38,449 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 13847.68it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:13<1:53:12, 13.29s/it, est. speed input: 1.20 toks/s, output: 19.26 toks/s]
Processed prompts:   0%|          | 2/512 [00:13<47:04,  5.54s/it, est. speed input: 2.39 toks/s, output: 38.20 toks/s]  
Processed prompts:  12%|        | 63/512 [00:13<00:48,  9.17it/s, est. speed input: 74.16 toks/s, output: 1186.57 toks/s]
Processed prompts:  23%|       | 117/512 [00:13<00:19, 19.82it/s, est. speed input: 135.98 toks/s, output: 2175.64 toks/s]
Processed prompts:  32%|      | 164/512 [00:13<00:10, 32.36it/s, est. speed input: 188.99 toks/s, output: 3023.78 toks/s]
Processed prompts:  40%|      | 205/512 [00:13<00:06, 46.86it/s, est. speed input: 234.49 toks/s, output: 3751.89 toks/s]
Processed prompts:  50%|     | 258/512 [00:14<00:03, 70.25it/s, est. speed input: 292.00 toks/s, output: 4671.99 toks/s]
Processed prompts:  59%|    | 302/512 [00:14<00:02, 93.68it/s, est. speed input: 338.79 toks/s, output: 5420.70 toks/s]
Processed prompts:  66%|   | 338/512 [00:14<00:01, 114.05it/s, est. speed input: 375.81 toks/s, output: 6013.02 toks/s]
Processed prompts:  73%|  | 376/512 [00:14<00:00, 139.47it/s, est. speed input: 414.55 toks/s, output: 6632.80 toks/s]
Processed prompts:  80%|  | 409/512 [00:14<00:00, 161.80it/s, est. speed input: 447.47 toks/s, output: 7159.47 toks/s]
Processed prompts:  86%| | 441/512 [00:14<00:00, 176.26it/s, est. speed input: 478.00 toks/s, output: 7647.98 toks/s]
Processed prompts:  92%|| 470/512 [00:14<00:00, 167.69it/s, est. speed input: 502.71 toks/s, output: 8043.42 toks/s]
Processed prompts:  97%|| 495/512 [00:15<00:00, 142.53it/s, est. speed input: 520.46 toks/s, output: 8327.31 toks/s]
Processed prompts: 100%|| 512/512 [00:17<00:00, 142.53it/s, est. speed input: 467.97 toks/s, output: 7487.46 toks/s]
Processed prompts: 100%|| 512/512 [00:17<00:00, 29.25it/s, est. speed input: 467.97 toks/s, output: 7487.46 toks/s] 
[rank0]:[W127 17:00:56.833563717 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:29:12
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-3B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:29:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:29:16 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2871768) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2871768) WARNING 01-27 17:29:57 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 6.85 requests/s, 1864.50 total tokens/s, 1754.83 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:29:16] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:29:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:29:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:29:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:29:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:29:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:29:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:29:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:29:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:29:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:29:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:29:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:29:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:29:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:29:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:29:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:29:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:29:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:29:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:29:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:29:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:29:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:29:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:29:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:29:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:29:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:29:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:29:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2871768) [2026-01-27 17:29:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2871768) [2026-01-27 17:29:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2871768) [2026-01-27 17:29:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2871768) [2026-01-27 17:29:20] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2871768) [2026-01-27 17:29:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2871768) [2026-01-27 17:29:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2871768) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2871768) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:29<00:00, 29.69s/it]
(EngineCore_DP0 pid=2871768) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:29<00:00, 29.69s/it]
(EngineCore_DP0 pid=2871768) 
(EngineCore_DP0 pid=2871768) [2026-01-27 17:29:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=2871768) [2026-01-27 17:29:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=2871768) [2026-01-27 17:29:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=2871768) [2026-01-27 17:29:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=2871768) [2026-01-27 17:29:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=2871768) [2026-01-27 17:29:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=2871768) [2026-01-27 17:29:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=2871768) [2026-01-27 17:29:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=2871768) 2026-01-27 17:29:57,185 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2871768) 2026-01-27 17:29:57,199 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 12090.05it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:09<09:34,  9.12s/it, est. speed input: 1.75 toks/s, output: 28.06 toks/s]
Processed prompts:  33%|      | 21/64 [00:09<00:13,  3.19it/s, est. speed input: 36.43 toks/s, output: 582.83 toks/s]
Processed prompts:  98%|| 63/64 [00:09<00:00, 12.16it/s, est. speed input: 108.04 toks/s, output: 1728.57 toks/s]
Processed prompts: 100%|| 64/64 [00:09<00:00, 12.16it/s, est. speed input: 109.75 toks/s, output: 1755.98 toks/s]
Processed prompts: 100%|| 64/64 [00:09<00:00,  6.86it/s, est. speed input: 109.75 toks/s, output: 1755.98 toks/s]
[rank0]:[W127 17:30:07.276004222 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:30:09
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-3B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:30:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:30:13 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2872737) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2872737) WARNING 01-27 17:30:52 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.81 requests/s, 2939.37 total tokens/s, 2766.46 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:30:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:30:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:30:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:30:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:30:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:30:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:30:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:30:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:30:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:30:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:30:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:30:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:30:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:30:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:30:16] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:30:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:30:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:30:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:30:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:30:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:30:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:30:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:30:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:30:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:30:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:30:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:30:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:30:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2872737) [2026-01-27 17:30:17] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2872737) [2026-01-27 17:30:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2872737) [2026-01-27 17:30:17] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2872737) [2026-01-27 17:30:17] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2872737) [2026-01-27 17:30:17] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2872737) [2026-01-27 17:30:17] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2872737) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2872737) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:28<00:00, 28.24s/it]
(EngineCore_DP0 pid=2872737) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:28<00:00, 28.24s/it]
(EngineCore_DP0 pid=2872737) 
(EngineCore_DP0 pid=2872737) [2026-01-27 17:30:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=2872737) [2026-01-27 17:30:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=2872737) [2026-01-27 17:30:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=2872737) [2026-01-27 17:30:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=2872737) [2026-01-27 17:30:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=2872737) [2026-01-27 17:30:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=2872737) [2026-01-27 17:30:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=2872737) [2026-01-27 17:30:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=2872737) 2026-01-27 17:30:52,261 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2872737) 2026-01-27 17:30:52,271 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 4308.44it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:11<23:57, 11.32s/it, est. speed input: 1.41 toks/s, output: 22.61 toks/s]
Processed prompts:  15%|        | 19/128 [00:11<00:46,  2.33it/s, est. speed input: 26.58 toks/s, output: 425.36 toks/s]
Processed prompts:  38%|      | 48/128 [00:11<00:10,  7.35it/s, est. speed input: 66.57 toks/s, output: 1065.13 toks/s]
Processed prompts:  59%|    | 75/128 [00:11<00:03, 13.72it/s, est. speed input: 103.07 toks/s, output: 1649.07 toks/s]
Processed prompts:  84%| | 108/128 [00:11<00:00, 24.28it/s, est. speed input: 147.08 toks/s, output: 2353.28 toks/s]
Processed prompts: 100%|| 128/128 [00:11<00:00, 24.28it/s, est. speed input: 173.35 toks/s, output: 2773.68 toks/s]
Processed prompts: 100%|| 128/128 [00:11<00:00, 10.83it/s, est. speed input: 173.35 toks/s, output: 2773.68 toks/s]
[rank0]:[W127 17:31:04.011473274 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:31:07
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-3B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:31:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:31:10 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2873743) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2873743) WARNING 01-27 17:31:50 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 13.94 requests/s, 3792.35 total tokens/s, 3569.27 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:31:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:31:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:31:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:31:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:31:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:31:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:31:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:31:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:31:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:31:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:31:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:31:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:31:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:31:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:31:14] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:31:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:31:14] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:31:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:31:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:31:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:31:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:31:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:31:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:31:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:31:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:31:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:31:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:31:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2873743) [2026-01-27 17:31:15] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2873743) [2026-01-27 17:31:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2873743) [2026-01-27 17:31:15] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2873743) [2026-01-27 17:31:15] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2873743) [2026-01-27 17:31:15] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2873743) [2026-01-27 17:31:15] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2873743) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2873743) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:28<00:00, 28.69s/it]
(EngineCore_DP0 pid=2873743) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:28<00:00, 28.69s/it]
(EngineCore_DP0 pid=2873743) 
(EngineCore_DP0 pid=2873743) [2026-01-27 17:31:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=2873743) [2026-01-27 17:31:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=2873743) [2026-01-27 17:31:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=2873743) [2026-01-27 17:31:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=2873743) [2026-01-27 17:31:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=2873743) [2026-01-27 17:31:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=2873743) [2026-01-27 17:31:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=2873743) [2026-01-27 17:31:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=2873743) 2026-01-27 17:31:50,294 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2873743) 2026-01-27 17:31:50,304 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 4817.30it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:16<1:08:05, 16.02s/it, est. speed input: 1.00 toks/s, output: 15.98 toks/s]
Processed prompts:   7%|         | 18/256 [00:16<02:33,  1.55it/s, est. speed input: 17.78 toks/s, output: 284.54 toks/s]
Processed prompts:  19%|        | 48/256 [00:16<00:39,  5.22it/s, est. speed input: 46.97 toks/s, output: 751.59 toks/s]
Processed prompts:  29%|       | 75/256 [00:16<00:18,  9.70it/s, est. speed input: 72.71 toks/s, output: 1163.42 toks/s]
Processed prompts:  38%|      | 98/256 [00:16<00:10, 14.80it/s, est. speed input: 94.19 toks/s, output: 1507.03 toks/s]
Processed prompts:  46%|     | 118/256 [00:16<00:06, 20.70it/s, est. speed input: 112.58 toks/s, output: 1801.33 toks/s]
Processed prompts:  53%|    | 136/256 [00:16<00:04, 27.52it/s, est. speed input: 128.85 toks/s, output: 2061.60 toks/s]
Processed prompts:  59%|    | 152/256 [00:16<00:02, 35.18it/s, est. speed input: 143.10 toks/s, output: 2289.56 toks/s]
Processed prompts:  66%|   | 168/256 [00:17<00:01, 44.70it/s, est. speed input: 157.20 toks/s, output: 2515.13 toks/s]
Processed prompts:  72%|  | 184/256 [00:17<00:01, 53.94it/s, est. speed input: 170.76 toks/s, output: 2732.09 toks/s]
Processed prompts:  78%|  | 199/256 [00:17<00:00, 63.27it/s, est. speed input: 183.31 toks/s, output: 2932.92 toks/s]
Processed prompts:  83%| | 213/256 [00:17<00:00, 68.65it/s, est. speed input: 194.45 toks/s, output: 3111.20 toks/s]
Processed prompts:  88%| | 226/256 [00:17<00:00, 73.32it/s, est. speed input: 204.64 toks/s, output: 3274.29 toks/s]
Processed prompts:  93%|| 238/256 [00:17<00:00, 70.91it/s, est. speed input: 213.27 toks/s, output: 3412.36 toks/s]
Processed prompts:  97%|| 248/256 [00:18<00:00, 62.31it/s, est. speed input: 219.48 toks/s, output: 3511.67 toks/s]
Processed prompts: 100%|| 256/256 [00:18<00:00, 62.31it/s, est. speed input: 223.74 toks/s, output: 3579.86 toks/s]
Processed prompts: 100%|| 256/256 [00:18<00:00, 13.98it/s, est. speed input: 223.74 toks/s, output: 3579.86 toks/s]
[rank0]:[W127 17:32:09.424651323 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:32:11
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Llama3.2-3B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:32:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:32:15 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2874811) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2874811) WARNING 01-27 17:32:55 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.39 requests/s, 3913.47 total tokens/s, 3683.27 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:32:15] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:32:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:32:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:32:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:32:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:32:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:32:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:32:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:32:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:32:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:32:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:32:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:32:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:32:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:32:18] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:32:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:32:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:32:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:32:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:32:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:32:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:32:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:32:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:32:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:32:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:32:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:32:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:32:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2874811) [2026-01-27 17:32:19] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2874811) [2026-01-27 17:32:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2874811) [2026-01-27 17:32:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2874811) [2026-01-27 17:32:19] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2874811) [2026-01-27 17:32:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2874811) [2026-01-27 17:32:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2874811) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2874811) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:28<00:00, 28.58s/it]
(EngineCore_DP0 pid=2874811) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:28<00:00, 28.58s/it]
(EngineCore_DP0 pid=2874811) 
(EngineCore_DP0 pid=2874811) [2026-01-27 17:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=2874811) [2026-01-27 17:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=2874811) [2026-01-27 17:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=2874811) [2026-01-27 17:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=2874811) [2026-01-27 17:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=2874811) [2026-01-27 17:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=2874811) [2026-01-27 17:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=2874811) [2026-01-27 17:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=2874811) 2026-01-27 17:32:54,602 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2874811) 2026-01-27 17:32:54,612 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 14268.33it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:26<3:42:47, 26.16s/it, est. speed input: 0.61 toks/s, output: 9.79 toks/s]
Processed prompts:   0%|          | 2/512 [00:26<1:32:50, 10.92s/it, est. speed input: 1.21 toks/s, output: 19.38 toks/s]
Processed prompts:   6%|         | 33/512 [00:26<03:16,  2.43it/s, est. speed input: 19.81 toks/s, output: 316.92 toks/s]
Processed prompts:  12%|        | 63/512 [00:26<01:21,  5.53it/s, est. speed input: 37.48 toks/s, output: 599.75 toks/s]
Processed prompts:  18%|        | 91/512 [00:27<00:44,  9.43it/s, est. speed input: 53.71 toks/s, output: 859.43 toks/s]
Processed prompts:  23%|       | 117/512 [00:27<00:27, 14.20it/s, est. speed input: 68.51 toks/s, output: 1096.12 toks/s]
Processed prompts:  28%|       | 141/512 [00:27<00:18, 20.36it/s, est. speed input: 82.22 toks/s, output: 1315.52 toks/s]
Processed prompts:  32%|      | 164/512 [00:27<00:12, 28.18it/s, est. speed input: 95.25 toks/s, output: 1523.93 toks/s]
Processed prompts:  36%|      | 185/512 [00:27<00:08, 37.43it/s, est. speed input: 107.04 toks/s, output: 1712.71 toks/s]
Processed prompts:  40%|      | 205/512 [00:27<00:06, 48.42it/s, est. speed input: 118.18 toks/s, output: 1890.91 toks/s]
Processed prompts:  44%|     | 224/512 [00:27<00:04, 60.68it/s, est. speed input: 128.65 toks/s, output: 2058.39 toks/s]
Processed prompts:  50%|     | 258/512 [00:28<00:03, 83.74it/s, est. speed input: 147.19 toks/s, output: 2355.01 toks/s]
Processed prompts:  56%|    | 288/512 [00:28<00:02, 103.20it/s, est. speed input: 163.37 toks/s, output: 2613.85 toks/s]
Processed prompts:  62%|   | 315/512 [00:28<00:01, 117.49it/s, est. speed input: 177.69 toks/s, output: 2843.02 toks/s]
Processed prompts:  66%|   | 338/512 [00:28<00:01, 125.74it/s, est. speed input: 189.67 toks/s, output: 3034.73 toks/s]
Processed prompts:  70%|   | 358/512 [00:28<00:01, 132.12it/s, est. speed input: 199.99 toks/s, output: 3199.87 toks/s]
Processed prompts:  73%|  | 376/512 [00:28<00:01, 135.83it/s, est. speed input: 209.16 toks/s, output: 3346.61 toks/s]
Processed prompts:  77%|  | 393/512 [00:28<00:00, 137.23it/s, est. speed input: 217.71 toks/s, output: 3483.39 toks/s]
Processed prompts:  80%|  | 409/512 [00:28<00:00, 140.50it/s, est. speed input: 225.75 toks/s, output: 3611.98 toks/s]
Processed prompts:  83%| | 425/512 [00:29<00:00, 131.16it/s, est. speed input: 233.42 toks/s, output: 3734.64 toks/s]
Processed prompts:  86%| | 440/512 [00:29<00:00, 124.95it/s, est. speed input: 240.53 toks/s, output: 3848.46 toks/s]
Processed prompts:  89%| | 454/512 [00:29<00:00, 110.90it/s, est. speed input: 246.77 toks/s, output: 3948.37 toks/s]
Processed prompts:  91%| | 466/512 [00:29<00:00, 100.54it/s, est. speed input: 251.97 toks/s, output: 4031.55 toks/s]
Processed prompts:  93%|| 477/512 [00:29<00:00, 89.81it/s, est. speed input: 256.50 toks/s, output: 4103.96 toks/s] 
Processed prompts:  95%|| 487/512 [00:30<00:00, 68.06it/s, est. speed input: 259.59 toks/s, output: 4153.50 toks/s]
Processed prompts:  97%|| 495/512 [00:30<00:00, 56.81it/s, est. speed input: 261.88 toks/s, output: 4190.08 toks/s]
Processed prompts:  98%|| 502/512 [00:35<00:01,  5.82it/s, est. speed input: 226.27 toks/s, output: 3620.32 toks/s]
Processed prompts: 100%|| 512/512 [00:35<00:00,  5.82it/s, est. speed input: 230.44 toks/s, output: 3687.09 toks/s]
Processed prompts: 100%|| 512/512 [00:35<00:00, 14.40it/s, est. speed input: 230.44 toks/s, output: 3687.09 toks/s]
[rank0]:[W127 17:33:31.036089456 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 18:17:26
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:17:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:17:33 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2919555) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2919555) WARNING 01-27 18:18:59 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.51 requests/s, 1226.16 total tokens/s, 1154.03 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 18:17:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:17:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:17:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:17:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:17:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:17:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:17:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:17:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:17:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:17:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:17:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:17:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:17:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:17:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:17:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:17:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:17:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:17:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:17:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:17:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:17:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:17:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:17:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:17:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:17:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:17:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:17:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:17:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2919555) [2026-01-27 18:17:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2919555) [2026-01-27 18:17:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2919555) [2026-01-27 18:17:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2919555) [2026-01-27 18:17:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2919555) [2026-01-27 18:17:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2919555) [2026-01-27 18:17:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2919555) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2919555) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.93s/it]
(EngineCore_DP0 pid=2919555) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:04<00:00, 33.25s/it]
(EngineCore_DP0 pid=2919555) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:04<00:00, 32.46s/it]
(EngineCore_DP0 pid=2919555) 
(EngineCore_DP0 pid=2919555) [2026-01-27 18:18:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=2919555) [2026-01-27 18:18:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=2919555) [2026-01-27 18:18:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=2919555) [2026-01-27 18:18:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=2919555) [2026-01-27 18:18:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=2919555) [2026-01-27 18:18:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=2919555) [2026-01-27 18:18:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=2919555) [2026-01-27 18:18:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=2919555) 2026-01-27 18:18:53,139 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2919555) 2026-01-27 18:18:53,384 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|         | 1/64 [00:00<00:09,  6.62it/s]
Adding requests:   6%|         | 4/64 [00:00<00:03, 15.73it/s]
Adding requests:  14%|        | 9/64 [00:00<00:02, 25.83it/s]
Adding requests:  27%|       | 17/64 [00:00<00:01, 42.70it/s]
Adding requests:  52%|    | 33/64 [00:00<00:00, 79.02it/s]
Adding requests:  84%| | 54/64 [00:00<00:00, 118.82it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 84.71it/s] 

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:13<13:43, 13.08s/it, est. speed input: 1.22 toks/s, output: 19.58 toks/s]
Processed prompts:  12%|        | 8/64 [00:13<01:07,  1.20s/it, est. speed input: 9.71 toks/s, output: 155.37 toks/s]
Processed prompts:  61%|    | 39/64 [00:13<00:04,  5.51it/s, est. speed input: 46.91 toks/s, output: 750.48 toks/s]
Processed prompts: 100%|| 64/64 [00:13<00:00,  5.51it/s, est. speed input: 76.70 toks/s, output: 1227.28 toks/s]
Processed prompts: 100%|| 64/64 [00:13<00:00,  4.79it/s, est. speed input: 76.70 toks/s, output: 1227.28 toks/s]
[rank0]:[W127 18:19:14.875476676 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 18:19:31
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:19:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:19:36 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2921455) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2921455) WARNING 01-27 18:20:54 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.69 requests/s, 2090.98 total tokens/s, 1967.98 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 18:19:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:19:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:19:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:19:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:19:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:19:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:19:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:19:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:19:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:19:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:19:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:19:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:19:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:19:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:19:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:19:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:19:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:19:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:19:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:19:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:19:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:19:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:19:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:19:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:19:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:19:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:19:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:19:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2921455) [2026-01-27 18:19:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2921455) [2026-01-27 18:19:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2921455) [2026-01-27 18:19:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2921455) [2026-01-27 18:19:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2921455) [2026-01-27 18:19:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2921455) [2026-01-27 18:19:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2921455) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2921455) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.54s/it]
(EngineCore_DP0 pid=2921455) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:03<00:00, 32.62s/it]
(EngineCore_DP0 pid=2921455) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:03<00:00, 31.86s/it]
(EngineCore_DP0 pid=2921455) 
(EngineCore_DP0 pid=2921455) [2026-01-27 18:20:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=2921455) [2026-01-27 18:20:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=2921455) [2026-01-27 18:20:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=2921455) [2026-01-27 18:20:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=2921455) [2026-01-27 18:20:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=2921455) [2026-01-27 18:20:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=2921455) [2026-01-27 18:20:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=2921455) [2026-01-27 18:20:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=2921455) 2026-01-27 18:20:53,101 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2921455) 2026-01-27 18:20:53,114 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 3886.62it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:15<33:45, 15.95s/it, est. speed input: 1.00 toks/s, output: 16.05 toks/s]
Processed prompts:  14%|        | 18/128 [00:16<01:10,  1.56it/s, est. speed input: 17.89 toks/s, output: 286.22 toks/s]
Processed prompts:  38%|      | 48/128 [00:16<00:15,  5.27it/s, est. speed input: 47.33 toks/s, output: 757.22 toks/s]
Processed prompts:  59%|    | 75/128 [00:16<00:05,  9.82it/s, est. speed input: 73.32 toks/s, output: 1173.14 toks/s]
Processed prompts:  77%|  | 98/128 [00:16<00:01, 15.06it/s, est. speed input: 95.11 toks/s, output: 1521.75 toks/s]
Processed prompts:  99%|| 127/128 [00:16<00:00, 23.96it/s, est. speed input: 122.29 toks/s, output: 1956.71 toks/s]
Processed prompts: 100%|| 128/128 [00:16<00:00, 23.96it/s, est. speed input: 123.26 toks/s, output: 1972.10 toks/s]
Processed prompts: 100%|| 128/128 [00:16<00:00,  7.70it/s, est. speed input: 123.26 toks/s, output: 1972.10 toks/s]
[rank0]:[W127 18:21:10.973200041 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 18:21:13
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:21:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:21:19 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2923052) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2923052) WARNING 01-27 18:22:37 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.19 requests/s, 2771.43 total tokens/s, 2608.40 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 18:21:18] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:21:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:21:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:21:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:21:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:21:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:21:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:21:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:21:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:21:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:21:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:21:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:21:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:21:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:21:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:21:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:21:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:21:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:21:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:21:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:21:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:21:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:21:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:21:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:21:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:21:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:21:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:21:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2923052) [2026-01-27 18:21:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2923052) [2026-01-27 18:21:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2923052) [2026-01-27 18:21:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2923052) [2026-01-27 18:21:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2923052) [2026-01-27 18:21:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2923052) [2026-01-27 18:21:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2923052) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2923052) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:28<00:28, 28.12s/it]
(EngineCore_DP0 pid=2923052) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:05<00:00, 33.30s/it]
(EngineCore_DP0 pid=2923052) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:05<00:00, 32.53s/it]
(EngineCore_DP0 pid=2923052) 
(EngineCore_DP0 pid=2923052) [2026-01-27 18:22:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=2923052) [2026-01-27 18:22:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=2923052) [2026-01-27 18:22:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=2923052) [2026-01-27 18:22:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=2923052) [2026-01-27 18:22:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=2923052) [2026-01-27 18:22:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=2923052) [2026-01-27 18:22:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=2923052) [2026-01-27 18:22:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=2923052) 2026-01-27 18:22:36,649 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2923052) 2026-01-27 18:22:36,662 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 11254.57it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:21<1:33:22, 21.97s/it, est. speed input: 0.73 toks/s, output: 11.65 toks/s]
Processed prompts:   1%|          | 2/256 [00:22<38:33,  9.11s/it, est. speed input: 1.45 toks/s, output: 23.19 toks/s]  
Processed prompts:   7%|         | 18/256 [00:22<02:31,  1.57it/s, est. speed input: 12.98 toks/s, output: 207.76 toks/s]
Processed prompts:  13%|        | 33/256 [00:22<01:04,  3.48it/s, est. speed input: 23.69 toks/s, output: 379.08 toks/s]
Processed prompts:  24%|       | 62/256 [00:22<00:22,  8.54it/s, est. speed input: 44.16 toks/s, output: 706.60 toks/s]
Processed prompts:  34%|      | 87/256 [00:22<00:11, 14.24it/s, est. speed input: 61.47 toks/s, output: 983.57 toks/s]
Processed prompts:  42%|     | 108/256 [00:22<00:07, 20.43it/s, est. speed input: 75.79 toks/s, output: 1212.58 toks/s]
Processed prompts:  50%|     | 127/256 [00:22<00:04, 27.38it/s, est. speed input: 88.52 toks/s, output: 1416.31 toks/s]
Processed prompts:  56%|    | 144/256 [00:23<00:03, 34.72it/s, est. speed input: 99.71 toks/s, output: 1595.31 toks/s]
Processed prompts:  62%|   | 159/256 [00:23<00:02, 42.30it/s, est. speed input: 109.45 toks/s, output: 1751.21 toks/s]
Processed prompts:  67%|   | 172/256 [00:23<00:01, 49.20it/s, est. speed input: 117.74 toks/s, output: 1883.80 toks/s]
Processed prompts:  72%|  | 184/256 [00:23<00:01, 55.85it/s, est. speed input: 125.28 toks/s, output: 2004.53 toks/s]
Processed prompts:  76%|  | 195/256 [00:23<00:00, 61.73it/s, est. speed input: 132.10 toks/s, output: 2113.63 toks/s]
Processed prompts:  80%|  | 206/256 [00:23<00:00, 61.93it/s, est. speed input: 138.52 toks/s, output: 2216.35 toks/s]
Processed prompts:  84%| | 216/256 [00:23<00:00, 61.64it/s, est. speed input: 144.25 toks/s, output: 2307.98 toks/s]
Processed prompts:  88%| | 225/256 [00:24<00:00, 60.53it/s, est. speed input: 149.28 toks/s, output: 2388.50 toks/s]
Processed prompts:  91%| | 233/256 [00:24<00:00, 59.48it/s, est. speed input: 153.68 toks/s, output: 2458.94 toks/s]
Processed prompts:  94%|| 240/256 [00:24<00:00, 53.03it/s, est. speed input: 157.13 toks/s, output: 2514.10 toks/s]
Processed prompts:  96%|| 246/256 [00:24<00:00, 44.21it/s, est. speed input: 159.64 toks/s, output: 2554.31 toks/s]
Processed prompts:  98%|| 252/256 [00:24<00:00, 36.66it/s, est. speed input: 161.87 toks/s, output: 2589.99 toks/s]
Processed prompts: 100%|| 256/256 [00:25<00:00, 36.66it/s, est. speed input: 163.18 toks/s, output: 2610.87 toks/s]
Processed prompts: 100%|| 256/256 [00:25<00:00, 10.20it/s, est. speed input: 163.18 toks/s, output: 2610.87 toks/s]
[rank0]:[W127 18:23:02.956508309 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 18:23:05
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-7B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:23:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:23:10 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2924790) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2924790) WARNING 01-27 18:24:36 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 9.86 requests/s, 2682.75 total tokens/s, 2524.94 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 18:23:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:23:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:23:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:23:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:23:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:23:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:23:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:23:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:23:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:23:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:23:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:23:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:23:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:23:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:23:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:23:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:23:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:23:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:23:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:23:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:23:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:23:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:23:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:23:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:23:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:23:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:23:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:23:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2924790) [2026-01-27 18:23:15] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2924790) [2026-01-27 18:23:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2924790) [2026-01-27 18:23:15] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2924790) [2026-01-27 18:23:15] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2924790) [2026-01-27 18:23:15] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2924790) [2026-01-27 18:23:15] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2924790) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2924790) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:28<00:28, 28.56s/it]
(EngineCore_DP0 pid=2924790) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:05<00:00, 33.76s/it]
(EngineCore_DP0 pid=2924790) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:05<00:00, 32.98s/it]
(EngineCore_DP0 pid=2924790) 
(EngineCore_DP0 pid=2924790) [2026-01-27 18:24:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=2924790) [2026-01-27 18:24:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=2924790) [2026-01-27 18:24:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=2924790) [2026-01-27 18:24:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=2924790) [2026-01-27 18:24:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=2924790) [2026-01-27 18:24:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=2924790) [2026-01-27 18:24:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=2924790) [2026-01-27 18:24:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=2924790) 2026-01-27 18:24:29,225 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2924790) 2026-01-27 18:24:29,238 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/512 [00:00<01:53,  4.50it/s]
Adding requests:   1%|          | 3/512 [00:00<00:55,  9.20it/s]
Adding requests:   1%|          | 6/512 [00:00<00:33, 15.25it/s]
Adding requests:   2%|         | 9/512 [00:00<00:25, 19.65it/s]
Adding requests:   3%|         | 13/512 [00:00<00:19, 25.70it/s]
Adding requests:   4%|         | 20/512 [00:00<00:12, 38.07it/s]
Adding requests:   6%|         | 30/512 [00:00<00:08, 55.51it/s]
Adding requests:   8%|         | 40/512 [00:00<00:06, 68.56it/s]
Adding requests:  11%|         | 55/512 [00:01<00:04, 91.68it/s]
Adding requests:  14%|        | 73/512 [00:01<00:03, 115.91it/s]
Adding requests:  18%|        | 93/512 [00:01<00:02, 139.74it/s]
Adding requests:  22%|       | 112/512 [00:01<00:02, 154.39it/s]
Adding requests:  27%|       | 136/512 [00:01<00:02, 177.27it/s]
Adding requests:  31%|       | 159/512 [00:01<00:01, 191.62it/s]
Adding requests:  35%|      | 181/512 [00:01<00:01, 198.30it/s]
Adding requests:  39%|      | 201/512 [00:01<00:01, 193.45it/s]
Adding requests:  45%|     | 232/512 [00:01<00:01, 226.70it/s]
Adding requests:  52%|    | 265/512 [00:02<00:00, 256.24it/s]
Adding requests:  62%|   | 319/512 [00:02<00:00, 339.64it/s]
Adding requests:  77%|  | 395/512 [00:02<00:00, 462.28it/s]
Adding requests:  98%|| 501/512 [00:02<00:00, 638.45it/s]
Adding requests: 100%|| 512/512 [00:02<00:00, 219.18it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:32<4:35:37, 32.36s/it, est. speed input: 0.49 toks/s, output: 7.91 toks/s]
Processed prompts:   0%|          | 2/512 [00:32<1:54:55, 13.52s/it, est. speed input: 0.98 toks/s, output: 15.66 toks/s]
Processed prompts:   1%|          | 4/512 [00:33<43:36,  5.15s/it, est. speed input: 1.94 toks/s, output: 31.00 toks/s]  
Processed prompts:   1%|          | 6/512 [00:33<23:32,  2.79s/it, est. speed input: 2.88 toks/s, output: 46.06 toks/s]
Processed prompts:   1%|         | 7/512 [00:33<18:16,  2.17s/it, est. speed input: 3.33 toks/s, output: 53.20 toks/s]
Processed prompts:   2%|         | 9/512 [00:34<11:09,  1.33s/it, est. speed input: 4.23 toks/s, output: 67.75 toks/s]
Processed prompts:   2%|         | 11/512 [00:34<07:28,  1.12it/s, est. speed input: 5.12 toks/s, output: 81.99 toks/s]
Processed prompts:   2%|         | 12/512 [00:34<06:27,  1.29it/s, est. speed input: 5.54 toks/s, output: 88.61 toks/s]
Processed prompts:   3%|         | 14/512 [00:35<04:31,  1.83it/s, est. speed input: 6.40 toks/s, output: 102.39 toks/s]
Processed prompts:   3%|         | 17/512 [00:35<02:53,  2.85it/s, est. speed input: 7.70 toks/s, output: 123.18 toks/s]
Processed prompts:   4%|         | 21/512 [00:35<01:51,  4.40it/s, est. speed input: 9.42 toks/s, output: 150.77 toks/s]
Processed prompts:   5%|         | 27/512 [00:35<01:08,  7.07it/s, est. speed input: 12.01 toks/s, output: 192.09 toks/s]
Processed prompts:   7%|         | 34/512 [00:36<00:47, 10.09it/s, est. speed input: 14.98 toks/s, output: 239.66 toks/s]
Processed prompts:   8%|         | 41/512 [00:36<00:36, 12.95it/s, est. speed input: 17.91 toks/s, output: 286.60 toks/s]
Processed prompts:  10%|         | 52/512 [00:36<00:22, 20.89it/s, est. speed input: 22.62 toks/s, output: 361.91 toks/s]
Processed prompts:  13%|        | 67/512 [00:36<00:13, 33.74it/s, est. speed input: 29.03 toks/s, output: 464.48 toks/s]
Processed prompts:  16%|        | 82/512 [00:37<00:09, 46.36it/s, est. speed input: 35.39 toks/s, output: 566.23 toks/s]
Processed prompts:  20%|        | 100/512 [00:37<00:06, 62.38it/s, est. speed input: 42.99 toks/s, output: 687.83 toks/s]
Processed prompts:  24%|       | 121/512 [00:37<00:04, 81.44it/s, est. speed input: 51.82 toks/s, output: 829.11 toks/s]
Processed prompts:  28%|       | 145/512 [00:37<00:03, 103.67it/s, est. speed input: 61.87 toks/s, output: 989.95 toks/s]
Processed prompts:  33%|      | 168/512 [00:37<00:02, 123.32it/s, est. speed input: 71.45 toks/s, output: 1143.26 toks/s]
Processed prompts:  37%|      | 189/512 [00:37<00:02, 134.52it/s, est. speed input: 80.12 toks/s, output: 1281.90 toks/s]
Processed prompts:  41%|      | 209/512 [00:37<00:02, 148.91it/s, est. speed input: 88.36 toks/s, output: 1413.77 toks/s]
Processed prompts:  45%|     | 228/512 [00:37<00:01, 147.96it/s, est. speed input: 96.06 toks/s, output: 1537.00 toks/s]
Processed prompts:  51%|     | 261/512 [00:38<00:01, 154.30it/s, est. speed input: 109.39 toks/s, output: 1750.19 toks/s]
Processed prompts:  54%|    | 278/512 [00:38<00:01, 156.94it/s, est. speed input: 116.20 toks/s, output: 1859.21 toks/s]
Processed prompts:  59%|    | 303/512 [00:38<00:01, 154.46it/s, est. speed input: 126.10 toks/s, output: 2017.60 toks/s]
Processed prompts:  64%|   | 328/512 [00:38<00:01, 148.88it/s, est. speed input: 135.87 toks/s, output: 2173.85 toks/s]
Processed prompts:  68%|   | 349/512 [00:38<00:01, 142.63it/s, est. speed input: 143.96 toks/s, output: 2303.29 toks/s]
Processed prompts:  72%|  | 368/512 [00:38<00:01, 139.68it/s, est. speed input: 151.23 toks/s, output: 2419.71 toks/s]
Processed prompts:  75%|  | 385/512 [00:39<00:00, 134.73it/s, est. speed input: 157.65 toks/s, output: 2522.46 toks/s]
Processed prompts:  78%|  | 400/512 [00:39<00:00, 123.60it/s, est. speed input: 163.16 toks/s, output: 2610.53 toks/s]
Processed prompts:  81%|  | 413/512 [00:39<00:00, 108.62it/s, est. speed input: 167.74 toks/s, output: 2683.86 toks/s]
Processed prompts:  83%| | 425/512 [00:39<00:00, 104.00it/s, est. speed input: 172.04 toks/s, output: 2752.65 toks/s]
Processed prompts:  85%| | 436/512 [00:39<00:00, 103.13it/s, est. speed input: 176.01 toks/s, output: 2816.08 toks/s]
Processed prompts:  87%| | 447/512 [00:39<00:00, 90.87it/s, est. speed input: 179.70 toks/s, output: 2875.22 toks/s] 
Processed prompts:  89%| | 457/512 [00:39<00:00, 78.12it/s, est. speed input: 182.88 toks/s, output: 2926.07 toks/s]
Processed prompts:  91%| | 466/512 [00:40<00:00, 69.11it/s, est. speed input: 185.66 toks/s, output: 2970.48 toks/s]
Processed prompts:  93%|| 474/512 [00:40<00:00, 59.88it/s, est. speed input: 187.94 toks/s, output: 3007.09 toks/s]
Processed prompts:  94%|| 481/512 [00:40<00:00, 57.23it/s, est. speed input: 190.05 toks/s, output: 3040.88 toks/s]
Processed prompts:  95%|| 487/512 [00:40<00:00, 42.44it/s, est. speed input: 191.14 toks/s, output: 3058.16 toks/s]
Processed prompts:  96%|| 492/512 [00:40<00:00, 35.74it/s, est. speed input: 192.04 toks/s, output: 3072.56 toks/s]
Processed prompts:  97%|| 497/512 [00:41<00:00, 31.36it/s, est. speed input: 192.92 toks/s, output: 3086.75 toks/s]
Processed prompts:  98%|| 501/512 [00:49<00:04,  2.32it/s, est. speed input: 163.26 toks/s, output: 2612.23 toks/s]
Processed prompts:  98%|| 504/512 [00:49<00:02,  2.77it/s, est. speed input: 163.78 toks/s, output: 2620.48 toks/s]
Processed prompts:  99%|| 507/512 [00:49<00:01,  3.39it/s, est. speed input: 164.37 toks/s, output: 2629.87 toks/s]
Processed prompts: 100%|| 511/512 [00:49<00:00,  4.52it/s, est. speed input: 165.24 toks/s, output: 2643.84 toks/s]
Processed prompts: 100%|| 512/512 [00:49<00:00,  4.52it/s, est. speed input: 165.56 toks/s, output: 2649.01 toks/s]
Processed prompts: 100%|| 512/512 [00:49<00:00, 10.35it/s, est. speed input: 165.56 toks/s, output: 2649.01 toks/s]
[rank0]:[W127 18:25:30.117512195 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 19:47:10
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:47:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:47:17 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3013490) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3013490) WARNING 01-27 19:49:40 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.32 requests/s, 630.48 total tokens/s, 593.39 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 19:47:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:47:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:47:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:47:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:47:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:47:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:47:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:47:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:47:21] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:47:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:47:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:47:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:47:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:47:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:47:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:47:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3013490) [2026-01-27 19:47:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3013490) [2026-01-27 19:47:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3013490) [2026-01-27 19:47:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3013490) [2026-01-27 19:47:22] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3013490) [2026-01-27 19:47:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3013490) [2026-01-27 19:47:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3013490) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3013490) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.52s/it]
(EngineCore_DP0 pid=3013490) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:40<00:44, 22.07s/it]
(EngineCore_DP0 pid=3013490) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:15<00:28, 28.31s/it]
(EngineCore_DP0 pid=3013490) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:57<00:00, 33.50s/it]
(EngineCore_DP0 pid=3013490) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:57<00:00, 29.32s/it]
(EngineCore_DP0 pid=3013490) 
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=3013490) 2026-01-27 19:49:32,667 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3013490) 2026-01-27 19:49:33,039 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|         | 1/64 [00:00<00:14,  4.28it/s]
Adding requests:   5%|         | 3/64 [00:00<00:06,  9.42it/s]
Adding requests:   9%|         | 6/64 [00:00<00:03, 16.07it/s]
Adding requests:  16%|        | 10/64 [00:00<00:02, 23.20it/s]
Adding requests:  22%|       | 14/64 [00:00<00:01, 28.32it/s]
Adding requests:  34%|      | 22/64 [00:00<00:00, 43.50it/s]
Adding requests:  55%|    | 35/64 [00:00<00:00, 68.10it/s]
Adding requests:  80%|  | 51/64 [00:00<00:00, 95.09it/s]
Adding requests: 100%|| 64/64 [00:01<00:00, 61.11it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:25<27:13, 25.93s/it, est. speed input: 0.62 toks/s, output: 9.87 toks/s]
Processed prompts:   3%|         | 2/64 [00:26<11:05, 10.74s/it, est. speed input: 1.23 toks/s, output: 19.66 toks/s]
Processed prompts:  11%|         | 7/64 [00:26<01:58,  2.08s/it, est. speed input: 4.28 toks/s, output: 68.54 toks/s]
Processed prompts:  36%|      | 23/64 [00:26<00:18,  2.23it/s, est. speed input: 14.02 toks/s, output: 224.27 toks/s]
Processed prompts:  59%|    | 38/64 [00:26<00:05,  4.50it/s, est. speed input: 23.06 toks/s, output: 369.01 toks/s]
Processed prompts:  81%| | 52/64 [00:26<00:01,  7.41it/s, est. speed input: 31.44 toks/s, output: 503.05 toks/s]
Processed prompts: 100%|| 64/64 [00:26<00:00,  7.41it/s, est. speed input: 38.70 toks/s, output: 619.13 toks/s]
Processed prompts: 100%|| 64/64 [00:26<00:00,  2.42it/s, est. speed input: 38.70 toks/s, output: 619.13 toks/s]
[rank0]:[W127 19:50:09.095252422 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 19:50:25
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:50:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:50:31 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3016363) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3016363) WARNING 01-27 19:52:47 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 3.90 requests/s, 1059.48 total tokens/s, 997.16 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 19:50:30] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:50:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:50:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:50:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:50:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:50:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:50:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:50:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:50:34] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:50:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:50:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:50:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:50:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:50:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:50:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:50:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3016363) [2026-01-27 19:50:35] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3016363) [2026-01-27 19:50:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3016363) [2026-01-27 19:50:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3016363) [2026-01-27 19:50:35] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3016363) [2026-01-27 19:50:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3016363) [2026-01-27 19:50:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3016363) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3016363) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.66s/it]
(EngineCore_DP0 pid=3016363) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:41<00:45, 22.84s/it]
(EngineCore_DP0 pid=3016363) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:17<00:28, 28.78s/it]
(EngineCore_DP0 pid=3016363) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 33.64s/it]
(EngineCore_DP0 pid=3016363) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 29.59s/it]
(EngineCore_DP0 pid=3016363) 
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=3016363) 2026-01-27 19:52:45,865 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3016363) 2026-01-27 19:52:45,941 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   1%|          | 1/128 [00:00<00:29,  4.33it/s]
Adding requests:   2%|         | 3/128 [00:00<00:14,  8.71it/s]
Adding requests:   5%|         | 6/128 [00:00<00:08, 14.97it/s]
Adding requests:   8%|         | 10/128 [00:00<00:05, 21.87it/s]
Adding requests:  11%|         | 14/128 [00:00<00:04, 26.37it/s]
Adding requests:  17%|        | 22/128 [00:00<00:02, 41.48it/s]
Adding requests:  27%|       | 35/128 [00:00<00:01, 65.79it/s]
Adding requests:  39%|      | 50/128 [00:01<00:00, 88.28it/s]
Adding requests:  54%|    | 69/128 [00:01<00:00, 117.13it/s]
Adding requests:  71%|   | 91/128 [00:01<00:00, 145.55it/s]
Adding requests:  92%|| 118/128 [00:01<00:00, 180.77it/s]
Adding requests: 100%|| 128/128 [00:01<00:00, 92.90it/s] 

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:29<1:03:15, 29.88s/it, est. speed input: 0.54 toks/s, output: 8.57 toks/s]
Processed prompts:   2%|         | 2/128 [00:30<25:59, 12.38s/it, est. speed input: 1.07 toks/s, output: 17.06 toks/s] 
Processed prompts:   5%|         | 6/128 [00:30<05:49,  2.86s/it, est. speed input: 3.19 toks/s, output: 50.96 toks/s]
Processed prompts:   9%|         | 11/128 [00:30<02:22,  1.22s/it, est. speed input: 5.81 toks/s, output: 93.03 toks/s]
Processed prompts:  11%|         | 14/128 [00:30<01:34,  1.21it/s, est. speed input: 7.37 toks/s, output: 117.90 toks/s]
Processed prompts:  18%|        | 23/128 [00:30<00:36,  2.84it/s, est. speed input: 12.05 toks/s, output: 192.77 toks/s]
Processed prompts:  30%|       | 38/128 [00:30<00:13,  6.60it/s, est. speed input: 19.83 toks/s, output: 317.22 toks/s]
Processed prompts:  41%|      | 52/128 [00:30<00:06, 11.26it/s, est. speed input: 27.03 toks/s, output: 432.48 toks/s]
Processed prompts:  51%|     | 65/128 [00:30<00:03, 16.87it/s, est. speed input: 33.67 toks/s, output: 538.71 toks/s]
Processed prompts:  61%|    | 78/128 [00:30<00:02, 24.03it/s, est. speed input: 40.27 toks/s, output: 644.30 toks/s]
Processed prompts:  70%|   | 90/128 [00:31<00:01, 31.71it/s, est. speed input: 46.30 toks/s, output: 740.75 toks/s]
Processed prompts:  79%|  | 101/128 [00:31<00:00, 39.61it/s, est. speed input: 51.77 toks/s, output: 828.39 toks/s]
Processed prompts:  95%|| 121/128 [00:31<00:00, 54.33it/s, est. speed input: 61.66 toks/s, output: 986.54 toks/s]
Processed prompts: 100%|| 128/128 [00:31<00:00, 54.33it/s, est. speed input: 65.23 toks/s, output: 1043.61 toks/s]
Processed prompts: 100%|| 128/128 [00:31<00:00,  4.08it/s, est. speed input: 65.23 toks/s, output: 1043.61 toks/s]
[rank0]:[W127 19:53:20.864606679 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 19:53:36
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:53:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:53:44 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3019222) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3019222) WARNING 01-27 19:56:11 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.11 requests/s, 1391.10 total tokens/s, 1309.27 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 19:53:43] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:53:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:53:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:53:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:53:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:53:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:53:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:53:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:53:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:53:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:53:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:53:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:53:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:53:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:53:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:53:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3019222) [2026-01-27 19:53:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3019222) [2026-01-27 19:53:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3019222) [2026-01-27 19:53:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3019222) [2026-01-27 19:53:48] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3019222) [2026-01-27 19:53:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3019222) [2026-01-27 19:53:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3019222) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3019222) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.54s/it]
(EngineCore_DP0 pid=3019222) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:41<00:45, 22.66s/it]
(EngineCore_DP0 pid=3019222) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:17<00:28, 28.88s/it]
(EngineCore_DP0 pid=3019222) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 33.81s/it]
(EngineCore_DP0 pid=3019222) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 29.68s/it]
(EngineCore_DP0 pid=3019222) 
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=3019222) 2026-01-27 19:56:01,104 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3019222) 2026-01-27 19:56:01,539 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:53,  4.78it/s]
Adding requests:   1%|          | 3/256 [00:00<00:26,  9.48it/s]
Adding requests:   2%|         | 6/256 [00:00<00:16, 15.25it/s]
Adding requests:   4%|         | 9/256 [00:00<00:13, 18.77it/s]
Adding requests:   5%|         | 13/256 [00:00<00:10, 23.79it/s]
Adding requests:   7%|         | 18/256 [00:00<00:07, 31.15it/s]
Adding requests:  10%|         | 26/256 [00:00<00:05, 44.36it/s]
Adding requests:  14%|        | 36/256 [00:01<00:03, 58.75it/s]
Adding requests:  19%|        | 49/256 [00:01<00:02, 78.21it/s]
Adding requests:  26%|       | 66/256 [00:01<00:01, 103.28it/s]
Adding requests:  33%|      | 84/256 [00:01<00:01, 122.20it/s]
Adding requests:  40%|      | 103/256 [00:01<00:01, 141.22it/s]
Adding requests:  49%|     | 126/256 [00:01<00:00, 166.60it/s]
Adding requests:  59%|    | 150/256 [00:01<00:00, 186.90it/s]
Adding requests:  66%|   | 169/256 [00:01<00:00, 125.81it/s]
Adding requests:  78%|  | 199/256 [00:02<00:00, 163.64it/s]
Adding requests:  92%|| 236/256 [00:02<00:00, 211.85it/s]
Adding requests: 100%|| 256/256 [00:02<00:00, 118.13it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:41<2:54:28, 41.05s/it, est. speed input: 0.39 toks/s, output: 6.24 toks/s]
Processed prompts:   1%|          | 2/256 [00:41<1:12:02, 17.02s/it, est. speed input: 0.78 toks/s, output: 12.41 toks/s]
Processed prompts:   3%|         | 7/256 [00:41<13:42,  3.30s/it, est. speed input: 2.70 toks/s, output: 43.25 toks/s]  
Processed prompts:   4%|         | 11/256 [00:41<07:05,  1.74s/it, est. speed input: 4.23 toks/s, output: 67.68 toks/s]
Processed prompts:   6%|         | 15/256 [00:41<04:12,  1.05s/it, est. speed input: 5.74 toks/s, output: 91.89 toks/s]
Processed prompts:   8%|         | 20/256 [00:41<02:27,  1.60it/s, est. speed input: 7.62 toks/s, output: 121.95 toks/s]
Processed prompts:  11%|         | 28/256 [00:42<01:15,  3.03it/s, est. speed input: 10.62 toks/s, output: 169.97 toks/s]
Processed prompts:  16%|        | 40/256 [00:42<00:36,  5.96it/s, est. speed input: 15.12 toks/s, output: 241.88 toks/s]
Processed prompts:  21%|        | 54/256 [00:42<00:19, 10.34it/s, est. speed input: 20.32 toks/s, output: 325.07 toks/s]
Processed prompts:  26%|       | 67/256 [00:42<00:12, 15.33it/s, est. speed input: 25.10 toks/s, output: 401.65 toks/s]
Processed prompts:  31%|       | 79/256 [00:42<00:08, 20.92it/s, est. speed input: 29.49 toks/s, output: 471.88 toks/s]
Processed prompts:  36%|      | 91/256 [00:43<00:06, 27.14it/s, est. speed input: 33.84 toks/s, output: 541.45 toks/s]
Processed prompts:  40%|      | 102/256 [00:43<00:04, 33.49it/s, est. speed input: 37.80 toks/s, output: 604.84 toks/s]
Processed prompts:  44%|     | 112/256 [00:43<00:03, 39.13it/s, est. speed input: 41.37 toks/s, output: 661.93 toks/s]
Processed prompts:  48%|     | 122/256 [00:43<00:03, 43.74it/s, est. speed input: 44.90 toks/s, output: 718.37 toks/s]
Processed prompts:  51%|     | 131/256 [00:43<00:02, 47.35it/s, est. speed input: 48.05 toks/s, output: 768.76 toks/s]
Processed prompts:  54%|    | 139/256 [00:43<00:02, 50.42it/s, est. speed input: 50.83 toks/s, output: 813.31 toks/s]
Processed prompts:  57%|    | 147/256 [00:43<00:02, 51.29it/s, est. speed input: 53.58 toks/s, output: 857.21 toks/s]
Processed prompts:  60%|    | 154/256 [00:44<00:01, 53.38it/s, est. speed input: 55.98 toks/s, output: 895.68 toks/s]
Processed prompts:  63%|   | 161/256 [00:44<00:01, 53.94it/s, est. speed input: 58.36 toks/s, output: 933.72 toks/s]
Processed prompts:  66%|   | 168/256 [00:44<00:01, 55.23it/s, est. speed input: 60.73 toks/s, output: 971.71 toks/s]
Processed prompts:  68%|   | 174/256 [00:44<00:01, 54.14it/s, est. speed input: 62.73 toks/s, output: 1003.75 toks/s]
Processed prompts:  70%|   | 180/256 [00:44<00:01, 53.25it/s, est. speed input: 64.73 toks/s, output: 1035.61 toks/s]
Processed prompts:  73%|  | 186/256 [00:44<00:01, 53.34it/s, est. speed input: 66.72 toks/s, output: 1067.45 toks/s]
Processed prompts:  75%|  | 192/256 [00:44<00:01, 53.78it/s, est. speed input: 68.70 toks/s, output: 1099.19 toks/s]
Processed prompts:  77%|  | 198/256 [00:44<00:01, 49.95it/s, est. speed input: 70.62 toks/s, output: 1129.96 toks/s]
Processed prompts:  80%|  | 204/256 [00:45<00:01, 41.01it/s, est. speed input: 72.42 toks/s, output: 1158.77 toks/s]
Processed prompts:  82%| | 209/256 [00:45<00:01, 42.76it/s, est. speed input: 74.03 toks/s, output: 1184.48 toks/s]
Processed prompts:  84%| | 214/256 [00:45<00:01, 34.93it/s, est. speed input: 75.44 toks/s, output: 1207.08 toks/s]
Processed prompts:  86%| | 220/256 [00:45<00:01, 33.06it/s, est. speed input: 77.21 toks/s, output: 1235.41 toks/s]
Processed prompts:  88%| | 226/256 [00:45<00:00, 31.76it/s, est. speed input: 78.96 toks/s, output: 1263.41 toks/s]
Processed prompts:  90%| | 230/256 [00:45<00:00, 28.57it/s, est. speed input: 80.03 toks/s, output: 1280.52 toks/s]
Processed prompts:  91%|| 234/256 [00:46<00:00, 26.42it/s, est. speed input: 81.10 toks/s, output: 1297.54 toks/s]
Processed prompts:  93%|| 238/256 [00:46<00:00, 25.04it/s, est. speed input: 82.16 toks/s, output: 1314.49 toks/s]
Processed prompts:  95%|| 242/256 [00:46<00:00, 24.21it/s, est. speed input: 83.21 toks/s, output: 1331.42 toks/s]
Processed prompts:  96%|| 245/256 [00:46<00:00, 19.25it/s, est. speed input: 83.77 toks/s, output: 1340.26 toks/s]
Processed prompts:  97%|| 248/256 [00:47<00:00, 16.62it/s, est. speed input: 84.33 toks/s, output: 1349.25 toks/s]
Processed prompts:  98%|| 250/256 [00:47<00:00, 15.40it/s, est. speed input: 84.70 toks/s, output: 1355.21 toks/s]
Processed prompts:  98%|| 252/256 [00:47<00:00, 14.22it/s, est. speed input: 85.05 toks/s, output: 1360.84 toks/s]
Processed prompts:  99%|| 254/256 [00:47<00:00, 13.61it/s, est. speed input: 85.43 toks/s, output: 1366.81 toks/s]
Processed prompts: 100%|| 256/256 [00:47<00:00, 12.35it/s, est. speed input: 85.72 toks/s, output: 1371.60 toks/s]
Processed prompts: 100%|| 256/256 [00:47<00:00, 12.35it/s, est. speed input: 85.72 toks/s, output: 1371.60 toks/s]
Processed prompts: 100%|| 256/256 [00:47<00:00,  5.36it/s, est. speed input: 85.72 toks/s, output: 1371.60 toks/s]
[rank0]:[W127 19:57:02.719461407 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 19:57:20
Backend: cuSPARSELt (2:10)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/json/Qwen2.5-14B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:57:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:57:26 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3022506) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3022506) WARNING 01-27 19:59:43 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.39 requests/s, 1465.18 total tokens/s, 1378.99 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 19:57:26] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:57:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:57:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:57:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:57:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:57:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:57:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:57:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:57:30] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:57:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:57:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:57:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:57:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:57:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:57:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:57:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3022506) [2026-01-27 19:57:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3022506) [2026-01-27 19:57:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3022506) [2026-01-27 19:57:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3022506) [2026-01-27 19:57:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3022506) [2026-01-27 19:57:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3022506) [2026-01-27 19:57:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3022506) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3022506) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.59s/it]
(EngineCore_DP0 pid=3022506) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:41<00:45, 22.99s/it]
(EngineCore_DP0 pid=3022506) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:17<00:28, 28.90s/it]
(EngineCore_DP0 pid=3022506) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 33.99s/it]
(EngineCore_DP0 pid=3022506) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 29.85s/it]
(EngineCore_DP0 pid=3022506) 
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=3022506) 2026-01-27 19:59:41,889 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3022506) 2026-01-27 19:59:41,960 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  33%|      | 168/512 [00:00<00:00, 1679.82it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 3356.82it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [01:06<9:28:56, 66.80s/it, est. speed input: 0.24 toks/s, output: 3.83 toks/s]
Processed prompts:   0%|          | 2/512 [01:07<3:57:30, 27.94s/it, est. speed input: 0.47 toks/s, output: 7.58 toks/s]
Processed prompts:   6%|         | 33/512 [01:08<08:25,  1.05s/it, est. speed input: 7.74 toks/s, output: 123.79 toks/s]
Processed prompts:  12%|        | 63/512 [01:08<03:28,  2.15it/s, est. speed input: 14.63 toks/s, output: 234.04 toks/s]
Processed prompts:  18%|        | 91/512 [01:09<01:55,  3.65it/s, est. speed input: 20.94 toks/s, output: 334.98 toks/s]
Processed prompts:  23%|       | 117/512 [01:10<01:12,  5.47it/s, est. speed input: 26.68 toks/s, output: 426.82 toks/s]
Processed prompts:  28%|       | 141/512 [01:10<00:47,  7.87it/s, est. speed input: 32.03 toks/s, output: 512.42 toks/s]
Processed prompts:  32%|      | 164/512 [01:10<00:31, 10.93it/s, est. speed input: 37.11 toks/s, output: 593.83 toks/s]
Processed prompts:  36%|      | 185/512 [01:10<00:22, 14.57it/s, est. speed input: 41.72 toks/s, output: 667.55 toks/s]
Processed prompts:  40%|      | 205/512 [01:11<00:16, 18.97it/s, est. speed input: 46.08 toks/s, output: 737.25 toks/s]
Processed prompts:  44%|     | 224/512 [01:11<00:11, 24.10it/s, est. speed input: 50.19 toks/s, output: 803.01 toks/s]
Processed prompts:  47%|     | 242/512 [01:11<00:09, 29.80it/s, est. speed input: 54.05 toks/s, output: 864.86 toks/s]
Processed prompts:  50%|     | 258/512 [01:11<00:07, 35.26it/s, est. speed input: 57.46 toks/s, output: 919.30 toks/s]
Processed prompts:  53%|    | 273/512 [01:12<00:05, 40.60it/s, est. speed input: 60.62 toks/s, output: 969.97 toks/s]
Processed prompts:  56%|    | 288/512 [01:12<00:04, 47.38it/s, est. speed input: 63.80 toks/s, output: 1020.82 toks/s]
Processed prompts:  59%|    | 302/512 [01:12<00:03, 52.57it/s, est. speed input: 66.73 toks/s, output: 1067.72 toks/s]
Processed prompts:  62%|   | 315/512 [01:12<00:03, 56.71it/s, est. speed input: 69.44 toks/s, output: 1110.96 toks/s]
Processed prompts:  64%|   | 327/512 [01:12<00:03, 59.68it/s, est. speed input: 71.91 toks/s, output: 1150.58 toks/s]
Processed prompts:  66%|   | 338/512 [01:12<00:02, 60.90it/s, est. speed input: 74.16 toks/s, output: 1186.52 toks/s]
Processed prompts:  68%|   | 348/512 [01:13<00:02, 62.68it/s, est. speed input: 76.20 toks/s, output: 1219.20 toks/s]
Processed prompts:  70%|   | 358/512 [01:13<00:02, 62.75it/s, est. speed input: 78.22 toks/s, output: 1251.51 toks/s]
Processed prompts:  72%|  | 367/512 [01:13<00:02, 63.04it/s, est. speed input: 80.03 toks/s, output: 1280.52 toks/s]
Processed prompts:  73%|  | 376/512 [01:13<00:02, 62.03it/s, est. speed input: 81.83 toks/s, output: 1309.21 toks/s]
Processed prompts:  75%|  | 384/512 [01:13<00:02, 61.25it/s, est. speed input: 83.41 toks/s, output: 1334.61 toks/s]
Processed prompts:  77%|  | 392/512 [01:13<00:02, 57.19it/s, est. speed input: 84.96 toks/s, output: 1359.35 toks/s]
Processed prompts:  78%|  | 399/512 [01:13<00:01, 56.70it/s, est. speed input: 86.33 toks/s, output: 1381.26 toks/s]
Processed prompts:  79%|  | 406/512 [01:14<00:01, 55.12it/s, est. speed input: 87.68 toks/s, output: 1402.89 toks/s]
Processed prompts:  80%|  | 412/512 [01:14<00:01, 53.74it/s, est. speed input: 88.83 toks/s, output: 1421.31 toks/s]
Processed prompts:  82%| | 418/512 [01:14<00:01, 51.32it/s, est. speed input: 89.97 toks/s, output: 1439.45 toks/s]
Processed prompts:  83%| | 424/512 [01:14<00:01, 51.32it/s, est. speed input: 91.11 toks/s, output: 1457.82 toks/s]
Processed prompts:  84%| | 430/512 [01:14<00:01, 51.87it/s, est. speed input: 92.26 toks/s, output: 1476.22 toks/s]
Processed prompts:  85%| | 436/512 [01:14<00:01, 48.69it/s, est. speed input: 93.37 toks/s, output: 1493.97 toks/s]
Processed prompts:  86%| | 441/512 [01:14<00:01, 47.61it/s, est. speed input: 94.30 toks/s, output: 1508.84 toks/s]
Processed prompts:  87%| | 446/512 [01:15<00:01, 36.63it/s, est. speed input: 95.09 toks/s, output: 1521.40 toks/s]
Processed prompts:  88%| | 451/512 [01:15<00:01, 36.67it/s, est. speed input: 95.98 toks/s, output: 1535.67 toks/s]
Processed prompts:  89%| | 455/512 [01:15<00:01, 37.06it/s, est. speed input: 96.70 toks/s, output: 1547.14 toks/s]
Processed prompts:  90%| | 459/512 [01:15<00:01, 30.12it/s, est. speed input: 97.28 toks/s, output: 1556.49 toks/s]
Processed prompts:  91%| | 465/512 [01:15<00:01, 29.42it/s, est. speed input: 98.28 toks/s, output: 1572.41 toks/s]
Processed prompts:  92%|| 470/512 [01:15<00:01, 28.15it/s, est. speed input: 99.08 toks/s, output: 1585.22 toks/s]
Processed prompts:  92%|| 473/512 [01:16<00:01, 28.38it/s, est. speed input: 99.57 toks/s, output: 1593.19 toks/s]
Processed prompts:  93%|| 476/512 [01:16<00:01, 23.95it/s, est. speed input: 99.95 toks/s, output: 1599.23 toks/s]
Processed prompts:  94%|| 480/512 [01:16<00:01, 23.13it/s, est. speed input: 100.55 toks/s, output: 1608.73 toks/s]
Processed prompts:  94%|| 483/512 [01:16<00:01, 20.45it/s, est. speed input: 100.91 toks/s, output: 1614.56 toks/s]
Processed prompts:  95%|| 486/512 [01:16<00:01, 16.67it/s, est. speed input: 101.17 toks/s, output: 1618.76 toks/s]
Processed prompts:  95%|| 488/512 [01:17<00:01, 15.04it/s, est. speed input: 101.34 toks/s, output: 1621.50 toks/s]
Processed prompts:  96%|| 490/512 [01:17<00:01, 13.85it/s, est. speed input: 101.52 toks/s, output: 1624.25 toks/s]
Processed prompts:  96%|| 492/512 [01:17<00:01, 13.14it/s, est. speed input: 101.70 toks/s, output: 1627.14 toks/s]
Processed prompts:  96%|| 494/512 [01:17<00:01, 12.62it/s, est. speed input: 101.88 toks/s, output: 1630.03 toks/s]
Processed prompts:  97%|| 496/512 [01:17<00:01, 12.26it/s, est. speed input: 102.06 toks/s, output: 1632.92 toks/s]
Processed prompts:  97%|| 497/512 [01:29<00:01, 12.26it/s, est. speed input: 102.15 toks/s, output: 1634.43 toks/s]
Processed prompts:  97%|| 498/512 [01:34<00:33,  2.37s/it, est. speed input: 84.33 toks/s, output: 1349.21 toks/s] 
Processed prompts:  98%|| 501/512 [01:34<00:16,  1.51s/it, est. speed input: 84.68 toks/s, output: 1354.84 toks/s]
Processed prompts: 100%|| 511/512 [01:34<00:00,  1.80it/s, est. speed input: 86.21 toks/s, output: 1379.42 toks/s]
Processed prompts: 100%|| 512/512 [01:34<00:00,  1.80it/s, est. speed input: 86.38 toks/s, output: 1382.12 toks/s]
Processed prompts: 100%|| 512/512 [01:34<00:00,  5.40it/s, est. speed input: 86.38 toks/s, output: 1382.12 toks/s]
[rank0]:[W127 20:01:18.888339858 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

