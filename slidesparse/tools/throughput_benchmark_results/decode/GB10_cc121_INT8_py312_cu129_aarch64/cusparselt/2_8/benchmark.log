
========== M=64 ==========
Time: 2026-01-27 16:56:06
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-1B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:56:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:56:10 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2835739) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2835739) WARNING 01-27 16:56:32 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.59 requests/s, 4241.68 total tokens/s, 3992.17 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 16:56:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:56:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:56:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:56:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:56:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:56:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:56:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:56:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:56:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:56:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:56:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:56:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:56:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:56:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:56:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:56:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:56:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:56:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2835739) [2026-01-27 16:56:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2835739) [2026-01-27 16:56:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2835739) [2026-01-27 16:56:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2835739) [2026-01-27 16:56:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2835739) [2026-01-27 16:56:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2835739) [2026-01-27 16:56:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2835739) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2835739) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.64s/it]
(EngineCore_DP0 pid=2835739) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.64s/it]
(EngineCore_DP0 pid=2835739) 
(EngineCore_DP0 pid=2835739) [2026-01-27 16:56:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2835739) [2026-01-27 16:56:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=2835739) [2026-01-27 16:56:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=2835739) [2026-01-27 16:56:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=2835739) [2026-01-27 16:56:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2835739) [2026-01-27 16:56:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=2835739) [2026-01-27 16:56:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=2835739) [2026-01-27 16:56:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=2835739) 2026-01-27 16:56:31,706 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2835739) 2026-01-27 16:56:31,724 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 12215.49it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:04<04:12,  4.00s/it, est. speed input: 4.00 toks/s, output: 63.99 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:04<00:00,  4.00s/it, est. speed input: 249.88 toks/s, output: 3998.13 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:04<00:00, 15.62it/s, est. speed input: 249.88 toks/s, output: 3998.13 toks/s]
[rank0]:[W127 16:56:36.582545249 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 16:56:38
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-1B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:56:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:56:42 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2836390) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2836390) WARNING 01-27 16:57:04 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 24.07 requests/s, 6548.17 total tokens/s, 6162.98 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 16:56:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:56:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:56:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:56:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:56:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:56:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:56:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:56:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:56:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:56:45] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:56:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:56:45] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:56:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:56:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:56:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:56:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:56:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:56:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:56:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2836390) [2026-01-27 16:56:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2836390) [2026-01-27 16:56:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2836390) [2026-01-27 16:56:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2836390) [2026-01-27 16:56:46] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2836390) [2026-01-27 16:56:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2836390) [2026-01-27 16:56:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2836390) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2836390) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.69s/it]
(EngineCore_DP0 pid=2836390) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.69s/it]
(EngineCore_DP0 pid=2836390) 
(EngineCore_DP0 pid=2836390) [2026-01-27 16:56:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2836390) [2026-01-27 16:56:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=2836390) [2026-01-27 16:56:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=2836390) [2026-01-27 16:56:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=2836390) [2026-01-27 16:56:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2836390) [2026-01-27 16:56:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=2836390) [2026-01-27 16:56:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=2836390) [2026-01-27 16:56:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=2836390) 2026-01-27 16:57:03,532 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2836390) 2026-01-27 16:57:03,539 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 13233.53it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:04<10:30,  4.97s/it, est. speed input: 3.22 toks/s, output: 51.54 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:05<02:46,  1.33s/it, est. speed input: 9.43 toks/s, output: 150.95 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:05<00:02, 23.20it/s, est. speed input: 190.99 toks/s, output: 3055.86 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:05<00:00, 50.82it/s, est. speed input: 356.40 toks/s, output: 5702.40 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 50.82it/s, est. speed input: 385.95 toks/s, output: 6175.24 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 24.12it/s, est. speed input: 385.95 toks/s, output: 6175.24 toks/s]
[rank0]:[W127 16:57:09.640905079 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 16:57:11
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-1B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:57:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:57:15 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2837035) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2837035) WARNING 01-27 16:57:37 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 29.69 requests/s, 8074.68 total tokens/s, 7599.70 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 16:57:15] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:57:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:57:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:57:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:57:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:57:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:57:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:57:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:57:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:57:18] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:57:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:57:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:57:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:57:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:57:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:57:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:57:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:57:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2837035) [2026-01-27 16:57:19] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2837035) [2026-01-27 16:57:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2837035) [2026-01-27 16:57:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2837035) [2026-01-27 16:57:19] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2837035) [2026-01-27 16:57:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2837035) [2026-01-27 16:57:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2837035) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2837035) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.78s/it]
(EngineCore_DP0 pid=2837035) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.78s/it]
(EngineCore_DP0 pid=2837035) 
(EngineCore_DP0 pid=2837035) [2026-01-27 16:57:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2837035) [2026-01-27 16:57:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=2837035) [2026-01-27 16:57:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=2837035) [2026-01-27 16:57:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=2837035) [2026-01-27 16:57:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2837035) [2026-01-27 16:57:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=2837035) [2026-01-27 16:57:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=2837035) [2026-01-27 16:57:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=2837035) 2026-01-27 16:57:36,863 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2837035) 2026-01-27 16:57:36,880 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13537.18it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<31:43,  7.47s/it, est. speed input: 2.14 toks/s, output: 34.29 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:07<00:33,  6.65it/s, est. speed input: 75.72 toks/s, output: 1211.58 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:07<00:10, 16.84it/s, est. speed input: 157.73 toks/s, output: 2523.75 toks/s]
Processed prompts:  46%|████▋     | 119/256 [00:07<00:04, 31.15it/s, est. speed input: 242.59 toks/s, output: 3881.50 toks/s]
Processed prompts:  60%|█████▉    | 153/256 [00:07<00:02, 45.68it/s, est. speed input: 307.52 toks/s, output: 4920.37 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:08<00:01, 62.11it/s, est. speed input: 364.79 toks/s, output: 5836.62 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:08<00:00, 79.20it/s, est. speed input: 415.71 toks/s, output: 6651.42 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:08<00:00, 94.99it/s, est. speed input: 460.51 toks/s, output: 7368.14 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 94.99it/s, est. speed input: 476.07 toks/s, output: 7617.14 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 29.75it/s, est. speed input: 476.07 toks/s, output: 7617.14 toks/s]
[rank0]:[W127 16:57:46.373371375 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 16:57:48
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-1B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:57:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:57:52 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2837745) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2837745) WARNING 01-27 16:58:11 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 30.86 requests/s, 8393.59 total tokens/s, 7899.85 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 16:57:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:57:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:57:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:57:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:57:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:57:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:57:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:57:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:57:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:57:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:57:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:57:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:57:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:57:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:57:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:57:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:57:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:57:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:57:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2837745) [2026-01-27 16:57:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2837745) [2026-01-27 16:57:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2837745) [2026-01-27 16:57:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2837745) [2026-01-27 16:57:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2837745) [2026-01-27 16:57:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2837745) [2026-01-27 16:57:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2837745) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2837745) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.93s/it]
(EngineCore_DP0 pid=2837745) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.93s/it]
(EngineCore_DP0 pid=2837745) 
(EngineCore_DP0 pid=2837745) [2026-01-27 16:58:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2837745) [2026-01-27 16:58:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=2837745) [2026-01-27 16:58:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=2837745) [2026-01-27 16:58:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=2837745) [2026-01-27 16:58:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2837745) [2026-01-27 16:58:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=2837745) [2026-01-27 16:58:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=2837745) [2026-01-27 16:58:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=2837745) 2026-01-27 16:58:10,873 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2837745) 2026-01-27 16:58:10,880 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 12567.06it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:12<1:48:12, 12.70s/it, est. speed input: 1.26 toks/s, output: 20.15 toks/s]
Processed prompts:   7%|▋         | 35/512 [00:12<02:04,  3.82it/s, est. speed input: 43.42 toks/s, output: 694.67 toks/s]
Processed prompts:  18%|█▊        | 92/512 [00:13<00:33, 12.45it/s, est. speed input: 112.66 toks/s, output: 1802.61 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:13<00:16, 22.69it/s, est. speed input: 172.03 toks/s, output: 2752.52 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:13<00:09, 34.94it/s, est. speed input: 223.58 toks/s, output: 3577.32 toks/s]
Processed prompts:  44%|████▍     | 225/512 [00:13<00:05, 49.18it/s, est. speed input: 268.37 toks/s, output: 4293.99 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:13<00:03, 71.53it/s, est. speed input: 323.41 toks/s, output: 5174.54 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:13<00:02, 94.57it/s, est. speed input: 368.73 toks/s, output: 5899.75 toks/s]
Processed prompts:  69%|██████▊   | 351/512 [00:13<00:01, 118.09it/s, est. speed input: 407.73 toks/s, output: 6523.61 toks/s]
Processed prompts:  76%|███████▌  | 387/512 [00:13<00:00, 142.17it/s, est. speed input: 445.59 toks/s, output: 7129.50 toks/s]
Processed prompts:  82%|████████▏ | 421/512 [00:14<00:00, 161.98it/s, est. speed input: 480.14 toks/s, output: 7682.23 toks/s]
Processed prompts:  88%|████████▊ | 453/512 [00:14<00:00, 171.63it/s, est. speed input: 510.94 toks/s, output: 8174.97 toks/s]
Processed prompts:  94%|█████████▍| 481/512 [00:14<00:00, 176.84it/s, est. speed input: 537.06 toks/s, output: 8592.96 toks/s]
Processed prompts:  99%|█████████▉| 507/512 [00:16<00:00, 40.56it/s, est. speed input: 493.30 toks/s, output: 7892.88 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:16<00:00, 40.56it/s, est. speed input: 495.68 toks/s, output: 7930.85 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:16<00:00, 30.98it/s, est. speed input: 495.68 toks/s, output: 7930.85 toks/s]
[rank0]:[W127 16:58:28.394995157 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:25:05
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-3B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:25:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:25:09 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2867577) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2867577) WARNING 01-27 17:25:48 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.75 requests/s, 2107.35 total tokens/s, 1983.39 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:25:09] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:25:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:25:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:25:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:25:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:25:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:25:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:25:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:25:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:25:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:25:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:25:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:25:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:25:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:25:12] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:25:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:25:12] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:25:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:25:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:25:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:25:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:25:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:25:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:25:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:25:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:25:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:25:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:25:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2867577) [2026-01-27 17:25:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2867577) [2026-01-27 17:25:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2867577) [2026-01-27 17:25:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2867577) [2026-01-27 17:25:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2867577) [2026-01-27 17:25:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2867577) [2026-01-27 17:25:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2867577) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2867577) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:27<00:00, 27.43s/it]
(EngineCore_DP0 pid=2867577) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:27<00:00, 27.43s/it]
(EngineCore_DP0 pid=2867577) 
(EngineCore_DP0 pid=2867577) [2026-01-27 17:25:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=2867577) [2026-01-27 17:25:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=2867577) [2026-01-27 17:25:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=2867577) [2026-01-27 17:25:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10616832 bytes
(EngineCore_DP0 pid=2867577) [2026-01-27 17:25:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=2867577) [2026-01-27 17:25:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56623104 bytes
(EngineCore_DP0 pid=2867577) [2026-01-27 17:25:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=2867577) [2026-01-27 17:25:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=2867577) 2026-01-27 17:25:48,018 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2867577) 2026-01-27 17:25:48,030 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 11833.17it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:08<08:28,  8.07s/it, est. speed input: 1.98 toks/s, output: 31.72 toks/s]
Processed prompts:  52%|█████▏    | 33/64 [00:08<00:05,  5.65it/s, est. speed input: 64.33 toks/s, output: 1029.30 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:08<00:00,  5.65it/s, est. speed input: 124.06 toks/s, output: 1984.90 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:08<00:00,  7.75it/s, est. speed input: 124.06 toks/s, output: 1984.90 toks/s]
[rank0]:[W127 17:25:57.037215257 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:25:59
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-3B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:26:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:26:03 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2868526) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2868526) WARNING 01-27 17:26:41 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 12.11 requests/s, 3293.43 total tokens/s, 3099.70 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:26:02] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:26:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:26:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:26:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:26:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:26:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:26:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:26:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:26:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:26:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:26:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:26:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:26:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:26:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:26:06] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:26:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:26:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:26:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:26:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:26:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:26:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:26:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:26:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:26:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:26:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:26:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:26:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:26:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2868526) [2026-01-27 17:26:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2868526) [2026-01-27 17:26:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2868526) [2026-01-27 17:26:07] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2868526) [2026-01-27 17:26:07] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2868526) [2026-01-27 17:26:07] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2868526) [2026-01-27 17:26:07] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2868526) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2868526) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:26<00:00, 26.69s/it]
(EngineCore_DP0 pid=2868526) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:26<00:00, 26.69s/it]
(EngineCore_DP0 pid=2868526) 
(EngineCore_DP0 pid=2868526) [2026-01-27 17:26:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=2868526) [2026-01-27 17:26:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=2868526) [2026-01-27 17:26:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=2868526) [2026-01-27 17:26:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10616832 bytes
(EngineCore_DP0 pid=2868526) [2026-01-27 17:26:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=2868526) [2026-01-27 17:26:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56623104 bytes
(EngineCore_DP0 pid=2868526) [2026-01-27 17:26:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=2868526) [2026-01-27 17:26:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=2868526) 2026-01-27 17:26:40,630 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2868526) 2026-01-27 17:26:40,641 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4264.03it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:10<21:21, 10.09s/it, est. speed input: 1.59 toks/s, output: 25.37 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:10<00:44,  2.47it/s, est. speed input: 28.25 toks/s, output: 451.99 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:10<00:06, 10.93it/s, est. speed input: 95.97 toks/s, output: 1535.55 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:10<00:01, 20.17it/s, est. speed input: 149.82 toks/s, output: 2397.07 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:10<00:00, 20.17it/s, est. speed input: 194.30 toks/s, output: 3108.87 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:10<00:00, 12.14it/s, est. speed input: 194.30 toks/s, output: 3108.87 toks/s]
[rank0]:[W127 17:26:52.111122283 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:26:54
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-3B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:26:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:26:57 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2869473) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2869473) WARNING 01-27 17:27:35 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.00 requests/s, 4079.56 total tokens/s, 3839.59 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:26:57] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:26:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:26:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:26:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:26:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:26:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:26:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:26:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:26:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:26:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:26:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:26:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:26:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:26:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:27:01] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:27:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:27:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:27:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:27:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:27:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:27:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:27:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:27:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:27:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:27:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:27:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:27:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:27:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2869473) [2026-01-27 17:27:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2869473) [2026-01-27 17:27:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2869473) [2026-01-27 17:27:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2869473) [2026-01-27 17:27:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2869473) [2026-01-27 17:27:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2869473) [2026-01-27 17:27:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2869473) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2869473) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:26<00:00, 26.54s/it]
(EngineCore_DP0 pid=2869473) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:26<00:00, 26.54s/it]
(EngineCore_DP0 pid=2869473) 
(EngineCore_DP0 pid=2869473) [2026-01-27 17:27:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=2869473) [2026-01-27 17:27:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=2869473) [2026-01-27 17:27:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=2869473) [2026-01-27 17:27:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10616832 bytes
(EngineCore_DP0 pid=2869473) [2026-01-27 17:27:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=2869473) [2026-01-27 17:27:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56623104 bytes
(EngineCore_DP0 pid=2869473) [2026-01-27 17:27:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=2869473) [2026-01-27 17:27:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=2869473) 2026-01-27 17:27:35,046 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2869473) 2026-01-27 17:27:35,057 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13671.62it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:14<1:03:30, 14.94s/it, est. speed input: 1.07 toks/s, output: 17.13 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:15<01:54,  2.04it/s, est. speed input: 23.30 toks/s, output: 372.84 toks/s]
Processed prompts:  20%|█▉        | 51/256 [00:15<00:35,  5.82it/s, est. speed input: 53.46 toks/s, output: 855.34 toks/s]
Processed prompts:  30%|███       | 77/256 [00:15<00:17, 10.45it/s, est. speed input: 80.02 toks/s, output: 1280.38 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:15<00:09, 15.90it/s, est. speed input: 102.99 toks/s, output: 1647.85 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:15<00:06, 22.25it/s, est. speed input: 122.72 toks/s, output: 1963.56 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:15<00:03, 29.62it/s, est. speed input: 140.18 toks/s, output: 2242.84 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:15<00:02, 40.67it/s, est. speed input: 161.11 toks/s, output: 2577.84 toks/s]
Processed prompts:  70%|██████▉   | 179/256 [00:16<00:01, 51.68it/s, est. speed input: 178.83 toks/s, output: 2861.33 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:16<00:00, 62.17it/s, est. speed input: 194.31 toks/s, output: 3108.89 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:16<00:00, 70.60it/s, est. speed input: 208.33 toks/s, output: 3333.25 toks/s]
Processed prompts:  89%|████████▊ | 227/256 [00:16<00:00, 75.37it/s, est. speed input: 220.88 toks/s, output: 3534.07 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:16<00:00, 74.78it/s, est. speed input: 231.03 toks/s, output: 3696.44 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:16<00:00, 64.88it/s, est. speed input: 238.84 toks/s, output: 3821.39 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:17<00:00, 64.88it/s, est. speed input: 240.25 toks/s, output: 3844.02 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:17<00:00, 15.02it/s, est. speed input: 240.25 toks/s, output: 3844.02 toks/s]
[rank0]:[W127 17:27:52.873942991 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:27:55
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Llama3.2-3B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:27:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:27:58 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2870497) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2870497) WARNING 01-27 17:28:37 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.39 requests/s, 4187.15 total tokens/s, 3940.85 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:27:58] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:27:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:27:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:27:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:27:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:27:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:27:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:27:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:27:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:27:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:27:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:27:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:27:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:27:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:28:01] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:28:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:28:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:28:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:28:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:28:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:28:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:28:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:28:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:28:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:28:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:28:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:28:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:28:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2870497) [2026-01-27 17:28:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2870497) [2026-01-27 17:28:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2870497) [2026-01-27 17:28:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2870497) [2026-01-27 17:28:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2870497) [2026-01-27 17:28:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2870497) [2026-01-27 17:28:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2870497) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2870497) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:26<00:00, 26.94s/it]
(EngineCore_DP0 pid=2870497) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:26<00:00, 26.94s/it]
(EngineCore_DP0 pid=2870497) 
(EngineCore_DP0 pid=2870497) [2026-01-27 17:28:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=2870497) [2026-01-27 17:28:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=2870497) [2026-01-27 17:28:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=2870497) [2026-01-27 17:28:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10616832 bytes
(EngineCore_DP0 pid=2870497) [2026-01-27 17:28:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=2870497) [2026-01-27 17:28:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56623104 bytes
(EngineCore_DP0 pid=2870497) [2026-01-27 17:28:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=2870497) [2026-01-27 17:28:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=2870497) 2026-01-27 17:28:36,417 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2870497) 2026-01-27 17:28:36,428 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 13782.18it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:25<3:33:02, 25.02s/it, est. speed input: 0.64 toks/s, output: 10.23 toks/s]
Processed prompts:   0%|          | 2/512 [00:25<1:28:48, 10.45s/it, est. speed input: 1.27 toks/s, output: 20.26 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:25<03:08,  2.54it/s, est. speed input: 20.70 toks/s, output: 331.16 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:25<01:17,  5.77it/s, est. speed input: 39.17 toks/s, output: 626.71 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:25<00:42,  9.84it/s, est. speed input: 56.11 toks/s, output: 897.82 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:26<00:26, 14.81it/s, est. speed input: 71.57 toks/s, output: 1145.04 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:26<00:17, 21.23it/s, est. speed input: 85.88 toks/s, output: 1374.16 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:26<00:11, 29.33it/s, est. speed input: 99.47 toks/s, output: 1591.55 toks/s]
Processed prompts:  40%|████      | 205/512 [00:26<00:06, 47.17it/s, est. speed input: 123.43 toks/s, output: 1974.89 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:26<00:04, 65.35it/s, est. speed input: 144.72 toks/s, output: 2315.58 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:26<00:02, 80.73it/s, est. speed input: 162.20 toks/s, output: 2595.22 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:27<00:02, 97.86it/s, est. speed input: 178.50 toks/s, output: 2855.99 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:27<00:01, 110.66it/s, est. speed input: 192.26 toks/s, output: 3076.22 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:27<00:01, 120.62it/s, est. speed input: 203.68 toks/s, output: 3258.91 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [00:27<00:01, 127.93it/s, est. speed input: 213.87 toks/s, output: 3421.89 toks/s]
Processed prompts:  75%|███████▌  | 385/512 [00:27<00:00, 135.56it/s, est. speed input: 223.48 toks/s, output: 3575.66 toks/s]
Processed prompts:  79%|███████▊  | 403/512 [00:27<00:00, 143.08it/s, est. speed input: 233.03 toks/s, output: 3728.55 toks/s]
Processed prompts:  82%|████████▏ | 421/512 [00:27<00:00, 137.04it/s, est. speed input: 242.16 toks/s, output: 3874.58 toks/s]
Processed prompts:  85%|████████▌ | 437/512 [00:27<00:00, 134.59it/s, est. speed input: 250.24 toks/s, output: 4003.82 toks/s]
Processed prompts:  88%|████████▊ | 452/512 [00:28<00:00, 123.72it/s, est. speed input: 257.45 toks/s, output: 4119.14 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:28<00:00, 110.39it/s, est. speed input: 263.85 toks/s, output: 4221.60 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:28<00:00, 97.88it/s, est. speed input: 269.05 toks/s, output: 4304.81 toks/s] 
Processed prompts:  96%|█████████▌| 489/512 [00:28<00:00, 76.46it/s, est. speed input: 272.89 toks/s, output: 4366.16 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:33<00:01,  8.50it/s, est. speed input: 240.48 toks/s, output: 3847.75 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:33<00:00,  8.50it/s, est. speed input: 246.59 toks/s, output: 3945.40 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:33<00:00, 15.41it/s, est. speed input: 246.59 toks/s, output: 3945.40 toks/s]
[rank0]:[W127 17:29:10.518694011 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 18:10:04
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-7B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:10:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:10:08 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2912649) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2912649) WARNING 01-27 18:11:22 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.81 requests/s, 1308.24 total tokens/s, 1231.28 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 18:10:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:10:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:10:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:10:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:10:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:10:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:10:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:10:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:10:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:10:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:10:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:10:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:10:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:10:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:10:11] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:10:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:10:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:10:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:10:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:10:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:10:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:10:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:10:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:10:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:10:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:10:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:10:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:10:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2912649) [2026-01-27 18:10:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2912649) [2026-01-27 18:10:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2912649) [2026-01-27 18:10:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2912649) [2026-01-27 18:10:12] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2912649) [2026-01-27 18:10:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2912649) [2026-01-27 18:10:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2912649) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2912649) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.38s/it]
(EngineCore_DP0 pid=2912649) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 30.58s/it]
(EngineCore_DP0 pid=2912649) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 30.10s/it]
(EngineCore_DP0 pid=2912649) 
(EngineCore_DP0 pid=2912649) [2026-01-27 18:11:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=2912649) [2026-01-27 18:11:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
(EngineCore_DP0 pid=2912649) [2026-01-27 18:11:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=2912649) [2026-01-27 18:11:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
(EngineCore_DP0 pid=2912649) [2026-01-27 18:11:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=2912649) [2026-01-27 18:11:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
(EngineCore_DP0 pid=2912649) [2026-01-27 18:11:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=2912649) [2026-01-27 18:11:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
(EngineCore_DP0 pid=2912649) 2026-01-27 18:11:21,925 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2912649) 2026-01-27 18:11:21,938 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 10192.33it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:13<13:41, 13.04s/it, est. speed input: 1.23 toks/s, output: 19.63 toks/s]
Processed prompts:  30%|██▉       | 19/64 [00:13<00:22,  2.02it/s, est. speed input: 23.10 toks/s, output: 369.61 toks/s]
Processed prompts:  97%|█████████▋| 62/64 [00:13<00:00,  8.48it/s, est. speed input: 74.59 toks/s, output: 1193.44 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:13<00:00,  8.48it/s, est. speed input: 77.00 toks/s, output: 1231.93 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:13<00:00,  4.81it/s, est. speed input: 77.00 toks/s, output: 1231.93 toks/s]
[rank0]:[W127 18:11:36.082109509 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 18:11:39
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-7B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:11:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:11:44 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2914153) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2914153) WARNING 01-27 18:12:53 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.96 requests/s, 2165.05 total tokens/s, 2037.69 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 18:11:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:11:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:11:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:11:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:11:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:11:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:11:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:11:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:11:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:11:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:11:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:11:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:11:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:11:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:11:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:11:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:11:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:11:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:11:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:11:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:11:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:11:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:11:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:11:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:11:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:11:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:11:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:11:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2914153) [2026-01-27 18:11:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2914153) [2026-01-27 18:11:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2914153) [2026-01-27 18:11:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2914153) [2026-01-27 18:11:48] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2914153) [2026-01-27 18:11:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2914153) [2026-01-27 18:11:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2914153) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2914153) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.96s/it]
(EngineCore_DP0 pid=2914153) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:55<00:00, 27.72s/it]
(EngineCore_DP0 pid=2914153) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:55<00:00, 27.75s/it]
(EngineCore_DP0 pid=2914153) 
(EngineCore_DP0 pid=2914153) [2026-01-27 18:12:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=2914153) [2026-01-27 18:12:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
(EngineCore_DP0 pid=2914153) [2026-01-27 18:12:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=2914153) [2026-01-27 18:12:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
(EngineCore_DP0 pid=2914153) [2026-01-27 18:12:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=2914153) [2026-01-27 18:12:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
(EngineCore_DP0 pid=2914153) [2026-01-27 18:12:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=2914153) [2026-01-27 18:12:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
(EngineCore_DP0 pid=2914153) 2026-01-27 18:12:52,402 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2914153) 2026-01-27 18:12:52,414 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3956.51it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:15<32:35, 15.39s/it, est. speed input: 1.04 toks/s, output: 16.63 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:15<01:07,  1.62it/s, est. speed input: 18.54 toks/s, output: 296.59 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:15<00:14,  5.45it/s, est. speed input: 49.03 toks/s, output: 784.44 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:15<00:05, 10.17it/s, est. speed input: 75.97 toks/s, output: 1215.58 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:15<00:01, 15.59it/s, est. speed input: 98.52 toks/s, output: 1576.36 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:16<00:00, 24.74it/s, est. speed input: 126.63 toks/s, output: 2026.06 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:16<00:00, 24.74it/s, est. speed input: 127.62 toks/s, output: 2042.00 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:16<00:00,  7.98it/s, est. speed input: 127.62 toks/s, output: 2042.00 toks/s]
[rank0]:[W127 18:13:09.474476144 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 18:13:11
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-7B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:13:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:13:15 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2915610) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2915610) WARNING 01-27 18:14:32 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.31 requests/s, 2804.04 total tokens/s, 2639.10 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 18:13:15] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:13:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:13:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:13:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:13:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:13:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:13:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:13:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:13:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:13:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:13:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:13:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:13:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:13:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:13:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:13:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:13:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:13:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:13:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:13:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:13:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:13:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:13:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:13:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:13:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:13:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:13:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:13:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2915610) [2026-01-27 18:13:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2915610) [2026-01-27 18:13:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2915610) [2026-01-27 18:13:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2915610) [2026-01-27 18:13:20] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2915610) [2026-01-27 18:13:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2915610) [2026-01-27 18:13:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2915610) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2915610) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:28<00:28, 28.08s/it]
(EngineCore_DP0 pid=2915610) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:02<00:00, 31.54s/it]
(EngineCore_DP0 pid=2915610) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:02<00:00, 31.02s/it]
(EngineCore_DP0 pid=2915610) 
(EngineCore_DP0 pid=2915610) [2026-01-27 18:14:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=2915610) [2026-01-27 18:14:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
(EngineCore_DP0 pid=2915610) [2026-01-27 18:14:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=2915610) [2026-01-27 18:14:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
(EngineCore_DP0 pid=2915610) [2026-01-27 18:14:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=2915610) [2026-01-27 18:14:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
(EngineCore_DP0 pid=2915610) [2026-01-27 18:14:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=2915610) [2026-01-27 18:14:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
(EngineCore_DP0 pid=2915610) 2026-01-27 18:14:30,479 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2915610) 2026-01-27 18:14:30,529 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:52,  4.83it/s]
Adding requests:   1%|          | 3/256 [00:00<00:27,  9.25it/s]
Adding requests:   2%|▏         | 5/256 [00:00<00:19, 12.77it/s]
Adding requests:   3%|▎         | 8/256 [00:00<00:13, 18.25it/s]
Adding requests:   5%|▍         | 12/256 [00:00<00:10, 23.14it/s]
Adding requests:   7%|▋         | 17/256 [00:00<00:07, 30.36it/s]
Adding requests:   9%|▉         | 23/256 [00:00<00:05, 38.88it/s]
Adding requests:  12%|█▎        | 32/256 [00:00<00:04, 53.30it/s]
Adding requests:  18%|█▊        | 45/256 [00:01<00:02, 75.05it/s]
Adding requests:  25%|██▍       | 63/256 [00:01<00:01, 105.58it/s]
Adding requests:  31%|███       | 79/256 [00:01<00:01, 120.86it/s]
Adding requests:  38%|███▊      | 98/256 [00:01<00:01, 141.08it/s]
Adding requests:  50%|████▉     | 127/256 [00:01<00:00, 183.78it/s]
Adding requests:  60%|██████    | 154/256 [00:01<00:00, 207.79it/s]
Adding requests:  73%|███████▎  | 188/256 [00:01<00:00, 244.63it/s]
Adding requests:  83%|████████▎ | 213/256 [00:01<00:00, 163.71it/s]
Adding requests:  99%|█████████▉| 253/256 [00:02<00:00, 215.34it/s]
Adding requests: 100%|██████████| 256/256 [00:02<00:00, 122.60it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:18<1:20:05, 18.84s/it, est. speed input: 0.85 toks/s, output: 13.58 toks/s]
Processed prompts:   2%|▏         | 5/256 [00:19<11:53,  2.84s/it, est. speed input: 4.21 toks/s, output: 67.33 toks/s]  
Processed prompts:   4%|▎         | 9/256 [00:19<05:22,  1.31s/it, est. speed input: 7.51 toks/s, output: 120.09 toks/s]
Processed prompts:   5%|▍         | 12/256 [00:19<03:25,  1.18it/s, est. speed input: 9.92 toks/s, output: 158.71 toks/s]
Processed prompts:   6%|▌         | 15/256 [00:19<02:17,  1.75it/s, est. speed input: 12.29 toks/s, output: 196.65 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:19<01:17,  3.03it/s, est. speed input: 16.23 toks/s, output: 259.75 toks/s]
Processed prompts:  10%|█         | 26/256 [00:19<00:45,  5.05it/s, est. speed input: 20.93 toks/s, output: 334.84 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:20<00:18, 11.41it/s, est. speed input: 31.93 toks/s, output: 510.95 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:20<00:07, 25.09it/s, est. speed input: 50.67 toks/s, output: 810.77 toks/s]
Processed prompts:  32%|███▏      | 83/256 [00:20<00:04, 38.48it/s, est. speed input: 65.39 toks/s, output: 1046.29 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:20<00:03, 49.20it/s, est. speed input: 78.18 toks/s, output: 1250.88 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:20<00:02, 63.90it/s, est. speed input: 93.14 toks/s, output: 1490.22 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:20<00:01, 75.77it/s, est. speed input: 106.37 toks/s, output: 1701.95 toks/s]
Processed prompts:  60%|█████▉    | 153/256 [00:20<00:01, 83.20it/s, est. speed input: 117.18 toks/s, output: 1874.87 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:21<00:00, 91.52it/s, est. speed input: 127.22 toks/s, output: 2035.59 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:21<00:00, 95.13it/s, est. speed input: 136.34 toks/s, output: 2181.43 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:21<00:00, 99.78it/s, est. speed input: 144.72 toks/s, output: 2315.54 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:21<00:00, 92.84it/s, est. speed input: 152.66 toks/s, output: 2442.57 toks/s]
Processed prompts:  84%|████████▍ | 215/256 [00:21<00:00, 88.37it/s, est. speed input: 159.83 toks/s, output: 2557.35 toks/s]
Processed prompts:  88%|████████▊ | 225/256 [00:21<00:00, 76.29it/s, est. speed input: 165.85 toks/s, output: 2653.60 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:21<00:00, 68.01it/s, est. speed input: 171.09 toks/s, output: 2737.47 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:22<00:00, 61.34it/s, est. speed input: 175.57 toks/s, output: 2809.04 toks/s]
Processed prompts:  97%|█████████▋| 249/256 [00:22<00:00, 45.62it/s, est. speed input: 178.33 toks/s, output: 2853.31 toks/s]
Processed prompts: 100%|█████████▉| 255/256 [00:22<00:00, 38.38it/s, est. speed input: 180.65 toks/s, output: 2890.38 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:22<00:00, 38.38it/s, est. speed input: 180.89 toks/s, output: 2894.25 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:22<00:00, 11.31it/s, est. speed input: 180.89 toks/s, output: 2894.25 toks/s]
[rank0]:[W127 18:14:58.857154220 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 18:15:14
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-7B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:15:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:15:19 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2917512) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2917512) WARNING 01-27 18:16:35 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.68 requests/s, 2904.30 total tokens/s, 2733.46 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 18:15:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:15:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:15:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:15:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:15:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:15:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:15:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:15:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:15:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:15:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:15:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:15:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:15:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:15:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:15:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:15:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:15:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:15:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:15:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:15:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:15:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:15:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:15:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:15:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:15:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:15:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:15:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:15:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2917512) [2026-01-27 18:15:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2917512) [2026-01-27 18:15:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2917512) [2026-01-27 18:15:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2917512) [2026-01-27 18:15:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2917512) [2026-01-27 18:15:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2917512) [2026-01-27 18:15:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2917512) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2917512) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:28<00:28, 28.39s/it]
(EngineCore_DP0 pid=2917512) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:01<00:00, 31.43s/it]
(EngineCore_DP0 pid=2917512) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:01<00:00, 30.97s/it]
(EngineCore_DP0 pid=2917512) 
(EngineCore_DP0 pid=2917512) [2026-01-27 18:16:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=2917512) [2026-01-27 18:16:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
(EngineCore_DP0 pid=2917512) [2026-01-27 18:16:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=2917512) [2026-01-27 18:16:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
(EngineCore_DP0 pid=2917512) [2026-01-27 18:16:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=2917512) [2026-01-27 18:16:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
(EngineCore_DP0 pid=2917512) [2026-01-27 18:16:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=2917512) [2026-01-27 18:16:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
(EngineCore_DP0 pid=2917512) 2026-01-27 18:16:34,063 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2917512) 2026-01-27 18:16:34,075 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 5562.03it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:34<4:53:58, 34.52s/it, est. speed input: 0.46 toks/s, output: 7.42 toks/s]
Processed prompts:   0%|          | 2/512 [00:34<2:02:29, 14.41s/it, est. speed input: 0.92 toks/s, output: 14.69 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:35<04:19,  1.84it/s, est. speed input: 15.01 toks/s, output: 240.20 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:35<01:47,  4.19it/s, est. speed input: 28.41 toks/s, output: 454.53 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:35<00:58,  7.15it/s, est. speed input: 40.70 toks/s, output: 651.25 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:36<00:36, 10.76it/s, est. speed input: 51.92 toks/s, output: 830.75 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:36<00:23, 15.51it/s, est. speed input: 62.35 toks/s, output: 997.62 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:36<00:16, 21.61it/s, est. speed input: 72.28 toks/s, output: 1156.44 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:36<00:11, 28.87it/s, est. speed input: 81.27 toks/s, output: 1300.30 toks/s]
Processed prompts:  40%|████      | 205/512 [00:36<00:08, 37.58it/s, est. speed input: 89.76 toks/s, output: 1436.14 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:36<00:06, 47.92it/s, est. speed input: 97.78 toks/s, output: 1564.54 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:36<00:04, 59.37it/s, est. speed input: 105.33 toks/s, output: 1685.25 toks/s]
Processed prompts:  51%|█████     | 259/512 [00:36<00:03, 71.08it/s, est. speed input: 112.39 toks/s, output: 1798.23 toks/s]
Processed prompts:  54%|█████▍    | 276/512 [00:36<00:02, 84.36it/s, est. speed input: 119.43 toks/s, output: 1910.91 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:37<00:02, 101.18it/s, est. speed input: 130.08 toks/s, output: 2081.23 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:37<00:01, 111.53it/s, est. speed input: 140.17 toks/s, output: 2242.65 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:37<00:01, 116.97it/s, est. speed input: 148.54 toks/s, output: 2376.61 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [00:37<00:01, 118.02it/s, est. speed input: 155.99 toks/s, output: 2495.90 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [00:37<00:01, 117.23it/s, est. speed input: 162.58 toks/s, output: 2601.30 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [00:37<00:01, 112.50it/s, est. speed input: 168.27 toks/s, output: 2692.24 toks/s]
Processed prompts:  80%|████████  | 412/512 [00:38<00:00, 109.50it/s, est. speed input: 173.16 toks/s, output: 2770.53 toks/s]
Processed prompts:  83%|████████▎ | 424/512 [00:38<00:00, 106.05it/s, est. speed input: 177.62 toks/s, output: 2841.91 toks/s]
Processed prompts:  85%|████████▍ | 435/512 [00:38<00:00, 98.11it/s, est. speed input: 181.57 toks/s, output: 2905.04 toks/s] 
Processed prompts:  87%|████████▋ | 446/512 [00:38<00:00, 89.41it/s, est. speed input: 185.40 toks/s, output: 2966.47 toks/s]
Processed prompts:  89%|████████▉ | 456/512 [00:38<00:00, 78.80it/s, est. speed input: 188.71 toks/s, output: 3019.42 toks/s]
Processed prompts:  91%|█████████ | 465/512 [00:38<00:00, 72.32it/s, est. speed input: 191.66 toks/s, output: 3066.58 toks/s]
Processed prompts:  92%|█████████▏| 473/512 [00:38<00:00, 68.23it/s, est. speed input: 194.26 toks/s, output: 3108.21 toks/s]
Processed prompts:  94%|█████████▍| 480/512 [00:39<00:00, 58.52it/s, est. speed input: 196.24 toks/s, output: 3139.88 toks/s]
Processed prompts:  95%|█████████▌| 487/512 [00:39<00:00, 44.27it/s, est. speed input: 197.70 toks/s, output: 3163.26 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [00:39<00:00, 37.56it/s, est. speed input: 198.66 toks/s, output: 3178.48 toks/s]
Processed prompts:  97%|█████████▋| 497/512 [00:39<00:00, 33.06it/s, est. speed input: 199.59 toks/s, output: 3193.47 toks/s]
Processed prompts:  98%|█████████▊| 501/512 [00:47<00:04,  2.35it/s, est. speed input: 167.78 toks/s, output: 2684.43 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:47<00:00,  2.35it/s, est. speed input: 171.18 toks/s, output: 2738.82 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:47<00:00, 10.70it/s, est. speed input: 171.18 toks/s, output: 2738.82 toks/s]
[rank0]:[W127 18:17:23.486978338 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 19:32:43
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-14B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:32:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:32:50 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3000363) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3000363) WARNING 01-27 19:35:15 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.42 requests/s, 658.85 total tokens/s, 620.09 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 19:32:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:32:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:32:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:32:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:32:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:32:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:32:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:32:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:32:53] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:32:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:32:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:32:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:32:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:32:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:32:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:32:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3000363) [2026-01-27 19:32:55] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3000363) [2026-01-27 19:32:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3000363) [2026-01-27 19:32:55] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3000363) [2026-01-27 19:32:55] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3000363) [2026-01-27 19:32:55] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3000363) [2026-01-27 19:32:55] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3000363) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3000363) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.67s/it]
(EngineCore_DP0 pid=3000363) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:47<00:52, 26.13s/it]
(EngineCore_DP0 pid=3000363) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:18<00:28, 28.43s/it]
(EngineCore_DP0 pid=3000363) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 33.26s/it]
(EngineCore_DP0 pid=3000363) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 29.71s/it]
(EngineCore_DP0 pid=3000363) 
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41287680 bytes
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29491200 bytes
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 159252480 bytes
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 79626240 bytes
(EngineCore_DP0 pid=3000363) 2026-01-27 19:35:07,044 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3000363) 2026-01-27 19:35:07,385 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 1/64 [00:00<00:13,  4.51it/s]
Adding requests:   5%|▍         | 3/64 [00:00<00:06,  9.18it/s]
Adding requests:   9%|▉         | 6/64 [00:00<00:03, 15.58it/s]
Adding requests:  16%|█▌        | 10/64 [00:00<00:02, 22.11it/s]
Adding requests:  22%|██▏       | 14/64 [00:00<00:01, 25.20it/s]
Adding requests:  33%|███▎      | 21/64 [00:00<00:01, 36.89it/s]
Adding requests:  52%|█████▏    | 33/64 [00:00<00:00, 60.29it/s]
Adding requests:  73%|███████▎  | 47/64 [00:01<00:00, 82.30it/s]
Adding requests: 100%|██████████| 64/64 [00:01<00:00, 57.10it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:24<25:46, 24.56s/it, est. speed input: 0.65 toks/s, output: 10.43 toks/s]
Processed prompts:   3%|▎         | 2/64 [00:24<10:30, 10.17s/it, est. speed input: 1.30 toks/s, output: 20.76 toks/s]
Processed prompts:   9%|▉         | 6/64 [00:24<02:16,  2.35s/it, est. speed input: 3.88 toks/s, output: 62.03 toks/s]
Processed prompts:  34%|███▍      | 22/64 [00:24<00:18,  2.27it/s, est. speed input: 14.15 toks/s, output: 226.45 toks/s]
Processed prompts:  58%|█████▊    | 37/64 [00:24<00:05,  4.66it/s, est. speed input: 23.70 toks/s, output: 379.17 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:25<00:00, 10.47it/s, est. speed input: 40.69 toks/s, output: 651.07 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:25<00:00, 10.47it/s, est. speed input: 40.69 toks/s, output: 651.07 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:25<00:00,  2.54it/s, est. speed input: 40.69 toks/s, output: 651.07 toks/s]
[rank0]:[W127 19:35:42.802230570 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 19:35:59
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-14B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:36:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:36:05 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3003332) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3003332) WARNING 01-27 19:38:28 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.08 requests/s, 1109.78 total tokens/s, 1044.50 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 19:36:05] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:36:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:36:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:36:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:36:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:36:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:36:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:36:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:36:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:36:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:36:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:36:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:36:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:36:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:36:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:36:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3003332) [2026-01-27 19:36:09] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3003332) [2026-01-27 19:36:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3003332) [2026-01-27 19:36:09] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3003332) [2026-01-27 19:36:09] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3003332) [2026-01-27 19:36:09] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3003332) [2026-01-27 19:36:09] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3003332) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3003332) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.45s/it]
(EngineCore_DP0 pid=3003332) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:48<00:53, 26.96s/it]
(EngineCore_DP0 pid=3003332) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:20<00:29, 29.16s/it]
(EngineCore_DP0 pid=3003332) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:01<00:00, 33.97s/it]
(EngineCore_DP0 pid=3003332) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:01<00:00, 30.37s/it]
(EngineCore_DP0 pid=3003332) 
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41287680 bytes
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29491200 bytes
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 159252480 bytes
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 79626240 bytes
(EngineCore_DP0 pid=3003332) 2026-01-27 19:38:23,160 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3003332) 2026-01-27 19:38:23,377 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   1%|          | 1/128 [00:00<00:28,  4.45it/s]
Adding requests:   2%|▏         | 3/128 [00:00<00:13,  9.42it/s]
Adding requests:   5%|▍         | 6/128 [00:00<00:08, 14.68it/s]
Adding requests:   8%|▊         | 10/128 [00:00<00:05, 21.29it/s]
Adding requests:  11%|█         | 14/128 [00:00<00:04, 25.86it/s]
Adding requests:  16%|█▋        | 21/128 [00:00<00:02, 37.91it/s]
Adding requests:  25%|██▌       | 32/128 [00:00<00:01, 56.93it/s]
Adding requests:  34%|███▍      | 44/128 [00:01<00:01, 73.91it/s]
Adding requests:  48%|████▊     | 61/128 [00:01<00:00, 100.35it/s]
Adding requests:  62%|██████▎   | 80/128 [00:01<00:00, 124.82it/s]
Adding requests:  80%|███████▉  | 102/128 [00:01<00:00, 151.73it/s]
Adding requests: 100%|██████████| 128/128 [00:01<00:00, 89.36it/s] 

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:28<59:52, 28.29s/it, est. speed input: 0.57 toks/s, output: 9.05 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:28<24:36, 11.72s/it, est. speed input: 1.13 toks/s, output: 18.02 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:28<04:35,  2.27s/it, est. speed input: 3.92 toks/s, output: 62.79 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:28<02:20,  1.20s/it, est. speed input: 6.14 toks/s, output: 98.26 toks/s]
Processed prompts:  11%|█         | 14/128 [00:28<01:31,  1.24it/s, est. speed input: 7.78 toks/s, output: 124.48 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:28<00:41,  2.58it/s, est. speed input: 11.62 toks/s, output: 185.92 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:29<00:18,  5.20it/s, est. speed input: 17.08 toks/s, output: 273.28 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:29<00:07, 10.59it/s, est. speed input: 25.26 toks/s, output: 404.11 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:29<00:03, 17.07it/s, est. speed input: 32.82 toks/s, output: 525.20 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:29<00:01, 30.47it/s, est. speed input: 46.17 toks/s, output: 738.77 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:29<00:00, 42.73it/s, est. speed input: 57.74 toks/s, output: 923.91 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:29<00:00, 53.39it/s, est. speed input: 67.60 toks/s, output: 1081.68 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:29<00:00, 53.39it/s, est. speed input: 68.68 toks/s, output: 1098.84 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:29<00:00,  4.29it/s, est. speed input: 68.68 toks/s, output: 1098.84 toks/s]
[rank0]:[W127 19:39:01.024307964 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 19:39:16
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-14B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:39:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:39:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3006356) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3006356) WARNING 01-27 19:41:46 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.46 requests/s, 1484.75 total tokens/s, 1397.41 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 19:39:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:39:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:39:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:39:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:39:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:39:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:39:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:39:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:39:26] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:39:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:39:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:39:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:39:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:39:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:39:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:39:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3006356) [2026-01-27 19:39:27] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3006356) [2026-01-27 19:39:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3006356) [2026-01-27 19:39:27] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3006356) [2026-01-27 19:39:27] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3006356) [2026-01-27 19:39:27] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3006356) [2026-01-27 19:39:27] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3006356) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3006356) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.39s/it]
(EngineCore_DP0 pid=3006356) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:47<00:53, 26.70s/it]
(EngineCore_DP0 pid=3006356) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:18<00:28, 28.52s/it]
(EngineCore_DP0 pid=3006356) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:00<00:00, 33.78s/it]
(EngineCore_DP0 pid=3006356) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:00<00:00, 30.11s/it]
(EngineCore_DP0 pid=3006356) 
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41287680 bytes
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29491200 bytes
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 159252480 bytes
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 79626240 bytes
(EngineCore_DP0 pid=3006356) 2026-01-27 19:41:39,428 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3006356) 2026-01-27 19:41:39,510 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:37,  6.78it/s]
Adding requests:   2%|▏         | 4/256 [00:00<00:16, 15.48it/s]
Adding requests:   3%|▎         | 8/256 [00:00<00:10, 24.18it/s]
Adding requests:   5%|▌         | 13/256 [00:00<00:07, 32.43it/s]
Adding requests:   8%|▊         | 20/256 [00:00<00:05, 43.56it/s]
Adding requests:  12%|█▎        | 32/256 [00:00<00:03, 66.18it/s]
Adding requests:  19%|█▉        | 49/256 [00:00<00:02, 96.64it/s]
Adding requests:  28%|██▊       | 72/256 [00:00<00:01, 136.24it/s]
Adding requests:  39%|███▊      | 99/256 [00:01<00:00, 174.95it/s]
Adding requests:  52%|█████▏    | 134/256 [00:01<00:00, 226.40it/s]
Adding requests:  66%|██████▋   | 170/256 [00:01<00:00, 266.06it/s]
Adding requests:  77%|███████▋  | 197/256 [00:01<00:00, 198.14it/s]
Adding requests:  98%|█████████▊| 250/256 [00:01<00:00, 276.82it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 165.27it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:39<2:46:46, 39.24s/it, est. speed input: 0.41 toks/s, output: 6.52 toks/s]
Processed prompts:   1%|          | 2/256 [00:39<1:08:50, 16.26s/it, est. speed input: 0.81 toks/s, output: 12.99 toks/s]
Processed prompts:   4%|▎         | 9/256 [00:39<09:49,  2.39s/it, est. speed input: 3.64 toks/s, output: 58.18 toks/s]  
Processed prompts:   6%|▌         | 15/256 [00:39<04:44,  1.18s/it, est. speed input: 6.04 toks/s, output: 96.57 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:39<02:31,  1.54it/s, est. speed input: 8.81 toks/s, output: 140.97 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:40<01:09,  3.19it/s, est. speed input: 13.56 toks/s, output: 216.90 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:40<00:35,  5.84it/s, est. speed input: 19.07 toks/s, output: 305.09 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:40<00:20,  9.29it/s, est. speed input: 24.52 toks/s, output: 392.34 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:40<00:13, 13.44it/s, est. speed input: 29.55 toks/s, output: 472.77 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:40<00:09, 18.06it/s, est. speed input: 34.14 toks/s, output: 546.19 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:40<00:06, 23.07it/s, est. speed input: 38.31 toks/s, output: 612.92 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:41<00:05, 28.41it/s, est. speed input: 42.08 toks/s, output: 673.21 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:41<00:04, 33.86it/s, est. speed input: 45.80 toks/s, output: 732.85 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:41<00:03, 39.24it/s, est. speed input: 49.14 toks/s, output: 786.29 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:41<00:02, 45.07it/s, est. speed input: 52.47 toks/s, output: 839.54 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:41<00:02, 49.12it/s, est. speed input: 55.40 toks/s, output: 886.34 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:41<00:01, 53.24it/s, est. speed input: 58.31 toks/s, output: 932.98 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:41<00:01, 54.33it/s, est. speed input: 60.82 toks/s, output: 973.12 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:41<00:01, 54.46it/s, est. speed input: 63.30 toks/s, output: 1012.87 toks/s]
Processed prompts:  68%|██████▊   | 173/256 [00:42<00:01, 56.78it/s, est. speed input: 65.80 toks/s, output: 1052.83 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:42<00:01, 56.68it/s, est. speed input: 68.26 toks/s, output: 1092.21 toks/s]
Processed prompts:  73%|███████▎  | 187/256 [00:42<00:01, 59.43it/s, est. speed input: 70.74 toks/s, output: 1131.91 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:42<00:01, 47.42it/s, est. speed input: 73.01 toks/s, output: 1168.19 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:42<00:01, 48.23it/s, est. speed input: 75.06 toks/s, output: 1200.98 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:42<00:01, 41.16it/s, est. speed input: 76.95 toks/s, output: 1231.15 toks/s]
Processed prompts:  82%|████████▏ | 211/256 [00:42<00:01, 42.12it/s, est. speed input: 78.61 toks/s, output: 1257.80 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:43<00:01, 36.59it/s, est. speed input: 80.12 toks/s, output: 1282.00 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:43<00:00, 35.21it/s, est. speed input: 82.00 toks/s, output: 1311.98 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:43<00:00, 36.01it/s, est. speed input: 83.28 toks/s, output: 1332.47 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:43<00:00, 30.86it/s, est. speed input: 84.39 toks/s, output: 1350.24 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:43<00:00, 28.84it/s, est. speed input: 85.53 toks/s, output: 1368.56 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:43<00:00, 26.92it/s, est. speed input: 86.65 toks/s, output: 1386.40 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:44<00:00, 25.99it/s, est. speed input: 87.77 toks/s, output: 1404.34 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:44<00:00, 20.70it/s, est. speed input: 88.37 toks/s, output: 1413.94 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:44<00:00, 17.83it/s, est. speed input: 88.97 toks/s, output: 1423.56 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:44<00:00, 16.17it/s, est. speed input: 89.35 toks/s, output: 1429.52 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:44<00:00, 15.20it/s, est. speed input: 89.74 toks/s, output: 1435.80 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:45<00:00, 14.47it/s, est. speed input: 90.13 toks/s, output: 1442.07 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:45<00:00, 13.52it/s, est. speed input: 90.48 toks/s, output: 1447.73 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:45<00:00, 13.52it/s, est. speed input: 90.48 toks/s, output: 1447.73 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:45<00:00,  5.66it/s, est. speed input: 90.48 toks/s, output: 1447.73 toks/s]
[rank0]:[W127 19:42:34.315219071 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 19:42:49
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/json/Qwen2.5-14B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:42:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:42:56 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3009587) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3009587) WARNING 01-27 19:45:23 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.55 requests/s, 1510.53 total tokens/s, 1421.68 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 19:42:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:42:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:42:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:42:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:42:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:42:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:42:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:42:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:42:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:42:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:42:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:42:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:42:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:42:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:42:59] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:43:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:43:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:43:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:43:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:43:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:43:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:43:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:43:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:43:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:43:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:43:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:43:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:43:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3009587) [2026-01-27 19:43:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3009587) [2026-01-27 19:43:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3009587) [2026-01-27 19:43:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3009587) [2026-01-27 19:43:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3009587) [2026-01-27 19:43:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3009587) [2026-01-27 19:43:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3009587) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3009587) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.42s/it]
(EngineCore_DP0 pid=3009587) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:48<00:53, 26.94s/it]
(EngineCore_DP0 pid=3009587) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:19<00:29, 29.05s/it]
(EngineCore_DP0 pid=3009587) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:01<00:00, 34.17s/it]
(EngineCore_DP0 pid=3009587) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:01<00:00, 30.47s/it]
(EngineCore_DP0 pid=3009587) 
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41287680 bytes
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29491200 bytes
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 159252480 bytes
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 79626240 bytes
(EngineCore_DP0 pid=3009587) 2026-01-27 19:45:14,499 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3009587) 2026-01-27 19:45:14,602 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/512 [00:00<01:09,  7.40it/s]
Adding requests:   1%|          | 4/512 [00:00<00:27, 18.73it/s]
Adding requests:   2%|▏         | 9/512 [00:00<00:15, 31.65it/s]
Adding requests:   3%|▎         | 14/512 [00:00<00:13, 36.68it/s]
Adding requests:   5%|▍         | 24/512 [00:00<00:08, 57.42it/s]
Adding requests:   8%|▊         | 42/512 [00:00<00:04, 96.44it/s]
Adding requests:  13%|█▎        | 67/512 [00:00<00:03, 144.64it/s]
Adding requests:  20%|█▉        | 100/512 [00:00<00:02, 200.29it/s]
Adding requests:  26%|██▌       | 131/512 [00:00<00:01, 232.06it/s]
Adding requests:  32%|███▏      | 165/512 [00:01<00:01, 263.38it/s]
Adding requests:  40%|████      | 207/512 [00:01<00:00, 307.48it/s]
Adding requests:  51%|█████     | 261/512 [00:01<00:00, 376.63it/s]
Adding requests:  66%|██████▌   | 339/512 [00:01<00:00, 496.21it/s]
Adding requests:  88%|████████▊ | 449/512 [00:01<00:00, 675.76it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 338.32it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [01:03<8:58:49, 63.27s/it, est. speed input: 0.25 toks/s, output: 4.05 toks/s]
Processed prompts:   0%|          | 2/512 [01:03<3:45:03, 26.48s/it, est. speed input: 0.50 toks/s, output: 8.00 toks/s]
Processed prompts:   3%|▎         | 14/512 [01:04<20:16,  2.44s/it, est. speed input: 3.46 toks/s, output: 55.38 toks/s]
Processed prompts:   5%|▌         | 27/512 [01:05<08:27,  1.05s/it, est. speed input: 6.60 toks/s, output: 105.67 toks/s]
Processed prompts:  10%|█         | 52/512 [01:06<03:12,  2.39it/s, est. speed input: 12.59 toks/s, output: 201.44 toks/s]
Processed prompts:  16%|█▌        | 80/512 [01:06<01:35,  4.51it/s, est. speed input: 19.18 toks/s, output: 306.93 toks/s]
Processed prompts:  21%|██        | 107/512 [01:07<00:57,  7.09it/s, est. speed input: 25.42 toks/s, output: 406.73 toks/s]
Processed prompts:  26%|██▌       | 132/512 [01:07<00:36, 10.50it/s, est. speed input: 31.24 toks/s, output: 499.90 toks/s]
Processed prompts:  30%|███       | 155/512 [01:07<00:24, 14.54it/s, est. speed input: 36.55 toks/s, output: 584.80 toks/s]
Processed prompts:  35%|███▍      | 177/512 [01:08<00:17, 19.42it/s, est. speed input: 41.58 toks/s, output: 665.36 toks/s]
Processed prompts:  39%|███▊      | 198/512 [01:08<00:12, 25.25it/s, est. speed input: 46.37 toks/s, output: 741.88 toks/s]
Processed prompts:  42%|████▏     | 217/512 [01:08<00:09, 31.24it/s, est. speed input: 50.65 toks/s, output: 810.41 toks/s]
Processed prompts:  46%|████▌     | 235/512 [01:08<00:07, 37.48it/s, est. speed input: 54.68 toks/s, output: 874.81 toks/s]
Processed prompts:  49%|████▉     | 252/512 [01:08<00:05, 43.54it/s, est. speed input: 58.45 toks/s, output: 935.14 toks/s]
Processed prompts:  52%|█████▏    | 268/512 [01:09<00:04, 50.09it/s, est. speed input: 61.99 toks/s, output: 991.85 toks/s]
Processed prompts:  55%|█████▌    | 283/512 [01:09<00:04, 55.91it/s, est. speed input: 65.29 toks/s, output: 1044.64 toks/s]
Processed prompts:  58%|█████▊    | 297/512 [01:09<00:03, 61.60it/s, est. speed input: 68.36 toks/s, output: 1093.78 toks/s]
Processed prompts:  61%|██████    | 310/512 [01:09<00:03, 64.92it/s, est. speed input: 71.18 toks/s, output: 1138.89 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [01:09<00:02, 67.49it/s, est. speed input: 73.77 toks/s, output: 1180.32 toks/s]
Processed prompts:  65%|██████▌   | 333/512 [01:09<00:02, 68.76it/s, est. speed input: 76.13 toks/s, output: 1218.01 toks/s]
Processed prompts:  67%|██████▋   | 344/512 [01:10<00:02, 68.78it/s, est. speed input: 78.46 toks/s, output: 1255.38 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [01:10<00:02, 67.68it/s, est. speed input: 80.56 toks/s, output: 1289.03 toks/s]
Processed prompts:  71%|███████   | 363/512 [01:10<00:02, 67.19it/s, est. speed input: 82.45 toks/s, output: 1319.23 toks/s]
Processed prompts:  73%|███████▎  | 372/512 [01:10<00:02, 65.67it/s, est. speed input: 84.32 toks/s, output: 1349.15 toks/s]
Processed prompts:  74%|███████▍  | 380/512 [01:10<00:02, 64.76it/s, est. speed input: 85.98 toks/s, output: 1375.65 toks/s]
Processed prompts:  76%|███████▌  | 388/512 [01:10<00:02, 60.72it/s, est. speed input: 87.60 toks/s, output: 1401.54 toks/s]
Processed prompts:  77%|███████▋  | 395/512 [01:10<00:01, 59.59it/s, est. speed input: 89.02 toks/s, output: 1424.33 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [01:11<00:01, 57.54it/s, est. speed input: 90.43 toks/s, output: 1446.85 toks/s]
Processed prompts:  80%|███████▉  | 408/512 [01:11<00:01, 56.27it/s, est. speed input: 91.63 toks/s, output: 1466.10 toks/s]
Processed prompts:  81%|████████  | 414/512 [01:11<00:01, 55.39it/s, est. speed input: 92.83 toks/s, output: 1485.30 toks/s]
Processed prompts:  82%|████████▏ | 420/512 [01:11<00:01, 52.68it/s, est. speed input: 94.01 toks/s, output: 1504.10 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [01:11<00:01, 53.45it/s, est. speed input: 95.21 toks/s, output: 1523.29 toks/s]
Processed prompts:  84%|████████▍ | 432/512 [01:11<00:01, 54.38it/s, est. speed input: 96.40 toks/s, output: 1542.47 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [01:11<00:01, 53.18it/s, est. speed input: 97.58 toks/s, output: 1561.30 toks/s]
Processed prompts:  87%|████████▋ | 444/512 [01:12<00:01, 42.49it/s, est. speed input: 98.63 toks/s, output: 1578.07 toks/s]
Processed prompts:  88%|████████▊ | 449/512 [01:12<00:01, 44.08it/s, est. speed input: 99.60 toks/s, output: 1593.60 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [01:12<00:01, 34.83it/s, est. speed input: 100.40 toks/s, output: 1606.33 toks/s]
Processed prompts:  90%|████████▉ | 460/512 [01:12<00:01, 33.45it/s, est. speed input: 101.45 toks/s, output: 1623.19 toks/s]
Processed prompts:  91%|█████████ | 466/512 [01:12<00:01, 32.22it/s, est. speed input: 102.49 toks/s, output: 1639.81 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [01:12<00:01, 28.99it/s, est. speed input: 103.11 toks/s, output: 1649.70 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [01:13<00:01, 26.66it/s, est. speed input: 103.72 toks/s, output: 1659.49 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [01:13<00:01, 25.42it/s, est. speed input: 104.34 toks/s, output: 1669.42 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [01:13<00:01, 24.01it/s, est. speed input: 104.94 toks/s, output: 1679.01 toks/s]
Processed prompts:  95%|█████████▍| 485/512 [01:13<00:01, 19.29it/s, est. speed input: 105.22 toks/s, output: 1683.48 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [01:14<00:01, 16.65it/s, est. speed input: 105.50 toks/s, output: 1688.01 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [01:14<00:01, 15.31it/s, est. speed input: 105.68 toks/s, output: 1690.91 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [01:14<00:01, 14.46it/s, est. speed input: 105.87 toks/s, output: 1694.00 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [01:14<00:01, 13.84it/s, est. speed input: 106.07 toks/s, output: 1697.11 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [01:14<00:01, 13.39it/s, est. speed input: 106.27 toks/s, output: 1700.25 toks/s]
Processed prompts:  97%|█████████▋| 497/512 [01:28<00:01, 13.39it/s, est. speed input: 106.36 toks/s, output: 1701.81 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [01:30<00:29,  2.09s/it, est. speed input: 88.35 toks/s, output: 1413.62 toks/s] 
Processed prompts:  98%|█████████▊| 500/512 [01:30<00:18,  1.54s/it, est. speed input: 88.55 toks/s, output: 1416.77 toks/s]
Processed prompts:  99%|█████████▊| 505/512 [01:30<00:05,  1.25it/s, est. speed input: 89.27 toks/s, output: 1428.40 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:30<00:00,  1.25it/s, est. speed input: 90.42 toks/s, output: 1446.79 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:30<00:00,  5.65it/s, est. speed input: 90.42 toks/s, output: 1446.79 toks/s]
[rank0]:[W127 19:46:56.222558008 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

