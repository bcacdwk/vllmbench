
========== M=16 ==========
Time: 2026-01-25 18:40:45
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-INT8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:40:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:40:49 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=290604) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=290604) 
(EngineCore_DP0 pid=290604) 
(EngineCore_DP0 pid=290604) ================================================================
(EngineCore_DP0 pid=290604) Internal Triton PTX codegen error
(EngineCore_DP0 pid=290604) `ptxas` stderr:
(EngineCore_DP0 pid=290604) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=290604) 
(EngineCore_DP0 pid=290604) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpiapmhd0v.ptx -o /tmp/tmpiapmhd0v.ptx.o
(EngineCore_DP0 pid=290604) 
(EngineCore_DP0 pid=290604) 
(EngineCore_DP0 pid=290604) //
(EngineCore_DP0 pid=290604) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=290604) //
(EngineCore_DP0 pid=290604) 
(EngineCore_DP0 pid=290604) .version 8.7
(EngineCore_DP0 pid=290604) .target sm_121a
(EngineCore_DP0 pid=290604) .address_size 64
(EngineCore_DP0 pid=290604) 
(EngineCore_DP0 pid=290604) 	// .globl	_quant_slide_int8_kernel // -- Begin function _quant_slide_int8_kernel
(EngineCore_DP0 pid=290604) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=290604)                                         // @_quant_slide_int8_kernel
(EngineCore_DP0 pid=290604) .visible .entry _quant_slide_int8_kernel(
(EngineCore_DP0 pid=290604) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_0,
(EngineCore_DP0 pid=290604) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_1,
(EngineCore_DP0 pid=290604) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_2,
(EngineCore_DP0 pid=290604) 	.param .u32 _quant_slide_int8_kernel_param_3,
(EngineCore_DP0 pid=290604) 	.param .u32 _quant_slide_int8_kernel_param_4,
(EngineCore_DP0 pid=290604) 	.param .u32 _quant_slide_int8_kernel_param_5,
(EngineCore_DP0 pid=290604) 	.param .u32 _quant_slide_int8_kernel_param_6,
(EngineCore_DP0 pid=290604) 	.param .u32 _quant_slide_int8_kernel_param_7,
(EngineCore_DP0 pid=290604) 	.param .u32 _quant_slide_int8_kernel_param_8,
(EngineCore_DP0 pid=290604) 	.param .u32 _quant_slide_int8_kernel_param_9,
(EngineCore_DP0 pid=290604) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_10,
(EngineCore_DP0 pid=290604) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_11
(EngineCore_DP0 pid=290604) )
(EngineCore_DP0 pid=290604) .reqntid 1024
(EngineCore_DP0 pid=290604) {
(EngineCore_DP0 pid=290604) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=290604) 	.reg .b16 	%rs<20>;
(EngineCore_DP0 pid=290604) 	.reg .b32 	%r<119>;
(EngineCore_DP0 pid=290604) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=290604) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=290604) $L__func_begin0:
(EngineCore_DP0 pid=290604) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=290604) 
(EngineCore_DP0 pid=290604) // %bb.0:
(EngineCore_DP0 pid=290604) 	ld.param.b32 	%r19, [_quant_slide_int8_kernel_param_7];
(EngineCore_DP0 pid=290604) 	ld.param.b32 	%r18, [_quant_slide_int8_kernel_param_5];
(EngineCore_DP0 pid=290604) 	ld.param.b32 	%r17, [_quant_slide_int8_kernel_param_4];
(EngineCore_DP0 pid=290604) 	ld.param.b64 	%rd3, [_quant_slide_int8_kernel_param_2];
(EngineCore_DP0 pid=290604) 	ld.param.b64 	%rd4, [_quant_slide_int8_kernel_param_0];
(EngineCore_DP0 pid=290604) $L__tmp0:
(EngineCore_DP0 pid=290604) 	.loc	1 288 24                        // quant_slide_tuned_Llama3.2-1B.py:288:24
(EngineCore_DP0 pid=290604) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=290604) 	ld.param.b32 	%r21, [_quant_slide_int8_kernel_param_8];
(EngineCore_DP0 pid=290604) 	.loc	1 293 26                        // quant_slide_tuned_Llama3.2-1B.py:293:26
(EngineCore_DP0 pid=290604) 	mul.lo.s32 	%r22, %r21, %r1;
(EngineCore_DP0 pid=290604) 	.loc	1 293 20                        // quant_slide_tuned_Llama3.2-1B.py:293:20
(EngineCore_DP0 pid=290604) 	mad.wide.s32 	%rd1, %r22, 2, %rd4;
(EngineCore_DP0 pid=290604) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=290604) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=290604) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=290604) 	setp.lt.s32 	%p1, %r18, 1;
(EngineCore_DP0 pid=290604) 	mov.b32 	%r117, 0f2B8CBCCC;
(EngineCore_DP0 pid=290604) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=290604) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=290604) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=290604) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=290604) 	shl.b32 	%r3, %r2, 2;
(EngineCore_DP0 pid=290604) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=290604) 	shr.u32 	%r31, %r2, 3;
(EngineCore_DP0 pid=290604) 	and.b32 	%r32, %r31, 124;
(EngineCore_DP0 pid=290604) 	mov.b32 	%r33, global_smem;
(EngineCore_DP0 pid=290604) 	add.s32 	%r39, %r33, %r32;
(EngineCore_DP0 pid=290604) 	add.s32 	%r42, %r33, %r3;
(EngineCore_DP0 pid=290604) 	mov.b32 	%r37, 0;
(EngineCore_DP0 pid=290604) 	mov.b32 	%r115, 0f00000000;
(EngineCore_DP0 pid=290604) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=290604) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=290604) 	mov.b32 	%r116, %r37;
(EngineCore_DP0 pid=290604) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=290604) 	.loc	1 300 22                        // quant_slide_tuned_Llama3.2-1B.py:300:22
(EngineCore_DP0 pid=290604) 	add.s32 	%r45, %r3, %r116;
(EngineCore_DP0 pid=290604) 	setp.lt.s32 	%p2, %r45, %r17;
(EngineCore_DP0 pid=290604) 	.loc	1 301 29                        // quant_slide_tuned_Llama3.2-1B.py:301:29
(EngineCore_DP0 pid=290604) 	mad.wide.s32 	%rd6, %r45, 2, %rd1;
(EngineCore_DP0 pid=290604) 	.loc	1 301 21                        // quant_slide_tuned_Llama3.2-1B.py:301:21
(EngineCore_DP0 pid=290604) 	// begin inline asm
(EngineCore_DP0 pid=290604) 	mov.u32 %r35, %r37;
(EngineCore_DP0 pid=290604) 	mov.u32 %r36, %r37;
(EngineCore_DP0 pid=290604) 	@%p2 ld.global.v2.b32 { %r35, %r36 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=290604) 	// end inline asm
(EngineCore_DP0 pid=290604) 	mov.b32 	{%rs1, %rs2}, %r35;
(EngineCore_DP0 pid=290604) 	mov.b32 	{%rs3, %rs4}, %r36;
(EngineCore_DP0 pid=290604) 	.loc	1 302 50                        // quant_slide_tuned_Llama3.2-1B.py:302:50
(EngineCore_DP0 pid=290604) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=290604) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=290604) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=290604) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=290604) $L__tmp1:
(EngineCore_DP0 pid=290604) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	bar.sync 	0;
(EngineCore_DP0 pid=290604) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=290604) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=290604) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=290604) 	cvt.f32.bf16 	%r46, %rs11;
(EngineCore_DP0 pid=290604) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	shfl.sync.bfly.b32 	%r47, %r46, 16, 31, -1;
(EngineCore_DP0 pid=290604) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	max.f32 	%r48, %r46, %r47;
(EngineCore_DP0 pid=290604) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	shfl.sync.bfly.b32 	%r49, %r48, 8, 31, -1;
(EngineCore_DP0 pid=290604) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	max.f32 	%r50, %r48, %r49;
(EngineCore_DP0 pid=290604) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	shfl.sync.bfly.b32 	%r51, %r50, 4, 31, -1;
(EngineCore_DP0 pid=290604) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	max.f32 	%r52, %r50, %r51;
(EngineCore_DP0 pid=290604) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	shfl.sync.bfly.b32 	%r53, %r52, 2, 31, -1;
(EngineCore_DP0 pid=290604) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	max.f32 	%r54, %r52, %r53;
(EngineCore_DP0 pid=290604) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	shfl.sync.bfly.b32 	%r55, %r54, 1, 31, -1;
(EngineCore_DP0 pid=290604) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	max.f32 	%r40, %r54, %r55;
(EngineCore_DP0 pid=290604) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	// begin inline asm
(EngineCore_DP0 pid=290604) 	@%p3 st.shared.b32 [ %r39 + 0 ], %r40;
(EngineCore_DP0 pid=290604) 	// end inline asm
(EngineCore_DP0 pid=290604) 	bar.sync 	0;
(EngineCore_DP0 pid=290604) 	// begin inline asm
(EngineCore_DP0 pid=290604) 	@%p4 ld.shared.b32 %r41, [ %r42 + 0 ];
(EngineCore_DP0 pid=290604) 	// end inline asm
(EngineCore_DP0 pid=290604) 	shfl.sync.bfly.b32 	%r56, %r41, 16, 31, -1;
(EngineCore_DP0 pid=290604) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	max.f32 	%r57, %r41, %r56;
(EngineCore_DP0 pid=290604) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	shfl.sync.bfly.b32 	%r58, %r57, 8, 31, -1;
(EngineCore_DP0 pid=290604) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	max.f32 	%r59, %r57, %r58;
(EngineCore_DP0 pid=290604) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	shfl.sync.bfly.b32 	%r60, %r59, 4, 31, -1;
(EngineCore_DP0 pid=290604) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	max.f32 	%r61, %r59, %r60;
(EngineCore_DP0 pid=290604) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	shfl.sync.bfly.b32 	%r62, %r61, 2, 31, -1;
(EngineCore_DP0 pid=290604) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	max.f32 	%r63, %r61, %r62;
(EngineCore_DP0 pid=290604) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	shfl.sync.bfly.b32 	%r64, %r63, 1, 31, -1;
(EngineCore_DP0 pid=290604) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	max.f32 	%r44, %r63, %r64;
(EngineCore_DP0 pid=290604) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=290604) 	// begin inline asm
(EngineCore_DP0 pid=290604) 	@%p19 st.shared.b32 [ %r42 + 0 ], %r44;
(EngineCore_DP0 pid=290604) 	// end inline asm
(EngineCore_DP0 pid=290604) 	bar.sync 	0;
(EngineCore_DP0 pid=290604) 	ld.shared.b32 	%r65, [global_smem];
(EngineCore_DP0 pid=290604) $L__tmp2:
(EngineCore_DP0 pid=290604) 	.loc	1 302 36                        // quant_slide_tuned_Llama3.2-1B.py:302:36
(EngineCore_DP0 pid=290604) 	max.f32 	%r115, %r115, %r65;
(EngineCore_DP0 pid=290604) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=290604) 	add.s32 	%r116, %r116, 4096;
(EngineCore_DP0 pid=290604) 	setp.lt.s32 	%p6, %r116, %r18;
(EngineCore_DP0 pid=290604) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=290604) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=290604) 	.loc	1 304 32                        // quant_slide_tuned_Llama3.2-1B.py:304:32
(EngineCore_DP0 pid=290604) 	max.f32 	%r117, %r115, 0f2B8CBCCC;
(EngineCore_DP0 pid=290604) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=290604) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=290604) 	mov.b32 	%r67, 0f42FE0000;
(EngineCore_DP0 pid=290604) 	.loc	1 305 32                        // quant_slide_tuned_Llama3.2-1B.py:305:32
(EngineCore_DP0 pid=290604) 	div.full.f32 	%r68, %r117, %r67;
(EngineCore_DP0 pid=290604) 	.loc	1 305 42                        // quant_slide_tuned_Llama3.2-1B.py:305:42
(EngineCore_DP0 pid=290604) 	max.f32 	%r66, %r68, 0f37810204;
(EngineCore_DP0 pid=290604) 	.loc	1 307 25                        // quant_slide_tuned_Llama3.2-1B.py:307:25
(EngineCore_DP0 pid=290604) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=290604) 	.loc	1 307 30                        // quant_slide_tuned_Llama3.2-1B.py:307:30
(EngineCore_DP0 pid=290604) 	// begin inline asm
(EngineCore_DP0 pid=290604) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r66 };
(EngineCore_DP0 pid=290604) 	// end inline asm
(EngineCore_DP0 pid=290604) 	.loc	1 310 29                        // quant_slide_tuned_Llama3.2-1B.py:310:29
(EngineCore_DP0 pid=290604) 	shl.b32 	%r14, %r19, 1;
(EngineCore_DP0 pid=290604) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=290604) 	setp.lt.s32 	%p8, %r14, 1;
(EngineCore_DP0 pid=290604) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=290604) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=290604) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=290604) 	ld.param.b32 	%r23, [_quant_slide_int8_kernel_param_9];
(EngineCore_DP0 pid=290604) 	shr.s32 	%r24, %r23, 31;
(EngineCore_DP0 pid=290604) 	shr.u32 	%r25, %r24, 30;
(EngineCore_DP0 pid=290604) 	add.s32 	%r26, %r23, %r25;
(EngineCore_DP0 pid=290604) 	shr.s32 	%r27, %r26, 2;
(EngineCore_DP0 pid=290604) 	ld.param.b64 	%rd5, [_quant_slide_int8_kernel_param_1];
(EngineCore_DP0 pid=290604) 	mul.lo.s32 	%r28, %r27, %r1;
(EngineCore_DP0 pid=290604) 	mad.wide.s32 	%rd2, %r28, 4, %rd5;
(EngineCore_DP0 pid=290604) 	div.full.f32 	%r13, %r67, %r117;
(EngineCore_DP0 pid=290604) 	mov.b32 	%r118, 0;
(EngineCore_DP0 pid=290604) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=290604)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=290604) 	.loc	1 314 30                        // quant_slide_tuned_Llama3.2-1B.py:314:30
(EngineCore_DP0 pid=290604) 	add.s32 	%r72, %r2, %r118;
(EngineCore_DP0 pid=290604) 	setp.lt.s32 	%p13, %r72, %r14;
(EngineCore_DP0 pid=290604) 	.loc	1 317 24                        // quant_slide_tuned_Llama3.2-1B.py:317:24
(EngineCore_DP0 pid=290604) 	shr.u32 	%r73, %r72, 31;
(EngineCore_DP0 pid=290604) 	add.s32 	%r74, %r72, %r73;
(EngineCore_DP0 pid=290604) 	shr.u32 	%r75, %r74, 1;
(EngineCore_DP0 pid=290604) 	.loc	1 318 23                        // quant_slide_tuned_Llama3.2-1B.py:318:23
(EngineCore_DP0 pid=290604) 	and.b32 	%r76, %r74, 2147483646;
(EngineCore_DP0 pid=290604) 	sub.s32 	%r77, %r72, %r76;
(EngineCore_DP0 pid=290604) 	.loc	1 319 30                        // quant_slide_tuned_Llama3.2-1B.py:319:30
(EngineCore_DP0 pid=290604) 	shl.b32 	%r78, %r77, 1;
(EngineCore_DP0 pid=290604) 	.loc	1 319 26                        // quant_slide_tuned_Llama3.2-1B.py:319:26
(EngineCore_DP0 pid=290604) 	mad.lo.s32 	%r79, %r75, 6, %r78;
(EngineCore_DP0 pid=290604) 	.loc	1 322 53                        // quant_slide_tuned_Llama3.2-1B.py:322:53
(EngineCore_DP0 pid=290604) 	setp.lt.s32 	%p14, %r79, %r17;
(EngineCore_DP0 pid=290604) 	.loc	1 322 37                        // quant_slide_tuned_Llama3.2-1B.py:322:37
(EngineCore_DP0 pid=290604) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=290604) 	.loc	1 321 29                        // quant_slide_tuned_Llama3.2-1B.py:321:29
(EngineCore_DP0 pid=290604) 	mad.wide.s32 	%rd8, %r79, 2, %rd1;
(EngineCore_DP0 pid=290604) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=290604) 	.loc	1 321 21                        // quant_slide_tuned_Llama3.2-1B.py:321:21
(EngineCore_DP0 pid=290604) 	// begin inline asm
(EngineCore_DP0 pid=290604) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=290604) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=290604) 	// end inline asm
(EngineCore_DP0 pid=290604) 	.loc	1 322 79                        // quant_slide_tuned_Llama3.2-1B.py:322:79
(EngineCore_DP0 pid=290604) 	cvt.f32.bf16 	%r80, %rs12;
(EngineCore_DP0 pid=290604) 	.loc	1 324 48                        // quant_slide_tuned_Llama3.2-1B.py:324:48
(EngineCore_DP0 pid=290604) 	or.b32 	%r81, %r79, 1;
(EngineCore_DP0 pid=290604) 	.loc	1 324 53                        // quant_slide_tuned_Llama3.2-1B.py:324:53
(EngineCore_DP0 pid=290604) 	setp.lt.s32 	%p15, %r81, %r17;
(EngineCore_DP0 pid=290604) 	.loc	1 324 37                        // quant_slide_tuned_Llama3.2-1B.py:324:37
(EngineCore_DP0 pid=290604) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=290604) 	.loc	1 323 39                        // quant_slide_tuned_Llama3.2-1B.py:323:39
(EngineCore_DP0 pid=290604) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=290604) 	.loc	1 323 21                        // quant_slide_tuned_Llama3.2-1B.py:323:21
(EngineCore_DP0 pid=290604) 	// begin inline asm
(EngineCore_DP0 pid=290604) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=290604) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=290604) 	// end inline asm
(EngineCore_DP0 pid=290604) 	.loc	1 324 79                        // quant_slide_tuned_Llama3.2-1B.py:324:79
(EngineCore_DP0 pid=290604) 	cvt.f32.bf16 	%r82, %rs14;
(EngineCore_DP0 pid=290604) 	.loc	1 326 48                        // quant_slide_tuned_Llama3.2-1B.py:326:48
(EngineCore_DP0 pid=290604) 	add.s32 	%r83, %r79, 2;
(EngineCore_DP0 pid=290604) 	.loc	1 326 53                        // quant_slide_tuned_Llama3.2-1B.py:326:53
(EngineCore_DP0 pid=290604) 	setp.lt.s32 	%p16, %r83, %r17;
(EngineCore_DP0 pid=290604) 	.loc	1 326 37                        // quant_slide_tuned_Llama3.2-1B.py:326:37
(EngineCore_DP0 pid=290604) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=290604) 	.loc	1 325 39                        // quant_slide_tuned_Llama3.2-1B.py:325:39
(EngineCore_DP0 pid=290604) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=290604) 	.loc	1 325 21                        // quant_slide_tuned_Llama3.2-1B.py:325:21
(EngineCore_DP0 pid=290604) 	// begin inline asm
(EngineCore_DP0 pid=290604) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=290604) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=290604) 	// end inline asm
(EngineCore_DP0 pid=290604) 	.loc	1 326 79                        // quant_slide_tuned_Llama3.2-1B.py:326:79
(EngineCore_DP0 pid=290604) 	cvt.f32.bf16 	%r84, %rs16;
(EngineCore_DP0 pid=290604) 	.loc	1 328 48                        // quant_slide_tuned_Llama3.2-1B.py:328:48
(EngineCore_DP0 pid=290604) 	add.s32 	%r85, %r79, 3;
(EngineCore_DP0 pid=290604) 	.loc	1 328 53                        // quant_slide_tuned_Llama3.2-1B.py:328:53
(EngineCore_DP0 pid=290604) 	setp.lt.s32 	%p17, %r85, %r17;
(EngineCore_DP0 pid=290604) 	.loc	1 328 37                        // quant_slide_tuned_Llama3.2-1B.py:328:37
(EngineCore_DP0 pid=290604) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=290604) 	.loc	1 327 39                        // quant_slide_tuned_Llama3.2-1B.py:327:39
(EngineCore_DP0 pid=290604) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=290604) 	.loc	1 327 21                        // quant_slide_tuned_Llama3.2-1B.py:327:21
(EngineCore_DP0 pid=290604) 	// begin inline asm
(EngineCore_DP0 pid=290604) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=290604) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=290604) 	// end inline asm
(EngineCore_DP0 pid=290604) 	.loc	1 328 79                        // quant_slide_tuned_Llama3.2-1B.py:328:79
(EngineCore_DP0 pid=290604) 	cvt.f32.bf16 	%r86, %rs18;
(EngineCore_DP0 pid=290604) 	.loc	1 330 56                        // quant_slide_tuned_Llama3.2-1B.py:330:56
(EngineCore_DP0 pid=290604) 	mul.f32 	%r87, %r13, %r80;
(EngineCore_DP0 pid=290604) 	.loc	1 330 51                        // quant_slide_tuned_Llama3.2-1B.py:330:51
(EngineCore_DP0 pid=290604) 	cvt.rni.f32.f32 	%r88, %r87;
(EngineCore_DP0 pid=290604) 	.loc	1 330 76                        // quant_slide_tuned_Llama3.2-1B.py:330:76
(EngineCore_DP0 pid=290604) 	max.f32 	%r89, %r88, 0fC3000000;
(EngineCore_DP0 pid=290604) 	min.f32 	%r90, %r89, 0f42FE0000;
(EngineCore_DP0 pid=290604) 	.loc	1 330 86                        // quant_slide_tuned_Llama3.2-1B.py:330:86
(EngineCore_DP0 pid=290604) 	cvt.rzi.s32.f32 	%r91, %r90;
(EngineCore_DP0 pid=290604) 	.loc	1 330 98                        // quant_slide_tuned_Llama3.2-1B.py:330:98
(EngineCore_DP0 pid=290604) 	and.b32 	%r92, %r91, 255;
(EngineCore_DP0 pid=290604) 	.loc	1 331 56                        // quant_slide_tuned_Llama3.2-1B.py:331:56
(EngineCore_DP0 pid=290604) 	mul.f32 	%r93, %r13, %r82;
(EngineCore_DP0 pid=290604) 	.loc	1 331 51                        // quant_slide_tuned_Llama3.2-1B.py:331:51
(EngineCore_DP0 pid=290604) 	cvt.rni.f32.f32 	%r94, %r93;
(EngineCore_DP0 pid=290604) 	.loc	1 332 56                        // quant_slide_tuned_Llama3.2-1B.py:332:56
(EngineCore_DP0 pid=290604) 	mul.f32 	%r95, %r13, %r84;
(EngineCore_DP0 pid=290604) 	.loc	1 332 51                        // quant_slide_tuned_Llama3.2-1B.py:332:51
(EngineCore_DP0 pid=290604) 	cvt.rni.f32.f32 	%r96, %r95;
(EngineCore_DP0 pid=290604) 	.loc	1 333 56                        // quant_slide_tuned_Llama3.2-1B.py:333:56
(EngineCore_DP0 pid=290604) 	mul.f32 	%r97, %r13, %r86;
(EngineCore_DP0 pid=290604) 	.loc	1 333 51                        // quant_slide_tuned_Llama3.2-1B.py:333:51
(EngineCore_DP0 pid=290604) 	cvt.rni.f32.f32 	%r98, %r97;
(EngineCore_DP0 pid=290604) 	.loc	1 333 76                        // quant_slide_tuned_Llama3.2-1B.py:333:76
(EngineCore_DP0 pid=290604) 	max.f32 	%r99, %r98, 0fC3000000;
(EngineCore_DP0 pid=290604) 	min.f32 	%r100, %r99, 0f42FE0000;
(EngineCore_DP0 pid=290604) 	.loc	1 333 86                        // quant_slide_tuned_Llama3.2-1B.py:333:86
(EngineCore_DP0 pid=290604) 	cvt.rzi.s32.f32 	%r101, %r100;
(EngineCore_DP0 pid=290604) 	.loc	1 331 76                        // quant_slide_tuned_Llama3.2-1B.py:331:76
(EngineCore_DP0 pid=290604) 	max.f32 	%r102, %r96, 0fC3000000;
(EngineCore_DP0 pid=290604) 	max.f32 	%r103, %r94, 0fC3000000;
(EngineCore_DP0 pid=290604) 	min.f32 	%r104, %r103, 0f42FE0000;
(EngineCore_DP0 pid=290604) 	min.f32 	%r105, %r102, 0f42FE0000;
(EngineCore_DP0 pid=290604) 	.loc	1 331 86                        // quant_slide_tuned_Llama3.2-1B.py:331:86
(EngineCore_DP0 pid=290604) 	cvt.rzi.s32.f32 	%r106, %r105;
(EngineCore_DP0 pid=290604) 	cvt.rzi.s32.f32 	%r107, %r104;
(EngineCore_DP0 pid=290604) 	.loc	1 335 30                        // quant_slide_tuned_Llama3.2-1B.py:335:30
(EngineCore_DP0 pid=290604) 	shl.b32 	%r108, %r107, 8;
(EngineCore_DP0 pid=290604) 	shl.b32 	%r109, %r106, 16;
(EngineCore_DP0 pid=290604) 	and.b32 	%r110, %r109, 16711680;
(EngineCore_DP0 pid=290604) 	and.b32 	%r111, %r108, 65280;
(EngineCore_DP0 pid=290604) 	.loc	1 335 24                        // quant_slide_tuned_Llama3.2-1B.py:335:24
(EngineCore_DP0 pid=290604) 	or.b32 	%r112, %r111, %r92;
(EngineCore_DP0 pid=290604) 	.loc	1 335 36                        // quant_slide_tuned_Llama3.2-1B.py:335:36
(EngineCore_DP0 pid=290604) 	or.b32 	%r113, %r112, %r110;
(EngineCore_DP0 pid=290604) 	.loc	1 335 55                        // quant_slide_tuned_Llama3.2-1B.py:335:55
(EngineCore_DP0 pid=290604) 	shl.b32 	%r114, %r101, 24;
(EngineCore_DP0 pid=290604) 	.loc	1 335 49                        // quant_slide_tuned_Llama3.2-1B.py:335:49
(EngineCore_DP0 pid=290604) 	or.b32 	%r70, %r113, %r114;
(EngineCore_DP0 pid=290604) 	.loc	1 336 29                        // quant_slide_tuned_Llama3.2-1B.py:336:29
(EngineCore_DP0 pid=290604) 	mad.wide.s32 	%rd12, %r72, 4, %rd2;
(EngineCore_DP0 pid=290604) 	.loc	1 336 39                        // quant_slide_tuned_Llama3.2-1B.py:336:39
(EngineCore_DP0 pid=290604) 	// begin inline asm
(EngineCore_DP0 pid=290604) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r70 };
(EngineCore_DP0 pid=290604) 	// end inline asm
(EngineCore_DP0 pid=290604) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=290604) 	add.s32 	%r118, %r118, 1024;
(EngineCore_DP0 pid=290604) 	setp.lt.s32 	%p18, %r118, %r14;
(EngineCore_DP0 pid=290604) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=290604) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=290604) 	.loc	1 312 4                         // quant_slide_tuned_Llama3.2-1B.py:312:4
(EngineCore_DP0 pid=290604) 	ret;
(EngineCore_DP0 pid=290604) $L__tmp3:
(EngineCore_DP0 pid=290604) $L__func_end0:
(EngineCore_DP0 pid=290604)                                         // -- End function
(EngineCore_DP0 pid=290604) }
(EngineCore_DP0 pid=290604) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=290604) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=290604) 	.section	.debug_abbrev
(EngineCore_DP0 pid=290604) 	{
(EngineCore_DP0 pid=290604) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=290604) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=290604) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=290604) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=290604) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=290604) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=290604) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=290604) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=290604) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=290604) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=290604) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=290604) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=290604) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=290604) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=290604) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=290604) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=290604) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=290604) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=290604) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=290604) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=290604) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=290604) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=290604) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=290604) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=290604) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=290604) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=290604) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=290604) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=290604) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=290604) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=290604) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=290604) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=290604) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=290604) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=290604) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=290604) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=290604) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=290604) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=290604) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=290604) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=290604) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=290604) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=290604) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=290604) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=290604) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=290604) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=290604) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=290604) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=290604) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=290604) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=290604) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=290604) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=290604) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=290604) 	}
(EngineCore_DP0 pid=290604) 	.section	.debug_info
(EngineCore_DP0 pid=290604) 	{
(EngineCore_DP0 pid=290604) .b32 224                                // Length of Unit
(EngineCore_DP0 pid=290604) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=290604) .b8 0
(EngineCore_DP0 pid=290604) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=290604) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=290604) .b8 1                                   // Abbrev [1] 0xb:0xd9 DW_TAG_compile_unit
(EngineCore_DP0 pid=290604) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=290604) .b8 114
(EngineCore_DP0 pid=290604) .b8 105
(EngineCore_DP0 pid=290604) .b8 116
(EngineCore_DP0 pid=290604) .b8 111
(EngineCore_DP0 pid=290604) .b8 110
(EngineCore_DP0 pid=290604) .b8 0
(EngineCore_DP0 pid=290604) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=290604) .b8 0
(EngineCore_DP0 pid=290604) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=290604) .b8 117
(EngineCore_DP0 pid=290604) .b8 97
(EngineCore_DP0 pid=290604) .b8 110
(EngineCore_DP0 pid=290604) .b8 116
(EngineCore_DP0 pid=290604) .b8 95
(EngineCore_DP0 pid=290604) .b8 115
(EngineCore_DP0 pid=290604) .b8 108
(EngineCore_DP0 pid=290604) .b8 105
(EngineCore_DP0 pid=290604) .b8 100
(EngineCore_DP0 pid=290604) .b8 101
(EngineCore_DP0 pid=290604) .b8 95
(EngineCore_DP0 pid=290604) .b8 116
(EngineCore_DP0 pid=290604) .b8 117
(EngineCore_DP0 pid=290604) .b8 110
(EngineCore_DP0 pid=290604) .b8 101
(EngineCore_DP0 pid=290604) .b8 100
(EngineCore_DP0 pid=290604) .b8 95
(EngineCore_DP0 pid=290604) .b8 76
(EngineCore_DP0 pid=290604) .b8 108
(EngineCore_DP0 pid=290604) .b8 97
(EngineCore_DP0 pid=290604) .b8 109
(EngineCore_DP0 pid=290604) .b8 97
(EngineCore_DP0 pid=290604) .b8 51
(EngineCore_DP0 pid=290604) .b8 46
(EngineCore_DP0 pid=290604) .b8 50
(EngineCore_DP0 pid=290604) .b8 45
(EngineCore_DP0 pid=290604) .b8 49
(EngineCore_DP0 pid=290604) .b8 66
(EngineCore_DP0 pid=290604) .b8 46
(EngineCore_DP0 pid=290604) .b8 112
(EngineCore_DP0 pid=290604) .b8 121
(EngineCore_DP0 pid=290604) .b8 0
(EngineCore_DP0 pid=290604) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=290604) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=290604) .b8 114
(EngineCore_DP0 pid=290604) .b8 111
(EngineCore_DP0 pid=290604) .b8 111
(EngineCore_DP0 pid=290604) .b8 116
(EngineCore_DP0 pid=290604) .b8 47
(EngineCore_DP0 pid=290604) .b8 118
(EngineCore_DP0 pid=290604) .b8 108
(EngineCore_DP0 pid=290604) .b8 108
(EngineCore_DP0 pid=290604) .b8 109
(EngineCore_DP0 pid=290604) .b8 98
(EngineCore_DP0 pid=290604) .b8 101
(EngineCore_DP0 pid=290604) .b8 110
(EngineCore_DP0 pid=290604) .b8 99
(EngineCore_DP0 pid=290604) .b8 104
(EngineCore_DP0 pid=290604) .b8 47
(EngineCore_DP0 pid=290604) .b8 115
(EngineCore_DP0 pid=290604) .b8 108
(EngineCore_DP0 pid=290604) .b8 105
(EngineCore_DP0 pid=290604) .b8 100
(EngineCore_DP0 pid=290604) .b8 101
(EngineCore_DP0 pid=290604) .b8 115
(EngineCore_DP0 pid=290604) .b8 112
(EngineCore_DP0 pid=290604) .b8 97
(EngineCore_DP0 pid=290604) .b8 114
(EngineCore_DP0 pid=290604) .b8 115
(EngineCore_DP0 pid=290604) .b8 101
(EngineCore_DP0 pid=290604) .b8 47
(EngineCore_DP0 pid=290604) .b8 99
(EngineCore_DP0 pid=290604) .b8 115
(EngineCore_DP0 pid=290604) .b8 114
(EngineCore_DP0 pid=290604) .b8 99
(EngineCore_DP0 pid=290604) .b8 47
(EngineCore_DP0 pid=290604) .b8 102
(EngineCore_DP0 pid=290604) .b8 117
(EngineCore_DP0 pid=290604) .b8 115
(EngineCore_DP0 pid=290604) .b8 101
(EngineCore_DP0 pid=290604) .b8 100
(EngineCore_DP0 pid=290604) .b8 95
(EngineCore_DP0 pid=290604) .b8 113
(EngineCore_DP0 pid=290604) .b8 117
(EngineCore_DP0 pid=290604) .b8 97
(EngineCore_DP0 pid=290604) .b8 110
(EngineCore_DP0 pid=290604) .b8 116
(EngineCore_DP0 pid=290604) .b8 95
(EngineCore_DP0 pid=290604) .b8 115
(EngineCore_DP0 pid=290604) .b8 108
(EngineCore_DP0 pid=290604) .b8 105
(EngineCore_DP0 pid=290604) .b8 100
(EngineCore_DP0 pid=290604) .b8 101
(EngineCore_DP0 pid=290604) .b8 95
(EngineCore_DP0 pid=290604) .b8 116
(EngineCore_DP0 pid=290604) .b8 114
(EngineCore_DP0 pid=290604) .b8 105
(EngineCore_DP0 pid=290604) .b8 116
(EngineCore_DP0 pid=290604) .b8 111
(EngineCore_DP0 pid=290604) .b8 110
(EngineCore_DP0 pid=290604) .b8 47
(EngineCore_DP0 pid=290604) .b8 98
(EngineCore_DP0 pid=290604) .b8 117
(EngineCore_DP0 pid=290604) .b8 105
(EngineCore_DP0 pid=290604) .b8 108
(EngineCore_DP0 pid=290604) .b8 100
(EngineCore_DP0 pid=290604) .b8 47
(EngineCore_DP0 pid=290604) .b8 71
(EngineCore_DP0 pid=290604) .b8 66
(EngineCore_DP0 pid=290604) .b8 49
(EngineCore_DP0 pid=290604) .b8 48
(EngineCore_DP0 pid=290604) .b8 95
(EngineCore_DP0 pid=290604) .b8 99
(EngineCore_DP0 pid=290604) .b8 99
(EngineCore_DP0 pid=290604) .b8 49
(EngineCore_DP0 pid=290604) .b8 50
(EngineCore_DP0 pid=290604) .b8 49
(EngineCore_DP0 pid=290604) .b8 95
(EngineCore_DP0 pid=290604) .b8 112
(EngineCore_DP0 pid=290604) .b8 121
(EngineCore_DP0 pid=290604) .b8 51
(EngineCore_DP0 pid=290604) .b8 49
(EngineCore_DP0 pid=290604) .b8 50
(EngineCore_DP0 pid=290604) .b8 95
(EngineCore_DP0 pid=290604) .b8 99
(EngineCore_DP0 pid=290604) .b8 117
(EngineCore_DP0 pid=290604) .b8 49
(EngineCore_DP0 pid=290604) .b8 50
(EngineCore_DP0 pid=290604) .b8 57
(EngineCore_DP0 pid=290604) .b8 95
(EngineCore_DP0 pid=290604) .b8 97
(EngineCore_DP0 pid=290604) .b8 97
(EngineCore_DP0 pid=290604) .b8 114
(EngineCore_DP0 pid=290604) .b8 99
(EngineCore_DP0 pid=290604) .b8 104
(EngineCore_DP0 pid=290604) .b8 54
(EngineCore_DP0 pid=290604) .b8 52
(EngineCore_DP0 pid=290604) .b8 0
(EngineCore_DP0 pid=290604) .b8 2                                   // Abbrev [2] 0x99:0x1b DW_TAG_subprogram
(EngineCore_DP0 pid=290604) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=290604) .b8 113
(EngineCore_DP0 pid=290604) .b8 117
(EngineCore_DP0 pid=290604) .b8 97
(EngineCore_DP0 pid=290604) .b8 110
(EngineCore_DP0 pid=290604) .b8 116
(EngineCore_DP0 pid=290604) .b8 95
(EngineCore_DP0 pid=290604) .b8 115
(EngineCore_DP0 pid=290604) .b8 108
(EngineCore_DP0 pid=290604) .b8 105
(EngineCore_DP0 pid=290604) .b8 100
(EngineCore_DP0 pid=290604) .b8 101
(EngineCore_DP0 pid=290604) .b8 95
(EngineCore_DP0 pid=290604) .b8 105
(EngineCore_DP0 pid=290604) .b8 110
(EngineCore_DP0 pid=290604) .b8 116
(EngineCore_DP0 pid=290604) .b8 56
(EngineCore_DP0 pid=290604) .b8 95
(EngineCore_DP0 pid=290604) .b8 107
(EngineCore_DP0 pid=290604) .b8 101
(EngineCore_DP0 pid=290604) .b8 114
(EngineCore_DP0 pid=290604) .b8 110
(EngineCore_DP0 pid=290604) .b8 101
(EngineCore_DP0 pid=290604) .b8 108
(EngineCore_DP0 pid=290604) .b8 0
(EngineCore_DP0 pid=290604) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=290604) .b8 3                                   // Abbrev [3] 0xb4:0x2f DW_TAG_subprogram
(EngineCore_DP0 pid=290604) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=290604) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=290604) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=290604) .b8 4                                   // Abbrev [4] 0xc9:0x19 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=290604) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=290604) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=290604) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=290604) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=290604) .b8 46                                  // DW_AT_call_line
(EngineCore_DP0 pid=290604) .b8 1
(EngineCore_DP0 pid=290604) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=290604) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=290604) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=290604) 	}
(EngineCore_DP0 pid=290604) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=290604) 
(EngineCore_DP0 pid=290604) ================================================================
(EngineCore_DP0 pid=290604) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=290604) 
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpiapmhd0v.ptx', '-o', '/tmp/tmpiapmhd0v.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866] 
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866] 
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866] 
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpiapmhd0v.ptx -o /tmp/tmpiapmhd0v.ptx.o
(EngineCore_DP0 pid=290604) ERROR 01-25 18:41:04 [core.py:866] 

STDERR:
[2026-01-25 18:40:48] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:40:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:40:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:40:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:40:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:40:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:40:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:40:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:40:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:40:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:40:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:40:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:40:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:40:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:40:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:40:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:40:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:40:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:40:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:40:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:40:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:40:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:40:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:40:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:40:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:40:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:40:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:40:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=290604) [2026-01-25 18:40:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=290604) [2026-01-25 18:40:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=290604) [2026-01-25 18:40:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=290604) [2026-01-25 18:40:53] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=290604) [2026-01-25 18:40:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=290604) [2026-01-25 18:40:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=290604) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=290604) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.24s/it]
(EngineCore_DP0 pid=290604) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.24s/it]
(EngineCore_DP0 pid=290604) 
(EngineCore_DP0 pid=290604) [2026-01-25 18:41:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=290604) [2026-01-25 18:41:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=290604) [2026-01-25 18:41:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=290604) [2026-01-25 18:41:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=290604) [2026-01-25 18:41:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=290604) [2026-01-25 18:41:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=290604) [2026-01-25 18:41:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=290604) [2026-01-25 18:41:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=290604) Process EngineCore_DP0:
(EngineCore_DP0 pid=290604) Traceback (most recent call last):
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=290604)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=290604)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=290604)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=290604) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpiapmhd0v.ptx', '-o', '/tmp/tmpiapmhd0v.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=290604) 
(EngineCore_DP0 pid=290604) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=290604) 
(EngineCore_DP0 pid=290604) Traceback (most recent call last):
(EngineCore_DP0 pid=290604)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=290604)     self.run()
(EngineCore_DP0 pid=290604)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=290604)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=290604)     raise e
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=290604)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=290604)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=290604)     super().__init__(
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=290604)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=290604)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=290604)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=290604)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=290604)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=290604)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=290604)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=290604)     return func(*args, **kwargs)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=290604)     return func(*args, **kwargs)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=290604)     self.model_runner.profile_run()
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=290604)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=290604)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=290604)     return func(*args, **kwargs)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=290604)     outputs = self.model(
(EngineCore_DP0 pid=290604)               ^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=290604)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=290604)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=290604)     model_output = self.model(
(EngineCore_DP0 pid=290604)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=290604)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=290604)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=290604)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=290604)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=290604)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=290604)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=290604)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=290604)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=290604)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=290604)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=290604)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=290604)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=290604)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=290604)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=290604)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=290604)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=290604)     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=290604)     return self._linear_fn(
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=290604)     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=290604)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=290604)     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=290604)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=290604)     return fn(input, L)
(EngineCore_DP0 pid=290604)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=290604)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=290604)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=290604)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=290604)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=290604)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=290604)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=290604)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=290604)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=290604)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=290604)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=290604)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=290604)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=290604)     raise PTXASError(error)
(EngineCore_DP0 pid=290604) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=290604) `ptxas` stderr:
(EngineCore_DP0 pid=290604) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=290604) 
(EngineCore_DP0 pid=290604) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpiapmhd0v.ptx -o /tmp/tmpiapmhd0v.ptx.o
(EngineCore_DP0 pid=290604) 
[rank0]:[W125 18:41:04.553511746 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=128 ==========
Time: 2026-01-25 18:41:05
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:41:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:41:09 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=291073) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=291073) 
(EngineCore_DP0 pid=291073) 
(EngineCore_DP0 pid=291073) ================================================================
(EngineCore_DP0 pid=291073) Internal Triton PTX codegen error
(EngineCore_DP0 pid=291073) `ptxas` stderr:
(EngineCore_DP0 pid=291073) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=291073) 
(EngineCore_DP0 pid=291073) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpmh_9pv6x.ptx -o /tmp/tmpmh_9pv6x.ptx.o
(EngineCore_DP0 pid=291073) 
(EngineCore_DP0 pid=291073) 
(EngineCore_DP0 pid=291073) //
(EngineCore_DP0 pid=291073) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=291073) //
(EngineCore_DP0 pid=291073) 
(EngineCore_DP0 pid=291073) .version 8.7
(EngineCore_DP0 pid=291073) .target sm_121a
(EngineCore_DP0 pid=291073) .address_size 64
(EngineCore_DP0 pid=291073) 
(EngineCore_DP0 pid=291073) 	// .globl	_quant_slide_int8_kernel // -- Begin function _quant_slide_int8_kernel
(EngineCore_DP0 pid=291073) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=291073)                                         // @_quant_slide_int8_kernel
(EngineCore_DP0 pid=291073) .visible .entry _quant_slide_int8_kernel(
(EngineCore_DP0 pid=291073) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_0,
(EngineCore_DP0 pid=291073) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_1,
(EngineCore_DP0 pid=291073) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_2,
(EngineCore_DP0 pid=291073) 	.param .u32 _quant_slide_int8_kernel_param_3,
(EngineCore_DP0 pid=291073) 	.param .u32 _quant_slide_int8_kernel_param_4,
(EngineCore_DP0 pid=291073) 	.param .u32 _quant_slide_int8_kernel_param_5,
(EngineCore_DP0 pid=291073) 	.param .u32 _quant_slide_int8_kernel_param_6,
(EngineCore_DP0 pid=291073) 	.param .u32 _quant_slide_int8_kernel_param_7,
(EngineCore_DP0 pid=291073) 	.param .u32 _quant_slide_int8_kernel_param_8,
(EngineCore_DP0 pid=291073) 	.param .u32 _quant_slide_int8_kernel_param_9,
(EngineCore_DP0 pid=291073) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_10,
(EngineCore_DP0 pid=291073) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_11
(EngineCore_DP0 pid=291073) )
(EngineCore_DP0 pid=291073) .reqntid 1024
(EngineCore_DP0 pid=291073) {
(EngineCore_DP0 pid=291073) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=291073) 	.reg .b16 	%rs<20>;
(EngineCore_DP0 pid=291073) 	.reg .b32 	%r<119>;
(EngineCore_DP0 pid=291073) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=291073) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=291073) $L__func_begin0:
(EngineCore_DP0 pid=291073) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=291073) 
(EngineCore_DP0 pid=291073) // %bb.0:
(EngineCore_DP0 pid=291073) 	ld.param.b32 	%r19, [_quant_slide_int8_kernel_param_7];
(EngineCore_DP0 pid=291073) 	ld.param.b32 	%r18, [_quant_slide_int8_kernel_param_5];
(EngineCore_DP0 pid=291073) 	ld.param.b32 	%r17, [_quant_slide_int8_kernel_param_4];
(EngineCore_DP0 pid=291073) 	ld.param.b64 	%rd3, [_quant_slide_int8_kernel_param_2];
(EngineCore_DP0 pid=291073) 	ld.param.b64 	%rd4, [_quant_slide_int8_kernel_param_0];
(EngineCore_DP0 pid=291073) $L__tmp0:
(EngineCore_DP0 pid=291073) 	.loc	1 288 24                        // quant_slide_tuned_Llama3.2-1B.py:288:24
(EngineCore_DP0 pid=291073) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=291073) 	ld.param.b32 	%r21, [_quant_slide_int8_kernel_param_8];
(EngineCore_DP0 pid=291073) 	.loc	1 293 26                        // quant_slide_tuned_Llama3.2-1B.py:293:26
(EngineCore_DP0 pid=291073) 	mul.lo.s32 	%r22, %r21, %r1;
(EngineCore_DP0 pid=291073) 	.loc	1 293 20                        // quant_slide_tuned_Llama3.2-1B.py:293:20
(EngineCore_DP0 pid=291073) 	mad.wide.s32 	%rd1, %r22, 2, %rd4;
(EngineCore_DP0 pid=291073) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=291073) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=291073) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=291073) 	setp.lt.s32 	%p1, %r18, 1;
(EngineCore_DP0 pid=291073) 	mov.b32 	%r117, 0f2B8CBCCC;
(EngineCore_DP0 pid=291073) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=291073) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=291073) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=291073) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=291073) 	shl.b32 	%r3, %r2, 2;
(EngineCore_DP0 pid=291073) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=291073) 	shr.u32 	%r31, %r2, 3;
(EngineCore_DP0 pid=291073) 	and.b32 	%r32, %r31, 124;
(EngineCore_DP0 pid=291073) 	mov.b32 	%r33, global_smem;
(EngineCore_DP0 pid=291073) 	add.s32 	%r39, %r33, %r32;
(EngineCore_DP0 pid=291073) 	add.s32 	%r42, %r33, %r3;
(EngineCore_DP0 pid=291073) 	mov.b32 	%r37, 0;
(EngineCore_DP0 pid=291073) 	mov.b32 	%r115, 0f00000000;
(EngineCore_DP0 pid=291073) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=291073) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=291073) 	mov.b32 	%r116, %r37;
(EngineCore_DP0 pid=291073) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=291073) 	.loc	1 300 22                        // quant_slide_tuned_Llama3.2-1B.py:300:22
(EngineCore_DP0 pid=291073) 	add.s32 	%r45, %r3, %r116;
(EngineCore_DP0 pid=291073) 	setp.lt.s32 	%p2, %r45, %r17;
(EngineCore_DP0 pid=291073) 	.loc	1 301 29                        // quant_slide_tuned_Llama3.2-1B.py:301:29
(EngineCore_DP0 pid=291073) 	mad.wide.s32 	%rd6, %r45, 2, %rd1;
(EngineCore_DP0 pid=291073) 	.loc	1 301 21                        // quant_slide_tuned_Llama3.2-1B.py:301:21
(EngineCore_DP0 pid=291073) 	// begin inline asm
(EngineCore_DP0 pid=291073) 	mov.u32 %r35, %r37;
(EngineCore_DP0 pid=291073) 	mov.u32 %r36, %r37;
(EngineCore_DP0 pid=291073) 	@%p2 ld.global.v2.b32 { %r35, %r36 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=291073) 	// end inline asm
(EngineCore_DP0 pid=291073) 	mov.b32 	{%rs1, %rs2}, %r35;
(EngineCore_DP0 pid=291073) 	mov.b32 	{%rs3, %rs4}, %r36;
(EngineCore_DP0 pid=291073) 	.loc	1 302 50                        // quant_slide_tuned_Llama3.2-1B.py:302:50
(EngineCore_DP0 pid=291073) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=291073) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=291073) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=291073) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=291073) $L__tmp1:
(EngineCore_DP0 pid=291073) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	bar.sync 	0;
(EngineCore_DP0 pid=291073) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=291073) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=291073) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=291073) 	cvt.f32.bf16 	%r46, %rs11;
(EngineCore_DP0 pid=291073) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	shfl.sync.bfly.b32 	%r47, %r46, 16, 31, -1;
(EngineCore_DP0 pid=291073) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	max.f32 	%r48, %r46, %r47;
(EngineCore_DP0 pid=291073) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	shfl.sync.bfly.b32 	%r49, %r48, 8, 31, -1;
(EngineCore_DP0 pid=291073) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	max.f32 	%r50, %r48, %r49;
(EngineCore_DP0 pid=291073) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	shfl.sync.bfly.b32 	%r51, %r50, 4, 31, -1;
(EngineCore_DP0 pid=291073) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	max.f32 	%r52, %r50, %r51;
(EngineCore_DP0 pid=291073) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	shfl.sync.bfly.b32 	%r53, %r52, 2, 31, -1;
(EngineCore_DP0 pid=291073) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	max.f32 	%r54, %r52, %r53;
(EngineCore_DP0 pid=291073) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	shfl.sync.bfly.b32 	%r55, %r54, 1, 31, -1;
(EngineCore_DP0 pid=291073) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	max.f32 	%r40, %r54, %r55;
(EngineCore_DP0 pid=291073) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	// begin inline asm
(EngineCore_DP0 pid=291073) 	@%p3 st.shared.b32 [ %r39 + 0 ], %r40;
(EngineCore_DP0 pid=291073) 	// end inline asm
(EngineCore_DP0 pid=291073) 	bar.sync 	0;
(EngineCore_DP0 pid=291073) 	// begin inline asm
(EngineCore_DP0 pid=291073) 	@%p4 ld.shared.b32 %r41, [ %r42 + 0 ];
(EngineCore_DP0 pid=291073) 	// end inline asm
(EngineCore_DP0 pid=291073) 	shfl.sync.bfly.b32 	%r56, %r41, 16, 31, -1;
(EngineCore_DP0 pid=291073) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	max.f32 	%r57, %r41, %r56;
(EngineCore_DP0 pid=291073) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	shfl.sync.bfly.b32 	%r58, %r57, 8, 31, -1;
(EngineCore_DP0 pid=291073) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	max.f32 	%r59, %r57, %r58;
(EngineCore_DP0 pid=291073) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	shfl.sync.bfly.b32 	%r60, %r59, 4, 31, -1;
(EngineCore_DP0 pid=291073) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	max.f32 	%r61, %r59, %r60;
(EngineCore_DP0 pid=291073) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	shfl.sync.bfly.b32 	%r62, %r61, 2, 31, -1;
(EngineCore_DP0 pid=291073) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	max.f32 	%r63, %r61, %r62;
(EngineCore_DP0 pid=291073) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	shfl.sync.bfly.b32 	%r64, %r63, 1, 31, -1;
(EngineCore_DP0 pid=291073) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	max.f32 	%r44, %r63, %r64;
(EngineCore_DP0 pid=291073) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291073) 	// begin inline asm
(EngineCore_DP0 pid=291073) 	@%p19 st.shared.b32 [ %r42 + 0 ], %r44;
(EngineCore_DP0 pid=291073) 	// end inline asm
(EngineCore_DP0 pid=291073) 	bar.sync 	0;
(EngineCore_DP0 pid=291073) 	ld.shared.b32 	%r65, [global_smem];
(EngineCore_DP0 pid=291073) $L__tmp2:
(EngineCore_DP0 pid=291073) 	.loc	1 302 36                        // quant_slide_tuned_Llama3.2-1B.py:302:36
(EngineCore_DP0 pid=291073) 	max.f32 	%r115, %r115, %r65;
(EngineCore_DP0 pid=291073) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=291073) 	add.s32 	%r116, %r116, 4096;
(EngineCore_DP0 pid=291073) 	setp.lt.s32 	%p6, %r116, %r18;
(EngineCore_DP0 pid=291073) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=291073) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=291073) 	.loc	1 304 32                        // quant_slide_tuned_Llama3.2-1B.py:304:32
(EngineCore_DP0 pid=291073) 	max.f32 	%r117, %r115, 0f2B8CBCCC;
(EngineCore_DP0 pid=291073) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=291073) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=291073) 	mov.b32 	%r67, 0f42FE0000;
(EngineCore_DP0 pid=291073) 	.loc	1 305 32                        // quant_slide_tuned_Llama3.2-1B.py:305:32
(EngineCore_DP0 pid=291073) 	div.full.f32 	%r68, %r117, %r67;
(EngineCore_DP0 pid=291073) 	.loc	1 305 42                        // quant_slide_tuned_Llama3.2-1B.py:305:42
(EngineCore_DP0 pid=291073) 	max.f32 	%r66, %r68, 0f37810204;
(EngineCore_DP0 pid=291073) 	.loc	1 307 25                        // quant_slide_tuned_Llama3.2-1B.py:307:25
(EngineCore_DP0 pid=291073) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=291073) 	.loc	1 307 30                        // quant_slide_tuned_Llama3.2-1B.py:307:30
(EngineCore_DP0 pid=291073) 	// begin inline asm
(EngineCore_DP0 pid=291073) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r66 };
(EngineCore_DP0 pid=291073) 	// end inline asm
(EngineCore_DP0 pid=291073) 	.loc	1 310 29                        // quant_slide_tuned_Llama3.2-1B.py:310:29
(EngineCore_DP0 pid=291073) 	shl.b32 	%r14, %r19, 1;
(EngineCore_DP0 pid=291073) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=291073) 	setp.lt.s32 	%p8, %r14, 1;
(EngineCore_DP0 pid=291073) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=291073) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=291073) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=291073) 	ld.param.b32 	%r23, [_quant_slide_int8_kernel_param_9];
(EngineCore_DP0 pid=291073) 	shr.s32 	%r24, %r23, 31;
(EngineCore_DP0 pid=291073) 	shr.u32 	%r25, %r24, 30;
(EngineCore_DP0 pid=291073) 	add.s32 	%r26, %r23, %r25;
(EngineCore_DP0 pid=291073) 	shr.s32 	%r27, %r26, 2;
(EngineCore_DP0 pid=291073) 	ld.param.b64 	%rd5, [_quant_slide_int8_kernel_param_1];
(EngineCore_DP0 pid=291073) 	mul.lo.s32 	%r28, %r27, %r1;
(EngineCore_DP0 pid=291073) 	mad.wide.s32 	%rd2, %r28, 4, %rd5;
(EngineCore_DP0 pid=291073) 	div.full.f32 	%r13, %r67, %r117;
(EngineCore_DP0 pid=291073) 	mov.b32 	%r118, 0;
(EngineCore_DP0 pid=291073) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=291073)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=291073) 	.loc	1 314 30                        // quant_slide_tuned_Llama3.2-1B.py:314:30
(EngineCore_DP0 pid=291073) 	add.s32 	%r72, %r2, %r118;
(EngineCore_DP0 pid=291073) 	setp.lt.s32 	%p13, %r72, %r14;
(EngineCore_DP0 pid=291073) 	.loc	1 317 24                        // quant_slide_tuned_Llama3.2-1B.py:317:24
(EngineCore_DP0 pid=291073) 	shr.u32 	%r73, %r72, 31;
(EngineCore_DP0 pid=291073) 	add.s32 	%r74, %r72, %r73;
(EngineCore_DP0 pid=291073) 	shr.u32 	%r75, %r74, 1;
(EngineCore_DP0 pid=291073) 	.loc	1 318 23                        // quant_slide_tuned_Llama3.2-1B.py:318:23
(EngineCore_DP0 pid=291073) 	and.b32 	%r76, %r74, 2147483646;
(EngineCore_DP0 pid=291073) 	sub.s32 	%r77, %r72, %r76;
(EngineCore_DP0 pid=291073) 	.loc	1 319 30                        // quant_slide_tuned_Llama3.2-1B.py:319:30
(EngineCore_DP0 pid=291073) 	shl.b32 	%r78, %r77, 1;
(EngineCore_DP0 pid=291073) 	.loc	1 319 26                        // quant_slide_tuned_Llama3.2-1B.py:319:26
(EngineCore_DP0 pid=291073) 	mad.lo.s32 	%r79, %r75, 6, %r78;
(EngineCore_DP0 pid=291073) 	.loc	1 322 53                        // quant_slide_tuned_Llama3.2-1B.py:322:53
(EngineCore_DP0 pid=291073) 	setp.lt.s32 	%p14, %r79, %r17;
(EngineCore_DP0 pid=291073) 	.loc	1 322 37                        // quant_slide_tuned_Llama3.2-1B.py:322:37
(EngineCore_DP0 pid=291073) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=291073) 	.loc	1 321 29                        // quant_slide_tuned_Llama3.2-1B.py:321:29
(EngineCore_DP0 pid=291073) 	mad.wide.s32 	%rd8, %r79, 2, %rd1;
(EngineCore_DP0 pid=291073) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=291073) 	.loc	1 321 21                        // quant_slide_tuned_Llama3.2-1B.py:321:21
(EngineCore_DP0 pid=291073) 	// begin inline asm
(EngineCore_DP0 pid=291073) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=291073) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=291073) 	// end inline asm
(EngineCore_DP0 pid=291073) 	.loc	1 322 79                        // quant_slide_tuned_Llama3.2-1B.py:322:79
(EngineCore_DP0 pid=291073) 	cvt.f32.bf16 	%r80, %rs12;
(EngineCore_DP0 pid=291073) 	.loc	1 324 48                        // quant_slide_tuned_Llama3.2-1B.py:324:48
(EngineCore_DP0 pid=291073) 	or.b32 	%r81, %r79, 1;
(EngineCore_DP0 pid=291073) 	.loc	1 324 53                        // quant_slide_tuned_Llama3.2-1B.py:324:53
(EngineCore_DP0 pid=291073) 	setp.lt.s32 	%p15, %r81, %r17;
(EngineCore_DP0 pid=291073) 	.loc	1 324 37                        // quant_slide_tuned_Llama3.2-1B.py:324:37
(EngineCore_DP0 pid=291073) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=291073) 	.loc	1 323 39                        // quant_slide_tuned_Llama3.2-1B.py:323:39
(EngineCore_DP0 pid=291073) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=291073) 	.loc	1 323 21                        // quant_slide_tuned_Llama3.2-1B.py:323:21
(EngineCore_DP0 pid=291073) 	// begin inline asm
(EngineCore_DP0 pid=291073) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=291073) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=291073) 	// end inline asm
(EngineCore_DP0 pid=291073) 	.loc	1 324 79                        // quant_slide_tuned_Llama3.2-1B.py:324:79
(EngineCore_DP0 pid=291073) 	cvt.f32.bf16 	%r82, %rs14;
(EngineCore_DP0 pid=291073) 	.loc	1 326 48                        // quant_slide_tuned_Llama3.2-1B.py:326:48
(EngineCore_DP0 pid=291073) 	add.s32 	%r83, %r79, 2;
(EngineCore_DP0 pid=291073) 	.loc	1 326 53                        // quant_slide_tuned_Llama3.2-1B.py:326:53
(EngineCore_DP0 pid=291073) 	setp.lt.s32 	%p16, %r83, %r17;
(EngineCore_DP0 pid=291073) 	.loc	1 326 37                        // quant_slide_tuned_Llama3.2-1B.py:326:37
(EngineCore_DP0 pid=291073) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=291073) 	.loc	1 325 39                        // quant_slide_tuned_Llama3.2-1B.py:325:39
(EngineCore_DP0 pid=291073) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=291073) 	.loc	1 325 21                        // quant_slide_tuned_Llama3.2-1B.py:325:21
(EngineCore_DP0 pid=291073) 	// begin inline asm
(EngineCore_DP0 pid=291073) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=291073) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=291073) 	// end inline asm
(EngineCore_DP0 pid=291073) 	.loc	1 326 79                        // quant_slide_tuned_Llama3.2-1B.py:326:79
(EngineCore_DP0 pid=291073) 	cvt.f32.bf16 	%r84, %rs16;
(EngineCore_DP0 pid=291073) 	.loc	1 328 48                        // quant_slide_tuned_Llama3.2-1B.py:328:48
(EngineCore_DP0 pid=291073) 	add.s32 	%r85, %r79, 3;
(EngineCore_DP0 pid=291073) 	.loc	1 328 53                        // quant_slide_tuned_Llama3.2-1B.py:328:53
(EngineCore_DP0 pid=291073) 	setp.lt.s32 	%p17, %r85, %r17;
(EngineCore_DP0 pid=291073) 	.loc	1 328 37                        // quant_slide_tuned_Llama3.2-1B.py:328:37
(EngineCore_DP0 pid=291073) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=291073) 	.loc	1 327 39                        // quant_slide_tuned_Llama3.2-1B.py:327:39
(EngineCore_DP0 pid=291073) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=291073) 	.loc	1 327 21                        // quant_slide_tuned_Llama3.2-1B.py:327:21
(EngineCore_DP0 pid=291073) 	// begin inline asm
(EngineCore_DP0 pid=291073) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=291073) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=291073) 	// end inline asm
(EngineCore_DP0 pid=291073) 	.loc	1 328 79                        // quant_slide_tuned_Llama3.2-1B.py:328:79
(EngineCore_DP0 pid=291073) 	cvt.f32.bf16 	%r86, %rs18;
(EngineCore_DP0 pid=291073) 	.loc	1 330 56                        // quant_slide_tuned_Llama3.2-1B.py:330:56
(EngineCore_DP0 pid=291073) 	mul.f32 	%r87, %r13, %r80;
(EngineCore_DP0 pid=291073) 	.loc	1 330 51                        // quant_slide_tuned_Llama3.2-1B.py:330:51
(EngineCore_DP0 pid=291073) 	cvt.rni.f32.f32 	%r88, %r87;
(EngineCore_DP0 pid=291073) 	.loc	1 330 76                        // quant_slide_tuned_Llama3.2-1B.py:330:76
(EngineCore_DP0 pid=291073) 	max.f32 	%r89, %r88, 0fC3000000;
(EngineCore_DP0 pid=291073) 	min.f32 	%r90, %r89, 0f42FE0000;
(EngineCore_DP0 pid=291073) 	.loc	1 330 86                        // quant_slide_tuned_Llama3.2-1B.py:330:86
(EngineCore_DP0 pid=291073) 	cvt.rzi.s32.f32 	%r91, %r90;
(EngineCore_DP0 pid=291073) 	.loc	1 330 98                        // quant_slide_tuned_Llama3.2-1B.py:330:98
(EngineCore_DP0 pid=291073) 	and.b32 	%r92, %r91, 255;
(EngineCore_DP0 pid=291073) 	.loc	1 331 56                        // quant_slide_tuned_Llama3.2-1B.py:331:56
(EngineCore_DP0 pid=291073) 	mul.f32 	%r93, %r13, %r82;
(EngineCore_DP0 pid=291073) 	.loc	1 331 51                        // quant_slide_tuned_Llama3.2-1B.py:331:51
(EngineCore_DP0 pid=291073) 	cvt.rni.f32.f32 	%r94, %r93;
(EngineCore_DP0 pid=291073) 	.loc	1 332 56                        // quant_slide_tuned_Llama3.2-1B.py:332:56
(EngineCore_DP0 pid=291073) 	mul.f32 	%r95, %r13, %r84;
(EngineCore_DP0 pid=291073) 	.loc	1 332 51                        // quant_slide_tuned_Llama3.2-1B.py:332:51
(EngineCore_DP0 pid=291073) 	cvt.rni.f32.f32 	%r96, %r95;
(EngineCore_DP0 pid=291073) 	.loc	1 333 56                        // quant_slide_tuned_Llama3.2-1B.py:333:56
(EngineCore_DP0 pid=291073) 	mul.f32 	%r97, %r13, %r86;
(EngineCore_DP0 pid=291073) 	.loc	1 333 51                        // quant_slide_tuned_Llama3.2-1B.py:333:51
(EngineCore_DP0 pid=291073) 	cvt.rni.f32.f32 	%r98, %r97;
(EngineCore_DP0 pid=291073) 	.loc	1 333 76                        // quant_slide_tuned_Llama3.2-1B.py:333:76
(EngineCore_DP0 pid=291073) 	max.f32 	%r99, %r98, 0fC3000000;
(EngineCore_DP0 pid=291073) 	min.f32 	%r100, %r99, 0f42FE0000;
(EngineCore_DP0 pid=291073) 	.loc	1 333 86                        // quant_slide_tuned_Llama3.2-1B.py:333:86
(EngineCore_DP0 pid=291073) 	cvt.rzi.s32.f32 	%r101, %r100;
(EngineCore_DP0 pid=291073) 	.loc	1 331 76                        // quant_slide_tuned_Llama3.2-1B.py:331:76
(EngineCore_DP0 pid=291073) 	max.f32 	%r102, %r96, 0fC3000000;
(EngineCore_DP0 pid=291073) 	max.f32 	%r103, %r94, 0fC3000000;
(EngineCore_DP0 pid=291073) 	min.f32 	%r104, %r103, 0f42FE0000;
(EngineCore_DP0 pid=291073) 	min.f32 	%r105, %r102, 0f42FE0000;
(EngineCore_DP0 pid=291073) 	.loc	1 331 86                        // quant_slide_tuned_Llama3.2-1B.py:331:86
(EngineCore_DP0 pid=291073) 	cvt.rzi.s32.f32 	%r106, %r105;
(EngineCore_DP0 pid=291073) 	cvt.rzi.s32.f32 	%r107, %r104;
(EngineCore_DP0 pid=291073) 	.loc	1 335 30                        // quant_slide_tuned_Llama3.2-1B.py:335:30
(EngineCore_DP0 pid=291073) 	shl.b32 	%r108, %r107, 8;
(EngineCore_DP0 pid=291073) 	shl.b32 	%r109, %r106, 16;
(EngineCore_DP0 pid=291073) 	and.b32 	%r110, %r109, 16711680;
(EngineCore_DP0 pid=291073) 	and.b32 	%r111, %r108, 65280;
(EngineCore_DP0 pid=291073) 	.loc	1 335 24                        // quant_slide_tuned_Llama3.2-1B.py:335:24
(EngineCore_DP0 pid=291073) 	or.b32 	%r112, %r111, %r92;
(EngineCore_DP0 pid=291073) 	.loc	1 335 36                        // quant_slide_tuned_Llama3.2-1B.py:335:36
(EngineCore_DP0 pid=291073) 	or.b32 	%r113, %r112, %r110;
(EngineCore_DP0 pid=291073) 	.loc	1 335 55                        // quant_slide_tuned_Llama3.2-1B.py:335:55
(EngineCore_DP0 pid=291073) 	shl.b32 	%r114, %r101, 24;
(EngineCore_DP0 pid=291073) 	.loc	1 335 49                        // quant_slide_tuned_Llama3.2-1B.py:335:49
(EngineCore_DP0 pid=291073) 	or.b32 	%r70, %r113, %r114;
(EngineCore_DP0 pid=291073) 	.loc	1 336 29                        // quant_slide_tuned_Llama3.2-1B.py:336:29
(EngineCore_DP0 pid=291073) 	mad.wide.s32 	%rd12, %r72, 4, %rd2;
(EngineCore_DP0 pid=291073) 	.loc	1 336 39                        // quant_slide_tuned_Llama3.2-1B.py:336:39
(EngineCore_DP0 pid=291073) 	// begin inline asm
(EngineCore_DP0 pid=291073) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r70 };
(EngineCore_DP0 pid=291073) 	// end inline asm
(EngineCore_DP0 pid=291073) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=291073) 	add.s32 	%r118, %r118, 1024;
(EngineCore_DP0 pid=291073) 	setp.lt.s32 	%p18, %r118, %r14;
(EngineCore_DP0 pid=291073) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=291073) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=291073) 	.loc	1 312 4                         // quant_slide_tuned_Llama3.2-1B.py:312:4
(EngineCore_DP0 pid=291073) 	ret;
(EngineCore_DP0 pid=291073) $L__tmp3:
(EngineCore_DP0 pid=291073) $L__func_end0:
(EngineCore_DP0 pid=291073)                                         // -- End function
(EngineCore_DP0 pid=291073) }
(EngineCore_DP0 pid=291073) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=291073) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=291073) 	.section	.debug_abbrev
(EngineCore_DP0 pid=291073) 	{
(EngineCore_DP0 pid=291073) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=291073) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=291073) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=291073) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=291073) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=291073) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=291073) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=291073) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=291073) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=291073) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=291073) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=291073) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=291073) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=291073) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=291073) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=291073) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=291073) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=291073) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=291073) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=291073) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=291073) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=291073) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=291073) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=291073) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=291073) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=291073) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=291073) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=291073) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=291073) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=291073) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=291073) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=291073) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=291073) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=291073) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=291073) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=291073) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=291073) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=291073) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=291073) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=291073) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=291073) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=291073) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=291073) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=291073) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=291073) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=291073) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=291073) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=291073) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=291073) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=291073) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=291073) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=291073) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=291073) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=291073) 	}
(EngineCore_DP0 pid=291073) 	.section	.debug_info
(EngineCore_DP0 pid=291073) 	{
(EngineCore_DP0 pid=291073) .b32 224                                // Length of Unit
(EngineCore_DP0 pid=291073) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=291073) .b8 0
(EngineCore_DP0 pid=291073) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=291073) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=291073) .b8 1                                   // Abbrev [1] 0xb:0xd9 DW_TAG_compile_unit
(EngineCore_DP0 pid=291073) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=291073) .b8 114
(EngineCore_DP0 pid=291073) .b8 105
(EngineCore_DP0 pid=291073) .b8 116
(EngineCore_DP0 pid=291073) .b8 111
(EngineCore_DP0 pid=291073) .b8 110
(EngineCore_DP0 pid=291073) .b8 0
(EngineCore_DP0 pid=291073) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=291073) .b8 0
(EngineCore_DP0 pid=291073) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=291073) .b8 117
(EngineCore_DP0 pid=291073) .b8 97
(EngineCore_DP0 pid=291073) .b8 110
(EngineCore_DP0 pid=291073) .b8 116
(EngineCore_DP0 pid=291073) .b8 95
(EngineCore_DP0 pid=291073) .b8 115
(EngineCore_DP0 pid=291073) .b8 108
(EngineCore_DP0 pid=291073) .b8 105
(EngineCore_DP0 pid=291073) .b8 100
(EngineCore_DP0 pid=291073) .b8 101
(EngineCore_DP0 pid=291073) .b8 95
(EngineCore_DP0 pid=291073) .b8 116
(EngineCore_DP0 pid=291073) .b8 117
(EngineCore_DP0 pid=291073) .b8 110
(EngineCore_DP0 pid=291073) .b8 101
(EngineCore_DP0 pid=291073) .b8 100
(EngineCore_DP0 pid=291073) .b8 95
(EngineCore_DP0 pid=291073) .b8 76
(EngineCore_DP0 pid=291073) .b8 108
(EngineCore_DP0 pid=291073) .b8 97
(EngineCore_DP0 pid=291073) .b8 109
(EngineCore_DP0 pid=291073) .b8 97
(EngineCore_DP0 pid=291073) .b8 51
(EngineCore_DP0 pid=291073) .b8 46
(EngineCore_DP0 pid=291073) .b8 50
(EngineCore_DP0 pid=291073) .b8 45
(EngineCore_DP0 pid=291073) .b8 49
(EngineCore_DP0 pid=291073) .b8 66
(EngineCore_DP0 pid=291073) .b8 46
(EngineCore_DP0 pid=291073) .b8 112
(EngineCore_DP0 pid=291073) .b8 121
(EngineCore_DP0 pid=291073) .b8 0
(EngineCore_DP0 pid=291073) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=291073) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=291073) .b8 114
(EngineCore_DP0 pid=291073) .b8 111
(EngineCore_DP0 pid=291073) .b8 111
(EngineCore_DP0 pid=291073) .b8 116
(EngineCore_DP0 pid=291073) .b8 47
(EngineCore_DP0 pid=291073) .b8 118
(EngineCore_DP0 pid=291073) .b8 108
(EngineCore_DP0 pid=291073) .b8 108
(EngineCore_DP0 pid=291073) .b8 109
(EngineCore_DP0 pid=291073) .b8 98
(EngineCore_DP0 pid=291073) .b8 101
(EngineCore_DP0 pid=291073) .b8 110
(EngineCore_DP0 pid=291073) .b8 99
(EngineCore_DP0 pid=291073) .b8 104
(EngineCore_DP0 pid=291073) .b8 47
(EngineCore_DP0 pid=291073) .b8 115
(EngineCore_DP0 pid=291073) .b8 108
(EngineCore_DP0 pid=291073) .b8 105
(EngineCore_DP0 pid=291073) .b8 100
(EngineCore_DP0 pid=291073) .b8 101
(EngineCore_DP0 pid=291073) .b8 115
(EngineCore_DP0 pid=291073) .b8 112
(EngineCore_DP0 pid=291073) .b8 97
(EngineCore_DP0 pid=291073) .b8 114
(EngineCore_DP0 pid=291073) .b8 115
(EngineCore_DP0 pid=291073) .b8 101
(EngineCore_DP0 pid=291073) .b8 47
(EngineCore_DP0 pid=291073) .b8 99
(EngineCore_DP0 pid=291073) .b8 115
(EngineCore_DP0 pid=291073) .b8 114
(EngineCore_DP0 pid=291073) .b8 99
(EngineCore_DP0 pid=291073) .b8 47
(EngineCore_DP0 pid=291073) .b8 102
(EngineCore_DP0 pid=291073) .b8 117
(EngineCore_DP0 pid=291073) .b8 115
(EngineCore_DP0 pid=291073) .b8 101
(EngineCore_DP0 pid=291073) .b8 100
(EngineCore_DP0 pid=291073) .b8 95
(EngineCore_DP0 pid=291073) .b8 113
(EngineCore_DP0 pid=291073) .b8 117
(EngineCore_DP0 pid=291073) .b8 97
(EngineCore_DP0 pid=291073) .b8 110
(EngineCore_DP0 pid=291073) .b8 116
(EngineCore_DP0 pid=291073) .b8 95
(EngineCore_DP0 pid=291073) .b8 115
(EngineCore_DP0 pid=291073) .b8 108
(EngineCore_DP0 pid=291073) .b8 105
(EngineCore_DP0 pid=291073) .b8 100
(EngineCore_DP0 pid=291073) .b8 101
(EngineCore_DP0 pid=291073) .b8 95
(EngineCore_DP0 pid=291073) .b8 116
(EngineCore_DP0 pid=291073) .b8 114
(EngineCore_DP0 pid=291073) .b8 105
(EngineCore_DP0 pid=291073) .b8 116
(EngineCore_DP0 pid=291073) .b8 111
(EngineCore_DP0 pid=291073) .b8 110
(EngineCore_DP0 pid=291073) .b8 47
(EngineCore_DP0 pid=291073) .b8 98
(EngineCore_DP0 pid=291073) .b8 117
(EngineCore_DP0 pid=291073) .b8 105
(EngineCore_DP0 pid=291073) .b8 108
(EngineCore_DP0 pid=291073) .b8 100
(EngineCore_DP0 pid=291073) .b8 47
(EngineCore_DP0 pid=291073) .b8 71
(EngineCore_DP0 pid=291073) .b8 66
(EngineCore_DP0 pid=291073) .b8 49
(EngineCore_DP0 pid=291073) .b8 48
(EngineCore_DP0 pid=291073) .b8 95
(EngineCore_DP0 pid=291073) .b8 99
(EngineCore_DP0 pid=291073) .b8 99
(EngineCore_DP0 pid=291073) .b8 49
(EngineCore_DP0 pid=291073) .b8 50
(EngineCore_DP0 pid=291073) .b8 49
(EngineCore_DP0 pid=291073) .b8 95
(EngineCore_DP0 pid=291073) .b8 112
(EngineCore_DP0 pid=291073) .b8 121
(EngineCore_DP0 pid=291073) .b8 51
(EngineCore_DP0 pid=291073) .b8 49
(EngineCore_DP0 pid=291073) .b8 50
(EngineCore_DP0 pid=291073) .b8 95
(EngineCore_DP0 pid=291073) .b8 99
(EngineCore_DP0 pid=291073) .b8 117
(EngineCore_DP0 pid=291073) .b8 49
(EngineCore_DP0 pid=291073) .b8 50
(EngineCore_DP0 pid=291073) .b8 57
(EngineCore_DP0 pid=291073) .b8 95
(EngineCore_DP0 pid=291073) .b8 97
(EngineCore_DP0 pid=291073) .b8 97
(EngineCore_DP0 pid=291073) .b8 114
(EngineCore_DP0 pid=291073) .b8 99
(EngineCore_DP0 pid=291073) .b8 104
(EngineCore_DP0 pid=291073) .b8 54
(EngineCore_DP0 pid=291073) .b8 52
(EngineCore_DP0 pid=291073) .b8 0
(EngineCore_DP0 pid=291073) .b8 2                                   // Abbrev [2] 0x99:0x1b DW_TAG_subprogram
(EngineCore_DP0 pid=291073) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=291073) .b8 113
(EngineCore_DP0 pid=291073) .b8 117
(EngineCore_DP0 pid=291073) .b8 97
(EngineCore_DP0 pid=291073) .b8 110
(EngineCore_DP0 pid=291073) .b8 116
(EngineCore_DP0 pid=291073) .b8 95
(EngineCore_DP0 pid=291073) .b8 115
(EngineCore_DP0 pid=291073) .b8 108
(EngineCore_DP0 pid=291073) .b8 105
(EngineCore_DP0 pid=291073) .b8 100
(EngineCore_DP0 pid=291073) .b8 101
(EngineCore_DP0 pid=291073) .b8 95
(EngineCore_DP0 pid=291073) .b8 105
(EngineCore_DP0 pid=291073) .b8 110
(EngineCore_DP0 pid=291073) .b8 116
(EngineCore_DP0 pid=291073) .b8 56
(EngineCore_DP0 pid=291073) .b8 95
(EngineCore_DP0 pid=291073) .b8 107
(EngineCore_DP0 pid=291073) .b8 101
(EngineCore_DP0 pid=291073) .b8 114
(EngineCore_DP0 pid=291073) .b8 110
(EngineCore_DP0 pid=291073) .b8 101
(EngineCore_DP0 pid=291073) .b8 108
(EngineCore_DP0 pid=291073) .b8 0
(EngineCore_DP0 pid=291073) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=291073) .b8 3                                   // Abbrev [3] 0xb4:0x2f DW_TAG_subprogram
(EngineCore_DP0 pid=291073) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=291073) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=291073) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=291073) .b8 4                                   // Abbrev [4] 0xc9:0x19 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=291073) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=291073) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=291073) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=291073) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=291073) .b8 46                                  // DW_AT_call_line
(EngineCore_DP0 pid=291073) .b8 1
(EngineCore_DP0 pid=291073) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=291073) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=291073) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=291073) 	}
(EngineCore_DP0 pid=291073) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=291073) 
(EngineCore_DP0 pid=291073) ================================================================
(EngineCore_DP0 pid=291073) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=291073) 
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpmh_9pv6x.ptx', '-o', '/tmp/tmpmh_9pv6x.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866] 
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866] 
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866] 
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpmh_9pv6x.ptx -o /tmp/tmpmh_9pv6x.ptx.o
(EngineCore_DP0 pid=291073) ERROR 01-25 18:41:24 [core.py:866] 

STDERR:
[2026-01-25 18:41:09] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:41:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:41:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:41:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:41:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:41:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:41:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:41:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:41:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:41:12] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:41:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:41:12] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:41:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:41:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:41:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:41:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:41:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:41:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=291073) [2026-01-25 18:41:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=291073) [2026-01-25 18:41:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=291073) [2026-01-25 18:41:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=291073) [2026-01-25 18:41:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=291073) [2026-01-25 18:41:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=291073) [2026-01-25 18:41:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=291073) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=291073) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.38s/it]
(EngineCore_DP0 pid=291073) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.38s/it]
(EngineCore_DP0 pid=291073) 
(EngineCore_DP0 pid=291073) [2026-01-25 18:41:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=291073) [2026-01-25 18:41:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=291073) [2026-01-25 18:41:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=291073) [2026-01-25 18:41:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=291073) [2026-01-25 18:41:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=291073) [2026-01-25 18:41:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=291073) [2026-01-25 18:41:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=291073) [2026-01-25 18:41:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=291073) Process EngineCore_DP0:
(EngineCore_DP0 pid=291073) Traceback (most recent call last):
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=291073)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=291073)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=291073)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=291073) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpmh_9pv6x.ptx', '-o', '/tmp/tmpmh_9pv6x.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=291073) 
(EngineCore_DP0 pid=291073) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=291073) 
(EngineCore_DP0 pid=291073) Traceback (most recent call last):
(EngineCore_DP0 pid=291073)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=291073)     self.run()
(EngineCore_DP0 pid=291073)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=291073)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=291073)     raise e
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=291073)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=291073)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=291073)     super().__init__(
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=291073)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=291073)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=291073)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=291073)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=291073)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=291073)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=291073)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=291073)     return func(*args, **kwargs)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=291073)     return func(*args, **kwargs)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=291073)     self.model_runner.profile_run()
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=291073)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=291073)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=291073)     return func(*args, **kwargs)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=291073)     outputs = self.model(
(EngineCore_DP0 pid=291073)               ^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291073)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291073)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=291073)     model_output = self.model(
(EngineCore_DP0 pid=291073)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=291073)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=291073)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=291073)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291073)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291073)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=291073)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=291073)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291073)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291073)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=291073)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=291073)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291073)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291073)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=291073)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=291073)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=291073)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=291073)     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=291073)     return self._linear_fn(
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=291073)     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=291073)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=291073)     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=291073)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=291073)     return fn(input, L)
(EngineCore_DP0 pid=291073)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=291073)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=291073)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=291073)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=291073)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=291073)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=291073)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=291073)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=291073)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=291073)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=291073)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=291073)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291073)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=291073)     raise PTXASError(error)
(EngineCore_DP0 pid=291073) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=291073) `ptxas` stderr:
(EngineCore_DP0 pid=291073) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=291073) 
(EngineCore_DP0 pid=291073) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpmh_9pv6x.ptx -o /tmp/tmpmh_9pv6x.ptx.o
(EngineCore_DP0 pid=291073) 
[rank0]:[W125 18:41:24.145853639 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=128

========== M=256 ==========
Time: 2026-01-25 18:41:26
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:41:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:41:30 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=291547) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=291547) 
(EngineCore_DP0 pid=291547) 
(EngineCore_DP0 pid=291547) ================================================================
(EngineCore_DP0 pid=291547) Internal Triton PTX codegen error
(EngineCore_DP0 pid=291547) `ptxas` stderr:
(EngineCore_DP0 pid=291547) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=291547) 
(EngineCore_DP0 pid=291547) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp0hrkqz6j.ptx -o /tmp/tmp0hrkqz6j.ptx.o
(EngineCore_DP0 pid=291547) 
(EngineCore_DP0 pid=291547) 
(EngineCore_DP0 pid=291547) //
(EngineCore_DP0 pid=291547) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=291547) //
(EngineCore_DP0 pid=291547) 
(EngineCore_DP0 pid=291547) .version 8.7
(EngineCore_DP0 pid=291547) .target sm_121a
(EngineCore_DP0 pid=291547) .address_size 64
(EngineCore_DP0 pid=291547) 
(EngineCore_DP0 pid=291547) 	// .globl	_quant_slide_int8_kernel // -- Begin function _quant_slide_int8_kernel
(EngineCore_DP0 pid=291547) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=291547)                                         // @_quant_slide_int8_kernel
(EngineCore_DP0 pid=291547) .visible .entry _quant_slide_int8_kernel(
(EngineCore_DP0 pid=291547) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_0,
(EngineCore_DP0 pid=291547) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_1,
(EngineCore_DP0 pid=291547) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_2,
(EngineCore_DP0 pid=291547) 	.param .u32 _quant_slide_int8_kernel_param_3,
(EngineCore_DP0 pid=291547) 	.param .u32 _quant_slide_int8_kernel_param_4,
(EngineCore_DP0 pid=291547) 	.param .u32 _quant_slide_int8_kernel_param_5,
(EngineCore_DP0 pid=291547) 	.param .u32 _quant_slide_int8_kernel_param_6,
(EngineCore_DP0 pid=291547) 	.param .u32 _quant_slide_int8_kernel_param_7,
(EngineCore_DP0 pid=291547) 	.param .u32 _quant_slide_int8_kernel_param_8,
(EngineCore_DP0 pid=291547) 	.param .u32 _quant_slide_int8_kernel_param_9,
(EngineCore_DP0 pid=291547) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_10,
(EngineCore_DP0 pid=291547) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_11
(EngineCore_DP0 pid=291547) )
(EngineCore_DP0 pid=291547) .reqntid 1024
(EngineCore_DP0 pid=291547) {
(EngineCore_DP0 pid=291547) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=291547) 	.reg .b16 	%rs<20>;
(EngineCore_DP0 pid=291547) 	.reg .b32 	%r<119>;
(EngineCore_DP0 pid=291547) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=291547) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=291547) $L__func_begin0:
(EngineCore_DP0 pid=291547) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=291547) 
(EngineCore_DP0 pid=291547) // %bb.0:
(EngineCore_DP0 pid=291547) 	ld.param.b32 	%r19, [_quant_slide_int8_kernel_param_7];
(EngineCore_DP0 pid=291547) 	ld.param.b32 	%r18, [_quant_slide_int8_kernel_param_5];
(EngineCore_DP0 pid=291547) 	ld.param.b32 	%r17, [_quant_slide_int8_kernel_param_4];
(EngineCore_DP0 pid=291547) 	ld.param.b64 	%rd3, [_quant_slide_int8_kernel_param_2];
(EngineCore_DP0 pid=291547) 	ld.param.b64 	%rd4, [_quant_slide_int8_kernel_param_0];
(EngineCore_DP0 pid=291547) $L__tmp0:
(EngineCore_DP0 pid=291547) 	.loc	1 288 24                        // quant_slide_tuned_Llama3.2-1B.py:288:24
(EngineCore_DP0 pid=291547) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=291547) 	ld.param.b32 	%r21, [_quant_slide_int8_kernel_param_8];
(EngineCore_DP0 pid=291547) 	.loc	1 293 26                        // quant_slide_tuned_Llama3.2-1B.py:293:26
(EngineCore_DP0 pid=291547) 	mul.lo.s32 	%r22, %r21, %r1;
(EngineCore_DP0 pid=291547) 	.loc	1 293 20                        // quant_slide_tuned_Llama3.2-1B.py:293:20
(EngineCore_DP0 pid=291547) 	mad.wide.s32 	%rd1, %r22, 2, %rd4;
(EngineCore_DP0 pid=291547) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=291547) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=291547) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=291547) 	setp.lt.s32 	%p1, %r18, 1;
(EngineCore_DP0 pid=291547) 	mov.b32 	%r117, 0f2B8CBCCC;
(EngineCore_DP0 pid=291547) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=291547) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=291547) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=291547) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=291547) 	shl.b32 	%r3, %r2, 2;
(EngineCore_DP0 pid=291547) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=291547) 	shr.u32 	%r31, %r2, 3;
(EngineCore_DP0 pid=291547) 	and.b32 	%r32, %r31, 124;
(EngineCore_DP0 pid=291547) 	mov.b32 	%r33, global_smem;
(EngineCore_DP0 pid=291547) 	add.s32 	%r39, %r33, %r32;
(EngineCore_DP0 pid=291547) 	add.s32 	%r42, %r33, %r3;
(EngineCore_DP0 pid=291547) 	mov.b32 	%r37, 0;
(EngineCore_DP0 pid=291547) 	mov.b32 	%r115, 0f00000000;
(EngineCore_DP0 pid=291547) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=291547) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=291547) 	mov.b32 	%r116, %r37;
(EngineCore_DP0 pid=291547) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=291547) 	.loc	1 300 22                        // quant_slide_tuned_Llama3.2-1B.py:300:22
(EngineCore_DP0 pid=291547) 	add.s32 	%r45, %r3, %r116;
(EngineCore_DP0 pid=291547) 	setp.lt.s32 	%p2, %r45, %r17;
(EngineCore_DP0 pid=291547) 	.loc	1 301 29                        // quant_slide_tuned_Llama3.2-1B.py:301:29
(EngineCore_DP0 pid=291547) 	mad.wide.s32 	%rd6, %r45, 2, %rd1;
(EngineCore_DP0 pid=291547) 	.loc	1 301 21                        // quant_slide_tuned_Llama3.2-1B.py:301:21
(EngineCore_DP0 pid=291547) 	// begin inline asm
(EngineCore_DP0 pid=291547) 	mov.u32 %r35, %r37;
(EngineCore_DP0 pid=291547) 	mov.u32 %r36, %r37;
(EngineCore_DP0 pid=291547) 	@%p2 ld.global.v2.b32 { %r35, %r36 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=291547) 	// end inline asm
(EngineCore_DP0 pid=291547) 	mov.b32 	{%rs1, %rs2}, %r35;
(EngineCore_DP0 pid=291547) 	mov.b32 	{%rs3, %rs4}, %r36;
(EngineCore_DP0 pid=291547) 	.loc	1 302 50                        // quant_slide_tuned_Llama3.2-1B.py:302:50
(EngineCore_DP0 pid=291547) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=291547) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=291547) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=291547) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=291547) $L__tmp1:
(EngineCore_DP0 pid=291547) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	bar.sync 	0;
(EngineCore_DP0 pid=291547) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=291547) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=291547) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=291547) 	cvt.f32.bf16 	%r46, %rs11;
(EngineCore_DP0 pid=291547) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	shfl.sync.bfly.b32 	%r47, %r46, 16, 31, -1;
(EngineCore_DP0 pid=291547) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	max.f32 	%r48, %r46, %r47;
(EngineCore_DP0 pid=291547) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	shfl.sync.bfly.b32 	%r49, %r48, 8, 31, -1;
(EngineCore_DP0 pid=291547) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	max.f32 	%r50, %r48, %r49;
(EngineCore_DP0 pid=291547) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	shfl.sync.bfly.b32 	%r51, %r50, 4, 31, -1;
(EngineCore_DP0 pid=291547) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	max.f32 	%r52, %r50, %r51;
(EngineCore_DP0 pid=291547) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	shfl.sync.bfly.b32 	%r53, %r52, 2, 31, -1;
(EngineCore_DP0 pid=291547) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	max.f32 	%r54, %r52, %r53;
(EngineCore_DP0 pid=291547) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	shfl.sync.bfly.b32 	%r55, %r54, 1, 31, -1;
(EngineCore_DP0 pid=291547) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	max.f32 	%r40, %r54, %r55;
(EngineCore_DP0 pid=291547) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	// begin inline asm
(EngineCore_DP0 pid=291547) 	@%p3 st.shared.b32 [ %r39 + 0 ], %r40;
(EngineCore_DP0 pid=291547) 	// end inline asm
(EngineCore_DP0 pid=291547) 	bar.sync 	0;
(EngineCore_DP0 pid=291547) 	// begin inline asm
(EngineCore_DP0 pid=291547) 	@%p4 ld.shared.b32 %r41, [ %r42 + 0 ];
(EngineCore_DP0 pid=291547) 	// end inline asm
(EngineCore_DP0 pid=291547) 	shfl.sync.bfly.b32 	%r56, %r41, 16, 31, -1;
(EngineCore_DP0 pid=291547) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	max.f32 	%r57, %r41, %r56;
(EngineCore_DP0 pid=291547) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	shfl.sync.bfly.b32 	%r58, %r57, 8, 31, -1;
(EngineCore_DP0 pid=291547) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	max.f32 	%r59, %r57, %r58;
(EngineCore_DP0 pid=291547) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	shfl.sync.bfly.b32 	%r60, %r59, 4, 31, -1;
(EngineCore_DP0 pid=291547) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	max.f32 	%r61, %r59, %r60;
(EngineCore_DP0 pid=291547) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	shfl.sync.bfly.b32 	%r62, %r61, 2, 31, -1;
(EngineCore_DP0 pid=291547) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	max.f32 	%r63, %r61, %r62;
(EngineCore_DP0 pid=291547) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	shfl.sync.bfly.b32 	%r64, %r63, 1, 31, -1;
(EngineCore_DP0 pid=291547) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	max.f32 	%r44, %r63, %r64;
(EngineCore_DP0 pid=291547) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=291547) 	// begin inline asm
(EngineCore_DP0 pid=291547) 	@%p19 st.shared.b32 [ %r42 + 0 ], %r44;
(EngineCore_DP0 pid=291547) 	// end inline asm
(EngineCore_DP0 pid=291547) 	bar.sync 	0;
(EngineCore_DP0 pid=291547) 	ld.shared.b32 	%r65, [global_smem];
(EngineCore_DP0 pid=291547) $L__tmp2:
(EngineCore_DP0 pid=291547) 	.loc	1 302 36                        // quant_slide_tuned_Llama3.2-1B.py:302:36
(EngineCore_DP0 pid=291547) 	max.f32 	%r115, %r115, %r65;
(EngineCore_DP0 pid=291547) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=291547) 	add.s32 	%r116, %r116, 4096;
(EngineCore_DP0 pid=291547) 	setp.lt.s32 	%p6, %r116, %r18;
(EngineCore_DP0 pid=291547) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=291547) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=291547) 	.loc	1 304 32                        // quant_slide_tuned_Llama3.2-1B.py:304:32
(EngineCore_DP0 pid=291547) 	max.f32 	%r117, %r115, 0f2B8CBCCC;
(EngineCore_DP0 pid=291547) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=291547) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=291547) 	mov.b32 	%r67, 0f42FE0000;
(EngineCore_DP0 pid=291547) 	.loc	1 305 32                        // quant_slide_tuned_Llama3.2-1B.py:305:32
(EngineCore_DP0 pid=291547) 	div.full.f32 	%r68, %r117, %r67;
(EngineCore_DP0 pid=291547) 	.loc	1 305 42                        // quant_slide_tuned_Llama3.2-1B.py:305:42
(EngineCore_DP0 pid=291547) 	max.f32 	%r66, %r68, 0f37810204;
(EngineCore_DP0 pid=291547) 	.loc	1 307 25                        // quant_slide_tuned_Llama3.2-1B.py:307:25
(EngineCore_DP0 pid=291547) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=291547) 	.loc	1 307 30                        // quant_slide_tuned_Llama3.2-1B.py:307:30
(EngineCore_DP0 pid=291547) 	// begin inline asm
(EngineCore_DP0 pid=291547) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r66 };
(EngineCore_DP0 pid=291547) 	// end inline asm
(EngineCore_DP0 pid=291547) 	.loc	1 310 29                        // quant_slide_tuned_Llama3.2-1B.py:310:29
(EngineCore_DP0 pid=291547) 	shl.b32 	%r14, %r19, 1;
(EngineCore_DP0 pid=291547) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=291547) 	setp.lt.s32 	%p8, %r14, 1;
(EngineCore_DP0 pid=291547) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=291547) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=291547) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=291547) 	ld.param.b32 	%r23, [_quant_slide_int8_kernel_param_9];
(EngineCore_DP0 pid=291547) 	shr.s32 	%r24, %r23, 31;
(EngineCore_DP0 pid=291547) 	shr.u32 	%r25, %r24, 30;
(EngineCore_DP0 pid=291547) 	add.s32 	%r26, %r23, %r25;
(EngineCore_DP0 pid=291547) 	shr.s32 	%r27, %r26, 2;
(EngineCore_DP0 pid=291547) 	ld.param.b64 	%rd5, [_quant_slide_int8_kernel_param_1];
(EngineCore_DP0 pid=291547) 	mul.lo.s32 	%r28, %r27, %r1;
(EngineCore_DP0 pid=291547) 	mad.wide.s32 	%rd2, %r28, 4, %rd5;
(EngineCore_DP0 pid=291547) 	div.full.f32 	%r13, %r67, %r117;
(EngineCore_DP0 pid=291547) 	mov.b32 	%r118, 0;
(EngineCore_DP0 pid=291547) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=291547)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=291547) 	.loc	1 314 30                        // quant_slide_tuned_Llama3.2-1B.py:314:30
(EngineCore_DP0 pid=291547) 	add.s32 	%r72, %r2, %r118;
(EngineCore_DP0 pid=291547) 	setp.lt.s32 	%p13, %r72, %r14;
(EngineCore_DP0 pid=291547) 	.loc	1 317 24                        // quant_slide_tuned_Llama3.2-1B.py:317:24
(EngineCore_DP0 pid=291547) 	shr.u32 	%r73, %r72, 31;
(EngineCore_DP0 pid=291547) 	add.s32 	%r74, %r72, %r73;
(EngineCore_DP0 pid=291547) 	shr.u32 	%r75, %r74, 1;
(EngineCore_DP0 pid=291547) 	.loc	1 318 23                        // quant_slide_tuned_Llama3.2-1B.py:318:23
(EngineCore_DP0 pid=291547) 	and.b32 	%r76, %r74, 2147483646;
(EngineCore_DP0 pid=291547) 	sub.s32 	%r77, %r72, %r76;
(EngineCore_DP0 pid=291547) 	.loc	1 319 30                        // quant_slide_tuned_Llama3.2-1B.py:319:30
(EngineCore_DP0 pid=291547) 	shl.b32 	%r78, %r77, 1;
(EngineCore_DP0 pid=291547) 	.loc	1 319 26                        // quant_slide_tuned_Llama3.2-1B.py:319:26
(EngineCore_DP0 pid=291547) 	mad.lo.s32 	%r79, %r75, 6, %r78;
(EngineCore_DP0 pid=291547) 	.loc	1 322 53                        // quant_slide_tuned_Llama3.2-1B.py:322:53
(EngineCore_DP0 pid=291547) 	setp.lt.s32 	%p14, %r79, %r17;
(EngineCore_DP0 pid=291547) 	.loc	1 322 37                        // quant_slide_tuned_Llama3.2-1B.py:322:37
(EngineCore_DP0 pid=291547) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=291547) 	.loc	1 321 29                        // quant_slide_tuned_Llama3.2-1B.py:321:29
(EngineCore_DP0 pid=291547) 	mad.wide.s32 	%rd8, %r79, 2, %rd1;
(EngineCore_DP0 pid=291547) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=291547) 	.loc	1 321 21                        // quant_slide_tuned_Llama3.2-1B.py:321:21
(EngineCore_DP0 pid=291547) 	// begin inline asm
(EngineCore_DP0 pid=291547) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=291547) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=291547) 	// end inline asm
(EngineCore_DP0 pid=291547) 	.loc	1 322 79                        // quant_slide_tuned_Llama3.2-1B.py:322:79
(EngineCore_DP0 pid=291547) 	cvt.f32.bf16 	%r80, %rs12;
(EngineCore_DP0 pid=291547) 	.loc	1 324 48                        // quant_slide_tuned_Llama3.2-1B.py:324:48
(EngineCore_DP0 pid=291547) 	or.b32 	%r81, %r79, 1;
(EngineCore_DP0 pid=291547) 	.loc	1 324 53                        // quant_slide_tuned_Llama3.2-1B.py:324:53
(EngineCore_DP0 pid=291547) 	setp.lt.s32 	%p15, %r81, %r17;
(EngineCore_DP0 pid=291547) 	.loc	1 324 37                        // quant_slide_tuned_Llama3.2-1B.py:324:37
(EngineCore_DP0 pid=291547) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=291547) 	.loc	1 323 39                        // quant_slide_tuned_Llama3.2-1B.py:323:39
(EngineCore_DP0 pid=291547) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=291547) 	.loc	1 323 21                        // quant_slide_tuned_Llama3.2-1B.py:323:21
(EngineCore_DP0 pid=291547) 	// begin inline asm
(EngineCore_DP0 pid=291547) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=291547) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=291547) 	// end inline asm
(EngineCore_DP0 pid=291547) 	.loc	1 324 79                        // quant_slide_tuned_Llama3.2-1B.py:324:79
(EngineCore_DP0 pid=291547) 	cvt.f32.bf16 	%r82, %rs14;
(EngineCore_DP0 pid=291547) 	.loc	1 326 48                        // quant_slide_tuned_Llama3.2-1B.py:326:48
(EngineCore_DP0 pid=291547) 	add.s32 	%r83, %r79, 2;
(EngineCore_DP0 pid=291547) 	.loc	1 326 53                        // quant_slide_tuned_Llama3.2-1B.py:326:53
(EngineCore_DP0 pid=291547) 	setp.lt.s32 	%p16, %r83, %r17;
(EngineCore_DP0 pid=291547) 	.loc	1 326 37                        // quant_slide_tuned_Llama3.2-1B.py:326:37
(EngineCore_DP0 pid=291547) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=291547) 	.loc	1 325 39                        // quant_slide_tuned_Llama3.2-1B.py:325:39
(EngineCore_DP0 pid=291547) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=291547) 	.loc	1 325 21                        // quant_slide_tuned_Llama3.2-1B.py:325:21
(EngineCore_DP0 pid=291547) 	// begin inline asm
(EngineCore_DP0 pid=291547) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=291547) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=291547) 	// end inline asm
(EngineCore_DP0 pid=291547) 	.loc	1 326 79                        // quant_slide_tuned_Llama3.2-1B.py:326:79
(EngineCore_DP0 pid=291547) 	cvt.f32.bf16 	%r84, %rs16;
(EngineCore_DP0 pid=291547) 	.loc	1 328 48                        // quant_slide_tuned_Llama3.2-1B.py:328:48
(EngineCore_DP0 pid=291547) 	add.s32 	%r85, %r79, 3;
(EngineCore_DP0 pid=291547) 	.loc	1 328 53                        // quant_slide_tuned_Llama3.2-1B.py:328:53
(EngineCore_DP0 pid=291547) 	setp.lt.s32 	%p17, %r85, %r17;
(EngineCore_DP0 pid=291547) 	.loc	1 328 37                        // quant_slide_tuned_Llama3.2-1B.py:328:37
(EngineCore_DP0 pid=291547) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=291547) 	.loc	1 327 39                        // quant_slide_tuned_Llama3.2-1B.py:327:39
(EngineCore_DP0 pid=291547) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=291547) 	.loc	1 327 21                        // quant_slide_tuned_Llama3.2-1B.py:327:21
(EngineCore_DP0 pid=291547) 	// begin inline asm
(EngineCore_DP0 pid=291547) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=291547) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=291547) 	// end inline asm
(EngineCore_DP0 pid=291547) 	.loc	1 328 79                        // quant_slide_tuned_Llama3.2-1B.py:328:79
(EngineCore_DP0 pid=291547) 	cvt.f32.bf16 	%r86, %rs18;
(EngineCore_DP0 pid=291547) 	.loc	1 330 56                        // quant_slide_tuned_Llama3.2-1B.py:330:56
(EngineCore_DP0 pid=291547) 	mul.f32 	%r87, %r13, %r80;
(EngineCore_DP0 pid=291547) 	.loc	1 330 51                        // quant_slide_tuned_Llama3.2-1B.py:330:51
(EngineCore_DP0 pid=291547) 	cvt.rni.f32.f32 	%r88, %r87;
(EngineCore_DP0 pid=291547) 	.loc	1 330 76                        // quant_slide_tuned_Llama3.2-1B.py:330:76
(EngineCore_DP0 pid=291547) 	max.f32 	%r89, %r88, 0fC3000000;
(EngineCore_DP0 pid=291547) 	min.f32 	%r90, %r89, 0f42FE0000;
(EngineCore_DP0 pid=291547) 	.loc	1 330 86                        // quant_slide_tuned_Llama3.2-1B.py:330:86
(EngineCore_DP0 pid=291547) 	cvt.rzi.s32.f32 	%r91, %r90;
(EngineCore_DP0 pid=291547) 	.loc	1 330 98                        // quant_slide_tuned_Llama3.2-1B.py:330:98
(EngineCore_DP0 pid=291547) 	and.b32 	%r92, %r91, 255;
(EngineCore_DP0 pid=291547) 	.loc	1 331 56                        // quant_slide_tuned_Llama3.2-1B.py:331:56
(EngineCore_DP0 pid=291547) 	mul.f32 	%r93, %r13, %r82;
(EngineCore_DP0 pid=291547) 	.loc	1 331 51                        // quant_slide_tuned_Llama3.2-1B.py:331:51
(EngineCore_DP0 pid=291547) 	cvt.rni.f32.f32 	%r94, %r93;
(EngineCore_DP0 pid=291547) 	.loc	1 332 56                        // quant_slide_tuned_Llama3.2-1B.py:332:56
(EngineCore_DP0 pid=291547) 	mul.f32 	%r95, %r13, %r84;
(EngineCore_DP0 pid=291547) 	.loc	1 332 51                        // quant_slide_tuned_Llama3.2-1B.py:332:51
(EngineCore_DP0 pid=291547) 	cvt.rni.f32.f32 	%r96, %r95;
(EngineCore_DP0 pid=291547) 	.loc	1 333 56                        // quant_slide_tuned_Llama3.2-1B.py:333:56
(EngineCore_DP0 pid=291547) 	mul.f32 	%r97, %r13, %r86;
(EngineCore_DP0 pid=291547) 	.loc	1 333 51                        // quant_slide_tuned_Llama3.2-1B.py:333:51
(EngineCore_DP0 pid=291547) 	cvt.rni.f32.f32 	%r98, %r97;
(EngineCore_DP0 pid=291547) 	.loc	1 333 76                        // quant_slide_tuned_Llama3.2-1B.py:333:76
(EngineCore_DP0 pid=291547) 	max.f32 	%r99, %r98, 0fC3000000;
(EngineCore_DP0 pid=291547) 	min.f32 	%r100, %r99, 0f42FE0000;
(EngineCore_DP0 pid=291547) 	.loc	1 333 86                        // quant_slide_tuned_Llama3.2-1B.py:333:86
(EngineCore_DP0 pid=291547) 	cvt.rzi.s32.f32 	%r101, %r100;
(EngineCore_DP0 pid=291547) 	.loc	1 331 76                        // quant_slide_tuned_Llama3.2-1B.py:331:76
(EngineCore_DP0 pid=291547) 	max.f32 	%r102, %r96, 0fC3000000;
(EngineCore_DP0 pid=291547) 	max.f32 	%r103, %r94, 0fC3000000;
(EngineCore_DP0 pid=291547) 	min.f32 	%r104, %r103, 0f42FE0000;
(EngineCore_DP0 pid=291547) 	min.f32 	%r105, %r102, 0f42FE0000;
(EngineCore_DP0 pid=291547) 	.loc	1 331 86                        // quant_slide_tuned_Llama3.2-1B.py:331:86
(EngineCore_DP0 pid=291547) 	cvt.rzi.s32.f32 	%r106, %r105;
(EngineCore_DP0 pid=291547) 	cvt.rzi.s32.f32 	%r107, %r104;
(EngineCore_DP0 pid=291547) 	.loc	1 335 30                        // quant_slide_tuned_Llama3.2-1B.py:335:30
(EngineCore_DP0 pid=291547) 	shl.b32 	%r108, %r107, 8;
(EngineCore_DP0 pid=291547) 	shl.b32 	%r109, %r106, 16;
(EngineCore_DP0 pid=291547) 	and.b32 	%r110, %r109, 16711680;
(EngineCore_DP0 pid=291547) 	and.b32 	%r111, %r108, 65280;
(EngineCore_DP0 pid=291547) 	.loc	1 335 24                        // quant_slide_tuned_Llama3.2-1B.py:335:24
(EngineCore_DP0 pid=291547) 	or.b32 	%r112, %r111, %r92;
(EngineCore_DP0 pid=291547) 	.loc	1 335 36                        // quant_slide_tuned_Llama3.2-1B.py:335:36
(EngineCore_DP0 pid=291547) 	or.b32 	%r113, %r112, %r110;
(EngineCore_DP0 pid=291547) 	.loc	1 335 55                        // quant_slide_tuned_Llama3.2-1B.py:335:55
(EngineCore_DP0 pid=291547) 	shl.b32 	%r114, %r101, 24;
(EngineCore_DP0 pid=291547) 	.loc	1 335 49                        // quant_slide_tuned_Llama3.2-1B.py:335:49
(EngineCore_DP0 pid=291547) 	or.b32 	%r70, %r113, %r114;
(EngineCore_DP0 pid=291547) 	.loc	1 336 29                        // quant_slide_tuned_Llama3.2-1B.py:336:29
(EngineCore_DP0 pid=291547) 	mad.wide.s32 	%rd12, %r72, 4, %rd2;
(EngineCore_DP0 pid=291547) 	.loc	1 336 39                        // quant_slide_tuned_Llama3.2-1B.py:336:39
(EngineCore_DP0 pid=291547) 	// begin inline asm
(EngineCore_DP0 pid=291547) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r70 };
(EngineCore_DP0 pid=291547) 	// end inline asm
(EngineCore_DP0 pid=291547) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=291547) 	add.s32 	%r118, %r118, 1024;
(EngineCore_DP0 pid=291547) 	setp.lt.s32 	%p18, %r118, %r14;
(EngineCore_DP0 pid=291547) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=291547) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=291547) 	.loc	1 312 4                         // quant_slide_tuned_Llama3.2-1B.py:312:4
(EngineCore_DP0 pid=291547) 	ret;
(EngineCore_DP0 pid=291547) $L__tmp3:
(EngineCore_DP0 pid=291547) $L__func_end0:
(EngineCore_DP0 pid=291547)                                         // -- End function
(EngineCore_DP0 pid=291547) }
(EngineCore_DP0 pid=291547) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=291547) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=291547) 	.section	.debug_abbrev
(EngineCore_DP0 pid=291547) 	{
(EngineCore_DP0 pid=291547) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=291547) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=291547) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=291547) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=291547) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=291547) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=291547) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=291547) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=291547) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=291547) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=291547) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=291547) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=291547) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=291547) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=291547) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=291547) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=291547) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=291547) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=291547) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=291547) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=291547) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=291547) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=291547) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=291547) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=291547) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=291547) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=291547) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=291547) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=291547) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=291547) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=291547) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=291547) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=291547) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=291547) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=291547) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=291547) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=291547) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=291547) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=291547) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=291547) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=291547) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=291547) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=291547) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=291547) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=291547) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=291547) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=291547) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=291547) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=291547) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=291547) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=291547) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=291547) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=291547) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=291547) 	}
(EngineCore_DP0 pid=291547) 	.section	.debug_info
(EngineCore_DP0 pid=291547) 	{
(EngineCore_DP0 pid=291547) .b32 224                                // Length of Unit
(EngineCore_DP0 pid=291547) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=291547) .b8 0
(EngineCore_DP0 pid=291547) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=291547) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=291547) .b8 1                                   // Abbrev [1] 0xb:0xd9 DW_TAG_compile_unit
(EngineCore_DP0 pid=291547) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=291547) .b8 114
(EngineCore_DP0 pid=291547) .b8 105
(EngineCore_DP0 pid=291547) .b8 116
(EngineCore_DP0 pid=291547) .b8 111
(EngineCore_DP0 pid=291547) .b8 110
(EngineCore_DP0 pid=291547) .b8 0
(EngineCore_DP0 pid=291547) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=291547) .b8 0
(EngineCore_DP0 pid=291547) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=291547) .b8 117
(EngineCore_DP0 pid=291547) .b8 97
(EngineCore_DP0 pid=291547) .b8 110
(EngineCore_DP0 pid=291547) .b8 116
(EngineCore_DP0 pid=291547) .b8 95
(EngineCore_DP0 pid=291547) .b8 115
(EngineCore_DP0 pid=291547) .b8 108
(EngineCore_DP0 pid=291547) .b8 105
(EngineCore_DP0 pid=291547) .b8 100
(EngineCore_DP0 pid=291547) .b8 101
(EngineCore_DP0 pid=291547) .b8 95
(EngineCore_DP0 pid=291547) .b8 116
(EngineCore_DP0 pid=291547) .b8 117
(EngineCore_DP0 pid=291547) .b8 110
(EngineCore_DP0 pid=291547) .b8 101
(EngineCore_DP0 pid=291547) .b8 100
(EngineCore_DP0 pid=291547) .b8 95
(EngineCore_DP0 pid=291547) .b8 76
(EngineCore_DP0 pid=291547) .b8 108
(EngineCore_DP0 pid=291547) .b8 97
(EngineCore_DP0 pid=291547) .b8 109
(EngineCore_DP0 pid=291547) .b8 97
(EngineCore_DP0 pid=291547) .b8 51
(EngineCore_DP0 pid=291547) .b8 46
(EngineCore_DP0 pid=291547) .b8 50
(EngineCore_DP0 pid=291547) .b8 45
(EngineCore_DP0 pid=291547) .b8 49
(EngineCore_DP0 pid=291547) .b8 66
(EngineCore_DP0 pid=291547) .b8 46
(EngineCore_DP0 pid=291547) .b8 112
(EngineCore_DP0 pid=291547) .b8 121
(EngineCore_DP0 pid=291547) .b8 0
(EngineCore_DP0 pid=291547) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=291547) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=291547) .b8 114
(EngineCore_DP0 pid=291547) .b8 111
(EngineCore_DP0 pid=291547) .b8 111
(EngineCore_DP0 pid=291547) .b8 116
(EngineCore_DP0 pid=291547) .b8 47
(EngineCore_DP0 pid=291547) .b8 118
(EngineCore_DP0 pid=291547) .b8 108
(EngineCore_DP0 pid=291547) .b8 108
(EngineCore_DP0 pid=291547) .b8 109
(EngineCore_DP0 pid=291547) .b8 98
(EngineCore_DP0 pid=291547) .b8 101
(EngineCore_DP0 pid=291547) .b8 110
(EngineCore_DP0 pid=291547) .b8 99
(EngineCore_DP0 pid=291547) .b8 104
(EngineCore_DP0 pid=291547) .b8 47
(EngineCore_DP0 pid=291547) .b8 115
(EngineCore_DP0 pid=291547) .b8 108
(EngineCore_DP0 pid=291547) .b8 105
(EngineCore_DP0 pid=291547) .b8 100
(EngineCore_DP0 pid=291547) .b8 101
(EngineCore_DP0 pid=291547) .b8 115
(EngineCore_DP0 pid=291547) .b8 112
(EngineCore_DP0 pid=291547) .b8 97
(EngineCore_DP0 pid=291547) .b8 114
(EngineCore_DP0 pid=291547) .b8 115
(EngineCore_DP0 pid=291547) .b8 101
(EngineCore_DP0 pid=291547) .b8 47
(EngineCore_DP0 pid=291547) .b8 99
(EngineCore_DP0 pid=291547) .b8 115
(EngineCore_DP0 pid=291547) .b8 114
(EngineCore_DP0 pid=291547) .b8 99
(EngineCore_DP0 pid=291547) .b8 47
(EngineCore_DP0 pid=291547) .b8 102
(EngineCore_DP0 pid=291547) .b8 117
(EngineCore_DP0 pid=291547) .b8 115
(EngineCore_DP0 pid=291547) .b8 101
(EngineCore_DP0 pid=291547) .b8 100
(EngineCore_DP0 pid=291547) .b8 95
(EngineCore_DP0 pid=291547) .b8 113
(EngineCore_DP0 pid=291547) .b8 117
(EngineCore_DP0 pid=291547) .b8 97
(EngineCore_DP0 pid=291547) .b8 110
(EngineCore_DP0 pid=291547) .b8 116
(EngineCore_DP0 pid=291547) .b8 95
(EngineCore_DP0 pid=291547) .b8 115
(EngineCore_DP0 pid=291547) .b8 108
(EngineCore_DP0 pid=291547) .b8 105
(EngineCore_DP0 pid=291547) .b8 100
(EngineCore_DP0 pid=291547) .b8 101
(EngineCore_DP0 pid=291547) .b8 95
(EngineCore_DP0 pid=291547) .b8 116
(EngineCore_DP0 pid=291547) .b8 114
(EngineCore_DP0 pid=291547) .b8 105
(EngineCore_DP0 pid=291547) .b8 116
(EngineCore_DP0 pid=291547) .b8 111
(EngineCore_DP0 pid=291547) .b8 110
(EngineCore_DP0 pid=291547) .b8 47
(EngineCore_DP0 pid=291547) .b8 98
(EngineCore_DP0 pid=291547) .b8 117
(EngineCore_DP0 pid=291547) .b8 105
(EngineCore_DP0 pid=291547) .b8 108
(EngineCore_DP0 pid=291547) .b8 100
(EngineCore_DP0 pid=291547) .b8 47
(EngineCore_DP0 pid=291547) .b8 71
(EngineCore_DP0 pid=291547) .b8 66
(EngineCore_DP0 pid=291547) .b8 49
(EngineCore_DP0 pid=291547) .b8 48
(EngineCore_DP0 pid=291547) .b8 95
(EngineCore_DP0 pid=291547) .b8 99
(EngineCore_DP0 pid=291547) .b8 99
(EngineCore_DP0 pid=291547) .b8 49
(EngineCore_DP0 pid=291547) .b8 50
(EngineCore_DP0 pid=291547) .b8 49
(EngineCore_DP0 pid=291547) .b8 95
(EngineCore_DP0 pid=291547) .b8 112
(EngineCore_DP0 pid=291547) .b8 121
(EngineCore_DP0 pid=291547) .b8 51
(EngineCore_DP0 pid=291547) .b8 49
(EngineCore_DP0 pid=291547) .b8 50
(EngineCore_DP0 pid=291547) .b8 95
(EngineCore_DP0 pid=291547) .b8 99
(EngineCore_DP0 pid=291547) .b8 117
(EngineCore_DP0 pid=291547) .b8 49
(EngineCore_DP0 pid=291547) .b8 50
(EngineCore_DP0 pid=291547) .b8 57
(EngineCore_DP0 pid=291547) .b8 95
(EngineCore_DP0 pid=291547) .b8 97
(EngineCore_DP0 pid=291547) .b8 97
(EngineCore_DP0 pid=291547) .b8 114
(EngineCore_DP0 pid=291547) .b8 99
(EngineCore_DP0 pid=291547) .b8 104
(EngineCore_DP0 pid=291547) .b8 54
(EngineCore_DP0 pid=291547) .b8 52
(EngineCore_DP0 pid=291547) .b8 0
(EngineCore_DP0 pid=291547) .b8 2                                   // Abbrev [2] 0x99:0x1b DW_TAG_subprogram
(EngineCore_DP0 pid=291547) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=291547) .b8 113
(EngineCore_DP0 pid=291547) .b8 117
(EngineCore_DP0 pid=291547) .b8 97
(EngineCore_DP0 pid=291547) .b8 110
(EngineCore_DP0 pid=291547) .b8 116
(EngineCore_DP0 pid=291547) .b8 95
(EngineCore_DP0 pid=291547) .b8 115
(EngineCore_DP0 pid=291547) .b8 108
(EngineCore_DP0 pid=291547) .b8 105
(EngineCore_DP0 pid=291547) .b8 100
(EngineCore_DP0 pid=291547) .b8 101
(EngineCore_DP0 pid=291547) .b8 95
(EngineCore_DP0 pid=291547) .b8 105
(EngineCore_DP0 pid=291547) .b8 110
(EngineCore_DP0 pid=291547) .b8 116
(EngineCore_DP0 pid=291547) .b8 56
(EngineCore_DP0 pid=291547) .b8 95
(EngineCore_DP0 pid=291547) .b8 107
(EngineCore_DP0 pid=291547) .b8 101
(EngineCore_DP0 pid=291547) .b8 114
(EngineCore_DP0 pid=291547) .b8 110
(EngineCore_DP0 pid=291547) .b8 101
(EngineCore_DP0 pid=291547) .b8 108
(EngineCore_DP0 pid=291547) .b8 0
(EngineCore_DP0 pid=291547) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=291547) .b8 3                                   // Abbrev [3] 0xb4:0x2f DW_TAG_subprogram
(EngineCore_DP0 pid=291547) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=291547) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=291547) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=291547) .b8 4                                   // Abbrev [4] 0xc9:0x19 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=291547) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=291547) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=291547) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=291547) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=291547) .b8 46                                  // DW_AT_call_line
(EngineCore_DP0 pid=291547) .b8 1
(EngineCore_DP0 pid=291547) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=291547) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=291547) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=291547) 	}
(EngineCore_DP0 pid=291547) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=291547) 
(EngineCore_DP0 pid=291547) ================================================================
(EngineCore_DP0 pid=291547) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=291547) 
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmp0hrkqz6j.ptx', '-o', '/tmp/tmp0hrkqz6j.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866] 
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866] 
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866] 
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp0hrkqz6j.ptx -o /tmp/tmp0hrkqz6j.ptx.o
(EngineCore_DP0 pid=291547) ERROR 01-25 18:41:45 [core.py:866] 

STDERR:
[2026-01-25 18:41:29] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:41:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:41:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:41:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:41:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:41:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:41:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:41:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:41:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:41:33] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:41:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:41:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:41:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:41:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:41:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:41:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:41:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:41:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:41:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=291547) [2026-01-25 18:41:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=291547) [2026-01-25 18:41:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=291547) [2026-01-25 18:41:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=291547) [2026-01-25 18:41:34] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=291547) [2026-01-25 18:41:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=291547) [2026-01-25 18:41:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=291547) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=291547) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.25s/it]
(EngineCore_DP0 pid=291547) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.25s/it]
(EngineCore_DP0 pid=291547) 
(EngineCore_DP0 pid=291547) [2026-01-25 18:41:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=291547) [2026-01-25 18:41:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=291547) [2026-01-25 18:41:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=291547) [2026-01-25 18:41:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=291547) [2026-01-25 18:41:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=291547) [2026-01-25 18:41:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=291547) [2026-01-25 18:41:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=291547) [2026-01-25 18:41:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=291547) Process EngineCore_DP0:
(EngineCore_DP0 pid=291547) Traceback (most recent call last):
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=291547)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=291547)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=291547)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=291547) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmp0hrkqz6j.ptx', '-o', '/tmp/tmp0hrkqz6j.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=291547) 
(EngineCore_DP0 pid=291547) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=291547) 
(EngineCore_DP0 pid=291547) Traceback (most recent call last):
(EngineCore_DP0 pid=291547)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=291547)     self.run()
(EngineCore_DP0 pid=291547)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=291547)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=291547)     raise e
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=291547)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=291547)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=291547)     super().__init__(
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=291547)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=291547)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=291547)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=291547)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=291547)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=291547)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=291547)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=291547)     return func(*args, **kwargs)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=291547)     return func(*args, **kwargs)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=291547)     self.model_runner.profile_run()
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=291547)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=291547)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=291547)     return func(*args, **kwargs)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=291547)     outputs = self.model(
(EngineCore_DP0 pid=291547)               ^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291547)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291547)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=291547)     model_output = self.model(
(EngineCore_DP0 pid=291547)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=291547)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=291547)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=291547)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291547)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291547)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=291547)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=291547)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291547)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291547)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=291547)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=291547)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=291547)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=291547)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=291547)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=291547)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=291547)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=291547)     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=291547)     return self._linear_fn(
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=291547)     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=291547)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=291547)     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=291547)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=291547)     return fn(input, L)
(EngineCore_DP0 pid=291547)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=291547)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=291547)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=291547)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=291547)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=291547)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=291547)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=291547)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=291547)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=291547)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=291547)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=291547)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=291547)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=291547)     raise PTXASError(error)
(EngineCore_DP0 pid=291547) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=291547) `ptxas` stderr:
(EngineCore_DP0 pid=291547) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=291547) 
(EngineCore_DP0 pid=291547) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp0hrkqz6j.ptx -o /tmp/tmp0hrkqz6j.ptx.o
(EngineCore_DP0 pid=291547) 
[rank0]:[W125 18:41:45.645224525 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=256

========== M=16 ==========
Time: 2026-01-26 02:28:14
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-INT8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:28:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:28:18 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=742802) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=742802) WARNING 01-26 02:28:37 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.48 requests/s, 1489.41 total tokens/s, 1401.80 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096

STDERR:
[2026-01-26 02:28:18] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:28:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:28:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:28:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:28:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:28:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:28:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:28:21] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:28:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:28:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:28:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:28:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:28:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:28:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:22] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=742802) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=742802) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.08s/it]
(EngineCore_DP0 pid=742802) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.08s/it]
(EngineCore_DP0 pid=742802) 
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=742802) 2026-01-26 02:28:37,433 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=742802) 2026-01-26 02:28:37,440 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|| 16/16 [00:00<00:00, 8644.71it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|         | 1/16 [00:02<00:43,  2.89s/it, est. speed input: 5.53 toks/s, output: 88.51 toks/s]
Processed prompts: 100%|| 16/16 [00:02<00:00,  2.89s/it, est. speed input: 87.69 toks/s, output: 1403.03 toks/s]
Processed prompts: 100%|| 16/16 [00:02<00:00,  5.48it/s, est. speed input: 87.69 toks/s, output: 1403.03 toks/s]
[rank0]:[W126 02:28:41.265358179 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 02:28:43
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:28:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:28:46 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=743364) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=743364) WARNING 01-26 02:29:06 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 23.55 requests/s, 6405.89 total tokens/s, 6029.07 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 02:28:46] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:28:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:28:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:28:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:28:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:28:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:28:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:28:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:28:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:50] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:28:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:28:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:28:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:28:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:28:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=743364) [2026-01-26 02:28:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=743364) [2026-01-26 02:28:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=743364) [2026-01-26 02:28:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=743364) [2026-01-26 02:28:51] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=743364) [2026-01-26 02:28:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=743364) [2026-01-26 02:28:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=743364) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=743364) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.18s/it]
(EngineCore_DP0 pid=743364) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.18s/it]
(EngineCore_DP0 pid=743364) 
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=743364) 2026-01-26 02:29:05,785 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=743364) 2026-01-26 02:29:05,792 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 13201.64it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:05<10:43,  5.07s/it, est. speed input: 3.16 toks/s, output: 50.51 toks/s]
Processed prompts:   2%|         | 3/128 [00:05<02:50,  1.36s/it, est. speed input: 9.24 toks/s, output: 147.90 toks/s]
Processed prompts:  48%|     | 62/128 [00:05<00:02, 22.66it/s, est. speed input: 186.84 toks/s, output: 2989.46 toks/s]
Processed prompts:  92%|| 118/128 [00:05<00:00, 49.65it/s, est. speed input: 348.68 toks/s, output: 5578.78 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 49.65it/s, est. speed input: 377.55 toks/s, output: 6040.80 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 23.60it/s, est. speed input: 377.55 toks/s, output: 6040.80 toks/s]
[rank0]:[W126 02:29:12.182585763 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 02:29:14
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:29:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:29:17 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=744003) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=744003) WARNING 01-26 02:29:34 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 29.38 requests/s, 7990.95 total tokens/s, 7520.90 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 02:29:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:29:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:29:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:29:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:29:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:29:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:29:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:29:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:29:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:29:20] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:29:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:29:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:29:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:29:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:29:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:29:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:29:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:29:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:21] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=744003) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=744003) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.60s/it]
(EngineCore_DP0 pid=744003) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.60s/it]
(EngineCore_DP0 pid=744003) 
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=744003) 2026-01-26 02:29:34,124 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=744003) 2026-01-26 02:29:34,131 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 13552.73it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<32:29,  7.65s/it, est. speed input: 2.09 toks/s, output: 33.48 toks/s]
Processed prompts:  13%|        | 33/256 [00:07<00:37,  5.96it/s, est. speed input: 67.90 toks/s, output: 1086.38 toks/s]
Processed prompts:  29%|       | 75/256 [00:07<00:11, 16.43it/s, est. speed input: 152.12 toks/s, output: 2433.91 toks/s]
Processed prompts:  42%|     | 108/256 [00:07<00:05, 27.36it/s, est. speed input: 216.26 toks/s, output: 3460.17 toks/s]
Processed prompts:  56%|    | 144/256 [00:08<00:02, 42.87it/s, est. speed input: 284.31 toks/s, output: 4548.96 toks/s]
Processed prompts:  70%|   | 178/256 [00:08<00:01, 60.55it/s, est. speed input: 345.97 toks/s, output: 5535.55 toks/s]
Processed prompts:  81%|  | 207/256 [00:08<00:00, 78.72it/s, est. speed input: 397.13 toks/s, output: 6354.09 toks/s]
Processed prompts:  92%|| 235/256 [00:08<00:00, 93.81it/s, est. speed input: 442.40 toks/s, output: 7078.36 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 93.81it/s, est. speed input: 471.12 toks/s, output: 7537.98 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 29.44it/s, est. speed input: 471.12 toks/s, output: 7537.98 toks/s]
[rank0]:[W126 02:29:43.824635377 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 16:53:46
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:53:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:53:49 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2833027) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2833027) WARNING 01-27 16:54:09 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.57 requests/s, 4234.72 total tokens/s, 3985.62 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 16:53:49] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:53:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:53:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:53:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:53:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:53:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:53:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:53:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:53:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:53:53] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:53:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:53:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:53:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:53:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:53:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:53:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:53:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:53:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2833027) [2026-01-27 16:53:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2833027) [2026-01-27 16:53:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2833027) [2026-01-27 16:53:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2833027) [2026-01-27 16:53:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2833027) [2026-01-27 16:53:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2833027) [2026-01-27 16:53:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2833027) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2833027) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.19s/it]
(EngineCore_DP0 pid=2833027) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.19s/it]
(EngineCore_DP0 pid=2833027) 
(EngineCore_DP0 pid=2833027) [2026-01-27 16:54:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=2833027) [2026-01-27 16:54:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=2833027) [2026-01-27 16:54:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=2833027) [2026-01-27 16:54:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=2833027) [2026-01-27 16:54:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=2833027) [2026-01-27 16:54:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=2833027) [2026-01-27 16:54:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=2833027) [2026-01-27 16:54:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=2833027) 2026-01-27 16:54:09,107 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2833027) 2026-01-27 16:54:09,115 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 11785.37it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:04<04:12,  4.01s/it, est. speed input: 3.99 toks/s, output: 63.90 toks/s]
Processed prompts: 100%|| 64/64 [00:04<00:00,  4.01s/it, est. speed input: 249.49 toks/s, output: 3991.83 toks/s]
Processed prompts: 100%|| 64/64 [00:04<00:00, 15.59it/s, est. speed input: 249.49 toks/s, output: 3991.83 toks/s]
[rank0]:[W127 16:54:14.021566938 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 16:54:16
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:54:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:54:19 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2833634) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2833634) WARNING 01-27 16:54:40 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 23.80 requests/s, 6473.82 total tokens/s, 6093.01 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 16:54:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:54:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:54:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:54:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:54:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:54:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:54:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:54:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:54:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:54:23] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:54:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:54:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:54:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:54:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:54:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:54:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:54:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:54:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2833634) [2026-01-27 16:54:24] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2833634) [2026-01-27 16:54:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2833634) [2026-01-27 16:54:24] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2833634) [2026-01-27 16:54:24] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2833634) [2026-01-27 16:54:24] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2833634) [2026-01-27 16:54:24] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2833634) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2833634) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.14s/it]
(EngineCore_DP0 pid=2833634) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.14s/it]
(EngineCore_DP0 pid=2833634) 
(EngineCore_DP0 pid=2833634) [2026-01-27 16:54:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=2833634) [2026-01-27 16:54:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=2833634) [2026-01-27 16:54:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=2833634) [2026-01-27 16:54:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=2833634) [2026-01-27 16:54:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=2833634) [2026-01-27 16:54:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=2833634) [2026-01-27 16:54:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=2833634) [2026-01-27 16:54:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=2833634) 2026-01-27 16:54:39,544 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2833634) 2026-01-27 16:54:39,551 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 13173.45it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:05<10:36,  5.01s/it, est. speed input: 3.19 toks/s, output: 51.08 toks/s]
Processed prompts:  28%|       | 36/128 [00:05<00:09,  9.89it/s, est. speed input: 112.64 toks/s, output: 1802.21 toks/s]
Processed prompts:  77%|  | 99/128 [00:05<00:00, 33.27it/s, est. speed input: 302.80 toks/s, output: 4844.82 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 33.27it/s, est. speed input: 381.56 toks/s, output: 6105.01 toks/s]
Processed prompts: 100%|| 128/128 [00:05<00:00, 23.85it/s, est. speed input: 381.56 toks/s, output: 6105.01 toks/s]
[rank0]:[W127 16:54:45.715065547 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 16:54:47
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:54:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:54:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2834273) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2834273) WARNING 01-27 16:55:12 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 28.96 requests/s, 7877.84 total tokens/s, 7414.43 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 16:54:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:54:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:54:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:54:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:54:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:54:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:54:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:54:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:54:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:54:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:54:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:54:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:54:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:54:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:54:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:54:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:54:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:54:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:54:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2834273) [2026-01-27 16:54:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2834273) [2026-01-27 16:54:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2834273) [2026-01-27 16:54:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2834273) [2026-01-27 16:54:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2834273) [2026-01-27 16:54:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2834273) [2026-01-27 16:54:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2834273) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2834273) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.41s/it]
(EngineCore_DP0 pid=2834273) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.41s/it]
(EngineCore_DP0 pid=2834273) 
(EngineCore_DP0 pid=2834273) [2026-01-27 16:55:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=2834273) [2026-01-27 16:55:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=2834273) [2026-01-27 16:55:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=2834273) [2026-01-27 16:55:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=2834273) [2026-01-27 16:55:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=2834273) [2026-01-27 16:55:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=2834273) [2026-01-27 16:55:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=2834273) [2026-01-27 16:55:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=2834273) 2026-01-27 16:55:11,760 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2834273) 2026-01-27 16:55:11,769 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 12736.24it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<32:28,  7.64s/it, est. speed input: 2.09 toks/s, output: 33.50 toks/s]
Processed prompts:  13%|        | 34/256 [00:07<00:36,  6.13it/s, est. speed input: 69.86 toks/s, output: 1117.82 toks/s]
Processed prompts:  29%|       | 75/256 [00:07<00:11, 16.34it/s, est. speed input: 152.03 toks/s, output: 2432.51 toks/s]
Processed prompts:  46%|     | 118/256 [00:08<00:04, 30.45it/s, est. speed input: 235.31 toks/s, output: 3764.92 toks/s]
Processed prompts:  59%|    | 152/256 [00:08<00:02, 44.29it/s, est. speed input: 298.14 toks/s, output: 4770.20 toks/s]
Processed prompts:  70%|   | 180/256 [00:08<00:01, 58.90it/s, est. speed input: 348.78 toks/s, output: 5580.42 toks/s]
Processed prompts:  81%| | 208/256 [00:08<00:00, 76.17it/s, est. speed input: 397.66 toks/s, output: 6362.60 toks/s]
Processed prompts:  92%|| 235/256 [00:08<00:00, 92.26it/s, est. speed input: 441.83 toks/s, output: 7069.25 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 92.26it/s, est. speed input: 464.51 toks/s, output: 7432.19 toks/s]
Processed prompts: 100%|| 256/256 [00:08<00:00, 29.03it/s, est. speed input: 464.51 toks/s, output: 7432.19 toks/s]
[rank0]:[W127 16:55:21.426207832 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 16:55:23
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-1B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:55:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:55:27 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2834959) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2834959) WARNING 01-27 16:55:47 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 30.61 requests/s, 8326.93 total tokens/s, 7837.11 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 16:55:27] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:55:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:55:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:55:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:55:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:55:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:55:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:55:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:55:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:55:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:55:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:55:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:55:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:55:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:55:30] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:55:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:55:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:55:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:55:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:55:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:55:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:55:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:55:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:55:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:55:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:55:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:55:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:55:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2834959) [2026-01-27 16:55:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2834959) [2026-01-27 16:55:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2834959) [2026-01-27 16:55:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2834959) [2026-01-27 16:55:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2834959) [2026-01-27 16:55:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2834959) [2026-01-27 16:55:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2834959) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2834959) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.01s/it]
(EngineCore_DP0 pid=2834959) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.01s/it]
(EngineCore_DP0 pid=2834959) 
(EngineCore_DP0 pid=2834959) [2026-01-27 16:55:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=2834959) [2026-01-27 16:55:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=2834959) [2026-01-27 16:55:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=2834959) [2026-01-27 16:55:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=2834959) [2026-01-27 16:55:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=2834959) [2026-01-27 16:55:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=2834959) [2026-01-27 16:55:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=2834959) [2026-01-27 16:55:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=2834959) 2026-01-27 16:55:46,951 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2834959) 2026-01-27 16:55:46,968 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 14702.95it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:12<1:48:28, 12.74s/it, est. speed input: 1.26 toks/s, output: 20.10 toks/s]
Processed prompts:   0%|          | 2/512 [00:12<45:06,  5.31s/it, est. speed input: 2.49 toks/s, output: 39.87 toks/s]  
Processed prompts:   6%|         | 33/512 [00:12<01:35,  5.02it/s, est. speed input: 40.79 toks/s, output: 652.61 toks/s]
Processed prompts:  18%|        | 91/512 [00:13<00:24, 17.27it/s, est. speed input: 111.01 toks/s, output: 1776.14 toks/s]
Processed prompts:  28%|       | 141/512 [00:13<00:11, 31.22it/s, est. speed input: 170.15 toks/s, output: 2722.44 toks/s]
Processed prompts:  36%|      | 185/512 [00:13<00:06, 47.50it/s, est. speed input: 221.43 toks/s, output: 3542.88 toks/s]
Processed prompts:  47%|     | 242/512 [00:13<00:03, 74.02it/s, est. speed input: 286.56 toks/s, output: 4584.88 toks/s]
Processed prompts:  54%|    | 278/512 [00:13<00:02, 92.54it/s, est. speed input: 326.18 toks/s, output: 5218.94 toks/s]
Processed prompts:  62%|   | 315/512 [00:13<00:01, 116.30it/s, est. speed input: 366.61 toks/s, output: 5865.74 toks/s]
Processed prompts:  70%|   | 358/512 [00:13<00:01, 146.11it/s, est. speed input: 412.64 toks/s, output: 6602.25 toks/s]
Processed prompts:  77%|  | 392/512 [00:14<00:00, 164.64it/s, est. speed input: 447.51 toks/s, output: 7160.19 toks/s]
Processed prompts:  83%| | 423/512 [00:14<00:00, 181.69it/s, est. speed input: 478.82 toks/s, output: 7661.12 toks/s]
Processed prompts:  88%| | 453/512 [00:14<00:00, 187.00it/s, est. speed input: 507.49 toks/s, output: 8119.87 toks/s]
Processed prompts:  94%|| 480/512 [00:14<00:00, 181.69it/s, est. speed input: 531.74 toks/s, output: 8507.85 toks/s]
Processed prompts:  98%|| 504/512 [00:16<00:00, 37.52it/s, est. speed input: 483.77 toks/s, output: 7740.32 toks/s] 
Processed prompts: 100%|| 512/512 [00:16<00:00, 37.52it/s, est. speed input: 490.87 toks/s, output: 7853.86 toks/s]
Processed prompts: 100%|| 512/512 [00:16<00:00, 30.68it/s, est. speed input: 490.87 toks/s, output: 7853.86 toks/s]
[rank0]:[W127 16:56:04.543356965 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:21:10
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-3B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:21:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:21:13 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2863556) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2863556) WARNING 01-27 17:21:50 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 8.15 requests/s, 2215.56 total tokens/s, 2085.23 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:21:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:21:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:21:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:21:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:21:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:21:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:21:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:21:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:21:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:21:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:21:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:21:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:21:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:21:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:21:16] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:21:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:21:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:21:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:21:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:21:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:21:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:21:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:21:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:21:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:21:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:21:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:21:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:21:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2863556) [2026-01-27 17:21:17] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2863556) [2026-01-27 17:21:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2863556) [2026-01-27 17:21:17] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2863556) [2026-01-27 17:21:17] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2863556) [2026-01-27 17:21:17] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2863556) [2026-01-27 17:21:17] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2863556) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2863556) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:25<00:00, 25.06s/it]
(EngineCore_DP0 pid=2863556) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:25<00:00, 25.06s/it]
(EngineCore_DP0 pid=2863556) 
(EngineCore_DP0 pid=2863556) [2026-01-27 17:21:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=2863556) [2026-01-27 17:21:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=2863556) [2026-01-27 17:21:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=2863556) [2026-01-27 17:21:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=2863556) [2026-01-27 17:21:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=2863556) [2026-01-27 17:21:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=2863556) [2026-01-27 17:21:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=2863556) [2026-01-27 17:21:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=2863556) 2026-01-27 17:21:50,011 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2863556) 2026-01-27 17:21:50,023 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 12133.77it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:07<08:03,  7.67s/it, est. speed input: 2.09 toks/s, output: 33.38 toks/s]
Processed prompts:  53%|    | 34/64 [00:07<00:04,  6.14it/s, est. speed input: 69.82 toks/s, output: 1117.09 toks/s]
Processed prompts: 100%|| 64/64 [00:07<00:00,  6.14it/s, est. speed input: 130.43 toks/s, output: 2086.96 toks/s]
Processed prompts: 100%|| 64/64 [00:07<00:00,  8.15it/s, est. speed input: 130.43 toks/s, output: 2086.96 toks/s]
[rank0]:[W127 17:21:58.605226595 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:22:00
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-3B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:22:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:22:04 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2864453) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2864453) WARNING 01-27 17:22:40 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 12.34 requests/s, 3355.25 total tokens/s, 3157.88 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:22:04] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:22:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:22:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:22:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:22:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:22:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:22:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:22:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:22:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:22:07] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:22:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:22:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:22:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:22:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:22:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:22:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:22:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:22:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2864453) [2026-01-27 17:22:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2864453) [2026-01-27 17:22:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2864453) [2026-01-27 17:22:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2864453) [2026-01-27 17:22:08] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2864453) [2026-01-27 17:22:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2864453) [2026-01-27 17:22:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2864453) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2864453) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.52s/it]
(EngineCore_DP0 pid=2864453) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.52s/it]
(EngineCore_DP0 pid=2864453) 
(EngineCore_DP0 pid=2864453) [2026-01-27 17:22:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=2864453) [2026-01-27 17:22:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=2864453) [2026-01-27 17:22:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=2864453) [2026-01-27 17:22:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=2864453) [2026-01-27 17:22:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=2864453) [2026-01-27 17:22:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=2864453) [2026-01-27 17:22:33] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=2864453) [2026-01-27 17:22:33] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=2864453) 2026-01-27 17:22:39,532 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2864453) 2026-01-27 17:22:39,543 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 12749.55it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:09<20:50,  9.85s/it, est. speed input: 1.63 toks/s, output: 26.00 toks/s]
Processed prompts:  16%|        | 20/128 [00:09<00:38,  2.82it/s, est. speed input: 32.17 toks/s, output: 514.66 toks/s]
Processed prompts:  38%|      | 49/128 [00:10<00:09,  8.55it/s, est. speed input: 77.96 toks/s, output: 1247.29 toks/s]
Processed prompts:  69%|   | 88/128 [00:10<00:02, 19.10it/s, est. speed input: 138.53 toks/s, output: 2216.41 toks/s]
Processed prompts: 100%|| 128/128 [00:10<00:00, 32.35it/s, est. speed input: 197.58 toks/s, output: 3161.26 toks/s]
Processed prompts: 100%|| 128/128 [00:10<00:00, 32.35it/s, est. speed input: 197.58 toks/s, output: 3161.26 toks/s]
Processed prompts: 100%|| 128/128 [00:10<00:00, 12.35it/s, est. speed input: 197.58 toks/s, output: 3161.26 toks/s]
[rank0]:[W127 17:22:50.813861504 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:22:53
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-3B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:22:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:22:56 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2865380) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2865380) WARNING 01-27 17:23:31 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.37 requests/s, 4181.54 total tokens/s, 3935.57 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:22:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:22:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:22:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:22:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:22:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:22:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:22:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:22:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:22:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:22:59] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:22:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:22:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:22:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:22:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:22:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:22:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:22:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:22:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:22:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2865380) [2026-01-27 17:23:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2865380) [2026-01-27 17:23:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2865380) [2026-01-27 17:23:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2865380) [2026-01-27 17:23:00] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2865380) [2026-01-27 17:23:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2865380) [2026-01-27 17:23:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2865380) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2865380) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.05s/it]
(EngineCore_DP0 pid=2865380) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.05s/it]
(EngineCore_DP0 pid=2865380) 
(EngineCore_DP0 pid=2865380) [2026-01-27 17:23:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=2865380) [2026-01-27 17:23:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=2865380) [2026-01-27 17:23:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=2865380) [2026-01-27 17:23:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=2865380) [2026-01-27 17:23:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=2865380) [2026-01-27 17:23:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=2865380) [2026-01-27 17:23:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=2865380) [2026-01-27 17:23:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=2865380) 2026-01-27 17:23:31,358 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2865380) 2026-01-27 17:23:31,369 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 4355.39it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:14<1:02:05, 14.61s/it, est. speed input: 1.10 toks/s, output: 17.52 toks/s]
Processed prompts:   7%|         | 18/256 [00:14<02:19,  1.70it/s, est. speed input: 19.51 toks/s, output: 312.09 toks/s]
Processed prompts:  19%|        | 48/256 [00:14<00:36,  5.72it/s, est. speed input: 51.50 toks/s, output: 824.03 toks/s]
Processed prompts:  29%|       | 75/256 [00:15<00:17, 10.61it/s, est. speed input: 79.65 toks/s, output: 1274.46 toks/s]
Processed prompts:  38%|      | 98/256 [00:15<00:09, 16.19it/s, est. speed input: 103.19 toks/s, output: 1651.05 toks/s]
Processed prompts:  46%|     | 118/256 [00:15<00:06, 22.64it/s, est. speed input: 123.34 toks/s, output: 1973.42 toks/s]
Processed prompts:  53%|    | 136/256 [00:15<00:03, 30.14it/s, est. speed input: 141.18 toks/s, output: 2258.84 toks/s]
Processed prompts:  62%|   | 159/256 [00:15<00:02, 41.98it/s, est. speed input: 163.60 toks/s, output: 2617.61 toks/s]
Processed prompts:  70%|   | 178/256 [00:15<00:01, 52.41it/s, est. speed input: 181.51 toks/s, output: 2904.17 toks/s]
Processed prompts:  76%|  | 194/256 [00:15<00:00, 62.40it/s, est. speed input: 196.38 toks/s, output: 3142.03 toks/s]
Processed prompts:  82%| | 210/256 [00:15<00:00, 71.21it/s, est. speed input: 210.73 toks/s, output: 3371.63 toks/s]
Processed prompts:  88%| | 225/256 [00:16<00:00, 76.60it/s, est. speed input: 223.61 toks/s, output: 3577.76 toks/s]
Processed prompts:  93%|| 238/256 [00:16<00:00, 79.12it/s, est. speed input: 234.38 toks/s, output: 3750.03 toks/s]
Processed prompts:  98%|| 250/256 [00:16<00:00, 71.39it/s, est. speed input: 242.93 toks/s, output: 3886.84 toks/s]
Processed prompts: 100%|| 256/256 [00:16<00:00, 71.39it/s, est. speed input: 246.86 toks/s, output: 3949.84 toks/s]
Processed prompts: 100%|| 256/256 [00:16<00:00, 15.43it/s, est. speed input: 246.86 toks/s, output: 3949.84 toks/s]
[rank0]:[W127 17:23:48.801705777 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:23:51
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Llama3.2-3B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:23:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:23:54 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2866351) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2866351) WARNING 01-27 17:24:30 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.73 requests/s, 4277.98 total tokens/s, 4026.33 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:23:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:23:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:23:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:23:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:23:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:23:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:23:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:23:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:23:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:23:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:23:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:23:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:23:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:23:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:23:57] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:23:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:23:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:23:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:23:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:23:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:23:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:23:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:23:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:23:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:23:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:23:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:23:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:23:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2866351) [2026-01-27 17:23:58] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2866351) [2026-01-27 17:23:58] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2866351) [2026-01-27 17:23:58] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2866351) [2026-01-27 17:23:58] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2866351) [2026-01-27 17:23:58] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2866351) [2026-01-27 17:23:58] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2866351) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2866351) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.73s/it]
(EngineCore_DP0 pid=2866351) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.73s/it]
(EngineCore_DP0 pid=2866351) 
(EngineCore_DP0 pid=2866351) [2026-01-27 17:24:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=2866351) [2026-01-27 17:24:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=2866351) [2026-01-27 17:24:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=2866351) [2026-01-27 17:24:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=2866351) [2026-01-27 17:24:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=2866351) [2026-01-27 17:24:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=2866351) [2026-01-27 17:24:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=2866351) [2026-01-27 17:24:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=2866351) 2026-01-27 17:24:29,957 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2866351) 2026-01-27 17:24:29,967 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 5834.11it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:24<3:28:47, 24.52s/it, est. speed input: 0.65 toks/s, output: 10.44 toks/s]
Processed prompts:   0%|          | 2/512 [00:24<1:27:03, 10.24s/it, est. speed input: 1.29 toks/s, output: 20.67 toks/s]
Processed prompts:   6%|         | 33/512 [00:24<03:04,  2.59it/s, est. speed input: 21.12 toks/s, output: 337.93 toks/s]
Processed prompts:  12%|        | 63/512 [00:25<01:16,  5.89it/s, est. speed input: 39.97 toks/s, output: 639.46 toks/s]
Processed prompts:  18%|        | 91/512 [00:25<00:41, 10.04it/s, est. speed input: 57.25 toks/s, output: 916.06 toks/s]
Processed prompts:  23%|       | 117/512 [00:25<00:26, 15.07it/s, est. speed input: 72.99 toks/s, output: 1167.78 toks/s]
Processed prompts:  28%|       | 141/512 [00:25<00:17, 21.61it/s, est. speed input: 87.59 toks/s, output: 1401.46 toks/s]
Processed prompts:  32%|      | 164/512 [00:25<00:11, 29.94it/s, est. speed input: 101.48 toks/s, output: 1623.61 toks/s]
Processed prompts:  40%|      | 205/512 [00:26<00:06, 48.13it/s, est. speed input: 125.91 toks/s, output: 2014.63 toks/s]
Processed prompts:  47%|     | 242/512 [00:26<00:04, 66.61it/s, est. speed input: 147.63 toks/s, output: 2362.03 toks/s]
Processed prompts:  53%|    | 273/512 [00:26<00:02, 82.22it/s, est. speed input: 165.45 toks/s, output: 2647.15 toks/s]
Processed prompts:  59%|    | 302/512 [00:26<00:02, 99.52it/s, est. speed input: 182.06 toks/s, output: 2912.96 toks/s]
Processed prompts:  64%|   | 327/512 [00:26<00:01, 111.44it/s, est. speed input: 196.03 toks/s, output: 3136.55 toks/s]
Processed prompts:  68%|   | 348/512 [00:26<00:01, 121.70it/s, est. speed input: 207.67 toks/s, output: 3322.79 toks/s]
Processed prompts:  72%|  | 367/512 [00:26<00:01, 129.94it/s, est. speed input: 218.08 toks/s, output: 3489.36 toks/s]
Processed prompts:  75%|  | 385/512 [00:27<00:00, 137.71it/s, est. speed input: 227.88 toks/s, output: 3646.11 toks/s]
Processed prompts:  79%|  | 403/512 [00:27<00:00, 146.53it/s, est. speed input: 237.66 toks/s, output: 3802.50 toks/s]
Processed prompts:  82%| | 421/512 [00:27<00:00, 139.21it/s, est. speed input: 246.93 toks/s, output: 3950.95 toks/s]
Processed prompts:  86%| | 438/512 [00:27<00:00, 129.57it/s, est. speed input: 255.44 toks/s, output: 4087.07 toks/s]
Processed prompts:  88%| | 453/512 [00:27<00:00, 122.21it/s, est. speed input: 262.82 toks/s, output: 4205.04 toks/s]
Processed prompts:  91%| | 467/512 [00:27<00:00, 114.91it/s, est. speed input: 269.53 toks/s, output: 4312.54 toks/s]
Processed prompts:  94%|| 480/512 [00:27<00:00, 98.62it/s, est. speed input: 275.17 toks/s, output: 4402.73 toks/s] 
Processed prompts:  96%|| 491/512 [00:28<00:00, 76.46it/s, est. speed input: 278.97 toks/s, output: 4463.49 toks/s]
Processed prompts:  98%|| 500/512 [00:32<00:01,  9.03it/s, est. speed input: 246.88 toks/s, output: 3950.13 toks/s]
Processed prompts: 100%|| 512/512 [00:32<00:00,  9.03it/s, est. speed input: 252.34 toks/s, output: 4037.38 toks/s]
Processed prompts: 100%|| 512/512 [00:32<00:00, 15.77it/s, est. speed input: 252.34 toks/s, output: 4037.38 toks/s]
[rank0]:[W127 17:25:03.388304905 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 18:03:17
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:03:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:03:21 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2906273) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2906273) WARNING 01-27 18:04:32 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.53 requests/s, 1233.05 total tokens/s, 1160.52 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 18:03:21] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:03:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:03:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:03:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:03:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:03:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:03:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:03:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:03:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:03:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:03:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:03:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:03:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:03:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:03:24] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:03:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:03:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:03:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:03:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:03:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:03:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:03:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:03:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:03:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:03:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:03:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:03:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:03:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2906273) [2026-01-27 18:03:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2906273) [2026-01-27 18:03:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2906273) [2026-01-27 18:03:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2906273) [2026-01-27 18:03:25] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2906273) [2026-01-27 18:03:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2906273) [2026-01-27 18:03:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2906273) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2906273) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:23<00:23, 23.97s/it]
(EngineCore_DP0 pid=2906273) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:57<00:00, 29.63s/it]
(EngineCore_DP0 pid=2906273) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:57<00:00, 28.78s/it]
(EngineCore_DP0 pid=2906273) 
(EngineCore_DP0 pid=2906273) [2026-01-27 18:04:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=2906273) [2026-01-27 18:04:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=2906273) [2026-01-27 18:04:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=2906273) [2026-01-27 18:04:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=2906273) [2026-01-27 18:04:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=2906273) [2026-01-27 18:04:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=2906273) [2026-01-27 18:04:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=2906273) [2026-01-27 18:04:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=2906273) 2026-01-27 18:04:31,458 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2906273) 2026-01-27 18:04:31,469 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 10349.92it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:13<14:31, 13.83s/it, est. speed input: 1.16 toks/s, output: 18.51 toks/s]
Processed prompts:  31%|      | 20/64 [00:13<00:21,  2.01it/s, est. speed input: 22.95 toks/s, output: 367.18 toks/s]
Processed prompts:  77%|  | 49/64 [00:14<00:02,  6.13it/s, est. speed input: 55.73 toks/s, output: 891.71 toks/s]
Processed prompts: 100%|| 64/64 [00:14<00:00,  6.13it/s, est. speed input: 72.57 toks/s, output: 1161.09 toks/s]
Processed prompts: 100%|| 64/64 [00:14<00:00,  4.54it/s, est. speed input: 72.57 toks/s, output: 1161.09 toks/s]
[rank0]:[W127 18:04:46.426406346 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 18:04:48
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:04:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:04:52 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2907719) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2907719) WARNING 01-27 18:06:02 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.52 requests/s, 2046.79 total tokens/s, 1926.39 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 18:04:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:04:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:04:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:04:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:04:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:04:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:04:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:04:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:04:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:04:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:04:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:04:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:04:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:04:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:04:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:04:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:04:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:04:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:04:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:04:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:04:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:04:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:04:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:04:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:04:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:04:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:04:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:04:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2907719) [2026-01-27 18:04:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2907719) [2026-01-27 18:04:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2907719) [2026-01-27 18:04:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2907719) [2026-01-27 18:04:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2907719) [2026-01-27 18:04:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2907719) [2026-01-27 18:04:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2907719) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2907719) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:23<00:23, 23.89s/it]
(EngineCore_DP0 pid=2907719) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:56<00:00, 29.28s/it]
(EngineCore_DP0 pid=2907719) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:56<00:00, 28.47s/it]
(EngineCore_DP0 pid=2907719) 
(EngineCore_DP0 pid=2907719) [2026-01-27 18:05:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=2907719) [2026-01-27 18:05:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=2907719) [2026-01-27 18:05:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=2907719) [2026-01-27 18:05:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=2907719) [2026-01-27 18:05:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=2907719) [2026-01-27 18:05:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=2907719) [2026-01-27 18:05:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=2907719) [2026-01-27 18:05:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=2907719) 2026-01-27 18:06:01,330 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2907719) 2026-01-27 18:06:01,352 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 3909.83it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:16<34:23, 16.25s/it, est. speed input: 0.98 toks/s, output: 15.75 toks/s]
Processed prompts:  14%|        | 18/128 [00:16<01:11,  1.53it/s, est. speed input: 17.54 toks/s, output: 280.72 toks/s]
Processed prompts:  38%|      | 48/128 [00:16<00:15,  5.16it/s, est. speed input: 46.41 toks/s, output: 742.54 toks/s]
Processed prompts:  59%|    | 75/128 [00:16<00:05,  9.59it/s, est. speed input: 71.82 toks/s, output: 1149.12 toks/s]
Processed prompts:  77%|  | 98/128 [00:16<00:02, 14.70it/s, est. speed input: 93.13 toks/s, output: 1490.02 toks/s]
Processed prompts:  99%|| 127/128 [00:16<00:00, 23.33it/s, est. speed input: 119.70 toks/s, output: 1915.21 toks/s]
Processed prompts: 100%|| 128/128 [00:16<00:00, 23.33it/s, est. speed input: 120.64 toks/s, output: 1930.29 toks/s]
Processed prompts: 100%|| 128/128 [00:16<00:00,  7.54it/s, est. speed input: 120.64 toks/s, output: 1930.29 toks/s]
[rank0]:[W127 18:06:19.306081908 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 18:06:21
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:06:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:06:25 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2909181) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2909181) WARNING 01-27 18:07:30 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 9.94 requests/s, 2702.92 total tokens/s, 2543.93 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 18:06:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:06:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:06:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:06:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:06:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:06:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:06:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:06:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:06:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:06:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:06:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:06:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:06:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:06:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:06:28] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:06:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:06:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:06:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:06:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:06:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:06:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:06:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:06:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:06:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:06:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:06:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:06:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:06:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2909181) [2026-01-27 18:06:29] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2909181) [2026-01-27 18:06:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2909181) [2026-01-27 18:06:29] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2909181) [2026-01-27 18:06:29] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2909181) [2026-01-27 18:06:29] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2909181) [2026-01-27 18:06:29] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2909181) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2909181) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:18<00:18, 18.29s/it]
(EngineCore_DP0 pid=2909181) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:52<00:00, 27.46s/it]
(EngineCore_DP0 pid=2909181) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:52<00:00, 26.09s/it]
(EngineCore_DP0 pid=2909181) 
(EngineCore_DP0 pid=2909181) [2026-01-27 18:07:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=2909181) [2026-01-27 18:07:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=2909181) [2026-01-27 18:07:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=2909181) [2026-01-27 18:07:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=2909181) [2026-01-27 18:07:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=2909181) [2026-01-27 18:07:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=2909181) [2026-01-27 18:07:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=2909181) [2026-01-27 18:07:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=2909181) 2026-01-27 18:07:29,464 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2909181) 2026-01-27 18:07:29,487 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 11158.66it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:22<1:35:31, 22.48s/it, est. speed input: 0.71 toks/s, output: 11.39 toks/s]
Processed prompts:   1%|          | 3/256 [00:22<24:44,  5.87s/it, est. speed input: 2.13 toks/s, output: 34.01 toks/s]  
Processed prompts:   7%|         | 19/256 [00:22<02:28,  1.60it/s, est. speed input: 13.40 toks/s, output: 214.41 toks/s]
Processed prompts:  19%|        | 48/256 [00:22<00:40,  5.18it/s, est. speed input: 33.58 toks/s, output: 537.36 toks/s]
Processed prompts:  29%|       | 75/256 [00:23<00:18,  9.67it/s, est. speed input: 52.05 toks/s, output: 832.81 toks/s]
Processed prompts:  38%|      | 98/256 [00:23<00:10, 14.69it/s, est. speed input: 67.49 toks/s, output: 1079.80 toks/s]
Processed prompts:  46%|     | 118/256 [00:23<00:06, 20.37it/s, est. speed input: 80.73 toks/s, output: 1291.62 toks/s]
Processed prompts:  53%|    | 136/256 [00:23<00:04, 26.74it/s, est. speed input: 92.44 toks/s, output: 1478.98 toks/s]
Processed prompts:  59%|    | 152/256 [00:23<00:03, 33.49it/s, est. speed input: 102.67 toks/s, output: 1642.65 toks/s]
Processed prompts:  65%|   | 166/256 [00:23<00:02, 40.29it/s, est. speed input: 111.48 toks/s, output: 1783.64 toks/s]
Processed prompts:  70%|   | 178/256 [00:23<00:01, 46.54it/s, est. speed input: 118.90 toks/s, output: 1902.39 toks/s]
Processed prompts:  74%|  | 189/256 [00:24<00:01, 53.01it/s, est. speed input: 125.65 toks/s, output: 2010.42 toks/s]
Processed prompts:  78%|  | 200/256 [00:24<00:00, 57.85it/s, est. speed input: 132.20 toks/s, output: 2115.25 toks/s]
Processed prompts:  82%| | 210/256 [00:24<00:00, 57.66it/s, est. speed input: 137.82 toks/s, output: 2205.06 toks/s]
Processed prompts:  86%| | 219/256 [00:24<00:00, 57.96it/s, est. speed input: 142.83 toks/s, output: 2285.24 toks/s]
Processed prompts:  89%| | 227/256 [00:24<00:00, 61.88it/s, est. speed input: 147.44 toks/s, output: 2359.04 toks/s]
Processed prompts:  92%|| 235/256 [00:24<00:00, 54.27it/s, est. speed input: 151.40 toks/s, output: 2422.35 toks/s]
Processed prompts:  95%|| 242/256 [00:25<00:00, 47.72it/s, est. speed input: 154.64 toks/s, output: 2474.26 toks/s]
Processed prompts:  97%|| 248/256 [00:25<00:00, 37.90it/s, est. speed input: 156.79 toks/s, output: 2508.57 toks/s]
Processed prompts:  99%|| 253/256 [00:25<00:00, 33.18it/s, est. speed input: 158.56 toks/s, output: 2537.04 toks/s]
Processed prompts: 100%|| 256/256 [00:25<00:00, 33.18it/s, est. speed input: 159.14 toks/s, output: 2546.28 toks/s]
Processed prompts: 100%|| 256/256 [00:25<00:00,  9.95it/s, est. speed input: 159.14 toks/s, output: 2546.28 toks/s]
[rank0]:[W127 18:07:56.343729551 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 18:07:58
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-7B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:08:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:08:02 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2910728) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2910728) WARNING 01-27 18:09:12 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.46 requests/s, 2844.91 total tokens/s, 2677.56 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 18:08:02] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:08:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:08:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:08:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:08:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:08:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:08:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:08:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:08:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:08:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:08:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:08:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:08:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:08:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:08:05] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:08:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:08:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:08:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:08:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:08:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:08:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:08:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:08:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:08:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:08:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:08:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:08:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:08:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2910728) [2026-01-27 18:08:06] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2910728) [2026-01-27 18:08:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2910728) [2026-01-27 18:08:06] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2910728) [2026-01-27 18:08:06] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2910728) [2026-01-27 18:08:06] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2910728) [2026-01-27 18:08:06] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2910728) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2910728) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:24<00:24, 24.02s/it]
(EngineCore_DP0 pid=2910728) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:57<00:00, 29.43s/it]
(EngineCore_DP0 pid=2910728) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:57<00:00, 28.62s/it]
(EngineCore_DP0 pid=2910728) 
(EngineCore_DP0 pid=2910728) [2026-01-27 18:09:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=2910728) [2026-01-27 18:09:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=2910728) [2026-01-27 18:09:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=2910728) [2026-01-27 18:09:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=2910728) [2026-01-27 18:09:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=2910728) [2026-01-27 18:09:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=2910728) [2026-01-27 18:09:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=2910728) [2026-01-27 18:09:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=2910728) 2026-01-27 18:09:11,392 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2910728) 2026-01-27 18:09:11,416 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 12158.09it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:34<4:54:07, 34.53s/it, est. speed input: 0.46 toks/s, output: 7.41 toks/s]
Processed prompts:   1%|          | 3/512 [00:34<1:16:59,  9.08s/it, est. speed input: 1.38 toks/s, output: 22.03 toks/s]
Processed prompts:   7%|         | 34/512 [00:35<04:13,  1.88it/s, est. speed input: 15.46 toks/s, output: 247.39 toks/s]
Processed prompts:  12%|        | 63/512 [00:35<01:48,  4.15it/s, est. speed input: 28.40 toks/s, output: 454.41 toks/s]
Processed prompts:  18%|        | 91/512 [00:35<00:59,  7.10it/s, est. speed input: 40.69 toks/s, output: 651.05 toks/s]
Processed prompts:  23%|       | 117/512 [00:36<00:36, 10.71it/s, est. speed input: 51.90 toks/s, output: 830.34 toks/s]
Processed prompts:  28%|       | 141/512 [00:36<00:24, 15.42it/s, est. speed input: 62.30 toks/s, output: 996.82 toks/s]
Processed prompts:  32%|      | 164/512 [00:36<00:16, 21.40it/s, est. speed input: 72.19 toks/s, output: 1155.00 toks/s]
Processed prompts:  36%|      | 185/512 [00:36<00:11, 28.49it/s, est. speed input: 81.14 toks/s, output: 1298.28 toks/s]
Processed prompts:  40%|      | 205/512 [00:36<00:08, 37.06it/s, est. speed input: 89.61 toks/s, output: 1433.78 toks/s]
Processed prompts:  44%|     | 224/512 [00:36<00:06, 46.99it/s, est. speed input: 97.60 toks/s, output: 1561.55 toks/s]
Processed prompts:  47%|     | 242/512 [00:36<00:04, 57.88it/s, est. speed input: 105.10 toks/s, output: 1681.63 toks/s]
Processed prompts:  50%|     | 258/512 [00:36<00:03, 67.23it/s, est. speed input: 111.67 toks/s, output: 1786.67 toks/s]
Processed prompts:  53%|    | 273/512 [00:37<00:03, 77.41it/s, est. speed input: 117.81 toks/s, output: 1885.02 toks/s]
Processed prompts:  59%|    | 302/512 [00:37<00:02, 98.68it/s, est. speed input: 129.70 toks/s, output: 2075.22 toks/s]
Processed prompts:  64%|   | 327/512 [00:37<00:01, 107.65it/s, est. speed input: 139.72 toks/s, output: 2235.55 toks/s]
Processed prompts:  68%|   | 348/512 [00:37<00:01, 113.09it/s, est. speed input: 148.05 toks/s, output: 2368.84 toks/s]
Processed prompts:  72%|  | 367/512 [00:37<00:01, 115.38it/s, est. speed input: 155.49 toks/s, output: 2487.87 toks/s]
Processed prompts:  75%|  | 384/512 [00:37<00:01, 114.72it/s, est. speed input: 162.05 toks/s, output: 2592.77 toks/s]
Processed prompts:  78%|  | 399/512 [00:38<00:01, 109.78it/s, est. speed input: 167.69 toks/s, output: 2683.10 toks/s]
Processed prompts:  80%|  | 412/512 [00:38<00:00, 106.41it/s, est. speed input: 172.55 toks/s, output: 2760.76 toks/s]
Processed prompts:  83%| | 424/512 [00:38<00:00, 102.64it/s, est. speed input: 176.97 toks/s, output: 2831.51 toks/s]
Processed prompts:  85%| | 435/512 [00:38<00:00, 94.24it/s, est. speed input: 180.86 toks/s, output: 2893.81 toks/s] 
Processed prompts:  87%| | 445/512 [00:38<00:00, 93.62it/s, est. speed input: 184.50 toks/s, output: 2951.96 toks/s]
Processed prompts:  89%| | 455/512 [00:38<00:00, 77.65it/s, est. speed input: 187.70 toks/s, output: 3003.16 toks/s]
Processed prompts:  91%| | 464/512 [00:38<00:00, 71.42it/s, est. speed input: 190.64 toks/s, output: 3050.19 toks/s]
Processed prompts:  92%|| 472/512 [00:39<00:00, 58.10it/s, est. speed input: 192.83 toks/s, output: 3085.30 toks/s]
Processed prompts:  94%|| 479/512 [00:39<00:00, 55.22it/s, est. speed input: 194.95 toks/s, output: 3119.26 toks/s]
Processed prompts:  95%|| 485/512 [00:39<00:00, 43.45it/s, est. speed input: 196.18 toks/s, output: 3138.89 toks/s]
Processed prompts:  96%|| 490/512 [00:39<00:00, 35.80it/s, est. speed input: 197.04 toks/s, output: 3152.58 toks/s]
Processed prompts:  97%|| 495/512 [00:40<00:00, 31.18it/s, est. speed input: 197.90 toks/s, output: 3166.40 toks/s]
Processed prompts:  97%|| 499/512 [00:48<00:06,  2.10it/s, est. speed input: 163.68 toks/s, output: 2618.94 toks/s]
Processed prompts: 100%|| 511/512 [00:48<00:00,  3.87it/s, est. speed input: 167.17 toks/s, output: 2674.73 toks/s]
Processed prompts: 100%|| 512/512 [00:48<00:00,  3.87it/s, est. speed input: 167.50 toks/s, output: 2679.96 toks/s]
Processed prompts: 100%|| 512/512 [00:48<00:00, 10.47it/s, est. speed input: 167.50 toks/s, output: 2679.96 toks/s]
[rank0]:[W127 18:10:01.698899598 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 19:19:25
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:19:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:19:32 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2986255) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2986255) WARNING 01-27 19:21:41 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.37 requests/s, 645.11 total tokens/s, 607.16 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 19:19:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:19:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:19:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:19:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:19:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:19:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:19:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:19:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:19:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:19:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:19:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:19:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:19:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:19:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:19:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:19:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2986255) [2026-01-27 19:19:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2986255) [2026-01-27 19:19:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2986255) [2026-01-27 19:19:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2986255) [2026-01-27 19:19:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2986255) [2026-01-27 19:19:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2986255) [2026-01-27 19:19:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2986255) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2986255) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.64s/it]
(EngineCore_DP0 pid=2986255) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:42<00:46, 23.46s/it]
(EngineCore_DP0 pid=2986255) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:14<00:27, 27.22s/it]
(EngineCore_DP0 pid=2986255) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:47<00:00, 29.66s/it]
(EngineCore_DP0 pid=2986255) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:47<00:00, 26.89s/it]
(EngineCore_DP0 pid=2986255) 
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36929536 bytes
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26378240 bytes
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 142442496 bytes
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70778880 bytes
(EngineCore_DP0 pid=2986255) 2026-01-27 19:21:36,401 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2986255) 2026-01-27 19:21:36,633 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|         | 1/64 [00:00<00:10,  6.21it/s]
Adding requests:   8%|         | 5/64 [00:00<00:02, 20.15it/s]
Adding requests:  17%|        | 11/64 [00:00<00:01, 34.31it/s]
Adding requests:  33%|      | 21/64 [00:00<00:00, 56.94it/s]
Adding requests:  62%|   | 40/64 [00:00<00:00, 99.86it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 94.69it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:25<26:57, 25.68s/it, est. speed input: 0.62 toks/s, output: 9.97 toks/s]
Processed prompts:   3%|         | 2/64 [00:25<10:59, 10.64s/it, est. speed input: 1.24 toks/s, output: 19.85 toks/s]
Processed prompts:  19%|        | 12/64 [00:25<00:59,  1.14s/it, est. speed input: 7.41 toks/s, output: 118.61 toks/s]
Processed prompts:  44%|     | 28/64 [00:26<00:13,  2.63it/s, est. speed input: 17.22 toks/s, output: 275.59 toks/s]
Processed prompts:  67%|   | 43/64 [00:26<00:04,  4.92it/s, est. speed input: 26.35 toks/s, output: 421.54 toks/s]
Processed prompts: 100%|| 64/64 [00:26<00:00,  4.92it/s, est. speed input: 39.09 toks/s, output: 625.41 toks/s]
Processed prompts: 100%|| 64/64 [00:26<00:00,  2.44it/s, est. speed input: 39.09 toks/s, output: 625.41 toks/s]
[rank0]:[W127 19:22:09.349039612 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 19:22:25
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:22:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:22:32 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2989442) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2989442) WARNING 01-27 19:24:39 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 3.98 requests/s, 1083.53 total tokens/s, 1019.79 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 19:22:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:22:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:22:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:22:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:22:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:22:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:22:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:22:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:22:35] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:22:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:22:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:22:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:22:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:22:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:22:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:22:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2989442) [2026-01-27 19:22:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2989442) [2026-01-27 19:22:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2989442) [2026-01-27 19:22:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2989442) [2026-01-27 19:22:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2989442) [2026-01-27 19:22:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2989442) [2026-01-27 19:22:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2989442) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2989442) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.50s/it]
(EngineCore_DP0 pid=2989442) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:43<00:48, 24.26s/it]
(EngineCore_DP0 pid=2989442) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:15<00:27, 27.76s/it]
(EngineCore_DP0 pid=2989442) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:49<00:00, 30.06s/it]
(EngineCore_DP0 pid=2989442) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:49<00:00, 27.32s/it]
(EngineCore_DP0 pid=2989442) 
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36929536 bytes
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26378240 bytes
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 142442496 bytes
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70778880 bytes
(EngineCore_DP0 pid=2989442) 2026-01-27 19:24:37,832 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2989442) 2026-01-27 19:24:37,906 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   1%|          | 1/128 [00:00<00:20,  6.18it/s]
Adding requests:   2%|         | 3/128 [00:00<00:10, 11.82it/s]
Adding requests:   5%|         | 6/128 [00:00<00:06, 18.52it/s]
Adding requests:   8%|         | 10/128 [00:00<00:04, 26.03it/s]
Adding requests:  12%|        | 15/128 [00:00<00:03, 32.97it/s]
Adding requests:  20%|        | 25/128 [00:00<00:01, 52.14it/s]
Adding requests:  29%|       | 37/128 [00:00<00:01, 70.69it/s]
Adding requests:  41%|      | 52/128 [00:00<00:00, 93.79it/s]
Adding requests:  56%|    | 72/128 [00:01<00:00, 123.17it/s]
Adding requests:  77%|  | 99/128 [00:01<00:00, 166.23it/s]
Adding requests: 100%|| 128/128 [00:01<00:00, 105.95it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:29<1:02:23, 29.48s/it, est. speed input: 0.54 toks/s, output: 8.68 toks/s]
Processed prompts:   2%|         | 2/128 [00:29<25:37, 12.20s/it, est. speed input: 1.08 toks/s, output: 17.30 toks/s] 
Processed prompts:   5%|         | 7/128 [00:29<04:46,  2.37s/it, est. speed input: 3.77 toks/s, output: 60.28 toks/s]
Processed prompts:  14%|        | 18/128 [00:29<01:14,  1.48it/s, est. speed input: 9.64 toks/s, output: 154.32 toks/s]
Processed prompts:  23%|       | 30/128 [00:29<00:31,  3.07it/s, est. speed input: 16.01 toks/s, output: 256.17 toks/s]
Processed prompts:  35%|      | 45/128 [00:30<00:14,  5.84it/s, est. speed input: 23.93 toks/s, output: 382.81 toks/s]
Processed prompts:  46%|     | 59/128 [00:30<00:07,  9.31it/s, est. speed input: 31.24 toks/s, output: 499.85 toks/s]
Processed prompts:  56%|    | 72/128 [00:30<00:04, 13.47it/s, est. speed input: 37.95 toks/s, output: 607.21 toks/s]
Processed prompts:  66%|   | 84/128 [00:30<00:02, 18.42it/s, est. speed input: 44.10 toks/s, output: 705.59 toks/s]
Processed prompts:  83%| | 106/128 [00:30<00:00, 29.58it/s, est. speed input: 55.30 toks/s, output: 884.84 toks/s]
Processed prompts:  98%|| 125/128 [00:30<00:00, 39.88it/s, est. speed input: 64.84 toks/s, output: 1037.45 toks/s]
Processed prompts: 100%|| 128/128 [00:30<00:00, 39.88it/s, est. speed input: 66.40 toks/s, output: 1062.35 toks/s]
Processed prompts: 100%|| 128/128 [00:30<00:00,  4.15it/s, est. speed input: 66.40 toks/s, output: 1062.35 toks/s]
[rank0]:[W127 19:25:11.781940006 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 19:25:25
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:25:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:25:32 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2992633) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2992633) WARNING 01-27 19:27:39 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.38 requests/s, 1464.03 total tokens/s, 1377.91 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 19:25:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:25:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:25:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:25:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:25:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:25:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:25:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:25:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:25:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:25:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:25:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:25:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:25:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:25:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:25:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:25:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2992633) [2026-01-27 19:25:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2992633) [2026-01-27 19:25:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2992633) [2026-01-27 19:25:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2992633) [2026-01-27 19:25:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2992633) [2026-01-27 19:25:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2992633) [2026-01-27 19:25:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2992633) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2992633) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.61s/it]
(EngineCore_DP0 pid=2992633) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:43<00:48, 24.05s/it]
(EngineCore_DP0 pid=2992633) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:15<00:27, 27.80s/it]
(EngineCore_DP0 pid=2992633) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:49<00:00, 30.04s/it]
(EngineCore_DP0 pid=2992633) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:49<00:00, 27.30s/it]
(EngineCore_DP0 pid=2992633) 
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36929536 bytes
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26378240 bytes
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 142442496 bytes
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70778880 bytes
(EngineCore_DP0 pid=2992633) 2026-01-27 19:27:37,246 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2992633) 2026-01-27 19:27:37,279 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:43,  5.80it/s]
Adding requests:   1%|          | 3/256 [00:00<00:22, 11.46it/s]
Adding requests:   2%|         | 6/256 [00:00<00:13, 18.30it/s]
Adding requests:   4%|         | 10/256 [00:00<00:09, 24.72it/s]
Adding requests:   5%|         | 14/256 [00:00<00:08, 28.58it/s]
Adding requests:   9%|         | 22/256 [00:00<00:05, 43.94it/s]
Adding requests:  13%|        | 33/256 [00:00<00:03, 63.20it/s]
Adding requests:  18%|        | 46/256 [00:00<00:02, 82.88it/s]
Adding requests:  25%|       | 65/256 [00:01<00:01, 113.21it/s]
Adding requests:  32%|      | 83/256 [00:01<00:01, 86.97it/s] 
Adding requests:  43%|     | 109/256 [00:01<00:01, 123.96it/s]
Adding requests:  53%|    | 136/256 [00:01<00:00, 157.03it/s]
Adding requests:  64%|   | 164/256 [00:01<00:00, 186.35it/s]
Adding requests:  80%|  | 204/256 [00:01<00:00, 240.61it/s]
Adding requests:  98%|| 252/256 [00:01<00:00, 305.27it/s]
Adding requests: 100%|| 256/256 [00:01<00:00, 139.86it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:39<2:46:53, 39.27s/it, est. speed input: 0.41 toks/s, output: 6.52 toks/s]
Processed prompts:   1%|          | 2/256 [00:39<1:08:51, 16.26s/it, est. speed input: 0.81 toks/s, output: 12.98 toks/s]
Processed prompts:   3%|         | 8/256 [00:39<11:13,  2.72s/it, est. speed input: 3.23 toks/s, output: 51.70 toks/s]  
Processed prompts:   5%|         | 12/256 [00:39<06:14,  1.53s/it, est. speed input: 4.83 toks/s, output: 77.22 toks/s]
Processed prompts:   7%|         | 18/256 [00:39<03:10,  1.25it/s, est. speed input: 7.20 toks/s, output: 115.26 toks/s]
Processed prompts:  11%|         | 27/256 [00:40<01:32,  2.49it/s, est. speed input: 10.76 toks/s, output: 172.16 toks/s]
Processed prompts:  16%|        | 41/256 [00:40<00:42,  5.12it/s, est. speed input: 16.27 toks/s, output: 260.24 toks/s]
Processed prompts:  21%|       | 55/256 [00:40<00:23,  8.59it/s, est. speed input: 21.72 toks/s, output: 347.60 toks/s]
Processed prompts:  27%|       | 68/256 [00:40<00:14, 12.70it/s, est. speed input: 26.75 toks/s, output: 427.99 toks/s]
Processed prompts:  31%|      | 80/256 [00:40<00:10, 17.46it/s, est. speed input: 31.35 toks/s, output: 501.66 toks/s]
Processed prompts:  36%|      | 92/256 [00:40<00:07, 23.19it/s, est. speed input: 35.92 toks/s, output: 574.79 toks/s]
Processed prompts:  40%|      | 103/256 [00:41<00:05, 28.86it/s, est. speed input: 40.07 toks/s, output: 641.07 toks/s]
Processed prompts:  44%|     | 113/256 [00:41<00:04, 33.86it/s, est. speed input: 43.79 toks/s, output: 700.56 toks/s]
Processed prompts:  48%|     | 123/256 [00:41<00:03, 40.13it/s, est. speed input: 47.51 toks/s, output: 760.13 toks/s]
Processed prompts:  52%|    | 132/256 [00:41<00:02, 43.77it/s, est. speed input: 50.80 toks/s, output: 812.72 toks/s]
Processed prompts:  55%|    | 140/256 [00:41<00:02, 48.17it/s, est. speed input: 53.72 toks/s, output: 859.56 toks/s]
Processed prompts:  58%|    | 148/256 [00:41<00:02, 51.83it/s, est. speed input: 56.63 toks/s, output: 906.03 toks/s]
Processed prompts:  61%|    | 155/256 [00:41<00:01, 53.00it/s, est. speed input: 59.13 toks/s, output: 946.10 toks/s]
Processed prompts:  63%|   | 162/256 [00:42<00:01, 52.46it/s, est. speed input: 61.60 toks/s, output: 985.60 toks/s]
Processed prompts:  66%|   | 169/256 [00:42<00:01, 54.45it/s, est. speed input: 64.09 toks/s, output: 1025.37 toks/s]
Processed prompts:  69%|   | 176/256 [00:42<00:01, 56.35it/s, est. speed input: 66.56 toks/s, output: 1064.99 toks/s]
Processed prompts:  71%|  | 183/256 [00:42<00:01, 58.10it/s, est. speed input: 69.03 toks/s, output: 1104.45 toks/s]
Processed prompts:  74%|  | 190/256 [00:42<00:01, 47.14it/s, est. speed input: 71.30 toks/s, output: 1140.86 toks/s]
Processed prompts:  77%|  | 196/256 [00:42<00:01, 46.12it/s, est. speed input: 73.32 toks/s, output: 1173.08 toks/s]
Processed prompts:  79%|  | 201/256 [00:42<00:01, 46.41it/s, est. speed input: 75.00 toks/s, output: 1200.05 toks/s]
Processed prompts:  80%|  | 206/256 [00:42<00:01, 46.81it/s, est. speed input: 76.68 toks/s, output: 1226.92 toks/s]
Processed prompts:  82%| | 211/256 [00:43<00:01, 36.49it/s, est. speed input: 78.14 toks/s, output: 1250.29 toks/s]
Processed prompts:  84%| | 216/256 [00:43<00:01, 39.38it/s, est. speed input: 79.81 toks/s, output: 1276.95 toks/s]
Processed prompts:  86%| | 221/256 [00:43<00:01, 34.37it/s, est. speed input: 81.30 toks/s, output: 1300.73 toks/s]
Processed prompts:  88%| | 226/256 [00:43<00:00, 30.83it/s, est. speed input: 82.75 toks/s, output: 1323.96 toks/s]
Processed prompts:  90%| | 230/256 [00:43<00:00, 27.91it/s, est. speed input: 83.86 toks/s, output: 1341.72 toks/s]
Processed prompts:  91%|| 234/256 [00:44<00:00, 25.96it/s, est. speed input: 84.96 toks/s, output: 1359.33 toks/s]
Processed prompts:  93%|| 238/256 [00:44<00:00, 24.73it/s, est. speed input: 86.05 toks/s, output: 1376.88 toks/s]
Processed prompts:  95%|| 242/256 [00:44<00:00, 24.20it/s, est. speed input: 87.16 toks/s, output: 1394.52 toks/s]
Processed prompts:  96%|| 245/256 [00:44<00:00, 19.34it/s, est. speed input: 87.73 toks/s, output: 1403.62 toks/s]
Processed prompts:  97%|| 248/256 [00:44<00:00, 16.71it/s, est. speed input: 88.30 toks/s, output: 1412.79 toks/s]
Processed prompts:  98%|| 250/256 [00:45<00:00, 15.50it/s, est. speed input: 88.68 toks/s, output: 1418.85 toks/s]
Processed prompts:  98%|| 252/256 [00:45<00:00, 14.46it/s, est. speed input: 89.05 toks/s, output: 1424.77 toks/s]
Processed prompts:  99%|| 254/256 [00:45<00:00, 13.85it/s, est. speed input: 89.43 toks/s, output: 1430.87 toks/s]
Processed prompts: 100%|| 256/256 [00:45<00:00, 12.60it/s, est. speed input: 89.74 toks/s, output: 1435.78 toks/s]
Processed prompts: 100%|| 256/256 [00:45<00:00, 12.60it/s, est. speed input: 89.74 toks/s, output: 1435.78 toks/s]
Processed prompts: 100%|| 256/256 [00:45<00:00,  5.61it/s, est. speed input: 89.74 toks/s, output: 1435.78 toks/s]
[rank0]:[W127 19:28:27.844541297 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 19:28:42
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/json/Qwen2.5-14B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:28:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:28:49 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2996099) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2996099) WARNING 01-27 19:30:55 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.48 requests/s, 1491.54 total tokens/s, 1403.81 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 19:28:48] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:28:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:28:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:28:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:28:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:28:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:28:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:28:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:28:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:28:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:28:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:28:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:28:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:28:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:28:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:28:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2996099) [2026-01-27 19:28:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2996099) [2026-01-27 19:28:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2996099) [2026-01-27 19:28:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2996099) [2026-01-27 19:28:53] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2996099) [2026-01-27 19:28:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2996099) [2026-01-27 19:28:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2996099) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2996099) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.48s/it]
(EngineCore_DP0 pid=2996099) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:43<00:48, 24.15s/it]
(EngineCore_DP0 pid=2996099) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:15<00:27, 27.75s/it]
(EngineCore_DP0 pid=2996099) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:49<00:00, 30.21s/it]
(EngineCore_DP0 pid=2996099) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:49<00:00, 27.40s/it]
(EngineCore_DP0 pid=2996099) 
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36929536 bytes
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26378240 bytes
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 142442496 bytes
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70778880 bytes
(EngineCore_DP0 pid=2996099) 2026-01-27 19:30:54,192 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2996099) 2026-01-27 19:30:54,272 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/512 [00:00<01:24,  6.07it/s]
Adding requests:   1%|          | 3/512 [00:00<00:45, 11.31it/s]
Adding requests:   1%|          | 5/512 [00:00<00:35, 14.21it/s]
Adding requests:   2%|         | 9/512 [00:00<00:23, 21.02it/s]
Adding requests:   3%|         | 14/512 [00:00<00:18, 27.48it/s]
Adding requests:   4%|         | 20/512 [00:00<00:13, 36.84it/s]
Adding requests:   6%|         | 31/512 [00:00<00:08, 56.86it/s]
Adding requests:   9%|         | 45/512 [00:00<00:05, 79.92it/s]
Adding requests:  12%|        | 63/512 [00:01<00:04, 108.58it/s]
Adding requests:  17%|        | 86/512 [00:01<00:02, 142.35it/s]
Adding requests:  21%|        | 108/512 [00:01<00:02, 162.82it/s]
Adding requests:  26%|       | 131/512 [00:01<00:02, 182.15it/s]
Adding requests:  31%|       | 159/512 [00:01<00:01, 209.07it/s]
Adding requests:  38%|      | 193/512 [00:01<00:01, 247.02it/s]
Adding requests:  45%|     | 230/512 [00:01<00:00, 282.81it/s]
Adding requests:  54%|    | 276/512 [00:01<00:00, 335.13it/s]
Adding requests:  64%|   | 329/512 [00:01<00:00, 392.25it/s]
Adding requests:  84%| | 430/512 [00:01<00:00, 574.43it/s]
Adding requests: 100%|| 512/512 [00:02<00:00, 249.49it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [01:01<8:41:22, 61.22s/it, est. speed input: 0.26 toks/s, output: 4.18 toks/s]
Processed prompts:   0%|          | 2/512 [01:01<3:37:49, 25.63s/it, est. speed input: 0.52 toks/s, output: 8.27 toks/s]
Processed prompts:   1%|         | 7/512 [01:02<42:26,  5.04s/it, est. speed input: 1.79 toks/s, output: 28.60 toks/s] 
Processed prompts:   2%|         | 11/512 [01:03<22:36,  2.71s/it, est. speed input: 2.78 toks/s, output: 44.45 toks/s]
Processed prompts:   3%|         | 16/512 [01:04<12:35,  1.52s/it, est. speed input: 4.00 toks/s, output: 63.94 toks/s]
Processed prompts:   4%|         | 22/512 [01:04<07:21,  1.11it/s, est. speed input: 5.44 toks/s, output: 86.96 toks/s]
Processed prompts:   7%|         | 34/512 [01:05<03:25,  2.32it/s, est. speed input: 8.31 toks/s, output: 132.98 toks/s]
Processed prompts:  11%|         | 54/512 [01:06<01:33,  4.90it/s, est. speed input: 13.07 toks/s, output: 209.08 toks/s]
Processed prompts:  16%|        | 82/512 [01:06<00:46,  9.29it/s, est. speed input: 19.65 toks/s, output: 314.40 toks/s]
Processed prompts:  21%|        | 108/512 [01:07<00:27, 14.85it/s, est. speed input: 25.78 toks/s, output: 412.41 toks/s]
Processed prompts:  26%|       | 133/512 [01:07<00:17, 21.38it/s, est. speed input: 31.62 toks/s, output: 505.88 toks/s]
Processed prompts:  30%|       | 156/512 [01:07<00:12, 28.65it/s, est. speed input: 36.96 toks/s, output: 591.34 toks/s]
Processed prompts:  35%|      | 178/512 [01:07<00:09, 36.36it/s, est. speed input: 42.02 toks/s, output: 672.39 toks/s]
Processed prompts:  39%|      | 198/512 [01:07<00:07, 43.64it/s, est. speed input: 46.59 toks/s, output: 745.44 toks/s]
Processed prompts:  42%|     | 217/512 [01:08<00:05, 50.45it/s, est. speed input: 50.89 toks/s, output: 814.28 toks/s]
Processed prompts:  46%|     | 235/512 [01:08<00:04, 56.63it/s, est. speed input: 54.94 toks/s, output: 879.06 toks/s]
Processed prompts:  49%|     | 252/512 [01:08<00:04, 61.81it/s, est. speed input: 58.74 toks/s, output: 939.82 toks/s]
Processed prompts:  52%|    | 268/512 [01:08<00:03, 66.56it/s, est. speed input: 62.30 toks/s, output: 996.73 toks/s]
Processed prompts:  55%|    | 283/512 [01:09<00:03, 69.79it/s, est. speed input: 65.61 toks/s, output: 1049.69 toks/s]
Processed prompts:  58%|    | 297/512 [01:09<00:02, 73.47it/s, est. speed input: 68.69 toks/s, output: 1099.04 toks/s]
Processed prompts:  61%|    | 310/512 [01:09<00:02, 73.19it/s, est. speed input: 71.51 toks/s, output: 1144.18 toks/s]
Processed prompts:  63%|   | 322/512 [01:09<00:02, 72.85it/s, est. speed input: 74.10 toks/s, output: 1185.61 toks/s]
Processed prompts:  65%|   | 333/512 [01:09<00:02, 73.16it/s, est. speed input: 76.47 toks/s, output: 1223.50 toks/s]
Processed prompts:  67%|   | 344/512 [01:09<00:02, 72.13it/s, est. speed input: 78.82 toks/s, output: 1261.05 toks/s]
Processed prompts:  69%|   | 354/512 [01:09<00:02, 70.04it/s, est. speed input: 80.93 toks/s, output: 1294.83 toks/s]
Processed prompts:  71%|   | 363/512 [01:10<00:02, 68.90it/s, est. speed input: 82.82 toks/s, output: 1325.16 toks/s]
Processed prompts:  73%|  | 372/512 [01:10<00:02, 66.63it/s, est. speed input: 84.70 toks/s, output: 1355.16 toks/s]
Processed prompts:  74%|  | 380/512 [01:10<00:02, 65.20it/s, est. speed input: 86.36 toks/s, output: 1381.73 toks/s]
Processed prompts:  76%|  | 388/512 [01:10<00:02, 61.80it/s, est. speed input: 87.99 toks/s, output: 1407.85 toks/s]
Processed prompts:  77%|  | 395/512 [01:10<00:01, 60.38it/s, est. speed input: 89.42 toks/s, output: 1430.74 toks/s]
Processed prompts:  79%|  | 402/512 [01:10<00:01, 59.64it/s, est. speed input: 90.85 toks/s, output: 1453.59 toks/s]
Processed prompts:  80%|  | 408/512 [01:10<00:01, 57.10it/s, est. speed input: 92.05 toks/s, output: 1472.81 toks/s]
Processed prompts:  81%|  | 414/512 [01:11<00:01, 55.44it/s, est. speed input: 93.25 toks/s, output: 1492.00 toks/s]
Processed prompts:  82%| | 420/512 [01:11<00:01, 53.36it/s, est. speed input: 94.44 toks/s, output: 1510.99 toks/s]
Processed prompts:  83%| | 426/512 [01:11<00:01, 54.81it/s, est. speed input: 95.65 toks/s, output: 1530.39 toks/s]
Processed prompts:  84%| | 432/512 [01:11<00:01, 54.73it/s, est. speed input: 96.85 toks/s, output: 1549.55 toks/s]
Processed prompts:  86%| | 438/512 [01:11<00:01, 55.14it/s, est. speed input: 98.05 toks/s, output: 1568.72 toks/s]
Processed prompts:  87%| | 444/512 [01:11<00:01, 42.44it/s, est. speed input: 99.08 toks/s, output: 1585.32 toks/s]
Processed prompts:  88%| | 449/512 [01:11<00:01, 43.79it/s, est. speed input: 100.05 toks/s, output: 1600.86 toks/s]
Processed prompts:  89%| | 454/512 [01:12<00:01, 34.05it/s, est. speed input: 100.84 toks/s, output: 1613.39 toks/s]
Processed prompts:  90%| | 460/512 [01:12<00:01, 33.14it/s, est. speed input: 101.90 toks/s, output: 1630.38 toks/s]
Processed prompts:  91%| | 466/512 [01:12<00:01, 31.56it/s, est. speed input: 102.93 toks/s, output: 1646.85 toks/s]
Processed prompts:  92%|| 470/512 [01:12<00:01, 27.91it/s, est. speed input: 103.53 toks/s, output: 1656.43 toks/s]
Processed prompts:  93%|| 474/512 [01:12<00:01, 25.87it/s, est. speed input: 104.14 toks/s, output: 1666.19 toks/s]
Processed prompts:  93%|| 478/512 [01:13<00:01, 24.58it/s, est. speed input: 104.75 toks/s, output: 1675.97 toks/s]
Processed prompts:  94%|| 482/512 [01:13<00:01, 23.22it/s, est. speed input: 105.34 toks/s, output: 1685.43 toks/s]
Processed prompts:  95%|| 485/512 [01:13<00:01, 18.62it/s, est. speed input: 105.60 toks/s, output: 1689.66 toks/s]
Processed prompts:  95%|| 488/512 [01:13<00:01, 15.94it/s, est. speed input: 105.86 toks/s, output: 1693.83 toks/s]
Processed prompts:  96%|| 490/512 [01:13<00:01, 14.81it/s, est. speed input: 106.04 toks/s, output: 1696.72 toks/s]
Processed prompts:  96%|| 492/512 [01:14<00:01, 13.95it/s, est. speed input: 106.23 toks/s, output: 1699.63 toks/s]
Processed prompts:  96%|| 494/512 [01:14<00:01, 13.29it/s, est. speed input: 106.41 toks/s, output: 1702.54 toks/s]
Processed prompts:  97%|| 496/512 [01:14<00:01, 12.84it/s, est. speed input: 106.59 toks/s, output: 1705.50 toks/s]
Processed prompts:  97%|| 497/512 [01:27<00:01, 12.84it/s, est. speed input: 106.69 toks/s, output: 1707.02 toks/s]
Processed prompts:  97%|| 498/512 [01:30<00:30,  2.16s/it, est. speed input: 88.11 toks/s, output: 1409.73 toks/s] 
Processed prompts:  98%|| 500/512 [01:30<00:19,  1.59s/it, est. speed input: 88.30 toks/s, output: 1412.72 toks/s]
Processed prompts:  98%|| 502/512 [01:30<00:11,  1.17s/it, est. speed input: 88.48 toks/s, output: 1415.66 toks/s]
Processed prompts:  99%|| 506/512 [01:30<00:04,  1.50it/s, est. speed input: 89.02 toks/s, output: 1424.34 toks/s]
Processed prompts: 100%|| 512/512 [01:31<00:00,  1.50it/s, est. speed input: 89.99 toks/s, output: 1439.89 toks/s]
Processed prompts: 100%|| 512/512 [01:31<00:00,  5.62it/s, est. speed input: 89.99 toks/s, output: 1439.89 toks/s]
[rank0]:[W127 19:32:29.497098329 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

